# One model to generate them all

Semi-Supervised Learning with Information Bottleneck


## Why linear association

> **Theorem 1**: The association between learned manifold and groundtruth factors must be linear.

It has bene proved that vanilla VAE pursuit the direction of PCA which ensures inidentifiability in the learned representation [[../papers/vae_pursue_pca.md]].  Study in [[../papers/rethinking_semi_vae.md]] proposes the ideas of notation to fabricate richer connection between the representation and the groundtruth labels. In practice, the observed labels are often incomplete and act as a _semantic delegate_ to the real observation, which means these are partially observed or generated via external process in parallel with data collection. It is essential to not treat the labels as a sole factors for generating the data even though its role in understanding the data is indisputable. A linear association between latents and grountruth labels would enable both richer and more flexible representation while retains absolute control of the generative model.


## Double descent in learning representation

There are two quantities of the data could be asseted for learning representation: 1) the complexity of data; 2) the quantities of data available for training. The complexity of data would strongly correlated to the capacity that fixed model could compress the inputs to a meaningful representation.

> **Theorem 2**: The compression capacity of generative model is closely tighted to the relevant information encoded.

## Sparsity in representation learning

The inactive latent units often perceived as the inconvenience during learning a VAE. Conversely, we hypothesize that this is a desirable property of VAE as the model manifest a process of feature selection. In practice, we don't know the exact number of relevant factors, and while training multiple VAEs of the same architecture results unidentifiable representation, the number of active units remains the same among different runs. This could be leverage as a powerful mechanism of VAEs.

> **Corollary 1**:

## Redirect information loss

[[../papers/vib.md]]

## Inductive biased in hierarchical latents models

> **Theorem 3**: The role of inductive biases is crucial in resolving the competing causes of hierarchical latents models.

---

[//begin]: # "Autogenerated link references for markdown compatibility"
[../papers/vae_pursue_pca.md]: ../papers/vae_pursue_pca "VAEs Pursue PCA"
[../papers/rethinking_semi_vae.md]: ../papers/rethinking_semi_vae "Rethinking SS-VAEs"
[../papers/vib.md]: ../papers/vib "VIB"
[//end]: # "Autogenerated link references"
