# Ideas

[[gtc21/modern_ai.md]]


## Information bottleneck

### The variational information bottleneck network:

$$I(Y, Z) - \beta I(X, Z)$$

### With semi-supervised extension

$$I(Y, Z) + I(X, Z) - \beta I(i, Z)$$

where $i$ is then index of data.

Issues:

1. Maximizing both the information of X and Y within Z is another bottleneck
2. It doesn't gaurantee identificability
3. Non-linear relationship between latents and factors are ignored,solution could be another deep network

$$f_\lambda(y|z)$$

### Mutual Information VAE

Factorizing while maximizing the mutual infoirmation:

$$I(X, Z_2) - \beta I(i, Z_1)  - \beta I(i, Z_2)$$

We often find the $Z_1$ posterior collapse, excessive amount of information stored in $Z_2$ without the constrain of factorization

### Semi-supervised with maximum mutual information latents

Two latents $z_1$ and $z_2$

$$I(Y, Z_2) + I(X, Z_1) - \beta I(i, Z_1) - \beta I(i, Z_2)$$

Specifically maximizing the mutual information of $z_2$ and $y$

### SemifoVAE

There are two way to widen information throughput:

- Extend the bandwidth
- Regularizing the traffic
- Determine which information will be flow

[//begin]: # "Autogenerated link references for markdown compatibility"
[gtc21/modern_ai.md]: gtc21/modern_ai "Modern Artificial Intelligence 1980s-2021 and Beyond"
[//end]: # "Autogenerated link references"
