# Reading list

## To-read

3. [ ] Coolen, A. C. C. A Beginner guide to mathematic of Neural Network.
4. [ ] Allen-Zhu, Z., Li, Y. & Song, Z. A Convergence Theory for Deep Learning via Over-Parameterization. arXiv:1811. 03962 [cs, math, stat] (2019).
6. [ ] Paquet, U., Ghaisas, S. K. & Tieleman, O. A Factorial Mixture Prior for Compositional Deep Generative Models. arXiv:1812. 07480 [cs, stat] (2018).
7. [ ] Mihail, E. A Machine Learning Primer. (2020).
8. [ ] Theis, L., Oord, A. van den & Bethge, M. A note on the evaluation of generative models. arXiv:1511. 01844 [cs, stat] (2016).
9. [ ] Lan, X. & Barner, K. E. A Probabilistic Representation of Deep Learning. arXiv:1908. 09772 [cs, stat] (2019).
11. [ ] Ou, Z. A Review of Learning with Deep Generative Models. (2019).
14. [ ] van Engelen, J. E. & Hoos, H. H. A survey on semi-supervised learning. Mach Learn 109, 373–440 (2020).
15. [ ] Xu, Y., Zhao, S., Song, J., Stewart, R. & Ermon, S. A Theory of Usable Information Under Computational Constraints. arXiv:2002. 10689 [cs, stat] (2020).
16. [ ] McCarthy, A. D., Li, X., Gu, J. & Dong, N. Addressing posterior collapse with mutual information for improved variational neural machine translation. in Proceedings of the 58th annual meeting of the association for computational linguistics 8512–8525 (Association for Computational Linguistics, 2020). doi:10. 18653/v1/2020. acl-main.753.
17. [ ] Zhang, C., Bütepage, J., Kjellström, H. & Mandt, S. Advances in Variational Inference. IEEE Transactions on Pattern Analysis and Machine Intelligence 41, 2008–2026 (2019).
18. [ ] Suzuki, J. An Estimator of Mutual Information and its Application to Independence Testing. Entropy 18, (2016).
19. [ ] Anonymous. An image is worth 16x16 words: Transformers for image recognition at scale. in Submitted to international conference on learning representations (2021).
20. [ ] Nalisnick, E., Hertel, L. & Smyth, P. Approximate Inference for Deep Latent Gaussian Mixtures. 4.
21. [ ] Huang, C.-W., Dinh, L. & Courville, A. Augmented Normalizing Flows: Bridging the Gap Between Generative Flows and Latent Variable Models. arXiv:2002. 07101 [cs, stat] (2020).
22. [ ] Chang, M. B., Gupta, A., Levine, S. & Griffiths, T. L. Automatically Composing Representation Transformations as a Means for Generalization. arXiv:1807. 04640 [cs, stat] (2019).
23. [ ] van der Schaar, M. AutoML and interpretability: Powering the machine learning revolution in healthcare. in Proceedings of the 2020 ACM-IMS on foundations of data science conference 1 (Association for Computing Machinery, 2020). doi:10. 1145/3412815. 3416879.
24. [ ] Lucas, T. & Verbeek, J. Auxiliary Guided Autoregressive Variational Autoencoders. arXiv:1711. 11479 [cs] (2017).
25. [ ] Lucas, T. & Verbeek, J. Auxiliary Guided Autoregressive Variational Autoencoders. arXiv:1711. 11479 [cs] (2019).
26. [ ] Dieng, A. B., Kim, Y., Rush, A. M. & Blei, D. M. Avoiding Latent Variable Collapse With Generative Skip Models. arXiv:1807. 04863 [cs, stat] (2018).
27. [ ] Asperti, A. & Trentin, M. Balancing reconstruction error and Kullback-Leibler divergence in Variational Autoencoders. arXiv:2002. 07514 [cs] (2020).
28. [ ] Martin, O. Bayesian Analysis with Python - Second Edition. (2018).
29. [ ] Morrow, R. & Chiu, W.-C. Benefiting Deep Latent Variable Models via Learning the Prior and Removing Latent Regularization. arXiv:2007. 03640 [cs, stat] (2020).
30. [ ] Zhao, S. et al. Bias and Generalization in Deep Generative Models: An Empirical Study. arXiv:1811. 03259 [cs, stat] (2018).
31. [ ] Grill, J.-B. et al. Bootstrap your own latent: A new approach to self-supervised Learning. arXiv:2006. 07733 [cs, stat] (2020).
32. [ ] Rey, L. A. P., Menkovski, V. & Portegies, J. W. Can VAEs capture topological properties? 12.
33. [ ] Hernán, M. A. & Robins, J. M. Causal Inference: What If. 311 (2020).
34. [ ] Fischer, I. & Alemi, A. A. CEB Improves Model Robustness. arXiv:2002. 05380 [cs, stat] (2020).
35. [ ] Martin, G. M., Frazier, D. T. & Robert, C. P. Computing Bayes: Bayesian Computation from 1763 to the 21st Century. arXiv:2004. 06425 [stat] (2020).
36. [ ] Molavipour, S., Bassi, G. & Skoglund, M. Conditional Mutual Information Neural Estimator. arXiv:1911. 02277 [cs, math] (2020) doi:10. 1109/ICASSP40776. 2020. 9053422.
37. [ ] Dai, B., Wang, Y., Aston, J., Hua, G. & Wipf, D. Connections with robust PCA and the role of emergent sparsity in variational autoencoder models. Journal of Machine Learning Research 19, 1–42 (2018).
38. [ ] Sinha, S., Odena, A. & Dieng, A. B. Consistency Regularization for Variational Auto-Encoders. 6 (2020).
39. [ ] Deasy, J., Simidjievski, N. & Liò, P. Constraining Variational Inference with Geometric Jensen-Shannon Divergence. arXiv:2006. 10599 [cs, stat] (2020).
40. [ ] Shao, H. et al. ControlVAE: Controllable Variational Autoencoder. arXiv:2004. 05988 [cs, stat] (2020).
41. [ ] Jónsson, H., Cherubini, G. & Eleftheriou, E. Convergence Behavior of DNNs with Mutual-Information-Based Regularization. Entropy 22, (2020).
42. [ ] Ma, X., Kong, X., Zhang, S. & Hovy, E. Decoupling Global and Local Representations from/for Image Generation. arXiv:2004. 11820 [cs, eess, stat] (2020).
43. [ ] Andreini, P., Izzo, C. & Ricco, G. Deep Dynamic Factor Models. arXiv:2007. 11887 [cs, econ] (2020).
44. [ ] Wipf, D. Deep Generative Models for Signal Processing and Beyond. 55.
45. [ ] Kim, Y. Deep Latent Variable Models of Natural Language. (2020).
46. [ ] Dilokthanakul, N. et al. Deep Unsupervised Clustering with Gaussian Mixture Variational Autoencoders. arXiv:1611. 02648 [cs, stat] (2017).
48. [ ] Gabbay, A. & Hoshen, Y. Demystifying Inter-Class Disentanglement. arXiv:1906. 11796 [cs, stat] (2020).
49. [ ] Dinh, L., Sohl-Dickstein, J. & Bengio, S. Density estimation using Real NVP. arXiv:1605. 08803 [cs, stat] (2017).
50. [ ] Sinha, S. et al. DIBS: Diversity inducing Information Bottleneck in Model Ensembles. arXiv:2003. 04514 [cs, stat] (2020).
51. [ ] Li, Y. et al. Disentangled Variational Auto-Encoder for Semi-supervised Learning. arXiv:1709. 05047 [cs] (2018).
52. [ ] Shi, W., Zhou, H., Miao, N. & Li, L. Dispersed Exponential Family Mixture VAEs for Interpretable Text Generation. arXiv:1906. 06719 [cs, stat] (2020).
53. [ ] Nakkiran, P. & Bansal, Y. Distributional Generalization: A New Kind of Generalization. arXiv:2009. 08092 [cs, math, stat] (2020).
54. [ ] Nalisnick, E., Matsukawa, A., Teh, Y. W., Gorur, D. & Lakshminarayanan, B. Do deep generative models know what they don’t know? in International conference on learning representations (2019).
55. [ ] Lucas, J., Tucker, G., Grosse, R. & Norouzi, M. Don’t Blame the ELBO! A Linear VAE Perspective on Posterior Collapse. arXiv:1911. 02469 [cs, stat] (2019).
56. [ ] Seybold, B., Fertig, E., Alemi, A. & Fischer, I. Dueling Decoders: Regularizing Variational Autoencoder Latent Spaces. arXiv:1905. 07478 [cs, stat] (2019).
57. [ ] Murphy, K. Dynamic Bayesian Networks: Representation, Inference and Learning. (UC Berkeley, 2002).
58. [ ] Cover, T. M. & Thomas, J. A. Elements of information theory (wiley series in telecommunications and signal processing). (Wiley-Interscience, 2006).
59. [ ] Ryabinin, M., Popov, S., Prokhorenkova, L. & Voita, E. Embedding Words in Non-Vector Space with Unsupervised Graph Learning. arXiv:2010. 02598 [cs] (2020).
60. [ ] Verdú, S. Empirical Estimation of Information Measures: A Literature Guide. Entropy 21, (2019).
61. [ ] Lopez, R., Gayoso, A. & Yosef, N. Enhancing scientific discoveries in molecular biology with deep generative models. Molecular Systems Biology 16, e9198 (2020).
62. [ ] Fromm, E. Escapse from Freedom. (1941).
63. [ ] Sepliarskaia, A., Kiseleva, J. & de Rijke, M. Evaluating Disentangled Representations. arXiv:1910. 05587 [cs, stat] (2019).
64. [ ] Whitney, W. F., Song, M. J., Brandfonbrener, D., Altosaar, J. & Cho, K. Evaluating representations by the complexity of learning low-loss predictors. arXiv:2009. 07368 [cs, stat] (2020).
65. [ ] Jacobsen, J.-H., Behrmann, J., Zemel, R. & Bethge, M. Excessive Invariance Causes Adversarial Vulnerability. arXiv:1811. 00401 [cs, stat] (2019).
66. [ ] Chan, J., Spence, J. P. & Song, Y. S. Exchangeable Variational Autoencoders with Applications to Genomic Data. 6.
67. [ ] Zhang, Y., Hu, J. & Okatani, T. Extending information maximization from a rate-distortion perspective. Neurocomputing 399, 285–295 (2020).
68. [ ] Yacoby, Y., Pan, W. & Doshi-Velez, F. Failure Modes of Variational Autoencoders and Their Effects on Downstream Tasks. arXiv:2007. 07124 [cs, stat] (2020).
69. [ ] Ghosh, P., Sajjadi, M. S. M., Vergari, A., Black, M. & Schölkopf, B. From Variational to Deterministic Autoencoders. arXiv:1903. 12436 [cs, stat] (2019).
70. [ ] Ghosh, P., Sajjadi, M. S. M., Vergari, A., Black, M. & Scholkopf, B. From variational to deterministic autoencoders. in International conference on learning representations (2020).
71. [ ] Foster, D. Generative Deep Learning.
72. [ ] Arvanitidis, G., Hauberg, S. & Schölkopf, B. Geometrically Enriched Latent Spaces. arXiv:2008. 00565 [cs, stat] (2020).
73. [ ] Alemi, A. A. & Fischer, I. GILBO: One Metric to Measure Them All. arXiv:1802. 04874 [cs, stat] (2019).
74. [ ] Feng, H., Kong, K., Zhang, T., Xue, S. & Chen, W. Good semi-supervised {VAE} requires tighter evidence lower bound. (2020).
75. [ ] Evci, U., Ioannou, Y. A., Keskin, C. & Dauphin, Y. Gradient Flow in Sparse Neural Networks and How Lottery Tickets Win. arXiv:2010. 03533 [cs] (2020).
76. [ ] Needham, M. & Hodler, A. E. Graph Algorithms. 266.
77. [ ] Deo, N. Graph theory with applications to engineering and computer science (prentice hall series in automatic computation). (Prentice-Hall, Inc., 1974).
78. [ ] Wainwright, M. J. & Jordan, M. I. Graphical Models, Exponential Families, and Variational Inference. FNT in Machine Learning 1, 1–305 (2007).
79. [ ] Levitt, S. D. Heads or Tails: The Impact of a Coin Toss on Major Life Decisions and Subsequent Happiness. 41 (2020).
80. [ ] Shin, S.-J., Song, K. & Moon, I.-C. Hierarchically Clustered Representation Learning. arXiv:1901. 09906 [cs, stat] (2019).
81. [ ] Livne, M., Swersky, K. & Fleet, D. J. High Mutual Information in Representation Learning with Symmetric Variational Inference. arXiv:1910. 04153 [cs, math, stat] (2019).
82. [ ] Gorelick, M. & Ozsvald, I. High performance Python. (O’Reilly, 2014).
83. [ ] Milgrom, P. & Tadelis, S. How Artificial Intelligence and Machine Learning Can Impact Market Design. (2019).
84. [ ] Xu, K. et al. How Neural Networks Extrapolate: From Feedforward to Graph Neural Networks. arXiv:2009. 11848 [cs, stat] (2020).
85. [ ] Makhzani, A. Implicit Autoencoders. arXiv:1805. 09804 [cs, stat] (2019).
86. [ ] Munjal, P., Paul, A. & Krishnan, N. C. Implicit Discriminator in Variational Autoencoder. arXiv:1909. 13062 [cs, stat] (2019).
87. [ ] Jing, L., Zbontar, J. & LeCun, Y. Implicit Rank-Minimizing Autoencoder. arXiv:2010. 00679 [cs, stat] (2020).
88. [ ] Figurnov, M., Mohamed, S. & Mnih, A. Implicit Reparameterization Gradients. arXiv:1805. 08498 [cs, stat] (2018).
89. [ ] Cheng, P. et al. Improving Disentangled Text Representation Learning with Information-Theoretic Guidance. arXiv:2006. 00693 [cs, stat] (2020).
90. [ ] Wu, J. et al. Improving Robustness and Generality of NLP Models Using Disentangled Representations. arXiv:2009. 09587 [cs] (2020).
91. [ ] Maaløe, L., Sønderby, C. K., Sønderby, S. K. & Winther, O. Improving Semi-Supervised Learning with Auxiliary Deep Generative Models. 5.
92. [ ] Marino, J., Chen, L., He, J. & Mandt, S. Improving sequential latent variable models with autoregressive flows. in (eds. Zhang, C., Ruiz, F., Bui, T., Dieng, A. B. & Liang, D.) vol. 118 1–16 (PMLR, 2020).
93. [ ] Kingma, D. P. et al. Improving Variational Inference with Inverse Autoregressive Flow. arXiv:1606. 04934 [cs, stat] (2017).
94. [ ] anonymous. Improving Zero-Shot Voice Style Transfer via Disentangled Representation Learning. https://openreview.net/pdf?id=TgSVWXw22FQ (2020).
95. [ ] Pineau, E. & Lelarge, M. InfoCatVAE: Representation Learning with Categorical Variational Autoencoders. arXiv:1806. 08240 [cs, stat] (2018).
96. [ ] Slonim, N., Atwal, G. S., Tkačik, G. & Bialek, W. Information-based clustering. Proceedings of the National Academy of Sciences 102, 18297–18302 (2005).
97. [ ] Kim, H. J. Interpretable Models in Probabilistic Machine Learning. 202.
98. [ ] Potapczynski, A., Loaiza-Ganem, G. & Cunningham, J. P. Invertible Gaussian Reparameterization: Revisiting the Gumbel-Softmax. arXiv:1912. 09588 [cs, stat] (2020).
99. [ ] Milgrom, P. Is Sympathy An Economic Value? Philosophy, Economics, and the Contigent Valuation Method. https://web.stanford.edu/~milgrom/publishedarticles/Is%20Sympathy%20An%20Economic%20Value.pdf (1993).
100. [ ] Lin, S. & Clark, R. LaDDer: Latent Data Distribution Modelling with a Generative Prior. arXiv:2009. 00088 [cs] (2020).
101. [ ] He, J., Spokoyny, D., Neubig, G. & Berg-Kirkpatrick, T. Lagging Inference Networks and Posterior Collapse in Variational Autoencoders. arXiv:1901. 05534 [cs, stat] (2019).
102. [ ] Tian, Y. & Engel, J. Latent Translation: Crossing Modalities by Bridging Generative Models. arXiv:1902. 08261 [cs, stat] (2019).
103. [ ] Wu, T., Fischer, I., Chuang, I. L. & Tegmark, M. Learnability for the Information Bottleneck. Entropy 21, (2019).
104. [ ] Hjelm, R. D. et al. Learning deep representations by mutual information estimation and maximization. arXiv:1808. 06670 [cs, stat] (2019).
105. [ ] Williams, J., Zhao, Y., Cooper, E. & Yamagishi, J. Learning Disentangled Phone and Speaker Representations in a Semi-Supervised VQ-VAE Paradigm. arXiv:2010. 10727 [cs, eess] (2020).
106. [ ] Siddharth, N. et al. Learning disentangled representations with semi-supervised deep generative models. in Proceedings of the 31st international conference on neural information processing systems 5927–5937 (Curran Associates Inc., 2017).
107. [ ] Parascandolo, G., Neitz, A., Orvieto, A., Gresele, L. & Schölkopf, B. Learning explanations that are hard to vary. arXiv:2009. 00329 [cs, stat] (2020).
108. [ ] Chen, N., Klushyn, A., Ferroni, F., Bayer, J. & van der Smagt, P. Learning Flat Latent Manifolds with VAEs. arXiv:2002. 04881 [cs, stat] (2020).
109. [ ] Gong, Y. et al. Learning from partially-observed multimodal data with variational autoencoders. (2020).
110. [ ] Parascandolo, G., Kilbertus, N., Rojas-Carulla, M. & Schölkopf, B. Learning Independent Causal Mechanisms. arXiv:1712. 00961 [cs, stat] (2018).
111. [ ] Li, B. et al. Learning Invariant Representations and Risks for Semi-supervised Domain Adaptation. arXiv:2010. 04647 [cs] (2020).
112. [ ] Wang, H.-P., Peng, W.-H. & Ko, W.-J. Learning Priors for Adversarial Autoencoders. arXiv:1909. 04443 [cs, stat] (2019).
113. [ ] Rezaabad, A. L. & Vishwanath, S. Learning Representations by Maximizing Mutual Information in Variational Autoencoders. arXiv:1912. 13361 [cs, stat] (2020).
114. [ ] Machine Learning Interviews_ Lessons from Both Sides - FSDL.pdf.
115. [ ] Germain, M., Gregor, K., Murray, I. & Larochelle, H. MADE: Masked Autoencoder for Distribution Estimation. arXiv:1502. 03509 [cs, stat] (2015).
116. [ ] Ma, X., Zhou, C. & Hovy, E. MAE: Mutual Posterior-Divergence Regularization for Variational Autoencoders. 16 (2019).
117. [ ] Papamakarios, G., Pavlakou, T. & Murray, I. Masked Autoregressive Flow for Density Estimation. arXiv:1705. 07057 [cs, stat] (2018).
118. [ ] Bonaccorso, G. Mastering Machine Learning Algorithms. (2020).
119. [ ] Deisenroth, M. P., Faisal, A. A. & Ong, C. S. Mathematics for Machine Learning.
120. [ ] Sinha, S., Goyal, A. & Garg, A. Maximum Entropy Models for Fast Adaptation. arXiv:2006. 16524 [cs, stat] (2020).
121. [ ] Livne, M., Swersky, K. & Fleet, D. J. MIM: Mutual Information Machine. arXiv:1910. 03175 [cs, math, stat] (2020).
122. [ ] Fazelnia, G., Ibrahim, M., Modarres, C., Wu, K. & Paisley, J. Mixed Membership Recurrent Neural Networks for Modeling Customer Purchases. New York 8 (2020).
123. [ ] Skopek, O., Ganea, O.-E. & Bécigneul, G. Mixed-curvature variational autoencoders. in International conference on learning representations (2020).
124. [ ] Rastgoufard, R. Multi-Label Latent Spaces with Semi-Supervised Deep Generative Models. (University of New Orleans, 2018).
125. [ ] Shi, Y., Pan, Y., Xu, D. & Tsang, I. W. Multi-view Alignment and Generation in CCA via Consistent Latent Encoding. arXiv:2005. 11716 [cs, stat] (2020).
126. [ ] Sutter, T. M., Daunhawer, I. & Vogt, J. E. Multimodal Generative Learning Utilizing Jensen-Shannon-Divergence. arXiv:2006. 08242 [cs, stat] (2020).
127. [ ] Aneja, J., Schwing, A., Kautz, J. & Vahdat, A. NCP-VAE: Variational Autoencoders with Noise Contrastive Priors. arXiv:2010. 02917 [cs, stat] (2020).
128. [ ] Goodfellow, I. NIPS 2016 Tutorial: Generative Adversarial Networks. arXiv:1701. 00160 [cs] (2017).
129. [ ] Huang, L. et al. Normalization Techniques in Training DNNs: Methodology, Analysis and Application. arXiv:2009. 12836 [cs, stat] (2020).
130. [ ] Wan, Z. et al. Old Photo Restoration via Deep Latent Space Translation. arXiv:2009. 07047 [cs] (2020).
131. [ ] Huang, C.-W., Ahmed, F., Kumar, K., Lacoste, A. & Courville, A. On Difficulties of Probability Distillation. (2018).
132. [ ] Kumar, A. & Poole, B. On Implicit Regularization in beta-VAEs. arXiv:2002. 00041 [cs, stat] (2020).
133. [ ] Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S. & Lucic, M. On Mutual Information Maximization for Representation Learning. arXiv:1907. 13625 [cs, stat] (2019).
134. [ ] Tschannen, M., Djolonga, J., Rubenstein, P. K., Gelly, S. & Lucic, M. On Mutual Information Maximization for Representation Learning. arXiv:1907. 13625 [cs, stat] (2020).
135. [ ] Molavipour, S., Bassi, G. & Skoglund, M. On Neural Estimators for Conditional Mutual Information Using Nearest Neighbors Sampling. arXiv:2006. 07225 [cs, math] (2020).
136. [ ] Lee, J., Tran, D., Firat, O. & Cho, K. On the Discrepancy between Density Estimation and Sequence Generation. arXiv:2002. 07233 [cs, stat] (2020).
137. [ ] Wu, C., Wang, P. Z. & Wang, W. Y. On the Encoder-Decoder Incompatibility in Variational Text Modeling and Beyond. arXiv:2004. 09189 [cs, stat] (2020).
138. [ ] Rezende, D. J. & Viola, F. On the properties of high-capacity VAEs. 10.
139. [ ] Echraibi, A., Flocon-Cholet, J., Gosselin, S. & Vaton, S. On the Variational Posterior of Dirichlet Process Deep Latent Gaussian Mixture Models. arXiv:2006. 08993 [cs, stat] (2020).
140. [ ] Poole, B., Ozair, S., Oord, A. van den, Alemi, A. A. & Tucker, G. On Variational Bounds of Mutual Information. arXiv:1905. 06922 [cs, stat] (2019).
141. [ ] Bartz, C., Bethge, J., Yang, H. & Meinel, C. One Model to Reconstruct Them All: A Novel Way to Use the Stochastic Noise in StyleGAN. arXiv:2010. 11113 [cs, eess] (2020).
142. [ ] Hauberg, S. Only Bayes should learn a manifold (on the estimation of differential geometric structure from data). arXiv:1806. 04994 [cs, stat] (2019).
143. [ ] Sun, M., Agarwal, S. & Kolter, J. Z. Poisoned classifiers are not only backdoored, they are fundamentally broken. arXiv:2010. 09080 [cs] (2020).
144. [ ] Long, T., Cao, Y. & Cheung, J. C. K. Preventing Posterior Collapse in Sequence VAEs with Pooling. arXiv:1911. 03976 [cs, stat] (2019).
145. [ ] Ghorbani, S., Wloka, C., Etemad, A., Brubaker, M. A. & Troje, N. F. Probabilistic character motion synthesis using a hierarchical deep latent variable model. Computer Graphics Forum (2020) doi:10. 1111/cgf.14116.
146. [ ] Koller, D. & Friedman, N. Probabilistic graphical models: principles and techniques. (MIT Press, 2009).
147. [ ] Chen, I. Y., Joshi, S., Ghassemi, M. & Ranganath, R. Probabilistic Machine Learning for Healthcare. arXiv:2009. 11087 [cs, stat] (2020).
148. [ ] Tran, D. Probabilistic Programming for Deep Learning. (2020).
149. [ ] Zhang, Y. et al. Pushing the Limits of Semi-Supervised Learning for Automatic Speech Recognition. arXiv:2010. 10504 [cs, eess] (2020).
150. [ ] Raschka, S. & Mirjalili, V. Python Machine Learning, Third Edition. (2019).
151. [ ] Nakagawa, A. & Kato, K. Quantitative Understanding of VAE by Interpreting ELBO as Rate Distortion Cost of Transform Coding. arXiv:2007. 15190 [cs, math, stat] (2020).
152. [ ] Kato, K., Zhou, J., Sasaki, T. & Nakagawa, A. Rate-Distortion Optimization Guided Autoencoder for Isometric Embedding in Euclidean Latent Space. arXiv:1910. 04329 [cs, stat] (2020).
153. [ ] Oliver, A., Odena, A., Raffel, C., Cubuk, E. D. & Goodfellow, I. J. Realistic evaluation of deep semi-supervised learning algorithms. in Proceedings of the 32nd international conference on neural information processing systems 3239–3250 (Curran Associates Inc., 2018).
154. [ ] Steinke, T. & Zakynthinou, L. Reasoning About Generalization via Conditional Mutual Information. arXiv:2001. 09122 [cs, math, stat] (2020).
155. [ ] Tan, S., Shen, Y., O’Donnell, T. J., Sordoni, A. & Courville, A. Recursive Top-Down Production for Sentence Generation with Latent Trees. arXiv:2010. 04704 [cs] (2020).
156. [ ] Litany, O., Morcos, A., Sridhar, S., Guibas, L. & Hoffman, J. Representation Learning Through Latent Canonicalizations. arXiv:2002. 11829 [cs, stat] (2020).
157. [ ] Mitrovic, J., McWilliams, B., Walker, J., Buesing, L. & Blundell, C. Representation Learning via Invariant Causal Mechanisms. arXiv:2010. 07922 [cs, stat] (2020).
158. [ ] Bengio, Y., Courville, A. & Vincent, P. Representation Learning: A Review and New Perspectives. IEEE Transactions on Pattern Analysis and Machine Intelligence 35, 1798–1828 (2014).
159. [ ] Shu, R., Zhao, S. & Kochenderfer, M. J. Rethinking Style and Content Disentanglement in Variational Autoencoders. 6 (2018).
160. [ ] Xiong, L. et al. SCALE method for single-cell ATAC-seq analysis via latent feature extraction. Nat Commun 10, 4576 (2019).
161. [ ] Gatopoulos, I. & Tomczak, J. M. Self-Supervised Variational Auto-Encoders. arXiv:2010. 02014 [cs, stat] (2020).
162. [ ] Kim, B.-K., Park, S., Kim, G. & Lee, S.-Y. Semi-supervised Disentanglement with Independent Vector Variational Autoencoders. arXiv:2003. 06581 [cs, stat] (2020).
163. [ ] Maaløe, L., Fraccaro, M. & Winther, O. Semi-Supervised Generation with Cluster-aware Generative Models. arXiv:1704. 00637 [cs, stat] (2017).
164. [ ] von Kügelgen, J., Mey, A., Loog, M. & Schölkopf, B. Semi-supervised learning, causality, and the conditional cluster assumption. in (eds. Peters, J. & Sontag, D.) vol. 124 1–10 (PMLR, 2020).
165. [ ] Willetts, M., Roberts, S. J. & Holmes, C. C. Semi-Unsupervised Learning with Deep Generative Models: Clustering and Classifying using Ultra-Sparse Labels. arXiv:1901. 08560 [cs, stat] (2019).
166. [ ] Xu, H. et al. Shallow VAEs with RealNVP Prior Can Perform as Well as Deep Hierarchical VAEs. 9.
167. [ ] Rybkin, O., Daniilidis, K. & Levine, S. Simple and Effective VAE Training with Calibrated Decoders. arXiv:2006. 13202 [cs, eess, stat] (2020).
168. [ ] Liu, J. Z. et al. Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness. arXiv:2006. 10108 [cs, stat] (2020).
169. [ ] Barello, G., Charles, A. S. & Pillow, J. W. Sparse-Coding Variational Auto-Encoders. http://biorxiv.org/lookup/doi/10. 1101/399246 (2018) doi:10. 1101/399246.
170. [ ] Bulinski, A. & Dimitrov, D. Statistical Estimation of the Shannon Entropy. Acta Mathematica Sinica, English Series 35, 17–46 (2019).
171. [ ] Nalisnick, E. & Smyth, P. Stick-Breaking Variational Autoencoders. arXiv:1605. 06197 [stat] (2017).
172. [ ] Zhao, H. Structured Bayesian Latent Factor Models with Meta-data. 193.
173. [ ] Nielsen, D., Jaini, P., Hoogeboom, E., Winther, O. & Welling, M. SurVAE Flows: Surjections to Bridge the Gap between VAEs and Flows. arXiv:2007. 02731 [cs, stat] (2020).
174. [ ] Alet, F., Kawaguchi, K., Lozano-Perez, T. & Kaelbling, L. P. Tailoring: encoding inductive biases by optimizing unsupervised objectives at prediction time. arXiv:2009. 10623 [cs, stat] (2020).
175. [ ] Agarwala, A., Pennington, J., Dauphin, Y. & Schoenholz, S. Temperature check: theory and practice for training models with softmax-cross-entropy losses. arXiv:2010. 07344 [cs] (2020).
176. [ ] Fromm, E. The Art of Loving. (1956).
177. [ ] Fischer, I. The Conditional Entropy Bottleneck. Entropy 22, (2020).
178. [ ] Leiva, R. A. G. The Mathematics of the Unknown. 242.
179. [ ] Parr, T. & Howard, J. The Matrix Calculus You Need For Deep Learning. arXiv:1802. 01528 [cs, stat] (2018).
180. [ ] Phuong, M., Welling, M., Kushman, N., Tomioka, R. & Nowozin, S. The mutual autoencoder: Controlling information in latent code representations. (2018).
181. [ ] Dai, B., Wang, Z. & Wipf, D. The Usual Suspects? Reassessing Blame for VAE Posterior Collapse. arXiv:1912. 10702 [cs, stat] (2019).
182. [ ] Goyal, A., Bengio, Y., Botvinick, M. & Levine, S. The variational bandwidth bottleneck: Stochastic evaluation on an information budget. in International conference on learning representations (2020).
183. [ ] Booch, G. et al. Thinking Fast and Slow in AI. arXiv:2010. 06002 [cs] (2020).
184. [ ] Rainforth, T. et al. Tighter variational bounds are not necessarily better. in (eds. Dy, J. & Krause, A.) vol. 80 4277–4285 (PMLR, 2018).
185. [ ] Fromm, E. To Have or To Be? (1976).
186. [ ] Mondal, A. K., Asnani, H., Singla, P. & AP, P. To Regularize or Not To Regularize? The Bias Variance Trade-off in Regularized AEs. arXiv:2006. 05838 [cs, stat] (2020).
187. [ ] E, W., Ma, C., Wojtowytsch, S. & Wu, L. Towards a Mathematical Understanding of Neural Network-Based Machine Learning: what we know and what we don’t. arXiv:2009. 10713 [cs, math, stat] (2020).
188. [ ] Shin, R., Alemi, A. A., Irving, G. & Vinyals, O. Tree-Structured Variational Autoencoder. 12 (2017).
189. [ ] Doersch, C. Tutorial on Variational Autoencoders. arXiv:1606. 05908 [cs, stat] (2016).
190. [ ] Alemi, A. A., Fischer, I. & Dillon, J. V. Uncertainty in the Variational Information Bottleneck. arXiv:1807. 00906 [cs, stat] (2018).
191. [ ] Shao, L., Song, Y. & Ermon, S. Understanding Classifier Mistakes with Generative Models. arXiv:2010. 02364 [cs] (2020).
192. [ ] Zhang, C., Bengio, S., Hardt, M., Recht, B. & Vinyals, O. Understanding deep learning requires rethinking generalization. arXiv:1611. 03530 [cs] (2017).
193. [ ] Lucas, J., Tucker, G., Grosse, R. & Norouzi, M. Understanding Posterior Collapse in Generative Latent Variable Models. 16 (2019).
194. [ ] Bau, D. et al. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences (2020) doi:10. 1073/pnas.1907375117.
195. [ ] Li, Z. et al. Unsupervised Clustering through Gaussian Mixture Variational AutoEncoder with Non-Reparameterized Variational Inference and Std Annealing. in 2020 International Joint Conference on Neural Networks (IJCNN) 1–8 (IEEE, 2020). doi:10. 1109/IJCNN48605. 2020. 9207493.
196. [ ] Makhzani, A. Unsupervised Representation Learning with Autoencoders. (2018).
197. [ ] Hattingh, C. Using Asyncio in Python. 166.
198. [ ] Xiao, Z., Kreis, K., Kautz, J. & Vahdat, A. VAEBM: A Symbiosis between Variational Autoencoders and Energy-based Models. arXiv:2010. 00654 [cs, stat] (2020).
199. [ ] Chen, W., Liu, W., Cai, Z., Xu, H. & Pei, D. VAEPP: Variational Autoencoder with a Pull-back Prior. 14 (2020).
202. [ ] Jiang, Z., Zheng, Y., Tan, H., Tang, B. & Zhou, H. Variational Deep Embedding: An Unsupervised and Generative Approach to Clustering. arXiv:1611. 05148 [cs] (2017).
203. [ ] Kingma, D. Variational Inference and Deep Learning. (2017).
205. [ ] Park, Y., Kim, C. & Kim, G. Variational Laplace autoencoders. in (eds. Chaudhuri, K. & Salakhutdinov, R.) vol. 97 5032–5041 (PMLR, 2019).
206. [ ] Chen, X. et al. Variational Lossy Autoencoder. arXiv:1611. 02731 [cs, stat] (2017).
207. [ ] Choudrey, R. A. Variational Methods for Bayesian Independent Component Analysis. 261.
208. [ ] Bishop, C. M. & Tipping, M. Variational Relevance Vector Machines. arXiv:1301. 3838 [cs, stat] (2013).
209. [ ] Anonymous. Very deep {VAE}s generalize autoregressive models and can outperform them on images. in Submitted to international conference on learning representations (2021).
210. [ ] Pearl, J. What is Gained from Past Learning. Journal of Causal Inference 6, (2018).
211. [ ] Dai, B. & Wipf, D. When Do Variational Autoencoders Know What They Don’t Know?

---

## Finished

1. [x] Fertig, E., Arbabi, A. & Alemi, A. A. $\beta$-VAEs can retain label information even at high compression. arXiv:1812. 02682 [cs, stat] (2018).
2. [x] Zhu, Q. et al. A Batch Normalized Inference Network Keeps the KL Vanishing Away. arXiv:2004. 12585 [cs] (2020).
5. [x] Sikka, H. A Deeper Look at the Unsupervised Learning of Disentangled Representations in $\beta$-VAE from the Perspective of Core Object Recognition. arXiv:2005. 07114 [cs, stat] (2020).
12. [x] Li, B., He, J., Neubig, G., Berg-Kirkpatrick, T. & Yang, Y. A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text. arXiv:1909. 00868 [cs, stat] (2019).
13. [x] Ridgeway, K. A Survey of Inductive Biases for Factorial Representation-Learning. arXiv:1612. 05299 [cs] (2016).
47. [x] Zheng, H., Yao, J., Zhang, Y. & Tsang, I. W. Degeneration in VAE: in the Light of Fisher Information Loss. arXiv:1802. 06677 [cs, stat] (2018).
204. [x] Voloshynovskiy, S., Taran, O., Kondah, M., Holotyak, T. & Rezende, D. Variational Information Bottleneck for Semi-Supervised Classification. Entropy 22, 943 (2020).
212. [x] Kalibhat, N. M., Balaji, Y. & Feizi, S. Winning Lottery Tickets in Deep Generative Models. arXiv:2010. 02350 [cs] (2020).
201. [x] Dony, L., König, M., Fischer, D. S. & Theis, F. J. Variational autoencoders with flexible priors enable robust distribution learning on single-cell RNA sequencing data. 5 (2020).
