
@misc{00_12VisionPower,
  title = {(12) {{Vision}}, {{Power}} and {{Agency}}: {{The Ascent}} of {{Ng\^o D\`inh Di\^em}}, 1945\textendash 54},
  shorttitle = {(12) {{Vision}}, {{Power}} and {{Agency}}},
  abstract = {ResearchGate is a network dedicated to science and research. Connect, collaborate and discover scientific publications, jobs and conferences. All for free.},
  file = {/home/trung/Zotero/storage/5ATZBP24/231890457_Vision_Power_and_Agency_The_Ascent_of_Ngo_Dinh_Diem_1945-54.html},
  howpublished = {https://www.researchgate.net/publication/231890457\_Vision\_Power\_and\_Agency\_The\_Ascent\_of\_Ngo\_Dinh\_Diem\_1945-54},
  journal = {ResearchGate},
  language = {en}
}

@misc{00_13CountriesAre,
  title = {(13) {{Some Countries Are Squashing}} the {{Coronavirus Curve}}. {{Vietnam Is One}}. - {{Coronavirus}} - {{Quora}}},
  file = {/home/trung/Zotero/storage/L3RREI6I/Some-Countries-Are-Squashing-the-Coronavirus-Curve-Vietnam-Is-One.html},
  howpublished = {https://www.quora.com/q/coronavirus/Some-Countries-Are-Squashing-the-Coronavirus-Curve-Vietnam-Is-One?ch=99\&share=f09960c1\&srid=nrCK}
}

@misc{00_AccurateEducationLtryptophan,
  title = {Accurate {{Education}} \textendash{} {{L}}-Tryptophan \& 5-{{HTP}} (5-{{Hydroxytryptophan}})},
  language = {en-US}
}

@misc{00_AdvancedFitnessResearchReview,
  title = {R/{{AdvancedFitness}} - {{Research Review}} \#2.1 - {{Do Athletes Benefit}} from {{Multivitamins}}?},
  abstract = {111 votes and 37 comments so far on Reddit},
  file = {/home/trung/Zotero/storage/43AMVG3Y/research_review_21_do_athletes_benefit_from.html},
  howpublished = {https://www.reddit.com/r/AdvancedFitness/comments/3t82wk/research\_review\_21\_do\_athletes\_benefit\_from/},
  journal = {reddit},
  language = {en-US}
}

@misc{00_AdvancingSelfSupervisedSemiSupervised,
  title = {Advancing {{Self}}-{{Supervised}} and {{Semi}}-{{Supervised Learning}} with {{SimCLR}}},
  abstract = {Posted by Ting Chen, Research Scientist, and Geoffrey Hinton, VP \& Engineering Fellow, Google Research     Recently, natural language proces...},
  file = {/home/trung/Zotero/storage/U6D25G3A/advancing-self-supervised-and-semi.html},
  journal = {Google AI Blog},
  keywords = {self-supervised},
  language = {en}
}

@misc{00_AdvancingSemisupervisedLearning,
  title = {Advancing {{Semi}}-Supervised {{Learning}} with {{Unsupervised Data Augmentation}}},
  abstract = {Posted by Qizhe Xie, Student Researcher and Thang Luong, Senior Research Scientist, Google Research, Brain Team     Success in deep learning...},
  file = {/home/trung/Zotero/storage/4X28R93Q/advancing-semi-supervised-learning-with.html},
  journal = {Google AI Blog},
  language = {en}
}

@misc{00_AmortizedOptimizationRui,
  title = {Amortized {{Optimization}} - {{Rui Shu}}},
  abstract = {PrologueOver the summer, I made it a goal to use the phrase ``amortized optimization'' as often as my friends would dar...},
  howpublished = {http://ruishu.io/2017/11/07/amortized-optimization/}
}

@article{00_AssessmentreportRhodiola,
  title = {Assessment Report on {{Rhodiola}} Rosea {{L}}., Rhizoma et Radix},
  pages = {33},
  file = {/home/trung/GoogleDrive/Zotero/assessment report on rhodiola rosea l.pdf},
  language = {en}
}

@misc{00_AsymmetricVariationalAutoencoders,
  title = {Asymmetric {{Variational Autoencoders}}},
  file = {/home/trung/GoogleDrive/Zotero/asymmetric variational autoencoders.pdf},
  howpublished = {https://drive.google.com/file/d/1dWxAXoyFpR8DZ768W9g2q453HB8nB4DF/view?usp=drive\_open\&usp=embed\_facebook},
  journal = {Google Docs},
  keywords = {auxiliary,important weight,variational}
}

@misc{00_AttentionAugmentedRecurrent,
  title = {Attention and {{Augmented Recurrent Neural Networks}}},
  file = {/home/trung/Zotero/storage/KRZGHNN8/augmented-rnns.html},
  howpublished = {https://distill.pub/2016/augmented-rnns/},
  keywords = {attention}
}

@article{00_AttentionDetail,
  title = {Attention {{In Detail}}},
  pages = {19},
  file = {/home/trung/GoogleDrive/Zotero/attention in detail.pdf},
  language = {en}
}

@misc{00_AutomaticPortraitSegmentation,
  title = {Automatic {{Portrait Segmentation}} for {{Image Stylization}} - {{Shen}} - 2016 - {{Computer Graphics Forum}} - {{Wiley Online Library}}},
  howpublished = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12814}
}

@misc{00_BayesianDataAnalysis,
  title = {Bayesian {{Data Analysis}} Course}
}

@misc{00_BayesianDeepLearning,
  title = {Bayesian {{Deep Learning Workshop}} | {{NeurIPS}} 2019},
  abstract = {Bayesian Deep Learning Workshop at NeurIPS 2019 \textemdash{} Friday, December 13, 2019 \textemdash{} Vancouver Convention Center, Vancouver, Canada.},
  file = {/home/trung/Zotero/storage/82IGAWAI/bayesiandeeplearning.org.html},
  howpublished = {http://bayesiandeeplearning.org}
}

@misc{00_BiasGeneralizationDeep,
  title = {Bias and {{Generalization}} in {{Deep Generative Models}}},
  file = {/home/trung/Zotero/storage/LS2I4K8V/bias-and-generalization-dgm.html},
  howpublished = {https://ermongroup.github.io/blog/bias-and-generalization-dgm/?fbclid=IwAR0ywH78NV4qY7mfPaOKDOA8\_1bSu97X2iIQsFdcwI4D3AqO1O0hAXqEUdg},
  keywords = {adversarial,generative,variational}
}

@misc{00_BiasGeneralizationDeepa,
  title = {Bias and {{Generalization}} in {{Deep Generative Models An Empirical Study}}},
  file = {/home/trung/Zotero/storage/9WTA3HKJ/view.html},
  howpublished = {https://drive.google.com/file/d/1khNmjtc1R0pYVDfKOQqPKh9RaQkmfYjR/view?usp=drive\_open\&usp=embed\_facebook},
  journal = {Google Docs},
  keywords = {bias,generalization,generative,variational}
}

@misc{00_Boostrapstatisticshuffling,
  title = {On {{Boostrap}} of {{U}} and {{V}} Statistic (Shuffling for Independent Factors)},
  file = {/home/trung/GoogleDrive/Zotero/on boostrap of u and v statistic (shuffling for independent factors).pdf}
}

@misc{00_BoostYourBrain,
  title = {Boost {{Your Brain Power}}: {{Creatine}}, {{A Compound Found In Muscle Tissue}}, {{Found To Improve Working Memory And General Intelligence}}},
  shorttitle = {Boost {{Your Brain Power}}},
  abstract = {Research undertaken by scientists at the University of Sydney and Macquarie University in Australia has shown that taking creatine, a compound found in muscle tissue, as a dietary supplement can give a significant boost to both working memory and general intelligence.},
  howpublished = {https://www.sciencedaily.com/releases/2003/08/030813070944.htm},
  journal = {ScienceDaily},
  language = {en}
}

@misc{00_BriefGuideRunning,
  title = {A {{Brief Guide}} to {{Running ML Systems}} in {{Production}} \textendash{} {{O}}'{{Reilly}}},
  howpublished = {https://www.oreilly.com/content/a-brief-guide-to-running-ml-systems-in-production/}
}

@misc{00_challengeslearninginference,
  title = {On the Challenges of Learning with Inference Networks on Sparse, High-Dimensional Data},
  file = {/home/trung/GoogleDrive/Zotero/on the challenges of learning with inference networks on sparse, high-dimensional data.pdf}
}

@misc{00_ChangeVariablesPrecursor,
  title = {Change of {{Variables}}: {{A Precursor}} to {{Normalizing Flow}} - {{Rui Shu}}},
  shorttitle = {Change of {{Variables}}},
  abstract = {Normalizing flow is a cool technique for density estimation that is fun to learn about and (tricky to) wrap your mind...},
  howpublished = {http://ruishu.io/2018/05/19/change-of-variables/}
}

@misc{00_ChemicalCompositionSelected,
  title = {Chemical {{Composition}} of {{Selected Edible Nut Seeds}}},
  doi = {10.1021/jf0606959},
  abstract = {Commercially important edible nut seeds were analyzed for chemical composition and moisture sorption. Moisture (1.47-9.51\%), protein (7.50-21.56\%), lipid (42.88-66.71\%), ash (1.16-3.28\%), total soluble sugars (0.55-3.96\%), tannins (0.01-0.88\%), and phytate (0.15-0.35\%) contents varied considerably. Regardless of the seed type, lipids were mainly composed of mono- and polyunsaturated fatty acids ({$>$}75\% of the total lipids). Fatty acid composition analysis indicated that oleic acid (C18:1) was the main constituent of monounsaturated lipids in all seed samples. With the exception of macadamia, linoleic acid (C18:2) was the major polyunsaturated fatty acid. In the case of walnuts, in addition to linoleic acid (59.79\%) linolenic acid (C18:3) also significantly contributed toward the total polyunsaturated lipids. Amino acid composition analyses indicated lysine (Brazil nut, cashew nut, hazelnut, pine nut, and walnut), sulfur amino acids methionine and cysteine (almond), tryptophan (macadamia, pecan), and threonine (peanut) to be the first limiting amino acid as compared to human (2-5 year old) amino acid requirements. The amino acid composition of the seeds was characterized by the dominance of hydrophobic (range = 37.16-44.54\%) and acidic (27.95-33.17\%) amino acids followed by basic (16.16-21.17\%) and hydrophilic (8.48-11.74\%) amino acids. Trypsin inhibitory activity, hemagglutinating activity, and proteolytic activity were not detected in the nut seed samples analyzed. Sorption isotherms (Aw range = 0.08-0.97) indicated a narrow range for monolayer water content (11-29 mg/g of dry matter). No visible mold growth was evident on any of the samples stored at Aw {$<$} 0.53 and 25 \textdegree C for 6 months.  Keywords:  Tree nuts; chemical composition; protein; lipids; fatty acids; phytates; tannins; amino acids; storage; sorption isotherm},
  howpublished = {https://pubs.acs.org/doi/pdf/10.1021/jf0606959},
  language = {en}
}

@misc{00_ClimbingNLUMeaning,
  title = {Climbing towards {{NLU}}: {{On Meaning}}, {{Form}}, and {{Understanding}} in the {{Age}} of {{Data}}},
  file = {/home/trung/GoogleDrive/Zotero/climbing towards nlu.pdf}
}

@misc{00_CLPCalculusTextbooks,
  title = {{{CLP Calculus Textbooks}}},
  file = {/home/trung/Zotero/storage/YCYBQP5N/~CLP.html},
  howpublished = {http://www.math.ubc.ca/\textasciitilde CLP/}
}

@misc{00_CompetitiveTrainingMixtures,
  title = {Competitive {{Training}} of {{Mixtures}} of {{Independent Deep Generative Models}}},
  file = {/home/trung/GoogleDrive/Zotero/competitive training of mixtures of independent deep generative models.pdf}
}

@misc{00_COMPSCI282BRInterpretability,
  title = {{{COMPSCI 282BR}}: {{Interpretability}} and {{Explainability}} in {{Machine Learning}}},
  file = {/home/trung/Zotero/storage/G4P7LCNM/interpretable-ml-class.github.io.html},
  howpublished = {https://interpretable-ml-class.github.io/?fbclid=IwAR2E5QJtu2MXbFTaBAHcs5IwGLCErSZiqBJSSHzNx9pBtECU1-9OsTpiRLI}
}

@misc{00_ComputerVision,
  title = {Computer {{Vision}}},
  file = {/home/trung/GoogleDrive/Zotero/computer vision.pdf}
}

@misc{00_conceptualmodelcoronavirus,
  title = {A Conceptual Model for the Coronavirus Disease 2019},
  file = {/home/trung/GoogleDrive/Zotero/a conceptual model for the coronavirus disease 2019.pdf}
}

@misc{00_ConditionalGenerationMolecules,
  title = {Conditional {{Generation}} of {{Molecules}} from {{Disentangled Representations}}},
  file = {/home/trung/Zotero/storage/EIC67NIK/31c4952a23bbe183258e3d24926a067cc0ed(1).pdf}
}

@misc{00_ConditionalGenerationMoleculesa,
  title = {Conditional {{Generation}} of {{Molecules}} from {{Disentangled Representations}}},
  file = {/home/trung/GoogleDrive/Zotero/conditional generation of molecules from disentangled representations.pdf}
}

@misc{00_ConvolutionsAutoregressiveNeural,
  title = {Convolutions in {{Autoregressive Neural Networks}}},
  abstract = {This post explains how to use one-dimensional causal and dilated convolutions in autoregressive neural networks such as WaveNet.},
  file = {/home/trung/Zotero/storage/EH5ZUX8L/convolution-in-autoregressive-neural-networks.html},
  howpublished = {https://theblog.github.io},
  journal = {The Blog},
  keywords = {wavenet}
}

@misc{00_COVID19ForecastingPrevention,
  title = {{{COVID}}-19 {{Forecasting}}, {{Prevention}} and {{Mitigation}}},
  file = {/home/trung/GoogleDrive/Zotero/covid-19 forecasting, prevention and mitigation.pdf}
}

@misc{00_CRUKCIBioinformaticsSummer,
  title = {{{CRUK}}-{{CI Bioinformatics Summer School Single Cell RNA}}-Sequencing},
  file = {/home/trung/Zotero/storage/GMNPXCRQ/QC_and_normalization.html},
  howpublished = {https://bioinformatics-core-shared-training.github.io/cruk-summer-school-2018/SingleCell/practical/QC\_and\_normalization.html}
}

@misc{00_CS188Introduction,
  title = {{{CS}} 188: {{Introduction}} to {{Artificial Intelligence}}, {{Fall}} 2018},
  file = {/home/trung/Zotero/storage/FY9HEZNQ/fa18.html},
  howpublished = {https://inst.eecs.berkeley.edu/\textasciitilde cs188/fa18/?fbclid=IwAR1A0FYti01pn-eRJtN9dKLoL3RjoEWr12sJzQF8wnBSf-dTRi9XQdcnk5E}
}

@misc{00_CS224NNaturalLanguage,
  title = {{{CS224N Natural Language Processing}} with {{Deep Learning}}}
}

@misc{00_CS231nPythonTutorial,
  title = {{{CS231n Python Tutorial With Google Colab}}},
  file = {/home/trung/Zotero/storage/2RGWQ94A/python-colab.html},
  howpublished = {https://colab.research.google.com/github/cs231n/cs231n.github.io/blob/master/python-colab.ipynb?fbclid=IwAR3kZTcxqJnSdB\_BIrOH8A0Cv\_cVHAOPEZ3MJbMP1EJlGiP4KWRt6TlR0xU},
  language = {en}
}

@misc{00_CS294131Special,
  title = {{{CS}} 294-131: {{Special Topics}} in {{Deep Learning}}},
  howpublished = {https://berkeley-deep-learning.github.io/cs294-131-s19/}
}

@misc{00_CS294158SP19DeepUnsupervised,
  title = {{{CS294}}-158-{{SP19 Deep Unsupervised Learning Spring}} 2019},
  file = {/home/trung/Zotero/storage/J74SV849/home.html},
  howpublished = {https://sites.google.com/view/berkeley-cs294-158-sp19/home},
  language = {en}
}

@misc{00_CS294158SP20DeepUnsupervised,
  title = {{{CS294}}-158-{{SP20 Deep Unsupervised Learning Spring}} 2020},
  file = {/home/trung/Zotero/storage/MNRIPM9D/home.html},
  howpublished = {https://sites.google.com/view/berkeley-cs294-158-sp20/home},
  language = {en}
}

@misc{00_CSC2541F18TopicsMachine,
  title = {{{CSC2541}}-{{F18 Topics}} in {{Machine Learning}} - {{Deep Reinforcement Learning}} ({{Fall}} 2018)},
  file = {/home/trung/Zotero/storage/XDXRW4ZV/csc2541-f18.github.io.html},
  howpublished = {https://csc2541-f18.github.io/}
}

@misc{00_CSC412Winter2020,
  title = {{{CSC412 Winter}} 2020: {{Probabilsitic Machine Learning}}},
  file = {/home/trung/Zotero/storage/Q8QK3KC9/csc412.html},
  howpublished = {https://probmlcourse.github.io/csc412/}
}

@misc{00_CSC4132516Neural,
  title = {{{CSC413}}/2516 {{Neural Networks}} and {{Deep Learning}} ({{Winter}} 2020)},
  file = {/home/trung/Zotero/storage/5APVK4RF/csc413-2020.github.io.html},
  howpublished = {https://csc413-2020.github.io/}
}

@misc{00_CURLContrastiveUnsupervised,
  title = {{{CURL}}: {{Contrastive Unsupervised Representations}} for {{Reinforcement Learning}}},
  file = {/home/trung/GoogleDrive/Zotero/curl.pdf;/home/trung/Zotero/storage/DDKSAFBA/curl.html},
  howpublished = {https://mishalaskin.github.io/curl/}
}

@misc{00_DailychartHow,
  title = {Daily Chart - {{How}} Much Would Giving up Meat Help the Environment\_ \_ {{Graphic}} Detail \_ {{The Economist}}.Pdf},
  file = {/home/trung/GoogleDrive/Zotero/daily chart - how much would giving up meat help the environment_ _ graphic detail _ the economist.pdf}
}

@misc{00_Datamanipulationcleaning,
  title = {Data Manipulation, Cleaning, and Processing in {{JavaScript}}},
  abstract = {This guide teaches the basics of manipulating data using JavaScript in the browser, or in node.js. Specifically, demonstrating tasks that are geared around preparing data for further analysis and visualization. This guide will demonstrate some basic techniques and how to implement them using core JavaScript API, the d3.js library and lodash.},
  howpublished = {http://learnjsdata.com},
  journal = {Learn JS Data}
}

@misc{00_DavidDuvenaudCourses,
  title = {David {{Duvenaud Courses}}},
  file = {/home/trung/Zotero/storage/EPNBCZUV/~duvenaud.html},
  howpublished = {http://www.cs.toronto.edu/\textasciitilde duvenaud/}
}

@misc{00_debiasgenerationpdf,
  title = {Debias\_generation.Pdf},
  file = {/home/trung/Zotero/storage/VYM5W5EC/view.html},
  howpublished = {https://drive.google.com/file/d/1NU-YsvoMF1X8jivr1QtZb4Aq63oRbQjU/view?usp=drive\_open\&usp=embed\_facebook},
  journal = {Google Docs}
}

@misc{00_DecodingThoughtVector,
  title = {Decoding the {{Thought Vector}}},
  file = {/home/trung/Zotero/storage/8H6ZEEJ8/ThoughtVectors.html},
  howpublished = {https://gabgoh.github.io/ThoughtVectors/}
}

@misc{00_DeepEquilibriumModels,
  title = {Deep {{Equilibrium Models}}},
  file = {/home/trung/Zotero/storage/7ETZ7W6B/1909.html},
  howpublished = {https://arxiv.org/abs/1909.01377}
}

@misc{00_DeepGenerativeModels,
  title = {Deep {{Generative Models CS236}} - {{Fall}} 2019},
  file = {/home/trung/Zotero/storage/BVN9328B/index.html},
  howpublished = {https://deepgenerativemodels.github.io/notes/index.html}
}

@misc{00_DEEPLEARNINGDeep,
  title = {{{DEEP LEARNING}} {$\cdot$} {{Deep Learning}}},
  file = {/home/trung/Zotero/storage/DEDM8NMY/pytorch-Deep-Learning.html},
  howpublished = {https://atcold.github.io/pytorch-Deep-Learning/?fbclid=IwAR2goAsgCkhdJFr6FXeIYXJQR8tqGXc\_1GJyYVMnwwPEKAb1mElJjaFsTpA}
}

@misc{00_DeepLearningDrizzle,
  title = {Deep {{Learning Drizzle}} ({{This}} Contain Everything)},
  file = {/home/trung/Zotero/storage/7P8GXRTA/deep-learning-drizzle.github.io.html},
  howpublished = {https://deep-learning-drizzle.github.io/?fbclid=IwAR2JO\_W4b1-M91CWUFJDRzoHxsaeMpOwQsOtLeTCeKhd9ge6HaCItHhqqrI}
}

@misc{00_DeepLearningOur,
  title = {Deep {{Learning}}: {{Our Miraculous Year}} 1990-1991},
  file = {/home/trung/Zotero/storage/JKZ5BG3X/deep-learning-miraculous-year-1990-1991.html},
  howpublished = {http://people.idsia.ch/\textasciitilde juergen/deep-learning-miraculous-year-1990-1991.html\#AC90}
}

@misc{00_DeepMindWhatif,
  title = {{DeepMind: What if solving one problem could unlock solutions to thousands more?}},
  shorttitle = {{DeepMind}},
  abstract = {We research and build safe AI systems that learn how to solve problems and advance scientific discovery for all. Explore our work: deepmind.com/research},
  file = {/home/trung/Zotero/storage/BBX6Q7AQ/Causal_Bayesian_Networks.html},
  howpublished = {/blog/article/Causal\_Bayesian\_Networks},
  journal = {Deepmind},
  keywords = {causal},
  language = {ALL}
}

@misc{00_DeepMultiscaleRecurrent,
  title = {On {{Deep Multiscale Recurrent Neural Networks}}},
  file = {/home/trung/GoogleDrive/Zotero/on deep multiscale recurrent neural networks.pdf},
  howpublished = {https://papyrus.bib.umontreal.ca/xmlui/bitstream/handle/1866/21588/chung\_junyoung\_2018\_these.pdf?sequence=2}
}

@misc{00_DeepProbabilisticModeling,
  title = {Deep {{Probabilistic Modeling Blog}}},
  file = {/home/trung/Zotero/storage/3ZVZA27X/blog.html},
  howpublished = {https://andresmasegosa.github.io/blog/},
  journal = {andres masegosa}
}

@misc{00_DeepUnsupervisedClustering,
  title = {Deep {{Unsupervised Clustering}} with {{Gaussian Mixture Variational Autoencoders}}},
  file = {/home/trung/Zotero/storage/FD36JGTJ/1611.html},
  howpublished = {https://arxiv.org/abs/1611.02648},
  keywords = {clustering,mixture,variational}
}

@misc{00_DensityEstimationMaximum,
  title = {Density {{Estimation}}: {{Maximum Likelihood}} - {{Rui Shu}}},
  shorttitle = {Density {{Estimation}}},
  abstract = {I'm starting a series of blog posts on the general topic of density estimation with deep generative models. The goal ...},
  howpublished = {http://ruishu.io/2018/02/12/dgm-intro/}
}

@misc{00_DensityEstimationVariational,
  title = {Density {{Estimation}}: {{Variational Autoencoders}} - {{Rui Shu}}},
  shorttitle = {Density {{Estimation}}},
  abstract = {One of the most popular models for density estimation is the Variational Autoencoder. It is a model that I have spent...},
  howpublished = {http://ruishu.io/2018/03/14/vae/}
}

@misc{00_Diagramprobabilitydistribution,
  title = {Diagram of Probability Distribution Relationships},
  howpublished = {https://www.johndcook.com/blog/distribution\_chart/}
}

@misc{00_DifferentiableInferenceGenerative,
  title = {Differentiable {{Inference}} and {{Generative Models}}},
  file = {/home/trung/Zotero/storage/RE54RD72/index.html},
  howpublished = {http://www.cs.toronto.edu/\textasciitilde duvenaud/courses/csc2541/index.html}
}

@misc{00_DiscreteFlowsInvertible,
  title = {Discrete {{Flows}}: {{Invertible Generative Models}} of {{Discrete Data}}},
  file = {/home/trung/GoogleDrive/Zotero/discrete flows.pdf}
}

@misc{00_EarlyVisualConcept,
  title = {Early {{Visual Concept Learning}} with {{Unsupervised Deep Learning}}},
  file = {/home/trung/GoogleDrive/Zotero/early visual concept learning with unsupervised deep learning.pdf},
  keywords = {disentanglement}
}

@misc{00_EffectiveEstimationDeep,
  title = {Effective {{Estimation}} of {{Deep Generative Language Models}}},
  file = {/home/trung/GoogleDrive/Zotero/effective estimation of deep generative language models.pdf},
  howpublished = {https://arxiv.org/pdf/1904.08194.pdf},
  keywords = {vae_issues}
}

@misc{00_EfficientBackprop,
  title = {Efficient {{Backprop}}},
  file = {/home/trung/GoogleDrive/Zotero/efficient backprop.pdf},
  howpublished = {http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf}
}

@misc{00_emergenceheathumidity,
  title = {The Emergence of Heat and Humidity Too Severe for Human Tolerance | {{Science Advances}}},
  howpublished = {https://advances.sciencemag.org/content/6/19/eaaw1838}
}

@misc{00_Environmentalimpactoffoodsbylifecyclestagepng,
  title = {Environmental-Impact-of-Foods-by-Life-Cycle-Stage.Png},
  file = {/home/trung/Zotero/storage/BWDUZJS2/Environmental-impact-of-foods-by-life-cycle-stage.png}
}

@misc{00_Evaluatingbiologicalplausibility,
  title = {Evaluating Biological Plausibility of Learning Algorithms the Lazy Way},
  file = {/home/trung/GoogleDrive/Zotero/evaluating biological plausibility of learning algorithms the lazy way.pdf},
  howpublished = {https://openreview.net/pdf?id=HJgPEXtIUS}
}

@misc{00_EvaluatingTestingUnintended,
  title = {Evaluating and {{Testing Unintended Memorization}} in {{Neural Networks}} \textendash{} {{The Berkeley Artificial Intelligence Research Blog}}},
  file = {/home/trung/Zotero/storage/664QIVL3/memorization.html;/home/trung/Zotero/storage/LUB2EPFN/memorization.html},
  howpublished = {https://bair.berkeley.edu/blog/2019/08/13/memorization/},
  keywords = {memory,overfitting}
}

@misc{00_ExploringAttentionBased,
  title = {Exploring {{Attention Based Model}} for {{Captioning Images}}},
  file = {/home/trung/GoogleDrive/Zotero/exploring attention based model for captioning images.pdf},
  howpublished = {https://papyrus.bib.umontreal.ca/xmlui/bitstream/handle/1866/20194/Xu\_Kelvin\_2017\_memoire.pdf?sequence=2}
}

@article{00_FasterStabilizedGan,
  title = {Towards {{Faster}} and {{Stabilized Gan Training}} for {{High}}-{{Fidelity Few}}-{{Shot Image Synthesis}}},
  file = {/home/trung/GoogleDrive/Zotero/towards faster and stabilized gan training for high-fidelity few-shot image synthesis.pdf}
}

@misc{00_FeedbackNetworks,
  title = {Feedback {{Networks}}},
  file = {/home/trung/GoogleDrive/Zotero/feedback networks.pdf},
  howpublished = {http://feedbacknet.stanford.edu/feedback\_networks\_2016.pdf}
}

@misc{00_Frontpage,
  title = {Front Page},
  abstract = {The Finnish National Agency for Education's core tasks are to develop education and training, early childhood education and lifelong learning and to promote internationalisation in Finland.},
  file = {/home/trung/Zotero/storage/HVMIQLVH/en.html},
  howpublished = {https://www.oph.fi/en},
  journal = {Finnish National Agency for Education},
  language = {en}
}

@misc{00_FullStackDeep,
  title = {Full {{Stack Deep Learning}}},
  abstract = {Full Stack Deep Learning helps you bridge the gap from training machine learning models to deploying AI systems in the real world.},
  howpublished = {https://course.fullstackdeeplearning.com/}
}

@misc{00_Fullydisentangledtexttoimagesynthesis,
  title = {Fully-Disentangled Text-to-Image Synthesis},
  file = {/home/trung/GoogleDrive/Zotero/fully-disentangled text-to-image synthesis.pdf}
}

@misc{00_GaussianMixtureVAE,
  title = {Gaussian {{Mixture VAE}}: {{Lessons}} in {{Variational Inference}}, {{Generative Models}}, and {{Deep Nets}} - {{Rui Shu}}},
  shorttitle = {Gaussian {{Mixture VAE}}},
  abstract = {Not too long ago, I came across this paper on unsupervised clustering with Gaussian Mixture VAEs. I was quite surpris...},
  file = {/home/trung/Zotero/storage/9PC7J3EK/gmvae.html},
  howpublished = {http://ruishu.io/2016/12/25/gmvae/},
  keywords = {mixture,variational}
}

@misc{00_GaussianMixtureVAEa,
  title = {Gaussian {{Mixture VAE}}: {{Lessons}} in {{Variational Inference}}, {{Generative Models}}, and {{Deep Nets}} - {{Rui Shu}}},
  shorttitle = {Gaussian {{Mixture VAE}}},
  abstract = {Not too long ago, I came across this paper on unsupervised clustering with Gaussian Mixture VAEs. I was quite surpris...},
  howpublished = {http://ruishu.io/2016/12/25/gmvae/}
}

@article{00_GaussianProcessPrior,
  title = {Gaussian {{Process Prior Variational Autoencoder}} - {{Francesco Paolo Casale}}, {{Adrian V Dalca}}, {{Luca Saglietti}}, {{Jennifer Listgarten}}, {{Nicolo Fusi}}},
  pages = {33},
  file = {/home/trung/GoogleDrive/Zotero/gaussian process prior variational autoencoder - francesco paolo casale, adrian v dalca, luca saglietti, jennifer listgarten, nicolo fusi.pdf},
  language = {en}
}

@misc{00_GeneratingDiverseHighFidelity,
  title = {Generating {{Diverse High}}-{{Fidelity Images}} with {{VQ}}-{{VAE}}-2},
  file = {/home/trung/GoogleDrive/Zotero/generating diverse high-fidelity images with vq-vae-2.pdf},
  howpublished = {https://arxiv.org/pdf/1906.00446.pdf}
}

@misc{00_GENERATIVEFEATUREMATCHING,
  title = {{{GENERATIVE FEATURE MATCHING NETWORKS}}},
  file = {/home/trung/GoogleDrive/Zotero/generative feature matching networks.pdf}
}

@misc{00_GenerativeModel,
  title = {Generative {{Model}} 1},
  file = {/home/trung/GoogleDrive/Zotero/generative model 1.pdf}
}

@misc{00_George2017What,
  title = {George\_2017\_{{What}} Can the Brain Teach Us about Building Artificial Intelligence.Pdf},
  file = {/home/trung/GoogleDrive/Zotero/george_2017_what can the brain teach us about building artificial intelligence.pdf}
}

@misc{00_GLUEBenchmark,
  title = {{{GLUE Benchmark}}},
  abstract = {The General Language Understanding Evaluation (GLUE) benchmark is a collection of resources for training, evaluating, and analyzing natural language understanding systems},
  howpublished = {https://gluebenchmark.com/},
  language = {en}
}

@misc{00_Goodisngood,
  title = {``{{Good}}'' Isn't Good Enough},
  file = {/home/trung/GoogleDrive/Zotero/“good” isn’t good enough.pdf}
}

@misc{00_GoogleColaboratory,
  title = {Google {{Colaboratory}}},
  file = {/home/trung/Zotero/storage/9WCALJGU/dna_sequencing_error_correction.html},
  howpublished = {https://colab.research.google.com/github/google/nucleus/blob/master/nucleus/examples/dna\_sequencing\_error\_correction.ipynb?fbclid=IwAR3MbpJIYCBwMzpgGHRiuj9UJxwe4lmSOkKpp-PeUwlCBRbNgVIzcBeuib0},
  language = {en}
}

@misc{00_GRAPHMIXREGULARIZEDTRAINING,
  title = {{{GRAPHMIX}}: {{REGULARIZED TRAINING OF GRAPH NEURAL NETWORKS FOR SEMI}}-{{SUPERVISED LEARNING}}},
  file = {/home/trung/GoogleDrive/Zotero/graphmix.pdf}
}

@misc{00_HaoWenDongHerman,
  title = {Hao-{{Wen Dong}} ({{Herman}})},
  abstract = {Homepage},
  file = {/home/trung/Zotero/storage/DHIPD8ME/salu133445.github.io.html},
  howpublished = {https://salu133445.github.io/},
  journal = {Hao-Wen Dong (Herman)},
  language = {en-US}
}

@misc{00_HighProteinDietsLongevity,
  title = {High-{{Protein Diets}} and {{Longevity}}},
  abstract = {ReferenceLevine ME, Suarez JA, Brandhorst S, et al. Low protein intake is associated with a major reduction in IGF-1, cancer, and overall mortality in the 65 and younger but not older population. Cell Metab. 2014;19(3):407-417.DesignDietary protein consumption was examined in a US population cohort and links sought to overall and disease specific mortality.ParticipantsThe cohort studied consisted of 6,381 adults aged 50 and over from the NHANES III, a nationally representative, cross-sectional study.},
  howpublished = {https://www.naturalmedicinejournal.com/journal/2014-05/high-protein-diets-and-longevity},
  journal = {Natural Medicine Journal},
  language = {en}
}

@misc{00_HighProteinDietsMiddle,
  title = {High-{{Protein Diets}} in {{Middle Age Might Shorten Life Span}}},
  abstract = {But same study found the reverse was true when people reached old age},
  howpublished = {https://www.webmd.com/diet/news/20140304/high-protein-diets-in-middle-age-might-shorten-life-span},
  journal = {WebMD},
  language = {en}
}

@misc{00_HighTemperatureHigh,
  title = {High {{Temperature}} and {{High Humidity Reduce}} the {{Transmission}} of {{COVID}}-19},
  file = {/home/trung/GoogleDrive/Zotero/high temperature and high humidity reduce the transmission of covid-19.pdf}
}

@misc{00_HowUMAPWorks,
  title = {How {{UMAP Works}} \textemdash{} Umap 0.4 Documentation},
  howpublished = {https://umap-learn.readthedocs.io/en/latest/how\_umap\_works.html}
}

@misc{00_HybridConvolutionalVariational,
  title = {A {{Hybrid Convolutional Variational Autoencoder}} for {{Text Generation}}},
  file = {/home/trung/Zotero/storage/JQ64PPRJ/1702.html},
  howpublished = {https://arxiv.org/abs/1702.02390},
  keywords = {variational}
}

@misc{00_HydroxychloroquinefinalDOI,
  title = {Hydroxychloroquine\_final\_{{DOI}}\_{{IJAA}}.Pdf},
  file = {/home/trung/GoogleDrive/Zotero/hydroxychloroquine_final_doi_ijaa.pdf}
}

@misc{00_ICML2019MetaLearning,
  title = {{{ICML}} 2019 {{Meta}}-{{Learning Tutorial}}},
  file = {/home/trung/Zotero/storage/5N4TLGAZ/icml19metalearning.html},
  howpublished = {https://sites.google.com/view/icml19metalearning},
  language = {en}
}

@misc{00_ICML2019MetaLearninga,
  title = {{{ICML}} 2019 {{Meta}}-{{Learning Tutorial}}},
  howpublished = {https://sites.google.com/view/icml19metalearning},
  language = {en}
}

@misc{00_ICML2020,
  title = {{{ICML}} 2020},
  abstract = {The International Conference on Machine Learning (ICML) is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence known as machine learning. ICML is globally renowned for presenting and publishing cutting-edge research on all aspects of machine learning used in closely related areas like artificial intelligence, statistics and data science, as well as important application areas such as machine vision, computational biology, speech recognition, and robotics. ICML is one of the fastest growing artificial intelligence conferences in the world. Participants at ICML span a wide range of backgrounds, from academic and industrial researchers, to entrepreneurs and engineers, to graduate students and postdocs.},
  howpublished = {https://slideslive.com/icml-2020},
  journal = {SlidesLive}
}

@misc{00_Ifyousurvive,
  title = {If You Survive the {{Coronavirus}} Can You Be Re-Infected?},
  file = {/home/trung/Zotero/storage/UMTXCK7W/CGTN-Social-Team.html},
  howpublished = {https://www.quora.com/If-you-survive-the-Coronavirus-can-you-be-re-infected/answer/CGTN-Social-Team?ch=99\&share=22343ece\&srid=nrCK}
}

@misc{00_Impactnonpharmaceuticalinterventions,
  title = {Impact of Non-Pharmaceutical Interventions ({{NPIs}}) to Reduce {{COVID}}- 19 Mortality and Healthcare Demand},
  file = {/home/trung/GoogleDrive/Zotero/impact of non-pharmaceutical interventions (npis) to reduce covid- 19 mortality and healthcare demand.pdf}
}

@misc{00_ImproveTrainingStability,
  title = {Improve {{Training Stability}} of {{Semi}}-{{Supervised Generative Adversarial Networks}} with {{Collaborative Training}}},
  file = {/home/trung/GoogleDrive/Zotero/improve training stability of semi-supervised generative adversarial networks with collaborative training.pdf}
}

@misc{00_ImprovevariationalautoEncoder,
  title = {Improve Variational {{autoEncoder}} with Auxiliary Softmax Multiclassifier},
  file = {/home/trung/Zotero/storage/2S4G42CI/1908.html},
  howpublished = {https://arxiv.org/abs/1908.06966},
  keywords = {variational}
}

@misc{00_ImprovingReproducibilityMachine,
  title = {Improving {{Reproducibility}} in {{Machine Learning Research}}},
  file = {/home/trung/GoogleDrive/Zotero/improving reproducibility in machine learning research.pdf}
}

@misc{00_InceptionismGoingDeeper,
  title = {Inceptionism: {{Going Deeper}} into {{Neural Networks}}},
  shorttitle = {Inceptionism},
  abstract = {Posted by Alexander Mordvintsev, Software Engineer, Christopher Olah, Software Engineering Intern and Mike Tyka, Software Engineer Update - ...},
  file = {/home/trung/Zotero/storage/KXQ3XBBI/inceptionism-going-deeper-into-neural.html},
  journal = {Google AI Blog},
  language = {en}
}

@misc{00_Inceptionpdf,
  title = {Inception.Pdf},
  file = {/home/trung/GoogleDrive/Zotero/inception.pdf}
}

@misc{00_IndependentComponentAnalysis,
  title = {Independent {{Component Analysis}}: {{A Tutorial}}},
  file = {/home/trung/GoogleDrive/Zotero/independent component analysis.pdf},
  howpublished = {https://www.ee.columbia.edu/\textasciitilde dpwe/papers/HyvO99-icatut.pdf}
}

@misc{00_InterviewDouglasHofstadter,
  title = {An {{Interview}} with {{Douglas R}}. {{Hofstadter}}, Following ''{{I}} Am a {{Strange Loop}}''},
  file = {/home/trung/Zotero/storage/4EYWRPJE/hofstadter_interview.html},
  howpublished = {http://tal.forum2.org/hofstadter\_interview}
}

@misc{00_IntroductionAppliedLinear,
  title = {Introduction to {{Applied Linear Algebra}} \textendash{} {{Vectors}}, {{Matrices}}, and {{Least Squares}}},
  file = {/home/trung/GoogleDrive/Zotero/introduction to applied linear algebra – vectors, matrices, and least squares.pdf;/home/trung/Zotero/storage/5CM37TJ6/vmls.html},
  howpublished = {https://web.stanford.edu/\textasciitilde boyd/vmls/?fbclid=IwAR2DJtu8h58Xd8KSBrWwe240pqHtENej3TaHVGhAbqOfdk3us-eRwweMwcA}
}

@misc{00_IntroductionAutomatedMachine,
  title = {Introduction to {{Automated Machine Learning Frameworks}} // {{SimonWenkel}}.Com},
  file = {/home/trung/Zotero/storage/XGX7LQCE/introduction-to-automated-machine-learning-frameworks.html},
  howpublished = {https://www.simonwenkel.com/projects/introduction-to-automated-machine-learning-frameworks.html}
}

@misc{00_Introductioninterpretabledeep,
  title = {Introduction to Interpretable Deep Learning},
  file = {/home/trung/GoogleDrive/Zotero/introduction to interpretable deep learning.pdf}
}

@book{00_IntroductionMachineLearning,
  title = {Introduction to {{Machine Learning}}},
  file = {/home/trung/GoogleDrive/Zotero/introduction to machine learning.pdf}
}

@misc{00_JohnConwayLife,
  title = {John {{Conway}}'s {{Life}} in {{Games}}},
  abstract = {The mathematician John Horton Conway's myriad accomplishments \textemdash{} including the Game of Life, sprouts and the surreal numbers \textemdash{} are the product of a mind at play.},
  file = {/home/trung/Zotero/storage/FEIGU8MT/john-conways-life-in-games-20150828.html},
  howpublished = {https://www.quantamagazine.org/john-conways-life-in-games-20150828/},
  journal = {Quanta Magazine},
  language = {en}
}

@misc{00_KnowledgeTransferLearning,
  title = {Knowledge {{Transfer}} for {{Learning Subject}}-{{Specific Causal Probabilistic Graphical Models}}},
  file = {/home/trung/GoogleDrive/Zotero/knowledge transfer for learning subject-specific causal probabilistic graphical models.pdf},
  howpublished = {http://ccc.inaoep.mx/archivos/CCC-19-004.pdf}
}

@misc{00_L1IntroductionCS294158SP20,
  title = {L1 {{Introduction}} -- {{CS294}}-158-{{SP20 Deep Unsupervised Learning}} -- {{UC Berkeley}}, {{Spring}} 2020},
  abstract = {Instructors: Pieter Abbeel \&amp; Aravind Srinivas Course Instructors: Pieter Abbeel, Peter Chen, Jonathan Ho, Aravind Srinivas, Alexander Li, Wilson Yan Course Website: https://sites.google.com/view/berkele...}
}

@misc{00_LanguageModelsare,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  file = {/home/trung/GoogleDrive/Zotero/language models are unsupervised multitask learners.pdf},
  howpublished = {https://d4mucfpksywv.cloudfront.net/better-language-models/language\_models\_are\_unsupervised\_multitask\_learners.pdf}
}

@misc{00_LearningDiscreteLatent,
  title = {Learning {{Discrete Latent Structure}}},
  file = {/home/trung/Zotero/storage/J6BGZA62/learn-discrete.html},
  howpublished = {https://duvenaud.github.io/learn-discrete/}
}

@misc{00_Learningexplanatoryrules,
  title = {{Learning explanatory rules from noisy data}},
  abstract = {Suppose you are playing football. The ball arrives at your feet, and you decide to pass it to the unmarked striker. What seems like one simple action requires two different kinds of thought.~First, you recognise that there is a football at your feet. This recognition requires intuitive perceptual thinking -~you cannot easily articulate how you come to know that there is a ball at your feet, you just see that it is there. Second, you decide to pass the ball to a particular striker. This decision requires conceptual thinking. Your decision is tied to a justification - the reason you passed the ball to the striker is because she was unmarked.The distinction is interesting to us because these two types of thinking correspond to two different approaches to machine learning: deep learning and symbolic program synthesis. Deep learning concentrates on intuitive perceptual thinking whereas symbolic program synthesis focuses on conceptual, rule-based thinking. Each system has different merits - deep learning systems are robust to noisy data but are difficult to interpret and require large amounts of data to train, whereas symbolic systems are much easier to interpret and require less training data but struggle with noisy data.},
  file = {/home/trung/Zotero/storage/FUP3UX67/learning-explanatory-rules-noisy-data.html},
  howpublished = {/blog/article/learning-explanatory-rules-noisy-data},
  journal = {Deepmind},
  keywords = {symbolic},
  language = {ALL}
}

@misc{00_LearningFactorialCodes,
  title = {Learning {{Factorial Codes}} by {{Predictability Minimization}}},
  file = {/home/trung/GoogleDrive/Zotero/learning factorial codes by predictability minimization.pdf}
}

@misc{00_LearningHierarchicalPriors,
  title = {Learning {{Hierarchical Priors}} in {{VAEs}} - Argmax.Ai},
  abstract = {We address the issue of learning informative latent representations of data. In the normal VAE, the latent space prior is a standard normal distribution. This over-regularises the posterior distribution, resulting in latent representations that do not represent well the structure of the data. This post, describing our 2019 NeurIPS publication, proposes and demonstrates a solution by using an hierarchical latent space prior.},
  howpublished = {/blog/vhp-vae/},
  language = {en}
}

@misc{00_Learninglearnusing,
  title = {Learning to Learn Using Gradient Descent},
  file = {/home/trung/GoogleDrive/Zotero/learning to learn using gradient descent.pdf}
}

@misc{00_LearningPriorsAdversarial,
  title = {Learning {{Priors}} for {{Adversarial Autoencoders}}},
  file = {/home/trung/Zotero/storage/CSHE8RPK/learning-priors-for-adversarial-autoencoders.html},
  howpublished = {https://www.groundai.com/project/learning-priors-for-adversarial-autoencoders/}
}

@misc{00_LearningSearch,
  title = {Learning to {{Search}}},
  file = {/home/trung/Zotero/storage/UTTHXD4R/learning-to-search.html},
  howpublished = {https://duvenaud.github.io/learning-to-search/}
}

@misc{00_LecturenotesMeasuretheoretic,
  title = {Lecture Notes on {{Measure}}-Theoretic {{Probability Theory}}},
  howpublished = {http://www.math.wisc.edu/\textasciitilde roch/grad-prob/index.html}
}

@misc{00_LessonsLearnedDeveloping,
  title = {Lessons {{Learned}} from {{Developing ML}} for {{Healthcare}}},
  abstract = {Posted by Yun Liu, Research Scientist and Po-Hsuan Cameron Chen, Research Engineer, Google Health     Machine learning (ML) methods are not ...},
  file = {/home/trung/Zotero/storage/Z3QW3S8I/lessons-learned-from-developing-ml-for.html},
  journal = {Google AI Blog},
  language = {en}
}

@misc{00_LiyuanLucasLiuRAdamVariance,
  title = {{{LiyuanLucasLiu}}/{{RAdam}}: {{On}} the {{Variance}} of the {{Adaptive Learning Rate}} and {{Beyond}}},
  file = {/home/trung/Zotero/storage/8IZKCMZA/RAdam.html},
  howpublished = {https://github.com/LiyuanLucasLiu/RAdam}
}

@misc{00_LogicalvsAnalogical,
  title = {Logical vs.{{Analogical}} or {{Symbolic}} vs. {{Connectionist}}},
  file = {/home/trung/Zotero/storage/6TN72REG/SymbolicVs.Connectionist.html},
  howpublished = {http://web.media.mit.edu/\textasciitilde minsky/papers/SymbolicVs.Connectionist.html},
  keywords = {symbolic}
}

@misc{00_MachineLearningHealthcare,
  title = {Machine {{Learning}} for {{Healthcare}}},
  abstract = {This course introduces students to machine learning in healthcare, including the nature of clinical data and the use of machine learning for risk stratification, disease progression modeling, precision medicine, diagnosis, subtype discovery, and improving clinical workflows.},
  howpublished = {https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-s897-machine-learning-for-healthcare-spring-2019/},
  journal = {MIT OpenCourseWare},
  language = {en}
}

@misc{00_MachineLearningHowdoes,
  title = {R/{{MachineLearning}} - [{{D}}] {{How}} Does Posterior Collapse in {{VAEs}} Happen?},
  abstract = {18 votes and 17 comments so far on Reddit},
  file = {/home/trung/Zotero/storage/VDCH55ZB/d_how_does_posterior_collapse_in_vaes_happen.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/comments/7fkdtm/d\_how\_does\_posterior\_collapse\_in\_vaes\_happen/},
  journal = {reddit},
  keywords = {vae_issues},
  language = {en-US}
}

@misc{00_MachineLearningInterviews,
  title = {Machine {{Learning Interviews}}\_ {{Lessons}} from {{Both Sides}} - {{FSDL}}.Pdf},
  file = {/home/trung/GoogleDrive/Zotero/machine learning interviews_ lessons from both sides - fsdl.pdf}
}

@misc{00_MAPPINGLANDSCAPEARTIFICIAL,
  title = {{{MAPPING THE LANDSCAPE OF ARTIFICIAL INTELLIGENCE APPLICATIONS AGAINST COVID}}-19},
  file = {/home/trung/GoogleDrive/Zotero/mapping the landscape of artificial intelligence applications against covid-19.pdf}
}

@misc{00_MathSisyphus,
  title = {The {{Math}} of {{Sisyphus}}},
  abstract = {"There is but one truly serious question in philosophy, and that is suicide," wrote Albert Camus in The Myth of Sisyphus. This is equally true for a human navigating an absurd existence, and an artificial intelligence navigating a morally insoluble situation.},
  file = {/home/trung/Zotero/storage/JCZPULRH/the-math-of-sisyphus.html},
  journal = {TechCrunch},
  language = {en-US}
}

@misc{00_matrixcalculusyou,
  title = {The Matrix Calculus You Need for Deep Learning},
  abstract = {Most of us last saw calculus in school, but derivatives are a critical part of machine learning, particularly deep neural networks, which are trained by optimizing a loss function. This article is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed.},
  file = {/home/trung/Zotero/storage/QJRCI62N/index.html},
  howpublished = {https://explained.ai/matrix-calculus/index.html}
}

@misc{00_MetalogicalAnalysisVagueness,
  title = {A {{Metalogical Analysis}} of {{Vagueness}}: {{An}} Exploratory Study into the Geometry of Logic},
  file = {/home/trung/GoogleDrive/Zotero/a metalogical analysis of vagueness.pdf}
}

@misc{00_MixMatchHolisticApproach,
  title = {{{MixMatch}}: {{A Holistic Approach}} to {{Semi}}-{{Supervised Learning}}},
  file = {/home/trung/GoogleDrive/Zotero/mixmatch.pdf}
}

@misc{00_MotorcycleConspicuityEffect,
  title = {Motorcycle {{Conspicuity}} and {{The Effect}} of {{Auxiliary Forward Lighting}}},
  file = {/home/trung/GoogleDrive/Zotero/motorcycle conspicuity and the effect of auxiliary forward lighting.pdf},
  howpublished = {https://www.nhtsa.gov/sites/nhtsa.dot.gov/files/811507.pdf}
}

@misc{00_MSGGANMultiScaleGradients,
  title = {{{MSG}}-{{GAN}}: {{Multi}}-{{Scale Gradients}} for {{Generative Adversarial Networks}}},
  file = {/home/trung/GoogleDrive/Zotero/msg-gan.pdf},
  howpublished = {https://arxiv.org/pdf/1903.06048.pdf}
}

@misc{00_NeuralnetworksFeedforward,
  title = {Neural Networks [1.1] : {{Feedforward}} Neural Network - Artificial Neuron},
  shorttitle = {Neural Networks [1.1]}
}

@misc{00_NeurIPS2019Disentanglement,
  title = {{{NeurIPS}} 2019 : {{Disentanglement Challenge}}},
  abstract = {Crowdsourcing AI to solve real-world problems},
  file = {/home/trung/Zotero/storage/67Z9DRN9/neurips-2019-disentanglement-challenge.html},
  howpublished = {https://www.aicrowd.com/challenges/neurips-2019-disentanglement-challenge},
  journal = {AIcrowd | Challenges},
  language = {en}
}

@misc{00_Neutralizingantibodyresponses,
  title = {Neutralizing Antibody Responses to {{SARS}}-{{CoV}}-2 in a {{COVID}}-19 Recovered Patient Cohort and Their Implications | {{medRxiv}}},
  file = {/home/trung/Zotero/storage/K8DB653S/2020.03.30.html},
  howpublished = {https://www.medrxiv.org/content/10.1101/2020.03.30.20047365v2?utm\_source=quora\&utm\_medium=referral}
}

@misc{00_NLPCourseYou,
  title = {{{NLP Course}} | {{For You}}},
  howpublished = {https://lena-voita.github.io/nlp\_course.html}
}

@misc{00_oxmlcsMLbazaar,
  title = {Oxmlcs/{{ML}}\_bazaar},
  abstract = {OxCSML research group reading groups and meetings at the Department of Statistics, University of Oxford. - oxmlcs/ML\_bazaar},
  file = {/home/trung/Zotero/storage/EIHQM8FQ/wiki.html},
  howpublished = {https://github.com/oxmlcs/ML\_bazaar},
  journal = {GitHub},
  language = {en}
}

@misc{00_PaddlePaddleERNIEImplementation,
  title = {{{PaddlePaddle}}/{{ERNIE}}: {{An Implementation}} of {{ERNIE For Language Understanding}} (Including {{Pre}}-Training Models and {{Fine}}-Tuning Tools)},
  file = {/home/trung/Zotero/storage/V4HE9YQM/ERNIE.html},
  howpublished = {https://github.com/PaddlePaddle/ERNIE}
}

@misc{00_PaperDeepLearning,
  title = {Paper by "{{Deep Learning Conspiracy}}" in {{Nature}}},
  file = {/home/trung/Zotero/storage/ALFDDNHR/deep-learning-conspiracy.html},
  howpublished = {http://people.idsia.ch/\textasciitilde juergen/deep-learning-conspiracy.html}
}

@misc{00_PapersCodeNeural,
  title = {Papers {{With Code}} : {{Neural Network Compression}}},
  shorttitle = {Papers {{With Code}}},
  abstract = {See leaderboards and papers with code for Neural Network Compression},
  file = {/home/trung/Zotero/storage/ZW8WTMK4/neural-network-compression.html},
  howpublished = {http://paperswithcode.com/task/neural-network-compression},
  language = {en}
}

@book{00_PatternRecognitionMachine,
  title = {Pattern {{Recognition}} and {{Machine Learning Solution}}},
  file = {/home/trung/GoogleDrive/Zotero/pattern recognition and machine learning solution.pdf}
}

@misc{00_PerceptualLossesRealTime,
  title = {Perceptual {{Losses}} for {{Real}}-{{Time Style Transfer}} and {{Super}}-{{Resolution}}},
  howpublished = {https://cs.stanford.edu/people/jcjohns/eccv16/}
}

@misc{00_PhosphatidylserinevsAlpha,
  title = {Phosphatidylserine vs. {{Alpha GPC}}},
  abstract = {Is phosphatidylserine REALLY better than alpha gpc? Read this important Phosphatidylserine vs. alpha gpc comparison to find hidden benefits, side...},
  howpublished = {https://trackmystack.com/phosphatidylserine-vs-alpha-gpc},
  journal = {TrackMyStack | Manage Your Health},
  language = {en}
}

@misc{00_PixelGANAutoencoders,
  title = {{{PixelGAN Autoencoders}}},
  file = {/home/trung/GoogleDrive/Zotero/pixelgan autoencoders.pdf},
  howpublished = {https://papers.nips.cc/paper/6793-pixelgan-autoencoders.pdf}
}

@misc{00_PixelVAEImprovedPixelVAE,
  title = {{{PixelVAE}}++: {{Improved PixelVAE}} with {{Discrete Prior}} - {{Google Search}}},
  file = {/home/trung/Zotero/storage/LM2ZIJNC/search.html},
  howpublished = {https://www.google.com/search?q=PixelVAE\%2B\%2B\%3A+Improved+PixelVAE+with+Discrete+Prior\&oq=PixelVAE\%2B\%2B\%3A+Improved+PixelVAE+with+Discrete+Prior\&aqs=chrome..69i57j69i58j69i60j69i61.879j0j4\&sourceid=chrome\&ie=UTF-8}
}

@misc{00_PotentialCOVID20193Clike,
  title = {Potential\_{{COVID}}-2019\_{{3C}}-Like\_{{Protease}}\_{{Inhibitors}}\_{{Designed}}\_{{Using}}\_{{Generative}}\_{{Deep}}\_{{Learning}}\_{{Approaches}}\_v2.Pdf},
  file = {/home/trung/GoogleDrive/Zotero/potential_covid-2019_3c-like_protease_inhibitors_designed_using_generative_deep_learning_approaches_v2.pdf}
}

@misc{00_PracticalDataEthics,
  title = {Practical {{Data Ethics}} | {{Data}} Ethics},
  howpublished = {http://ethics.fast.ai/?fbclid=IwAR3DhYUQugvvjq4Y\_-wc6Ux\_41IqXSSDf4k10\_Or2FUaTYTn9JSVvAYAZbM}
}

@misc{00_PracticalDeepLearning,
  title = {Practical {{Deep Learning}} for {{Coders}}, v3 | Fast.Ai Course V3},
  file = {/home/trung/Zotero/storage/8TRWRE9U/index.html},
  howpublished = {https://course.fast.ai/index.html}
}

@misc{00_Predictingcommerciallyavailable,
  title = {Predicting Commercially Available Antiviral Drugs That May Act on the Novel Coronavirus},
  file = {/home/trung/GoogleDrive/Zotero/predicting commercially available antiviral drugs that may act on the novel coronavirus.pdf}
}

@misc{00_PythonComputationalScience,
  title = {Python for {{Computational Science}} and {{Engineering}}},
  file = {/home/trung/GoogleDrive/Zotero/python for computational science and engineering.pdf}
}

@misc{00_pytorchfairseq,
  title = {Pytorch/Fairseq},
  abstract = {Facebook AI Research Sequence-to-Sequence Toolkit written in Python. - pytorch/fairseq},
  howpublished = {https://github.com/pytorch/fairseq},
  journal = {GitHub},
  language = {en}
}

@misc{00_ReasoningShapesProbability,
  title = {Reasoning about {{Shapes}} and {{Probability Distributions}} - {{Eric J}}. {{Ma}}'s {{Personal Site}}},
  howpublished = {https://ericmjl.github.io/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/}
}

@misc{00_ReformerEfficientTransformer,
  title = {Reformer: {{The Efficient Transformer}}},
  file = {/home/trung/GoogleDrive/Zotero/reformer.pdf},
  howpublished = {https://openreview.net/pdf?id=rkgNKkHtvB}
}

@misc{00_RepresentationLearningProbabilistic,
  title = {Representation {{Learning}}: {{A Probabilistic Perspective Spring}} 2020},
  file = {/home/trung/Zotero/storage/QXQFB548/index.html},
  howpublished = {http://www.cs.columbia.edu/\textasciitilde blei/seminar/2020-representation/index.html}
}

@misc{00_RevengeNeurons,
  title = {The {{Revenge}} of {{Neurons}}},
  abstract = {The invention of inductive machines and the controverse of Artificial Intelligence},
  file = {/home/trung/Zotero/storage/VBSS95LJ/neurovenge.antonomase.fr.html},
  howpublished = {https://neurovenge.antonomase.fr/},
  keywords = {knowledge,symbolic}
}

@misc{00_RulesMachineLearning,
  title = {Rules of {{Machine Learning}}: | {{ML Universal Guides}}},
  shorttitle = {Rules of {{Machine Learning}}},
  howpublished = {https://developers.google.com/machine-learning/guides/rules-of-ml},
  journal = {Google Developers},
  language = {en}
}

@misc{00_Salviaofficinalisoverview,
  title = {Salvia Officinalis - an Overview | {{ScienceDirect Topics}}},
  howpublished = {https://www.sciencedirect.com/topics/agricultural-and-biological-sciences/salvia-officinalis}
}

@misc{00_SARSCoV2expectedmutate,
  title = {{{SARS}}-{{CoV}}-2 Expected to Mutate Every Year the Way Other Coronaviruses like the Regular Flu Tend To?},
  file = {/home/trung/Zotero/storage/LGATMHBG/Huang-ZheYu.html},
  howpublished = {https://www.quora.com/Is-SARS-CoV-2-expected-to-mutate-every-year-the-way-other-coronaviruses-like-the-regular-flu-tend-to/answer/Huang-ZheYu?ch=99\&share=4bdf71b3\&srid=nrCK}
}

@misc{00_SCATACSEQEXPLORERPIPELINE,
  title = {{{SC}}-{{ATACSEQ}}-{{EXPLORER PIPELINE}}},
  file = {/home/trung/GoogleDrive/Zotero/sc-atacseq-explorer pipeline.pdf}
}

@misc{00_ScatteringRepresentationsRecognition,
  title = {Scattering {{Representations}} for {{Recognition}}},
  file = {/home/trung/GoogleDrive/Zotero/scattering representations for recognition.pdf}
}

@misc{00_SciHiveS4LSelfSupervised,
  title = {{{SciHive}} - {{S4L}}: {{Self}}-{{Supervised Semi}}-{{Supervised Learning}}},
  file = {/home/trung/Zotero/storage/ACWGFLVL/1905.html},
  howpublished = {https://www.scihive.org/paper/1905.03670}
}

@misc{00_Singlecelltranscriptomicsreveals,
  title = {Single-Cell Transcriptomics Reveals Expansion of Cytotoxic {{CD4 T}}-Cells in Supercentenarians | {{bioRxiv}}},
  file = {/home/trung/Zotero/storage/AVYIQ3PQ/643528v1.html},
  howpublished = {https://www.biorxiv.org/content/10.1101/643528v1}
}

@misc{00_SphericalConvolutionalNeural,
  title = {Spherical {{Convolutional Neural Networks}}},
  file = {/home/trung/GoogleDrive/Zotero/spherical convolutional neural networks.pdf}
}

@misc{00_StackedCapsuleAutoencoders,
  title = {Stacked {{Capsule Autoencoders}}},
  file = {/home/trung/Zotero/storage/LNFPECXZ/stacked_capsule_autoencoders.html},
  howpublished = {http://akosiorek.github.io/ml/2019/06/23/stacked\_capsule\_autoencoders.html}
}

@misc{00_StanfordCS224N,
  title = {Stanford {{CS 224N}} | {{Natural Language Processing}} with {{Deep Learning}}},
  file = {/home/trung/Zotero/storage/V6U8WEF4/cs224n.html},
  howpublished = {http://web.stanford.edu/class/cs224n/}
}

@book{00_STAT394Causal,
  title = {{{STAT}} 394: {{Causal Inference}}},
  shorttitle = {References | {{STAT}} 394},
  abstract = {This is the class website for Causal Inference at Macalester College.}
}

@misc{00_Studylinkschanges,
  title = {Study Links Changes in Collagen to Worse Pancreatic Cancer Prognosis},
  abstract = {The first evidence linking a disturbance of the most common protein in the body with a poor outcome in pancreatic cancer has been uncovered by a team of researchers.},
  howpublished = {https://www.sciencedaily.com/releases/2016/10/161021120640.htm},
  journal = {ScienceDaily},
  language = {en}
}

@misc{00_SulfurAminoAcids,
  title = {Sulfur {{Amino Acids}} - an Overview | {{ScienceDirect Topics}}},
  file = {/home/trung/Zotero/storage/RDMRZ6VH/sulfur-amino-acids.html},
  howpublished = {https://www.sciencedirect.com/topics/neuroscience/sulfur-amino-acids}
}

@misc{00_SummerSchoolDeep,
  title = {Summer {{School Deep}}|{{Bayes}}. {{Russia}}, {{Moscow}}, {{August}} 2019},
  abstract = {At the Deep|Bayes summer school, we will discuss how Bayesian Methods can be combined with Deep Learning and lead to better results in machine learning applications. Recent research has proven that the use of Bayesian approach can be beneficial in various ways. School participants will learn methods and techniques that are crucial for understanding current research in machine learning. They will also have hands-on experience with using probabilistic modeling to build neural generative and discriminative models, learn modern stochastic optimization methods and regularization techniques for neural networks, and master the ways to reason about the uncertainty about the weight of the neural networks and their predictions. Lectures will be followed by practical sessions.},
  file = {/home/trung/Zotero/storage/TVP5ESY9/deepbayes.ru.html},
  howpublished = {https://deepbayes.ru/}
}

@misc{00_SuperGLUEBenchmark,
  title = {{{SuperGLUE Benchmark}}},
  abstract = {SuperGLUE is a new benchmark styled after original GLUE benchmark with a set of more difficult language understanding tasks, improved resources, and a new public leaderboard.},
  file = {/home/trung/Zotero/storage/GR8XEPYF/leaderboard.html},
  howpublished = {https://super.gluebenchmark.com/},
  journal = {SuperGLUE Benchmark},
  language = {en}
}

@misc{00_SymbolbasedMachineLearning,
  title = {Symbol-Based {{Machine Learning}}},
  file = {/home/trung/GoogleDrive/Zotero/symbol-based machine learning.pdf},
  howpublished = {https://cs.appstate.edu/\textasciitilde blk/cs4440/chap13/concept\_learning\_brief.pdf}
}

@article{00_TestBackuptools,
  title = {Test of {{Back}}-up Tools for Rope Access - on a Loaded Rope},
  pages = {9},
  file = {/home/trung/GoogleDrive/Zotero/test of back-up tools for rope access - on a loaded rope.pdf},
  language = {en}
}

@misc{00_ThinkBayes,
  title = {Think {{Bayes}}},
  file = {/home/trung/GoogleDrive/Zotero/think bayes.pdf}
}

@misc{00_TutorialInformationMaximizing,
  title = {A {{Tutorial}} on {{Information Maximizing Variational Autoencoders}} ({{InfoVAE}})},
  file = {/home/trung/Zotero/storage/FKRBTPCE/a-tutorial-on-mmd-variational-autoencoders.html},
  howpublished = {https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/}
}

@misc{00_TutorialVariationalBayes,
  title = {Tutorial "{{Variational Bayes}} and beyond: {{Bayesian}} Inference for Big Data"},
  file = {/home/trung/Zotero/storage/HU7SIKAP/tutorial_2018_icml.html},
  howpublished = {http://www.tamarabroderick.com/tutorial\_2018\_icml.html}
}

@article{00_UnderstandingSARSCoV2drugs,
  title = {Understanding {{SARS}}-{{CoV}}-2 and the Drugs That Might Lessen Its Power},
  issn = {0013-0613},
  abstract = {Modest improvements in treatment could make a big difference},
  file = {/home/trung/Zotero/storage/FEM76D6K/understanding-sars-cov-2-and-the-drugs-that-might-lessen-its-power.html},
  journal = {The Economist}
}

@misc{00_Unsupervisedlearningcurious,
  title = {{Unsupervised learning: the curious pupil}},
  shorttitle = {{Unsupervised learning}},
  abstract = {One in a series of posts explaining the theories underpinning our researchOver the last decade, machine learning has made unprecedented progress in areas as diverse as image recognition, self-driving cars and playing complex games like Go. These successes have been largely realised by training deep neural networks with one of two learning paradigms\textemdash supervised learning and reinforcement learning. Both paradigms require training signals to be designed by a human and passed to the computer. In the case of supervised learning, these are the ``targets'' (such as the correct label for an image); in the case of reinforcement learning, they are the ``rewards'' for successful behaviour (such as getting a high score in an Atari game). The limits of learning are therefore defined by the human trainers. While some scientists contend that a sufficiently inclusive training regime\textemdash for example, the ability to complete a very wide variety of tasks\textemdash should be enough to give rise to general intelligence, others believe that true intelligence will require more independent learning strategies. Consider how a toddler learns, for instance. Her grandmother might sit with her and patiently point out examples of ducks (acting as the instructive signal in supervised learning), or reward her with applause for solving a woodblock puzzle (as in reinforcement learning).},
  file = {/home/trung/Zotero/storage/PL23B72V/unsupervised-learning.html},
  howpublished = {/blog/article/unsupervised-learning},
  journal = {Deepmind},
  language = {ALL}
}

@misc{00_UsingBernoulliVAE,
  title = {Using a {{Bernoulli VAE}} on {{Real}}-{{Valued Observations}} - {{Rui Shu}}},
  abstract = {The Bernoulli observation VAE is supposed is used when one's observed samples \$x \textbackslash in \textbackslash sset\{0, 1\}\^n\$ are vectors of bi...},
  howpublished = {http://ruishu.io/2018/03/19/bernoulli-vae/}
}

@misc{00_UVAQdataGroup,
  title = {{{UVA Qdata Group}}'s {{Deep Learning Reading Group}}},
  file = {/home/trung/Zotero/storage/R8535B89/deep2Read.html},
  howpublished = {https://qdata.github.io/deep2Read/?fbclid=IwAR1oQtgcY7m5MbhLoVSIQP7zO19i0N6xVbdTAhCbply2ggHtG9yhX5tTUeU}
}

@misc{00_VariationalBayesModel,
  title = {Variational {{Bayes}} under {{Model Misspecification}}},
  file = {/home/trung/Zotero/storage/2ZWTSSCR/1905.html},
  howpublished = {https://arxiv.org/abs/1905.10859},
  keywords = {variational}
}

@misc{00_WeightsBiasesTutorials,
  title = {Weights \& {{Biases Tutorials}}},
  abstract = {Keep up with exciting updates from Lukas Biewald and the team at Weights \& Biases. We're sharing peeks into different deep learning applications, tutorials for specific techniques, and hands-on code walkthroughs.},
  file = {/home/trung/Zotero/storage/AYM67I9J/tutorials.html},
  howpublished = {https://www.wandb.com/tutorials}
}

@misc{00_Whataresafest,
  title = {What Are the Safest Sources of Energy?},
  abstract = {Based on safety and carbon emissions, fossil fuels are the dirtiest and most dangerous, while nuclear and modern renewable energy sources are vastly safer and cleaner.},
  file = {/home/trung/Zotero/storage/HWFTLPZM/safest-sources-of-energy.html},
  howpublished = {https://ourworldindata.org/safest-sources-of-energy},
  journal = {Our World in Data}
}

@misc{00_Whatnewlydeveloped,
  title = {What Newly Developed Machine Learning Models Could Surpass Deep Learning?},
  howpublished = {https://www.quora.com/What-newly-developed-machine-learning-models-could-surpass-deep-learning/answer/Sridhar-Mahadevan-6?ch=99\&share=d7ad5088\&srid=nrCK}
}

@misc{00_WhatwrongVAEs,
  title = {What Is Wrong with {{VAEs}}?},
  file = {/home/trung/Zotero/storage/95M94P8F/what_is_wrong_with_vaes.html},
  howpublished = {http://akosiorek.github.io/ml/2018/03/14/what\_is\_wrong\_with\_vaes.html},
  keywords = {vae_issues}
}

@article{10.1561/2200000006,
  title = {Learning Deep Architectures for {{AI}}},
  author = {Bengio, Yoshua},
  year = {2009},
  month = jan,
  volume = {2},
  pages = {1--127},
  publisher = {{Now Publishers Inc.}},
  address = {{Hanover, MA, USA}},
  issn = {1935-8237},
  doi = {10.1561/2200000006},
  abstract = {Theoretical results suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g., in vision, language, and other AI-level tasks), one may need deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This monograph discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks.},
  file = {/home/trung/GoogleDrive/Zotero/bengio_2009_learning deep architectures for ai.pdf},
  issue_date = {January 2009},
  journal = {Found. Trends Mach. Learn.},
  number = {1}
}

@book{10.5555/2621980,
  title = {Understanding Machine Learning: {{From}} Theory to Algorithms},
  author = {{Shalev-Shwartz}, Shai and {Ben-David}, Shai},
  year = {2014},
  publisher = {{Cambridge University Press}},
  address = {{USA}},
  abstract = {Machine learning is one of the fastest growing areas of computer science, with far-reaching applications. The aim of this textbook is to introduce machine learning, and the algorithmic paradigms it offers, in a principled way. The book provides an extensive theoretical account of the fundamental ideas underlying machine learning and the mathematical derivations that transform these principles into practical algorithms. Following a presentation of the basics of the field, the book covers a wide array of central topics that have not been addressed by previous textbooks. These include a discussion of the computational complexity of learning and the concepts of convexity and stability; important algorithmic paradigms including stochastic gradient descent, neural networks, and structured output learning; and emerging theoretical concepts such as the PAC-Bayes approach and compression-based bounds. Designed for an advanced undergraduate or beginning graduate course, the text makes the fundamentals and algorithms of machine learning accessible to students and non-expert readers in statistics, computer science, mathematics, and engineering.},
  file = {/home/trung/GoogleDrive/Zotero/shalev-shwartz et al_2014_understanding machine learning.pdf},
  isbn = {1-107-05713-2}
}

@inproceedings{10.5555/3327144.3327244,
  title = {Realistic Evaluation of Deep Semi-Supervised Learning Algorithms},
  booktitle = {Proceedings of the 32nd International Conference on Neural Information Processing Systems},
  author = {Oliver, Avital and Odena, Augustus and Raffel, Colin and Cubuk, Ekin D. and Goodfellow, Ian J.},
  year = {2018},
  pages = {3239--3250},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, we argue that these benchmarks fail to address many issues that SSL algorithms would face in real-world applications. After creating a unified reimplementation of various widely-used SSL techniques, we test them in a suite of experiments designed to address these issues. We find that the performance of simple baselines which do not use unlabeled data is often underreported, SSL methods differ in sensitivity to the amount of labeled and unlabeled data, and performance can degrade substantially when the unlabeled dataset contains out-of-distribution examples. To help guide SSL research towards real-world applicability, we make our unified reimplemention and evaluation platform publicly available.},
  file = {/home/trung/GoogleDrive/Zotero/oliver et al_2018_realistic evaluation of deep semi-supervised learning algorithms.pdf},
  series = {{{NIPS}}'18}
}

@misc{17_MixedPrecisionTrainingDeep,
  title = {Mixed-{{Precision Training}} of {{Deep Neural Networks}}},
  year = {2017},
  month = oct,
  abstract = {Mixed-precision training techniques gain performance from FP16 computation (FP16) while maintaining the accuracy of FP32.},
  file = {/home/trung/Zotero/storage/2YG3ACNQ/mixed-precision-training-deep-neural-networks.html},
  howpublished = {https://devblogs.nvidia.com/mixed-precision-training-deep-neural-networks/},
  journal = {NVIDIA Developer Blog},
  language = {en-US}
}

@misc{18_AttentionAttention,
  title = {Attention? {{Attention}}!},
  shorttitle = {Attention?},
  year = {2018},
  month = jun,
  abstract = {Attention has been a fairly popular concept and a useful tool in the deep learning community in recent years. In this post, we are gonna look into how attention was invented, and various attention mechanisms and models, such as transformer and SNAIL.},
  file = {/home/trung/Zotero/storage/Z5NCL8KH/attention-attention.html},
  howpublished = {https://lilianweng.github.io/2018/06/24/attention-attention.html},
  journal = {Lil'Log},
  keywords = {attention},
  language = {en}
}

@misc{18_BlessingsMultipleCauses,
  title = {The {{Blessings}} of {{Multiple Causes}}: {{Causal Inference}} When You {{Can}}'t {{Measure Confounders}}},
  shorttitle = {The {{Blessings}} of {{Multiple Causes}}},
  year = {2018},
  month = sep,
  abstract = {Happy back-to-school time everyone!  After a long vacation (in blogging terms), I'm returning with a brief paper review which conveniently allows me to continue to explain a few ideas from causal reasoning. Since I wrote this intro to causality, I have read a lot more about it, especially how it...},
  howpublished = {https://www.inference.vc/blessings-of-multiple-causes-causal-inference-when-you-cant-measure-confounders/},
  journal = {inFERENCe},
  keywords = {causal},
  language = {en}
}

@misc{18_FlowbasedDeepGenerative,
  title = {Flow-Based {{Deep Generative Models}}},
  year = {2018},
  month = oct,
  abstract = {In this post, we are looking into the third type of generative models: flow-based generative models. Different from GAN and VAE, they explicitly learn the probability density function of the input data.},
  file = {/home/trung/Zotero/storage/JZ3WFM4V/flow-based-deep-generative-models.html},
  howpublished = {https://lilianweng.github.io/2018/10/13/flow-based-deep-generative-models.html},
  journal = {Lil'Log},
  language = {en}
}

@book{18_Integrateduncertaintyknowledge,
  title = {Integrated Uncertainty in Knowledge Modelling and Decision Making},
  year = {2018},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{New York, NY}},
  file = {/home/trung/GoogleDrive/Zotero/2018_integrated uncertainty in knowledge modelling and decision making.pdf},
  isbn = {978-3-319-75428-4},
  language = {en}
}

@misc{18_VariationalAttentionSequencetoSequence,
  title = {Variational {{Attention}} for {{Sequence}}-to-{{Sequence Models}}},
  year = {2018},
  file = {/home/trung/GoogleDrive/Zotero/2018_variational attention for sequence-to-sequence models.pdf;/home/trung/GoogleDrive/Zotero/2018_variational attention for sequence-to-sequence models2.pdf},
  howpublished = {https://drive.google.com/file/d/1PV0K3aai85PL8Fg5NBZ14e2cQ66vU2Ah/view?usp=drive\_open\&usp=embed\_facebook},
  journal = {Google Docs},
  keywords = {attention,sequence,variational}
}

@misc{19_AGuideforEthicalDataScienceFinalOct2019pdf,
  title = {A-{{Guide}}-for-{{Ethical}}-{{Data}}-{{Science}}-{{Final}}-{{Oct}}-2019.Pdf},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/2019_a-guide-for-ethical-data-science-final-oct-2019.pdf},
  howpublished = {https://www.rss.org.uk/Images/PDF/influencing-change/2019/A-Guide-for-Ethical-Data-Science-Final-Oct-2019.pdf}
}

@misc{19_AllWaysYou,
  title = {All {{The Ways You Can Compress BERT}}},
  year = {2019},
  month = nov,
  abstract = {Model compression reduces redundancy in a trained neural network. This is useful, since BERT barely fits on a GPU (BERT-Large does not) and definitely won't fit on your smart phone. Improved memory and inference speed efficiency can also save costs at scale.},
  file = {/home/trung/Zotero/storage/757HDK5J/all-the-ways-to-compress-BERT.html},
  howpublished = {http://mitchgordon.me/machine/learning/2019/11/18/all-the-ways-to-compress-BERT.html},
  journal = {Mitchell A. Gordon},
  language = {en}
}

@misc{19_bayesgroupdeepbayes2019,
  title = {Bayesgroup/Deepbayes-2019},
  year = {2019},
  month = nov,
  abstract = {Practical assignments of the Deep|Bayes summer school 2019},
  howpublished = {Bayesian Methods Research Group}
}

@misc{19_CollagenClimbersLatest,
  title = {Collagen for {{Climbers}} - {{Latest Research Supports Benefits}}!},
  year = {2019},
  month = jun,
  abstract = {Collagen for Climbers - Latest Research Supports Benefits!},
  file = {/home/trung/Zotero/storage/VNSBKNFM/collagen-for-climbers-latest-research-supports-benefits.html},
  howpublished = {https://trainingforclimbing.com/collagen-for-climbers-latest-research-supports-benefits/},
  journal = {Training For Climbing - by Eric H\"orst},
  language = {en-US}
}

@misc{19_DeepGaussianProcesses,
  title = {Deep {{Gaussian Processes}}},
  year = {2019},
  month = jan,
  abstract = {{$<$}p{$>$}Gaussian process models provide a flexible, non-parametric approach to modelling that sustains uncertainty about the function. However, computational demands and the joint Gaussian assumption ma...},
  file = {/home/trung/Zotero/storage/WF8MIR35/deep-gaussian-processes.html},
  howpublished = {http://inverseprobability.com/talks/notes/deep-gaussian-processes.html},
  journal = {PMLR},
  keywords = {gaussian process,variational}
}

@misc{19_DeepLearningDTU02456deeplearning,
  title = {{{DeepLearningDTU}}/02456-Deep-Learning},
  year = {2019},
  month = sep,
  abstract = {Exercises and supplementary material for the deep learning course 02456.},
  howpublished = {DeepLearningDTU}
}

@misc{19_DeepLearningDTU02456deeplearningwithPyTorch,
  title = {{{DeepLearningDTU}}/02456-Deep-Learning-with-{{PyTorch}}},
  year = {2019},
  month = oct,
  abstract = {Exercises and supplementary material for the deep learning course 02456 using PyTorch.},
  howpublished = {DeepLearningDTU}
}

@misc{19_DeepLearningDTUnvidiadeep,
  title = {{{DeepLearningDTU}}/Nvidia\_deep\_learning\_summercamp\_2016},
  year = {2019},
  month = may,
  abstract = {Lasagne / Theano tutorials for Nvidia Deep Learning Summercamp 2016},
  howpublished = {DeepLearningDTU}
}

@misc{19_DeepLearningDTUvariationalautoencoderssummerschool2016,
  title = {{{DeepLearningDTU}}/Variational-Autoencoders-Summerschool-2016},
  year = {2019},
  month = mar,
  abstract = {Exercises for the semi-supervised summer school https://semisupervised-learning.compute.dtu.dk.},
  howpublished = {DeepLearningDTU}
}

@misc{19_Deeplearninginterview,
  title = {Deep Learning Interview Topic},
  year = {2019},
  month = nov,
  abstract = {List of DL topics and resources essential for cracking interviews},
  howpublished = {Vision \& Language Group, IIT Roorkee},
  keywords = {computer-vision,deep-learning,generative-models,linear-algebra,natural-language-processing,probability-statistics}
}

@misc{19_DoubleEdgedSwordCollagen,
  title = {The {{Double}}-{{Edged Sword}} of {{Collagen}}},
  year = {2019},
  month = dec,
  abstract = {Reading time: 4 minutes Sara Musetti If you've heard of collagen, it's likely been in articles raving about its benefits. Maybe it was in the recent collagen broth craze, when everyone from Halle B\ldots},
  journal = {OncoBites},
  language = {en}
}

@misc{19_Efficientinferencedynamical,
  title = {Efficient Inference for Dynamical Models: {{Code}} and Data Now on {{GitHub}}},
  shorttitle = {Efficient Inference for Dynamical Models},
  year = {2019},
  month = sep,
  abstract = {Researchers at Microsoft Research Cambridge have released Python code and data for their work using machine learning to efficiently parameterize prescribed and neural dynamical systems models. Explore variational inference for hierarchical dynamical systems},
  file = {/home/trung/Zotero/storage/ETIAHMK5/efficient-inference-for-dynamical-models-using-variational-autoencoders.html},
  journal = {Microsoft Research},
  keywords = {variational},
  language = {en-US}
}

@misc{19_EmergentToolUse,
  title = {Emergent {{Tool Use}} from {{Multi}}-{{Agent Interaction}}},
  year = {2019},
  month = sep,
  abstract = {We've observed agents discovering progressively more complex tool use while playing a simple game of hide-and-seek.},
  file = {/home/trung/Zotero/storage/PTDVXRVU/emergent-tool-use.html},
  howpublished = {https://openai.com/blog/emergent-tool-use/},
  journal = {OpenAI},
  language = {en}
}

@misc{19_FutureAI,
  title = {The {{Future}} of {{AI}}},
  year = {2019},
  month = sep,
  abstract = {Waves of automation have driven human advance, and each wave requires humans to The promise of AI is to launch new systems of automated intellectual endeavour that will be the first systems to adap...},
  file = {/home/trung/Zotero/storage/B5EQG8KF/the-future-of-ai.html},
  howpublished = {http://inverseprobability.com/talks/health/the-future-of-ai.html},
  journal = {PMLR}
}

@misc{19_GaussianProcessesnot,
  title = {Gaussian {{Processes}}, Not Quite for Dummies},
  year = {2019},
  month = nov,
  abstract = {I recall always having this vague impression about Gaussian Processes (GPs) being a magical algorithm that is able to define probability distributions over sets of functions, but I had always procrastinated reading up on the details. It's not completely my fault though! Whenever I Google \&quot;Gaussian Processes\&quot;, I},
  file = {/home/trung/Zotero/storage/WYN7R8KT/gaussian-process-not-quite-for-dummies.html},
  howpublished = {https://thegradient.pub/gaussian-process-not-quite-for-dummies/},
  journal = {The Gradient},
  language = {en}
}

@misc{19_MultiTaskDeepNeural,
  title = {Multi-{{Task Deep Neural Networks}} for {{Natural Language Understanding}}},
  year = {2019},
  file = {/home/trung/Zotero/storage/XG9DGS8Z/1901.html},
  howpublished = {https://arxiv.org/abs/1901.11504}
}

@misc{19_MuseNet,
  title = {{{MuseNet}}},
  year = {2019},
  month = apr,
  abstract = {We've created Musenet, a deep neural network that can generate 4-minute musical compositions with 10 different instruments and can combine styles from country to Mozart to the Beatles.},
  file = {/home/trung/Zotero/storage/Z8J7UIJU/musenet.html},
  howpublished = {https://openai.com/blog/musenet/},
  journal = {OpenAI},
  language = {en}
}

@misc{19_NeurIPS2019Disentanglement,
  title = {{{NeurIPS}} 2019 {{Disentanglement Challenge}}},
  year = {2019},
  abstract = {Crowdsourcing AI to solve real-world problems},
  file = {/home/trung/Zotero/storage/Y8XQ3CLG/neurips-2019-disentanglement-challenge.html},
  howpublished = {https://www.aicrowd.com/challenges/neurips-2019-disentanglement-challenge},
  journal = {AIcrowd | Challenges},
  language = {en}
}

@misc{19_NLPCleverHans,
  title = {{{NLP}}'s {{Clever Hans Moment}} Has {{Arrived}}},
  year = {2019},
  month = aug,
  abstract = {A review of Timothy Niven and Hung-Yu Kao, 2019: Probing Neural Network Comprehension of Natural Language Arguments},
  file = {/home/trung/Zotero/storage/HJM7DZ3P/nlps-clever-hans-moment-has-arrived.html},
  howpublished = {https://thegradient.pub/nlps-clever-hans-moment-has-arrived/},
  journal = {The Gradient},
  language = {en}
}

@misc{19_OATMLbdlbenchmarks,
  title = {{{OATML}}/Bdl-Benchmarks},
  year = {2019},
  month = dec,
  abstract = {Bayesian Deep Learning Benchmarks. Contribute to OATML/bdl-benchmarks development by creating an account on GitHub.},
  copyright = {Apache-2.0},
  howpublished = {OATML}
}

@misc{19_pachterlabSP2019,
  title = {Pachterlab/{{SP}}\_2019},
  year = {2019},
  month = nov,
  abstract = {Contribute to pachterlab/SP\_2019 development by creating an account on GitHub.},
  howpublished = {Pachter Lab}
}

@misc{19_PGMLabAutumnschool,
  title = {{{PGM}}-{{Lab}}/{{Autumn}} School on {{Machine Learning}}},
  year = {2019},
  month = oct,
  abstract = {Repository for the course PPLs with DNNs at Autumn school on Machine Learning, Tbilisi, Georgia, October 3-11, 2019.},
  howpublished = {PGM-Lab}
}

@misc{19_PGMLabInferPy,
  title = {{{PGM}}-{{Lab}}/{{InferPy}}},
  year = {2019},
  month = nov,
  abstract = {InferPy: Deep Probabilistic Modeling with Tensorflow Made Easy},
  copyright = {Apache-2.0},
  howpublished = {PGM-Lab}
}

@misc{19_PGMLabProbabilisticModels,
  title = {{{PGM}}-{{Lab}}/{{Probabilistic Models}} with {{Deep Neural Networks}}},
  year = {2019},
  month = nov,
  abstract = {Contribute to PGM-Lab/ProbModelsDNNs development by creating an account on GitHub.},
  howpublished = {PGM-Lab}
}

@misc{19_PGMLabTutorialNordic,
  title = {{{PGM}}-{{Lab}}/{{Tutorial}} of the {{Nordic Probabilistic AI School}}},
  year = {2019},
  month = oct,
  abstract = {Repo for the Tutorials of Day1-Day3 of the Nordic Probabilistic AI  School (https://probabilistic.ai/)},
  howpublished = {PGM-Lab}
}

@misc{19_probabilisticaiNordicProbabilistic,
  title = {Probabilisticai/ {{Nordic Probabilistic AI School}} 2019.},
  year = {2019},
  month = sep,
  abstract = {Materials of the Nordic Probabilistic AI School 2019.},
  howpublished = {Probabilistic AI}
}

@misc{19_Proteinsupplementscontaining,
  title = {Protein Supplements Containing {{BCAA}} May Have `Detrimental Effects' on Health and Lifespan},
  year = {2019-05-01T10:38:00.0000000-04:00},
  abstract = {New research suggests that BCAAs in the form of pre-mixed protein powders, shakes and supplements may do more harm to health than good.},
  chapter = {Medical News},
  howpublished = {https://www.news-medical.net/news/20190501/Protein-supplement-BCAA-may-have-detrimental-effects-on-health-and-lifespan.aspx},
  journal = {News-Medical.net},
  language = {en}
}

@misc{19_UnsupervisedCrosslingualRepresentation,
  title = {Unsupervised {{Cross}}-Lingual {{Representation Learning}}},
  year = {2019},
  month = oct,
  abstract = {This post expands on the ACL 2019 tutorial on Unsupervised Cross-lingual Representation Learning. It highlights key insights and takeaways and provides updates based on recent work, particularly unsupervised deep multilingual models.},
  file = {/home/trung/Zotero/storage/NKRZBLMH/unsupervised-cross-lingual-learning.html},
  howpublished = {https://ruder.io/unsupervised-cross-lingual-learning/},
  journal = {Sebastian Ruder},
  language = {en}
}

@misc{19_vietaimlfoundationclass,
  title = {Vietai/Ml-Foundation-Class},
  year = {2019},
  month = nov,
  abstract = {VietAI Machine Learning Foundation Class Material. Contribute to vietai/ml-foundation-class development by creating an account on GitHub.},
  howpublished = {vietai},
  keywords = {machine-learning,vietai}
}

@misc{20_bayesgroupdeepbayes2019,
  title = {Bayesgroup/Deepbayes-2019},
  year = {2020},
  month = sep,
  abstract = {Practical assignments of the Deep|Bayes summer school 2019},
  howpublished = {Bayesian Methods Research Group}
}

@misc{20_deepchemdeepchem,
  title = {Deepchem/Deepchem},
  year = {2020},
  month = mar,
  abstract = {Democratizing Deep-Learning for Drug Discovery, Quantum Chemistry, Materials Science and Biology},
  copyright = {MIT},
  howpublished = {deepchem}
}

@misc{20_dsgiitrgraphnets,
  title = {Dsgiitr/Graph\_nets},
  year = {2020},
  month = feb,
  abstract = {PyTorch Implementation and Explanation of Graph Representation Learning papers involving DeepWalk, GCN, GraphSAGE, ChebNet \& GAT.},
  howpublished = {Data Science Group, IIT Roorkee}
}

@misc{20_EthicalMLawesomeproductionmachinelearning,
  title = {{{EthicalML}}/Awesome-Production-Machine-Learning},
  year = {2020},
  month = dec,
  abstract = {A curated list of awesome open source libraries to deploy, monitor, version and scale your machine learning},
  copyright = {MIT License         ,                 MIT License},
  howpublished = {The Institute for Ethical Machine Learning}
}

@misc{20_fastaifastbook,
  title = {Fastai/Fastbook},
  year = {2020},
  month = mar,
  abstract = {Draft of the fastai book. Contribute to fastai/fastbook development by creating an account on GitHub.},
  copyright = {GPL-3.0},
  howpublished = {fast.ai}
}

@misc{20_GoogleCloudPlatformtensorflowwithoutaphd,
  title = {{{GoogleCloudPlatform}}/Tensorflow-without-a-Phd},
  year = {2020},
  month = apr,
  abstract = {A crash course in six episodes for software developers who want to become machine learning practitioners.},
  copyright = {Apache-2.0},
  howpublished = {Google Cloud Platform}
}

@misc{20_GreenleafLabMPALSingleCell2019,
  title = {{{GreenleafLab}}/{{MPAL}}-{{Single}}-{{Cell}}-2019},
  year = {2020},
  month = apr,
  abstract = {Publication Page for MPAL Paper 2019. Contribute to GreenleafLab/MPAL-Single-Cell-2019 development by creating an account on GitHub.},
  howpublished = {GreenleafLab}
}

@misc{20_GreenleafLabMPALSingleCell2019a,
  title = {{{GreenleafLab}}/{{MPAL}}-{{Single}}-{{Cell}}-2019},
  year = {2020},
  month = may,
  abstract = {Publication Page for MPAL Paper 2019. Contribute to GreenleafLab/MPAL-Single-Cell-2019 development by creating an account on GitHub.},
  howpublished = {GreenleafLab}
}

@misc{20_Learningcomputationaltasks,
  title = {Learning Computational Tasks from Single Examples},
  year = {2020},
  month = apr,
  abstract = {New ``meta-learning'' approach improves on the state of the art in ``one-shot'' learning.},
  file = {/home/trung/Zotero/storage/MNR74KNT/learning-computational-tasks-from-single-examples.html},
  howpublished = {https://www.amazon.science/blog/learning-computational-tasks-from-single-examples},
  journal = {Amazon Science},
  language = {en}
}

@misc{20_lllyasvielstyle2paintssketch,
  title = {Lllyasviel/Style2paints: Sketch + Style = Paints ({{TOG2018}}/{{SIGGRAPH2018ASIA}})},
  year = {2020},
  file = {/home/trung/Zotero/storage/9X9E46FK/style2paints.html},
  howpublished = {https://github.com/lllyasviel/style2paints}
}

@misc{20_MachineLearningTokyogenerativedeep,
  title = {Machine-{{Learning}}-{{Tokyo}}/Generative\_deep\_learning},
  year = {2020},
  month = mar,
  abstract = {Generative Deep Learning Sessions led by Anugraha Sinha (Machine Learning Tokyo)},
  copyright = {MIT},
  howpublished = {Machine Learning Tokyo}
}

@misc{20_MichaelMannFought,
  title = {Michael {{Mann Fought Climate Denial}}. {{Now He}}'s {{Fighting Climate Doom}}.},
  year = {2020},
  month = jun,
  abstract = {ONE AUGUST AFTERNOON IN 2010, Michael Mann was opening mail in his office at Penn State University when a dusting of white powder emerged from an envelope. At first he thought it was his imagination. ``I figured maybe it's just an old dingy envelope or something,'' Mann recalled. His next thought: anthrax.},
  howpublished = {https://alumni.berkeley.edu/california-magazine/summer-2020/michael-mann-on-climate-denial-and-doom},
  journal = {Cal Alumni Association}
}

@misc{20_microsoftdowhy,
  title = {Microsoft/Dowhy},
  year = {2020},
  month = dec,
  abstract = {DoWhy is a Python library for causal inference that supports explicit modeling and testing of causal assumptions. DoWhy is based on a unified language for causal inference, combining causal graphic...},
  copyright = {MIT License         ,                 MIT License},
  howpublished = {Microsoft}
}

@misc{20_NVIDIADeepLearningExamples,
  title = {{{NVIDIA}}/{{DeepLearningExamples}}},
  year = {2020},
  month = apr,
  abstract = {Deep Learning Examples. Contribute to NVIDIA/DeepLearningExamples development by creating an account on GitHub.},
  howpublished = {NVIDIA Corporation}
}

@misc{20_practicalAIpracticalAI,
  title = {{{practicalAI}}/{{practicalAI}}},
  year = {2020},
  month = jan,
  abstract = {📚 A practical approach to machine learning to enable everyone to learn, explore and build.},
  copyright = {MIT},
  howpublished = {practicalAI}
}

@misc{20_SinglecellRNAseqanalysis,
  title = {Single-Cell {{RNA}}-Seq Analysis Workshop},
  year = {2020},
  month = may,
  abstract = {Contribute to hbctraining/scRNA-seq development by creating an account on GitHub.},
  howpublished = {Teaching materials at the Harvard Chan Bioinformatics Core}
}

@misc{20_StanfordCS330MultiTask,
  title = {Stanford {{CS330}}: {{Multi}}-{{Task}} and {{Meta}}-{{Learning}}, 2019},
  year = {2020}
}

@misc{20_theislabscvelo,
  title = {Theislab/Scvelo},
  year = {2020},
  month = mar,
  abstract = {RNA Velocity using dynamical modeling. Contribute to theislab/scvelo development by creating an account on GitHub.},
  copyright = {BSD-3-Clause},
  howpublished = {Theis Lab}
}

@misc{20_thunlpGNNPapers,
  title = {Thunlp/{{GNNPapers}}},
  year = {2020},
  month = apr,
  abstract = {Must-read papers on graph neural networks (GNN). Contribute to thunlp/GNNPapers development by creating an account on GitHub.},
  howpublished = {THUNLP}
}

@misc{20_UnsupervisedClusteringUsing,
  title = {Unsupervised {{Clustering Using Pseudo}}-{{Semi}}-{{Supervised Learning}}},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/2020_unsupervised clustering using pseudo-semi-supervised learning.pdf}
}

@inproceedings{49463,
  title = {{{ReMixMatch}}: {{Semi}}-Supervised Learning with Distribution Matching and Augmentation Anchoring},
  booktitle = {{{ICLR}}},
  author = {Kurakin, Alex and Raffel, Colin and Berthelot, David and Cubuk, Ekin Dogus and Zhang, Han and Sohn, Kihyuk and Carlini, Nicholas},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/kurakin et al_2020_remixmatch.pdf}
}

@article{9187857,
  title = {{{GRACE}}: {{A}} Graph-Based Cluster Ensemble Approach for Single-Cell {{RNA}}-{{Seq}} Data Clustering},
  author = {Guan, J. and Li, R. -Y. and Wang, J.},
  year = {2020},
  volume = {8},
  pages = {166730--166741},
  doi = {10.1109/ACCESS.2020.3022718},
  journal = {IEEE Access}
}

@article{9204375,
  title = {{{scLRTD}} : {{A}} Novel Low Rank Tensor Decomposition Method for Imputing Missing Values in Single-Cell Multi-Omics Sequencing Data},
  author = {Ni, Z. and Zheng, X. and Zheng, X. and Zou, X.},
  year = {2020},
  pages = {1--1},
  doi = {10.1109/TCBB.2020.3025804},
  journal = {IEEE/ACM Transactions on Computational Biology and Bioinformatics}
}

@article{abbasnejad16_InfiniteVariationalAutoencoder,
  title = {Infinite {{Variational Autoencoder}} for {{Semi}}-{{Supervised Learning}}},
  author = {Abbasnejad, Ehsan and Dick, Anthony and van den Hengel, Anton},
  year = {2016},
  month = nov,
  abstract = {This paper presents an infinite variational autoencoder (VAE) whose capacity adapts to suit the input data. This is achieved using a mixture model where the mixing coefficients are modeled by a Dirichlet process, allowing us to integrate over the coefficients when performing inference. Critically, this then allows us to automatically vary the number of autoencoders in the mixture based on the data. Experiments show the flexibility of our method, particularly for semi-supervised learning, where only a small number of training samples are available.},
  archivePrefix = {arXiv},
  eprint = {1611.07800},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/abbasnejad et al_2016_infinite variational autoencoder for semi-supervised learning.pdf},
  journal = {arXiv:1611.07800 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{abid19_ContrastiveVariationalAutoencoder,
  title = {Contrastive {{Variational Autoencoder Enhances Salient Features}}},
  author = {Abid, Abubakar and Zou, James},
  year = {2019},
  month = feb,
  abstract = {Variational autoencoders are powerful algorithms for identifying dominant latent structure in a single dataset. In many applications, however, we are interested in modeling latent structure and variation that are enriched in a target dataset compared to some background\textemdash e.g. enriched in patients compared to the general population. Contrastive learning is a principled framework to capture such enriched variation between the target and background, but state-of-the-art contrastive methods are limited to linear models. In this paper, we introduce the contrastive variational autoencoder (cVAE), which combines the benefits of contrastive learning with the power of deep generative models. The cVAE is designed to identify and enhance salient latent features. The cVAE is trained on two related but unpaired datasets, one of which has minimal contribution from the salient latent features. The cVAE explicitly models latent features that are shared between the datasets, as well as those that are enriched in one dataset relative to the other, which allows the algorithm to isolate and enhance the salient latent features. The algorithm is straightforward to implement, has a similar run-time to the standard VAE, and is robust to noise and dataset purity. We conduct experiments across diverse types of data, including gene expression and facial images, showing that the cVAE effectively uncovers latent structure that is salient in a particular analysis.},
  archivePrefix = {arXiv},
  eprint = {1902.04601},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/M654X23L/Abid and Zou - 2019 - Contrastive Variational Autoencoder Enhances Salie.pdf},
  journal = {arXiv:1902.04601 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{abid19_ContrastiveVariationalAutoencodera,
  title = {Contrastive {{Variational Autoencoder Enhances Salient Features}}},
  author = {Abid, Abubakar and Zou, James},
  year = {2019},
  month = feb,
  abstract = {Variational autoencoders are powerful algorithms for identifying dominant latent structure in a single dataset. In many applications, however, we are interested in modeling latent structure and variation that are enriched in a target dataset compared to some background---e.g. enriched in patients compared to the general population. Contrastive learning is a principled framework to capture such enriched variation between the target and background, but state-of-the-art contrastive methods are limited to linear models. In this paper, we introduce the contrastive variational autoencoder (cVAE), which combines the benefits of contrastive learning with the power of deep generative models. The cVAE is designed to identify and enhance salient latent features. The cVAE is trained on two related but unpaired datasets, one of which has minimal contribution from the salient latent features. The cVAE explicitly models latent features that are shared between the datasets, as well as those that are enriched in one dataset relative to the other, which allows the algorithm to isolate and enhance the salient latent features. The algorithm is straightforward to implement, has a similar run-time to the standard VAE, and is robust to noise and dataset purity. We conduct experiments across diverse types of data, including gene expression and facial images, showing that the cVAE effectively uncovers latent structure that is salient in a particular analysis.},
  archivePrefix = {arXiv},
  eprint = {1902.04601},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/abid et al_2019_contrastive variational autoencoder enhances salient features.pdf},
  journal = {arXiv:1902.04601 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{achille17_InformationDropoutLearning,
  title = {Information {{Dropout}}: {{Learning Optimal Representations Through Noisy Computation}}},
  shorttitle = {Information {{Dropout}}},
  author = {Achille, Alessandro and Soatto, Stefano},
  year = {2017},
  month = feb,
  abstract = {The cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations, but does not enforce some of the key properties. We show that this can be solved by adding a regularization term, which is in turn related to injecting multiplicative noise in the activations of a Deep Neural Network, a special case of which is the common practice of dropout. We show that our regularized loss function can be efficiently minimized using Information Dropout, a generalization of dropout rooted in information theoretic principles that automatically adapts to the data and can better exploit architectures of limited capacity. When the task is the reconstruction of the input, we show that our loss function yields a Variational Autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Finally, we prove that we can promote the creation of disentangled representations simply by enforcing a factorized prior, a fact that has been observed empirically in recent work. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1611.01353},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/H5R4VL9A/Achille and Soatto - 2017 - Information Dropout Learning Optimal Representati.pdf},
  journal = {arXiv:1611.01353 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{achille17_InformationDropoutLearninga,
  title = {Information {{Dropout}}: {{Learning Optimal Representations Through Noisy Computation}}},
  shorttitle = {Information {{Dropout}}},
  author = {Achille, Alessandro and Soatto, Stefano},
  year = {2017},
  month = feb,
  abstract = {The cross-entropy loss commonly used in deep learning is closely related to the defining properties of optimal representations, but does not enforce some of the key properties. We show that this can be solved by adding a regularization term, which is in turn related to injecting multiplicative noise in the activations of a Deep Neural Network, a special case of which is the common practice of dropout. We show that our regularized loss function can be efficiently minimized using Information Dropout, a generalization of dropout rooted in information theoretic principles that automatically adapts to the data and can better exploit architectures of limited capacity. When the task is the reconstruction of the input, we show that our loss function yields a Variational Autoencoder as a special case, thus providing a link between representation learning, information theory and variational inference. Finally, we prove that we can promote the creation of disentangled representations simply by enforcing a factorized prior, a fact that has been observed empirically in recent work. Our experiments validate the theoretical intuitions behind our method, and we find that information dropout achieves a comparable or better generalization performance than binary dropout, especially on smaller models, since it can automatically adapt the noise to the structure of the network, as well as to the test sample.},
  archivePrefix = {arXiv},
  eprint = {1611.01353},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/achille et al_2017_information dropout.pdf},
  journal = {arXiv:1611.01353 [cs, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, stat}
}

@article{achille18_EmergenceInvarianceDisentanglement,
  title = {Emergence of {{Invariance}} and {{Disentanglement}} in {{Deep Representations}}},
  author = {Achille, Alessandro and Soatto, Stefano},
  year = {2018},
  month = jun,
  abstract = {Using established principles from Statistics and Information Theory, we show that invariance to nuisance factors in a deep neural network is equivalent to information minimality of the learned representation, and that stacking layers and injecting noise during training naturally bias the network towards learning invariant representations. We then decompose the cross-entropy loss used during training and highlight the presence of an inherent overfitting term. We propose regularizing the loss by bounding such a term in two equivalent ways: One with a Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other using the information in the weights as a measure of complexity of a learned model, yielding a novel Information Bottleneck for the weights. Finally, we show that invariance and independence of the components of the representation learned by the network are bounded above and below by the information in the weights, and therefore are implicitly optimized during training. The theory enables us to quantify and predict sharp phase transitions between underfitting and overfitting of random labels when using our regularized loss, which we verify in experiments, and sheds light on the relation between the geometry of the loss function, invariance properties of the learned representation, and generalization error.},
  archivePrefix = {arXiv},
  eprint = {1706.01350},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/achille et al_2018_emergence of invariance and disentanglement in deep representations.pdf},
  journal = {arXiv:1706.01350 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{achille19_InformationComplexityLearning,
  title = {The {{Information Complexity}} of {{Learning Tasks}}, Their {{Structure}} and Their {{Distance}}},
  author = {Achille, Alessandro and Paolini, Giovanni and Mbeng, Glen and Soatto, Stefano},
  year = {2019},
  month = apr,
  abstract = {We introduce an asymmetric distance in the space of learning tasks, and a framework to compute their complexity. These concepts are foundational to the practice of transfer learning, ubiquitous in Deep Learning, whereby a parametric model is pre-trained for a task, and then used for another after fine-tuning. The framework we develop is intrinsically non-asymptotic, capturing the finite nature of the training dataset, yet it allows distinguishing learning from memorization. It encompasses, as special cases, classical notions from Kolmogorov complexity, Shannon, and Fisher Information. However, unlike some of those frameworks, it can be applied easily to large-scale models and real-world datasets. It is the first framework to explicitly account for the optimization scheme, which plays a crucial role in Deep Learning, in measuring complexity and information.},
  archivePrefix = {arXiv},
  eprint = {1904.03292},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/achille et al_2019_the information complexity of learning tasks, their structure and their distance.pdf},
  journal = {arXiv:1904.03292 [cs, math, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{achille19_WhereInformationDeep,
  title = {Where Is the {{Information}} in a {{Deep Neural Network}}?},
  author = {Achille, Alessandro and Soatto, Stefano},
  year = {2019},
  month = jun,
  abstract = {Whatever information a Deep Neural Network has gleaned from past data is encoded in its weights. How this information affects the response of the network to future data is largely an open question. In fact, even how to define and measure information in a network is still not settled. We introduce the notion of Information in the Weights as the optimal trade-off between accuracy of the network and complexity of the weights, relative to a prior. Depending on the prior, the definition reduces to known information measures such as Shannon Mutual Information and Fisher Information, but affords added flexibility that enables us to relate it to generalization, via the PAC-Bayes bound, and to invariance. This relation hinges not only on the architecture of the model, but surprisingly on how it is trained. We then introduce a notion of effective information in the activations, which are deterministic functions of future inputs, resolving inconsistencies in prior work. We relate this to the Information in the Weights, and use this result to show that models of low complexity not only generalize better, but are bound to learn invariant representations of future inputs.},
  archivePrefix = {arXiv},
  eprint = {1905.12213},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/achille et al_2019_where is the information in a deep neural network.pdf},
  journal = {arXiv:1905.12213 [cs, math, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, math, stat}
}

@misc{adaloglou20_GANscomputervision,
  title = {{{GANs}} in Computer Vision - {{Introduction}} to Generative Learning},
  author = {Adaloglou, Nikolaos},
  year = {2020},
  month = apr,
  abstract = {The first article of the GANs in computer vision series - an introduction to generative learning, adversarial learning, gan training algorithm, conditional image generation, mode collapse, mutual information},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/NWCDFXI6/gan-computer-vision.html},
  howpublished = {https://theaisummer.com/gan-computer-vision/},
  journal = {AI Summer},
  language = {en}
}

@article{adcock20_gaptheorypractice,
  title = {The Gap between Theory and Practice in Function Approximation with Deep Neural Networks},
  author = {Adcock, Ben and Dexter, Nick},
  year = {2020},
  month = jan,
  abstract = {Deep learning (DL) is transforming whole industries as complicated decision-making processes are being automated by Deep Neural Networks (DNNs) trained on real-world data. Driven in part by a rapidly-expanding literature on DNN approximation theory showing that DNNs can approximate a rich variety of functions, these tools are increasingly being considered for problems in scientific computing. Yet, unlike more traditional algorithms in this field, relatively little is known about DNNs from the principles of numerical analysis, namely, stability, accuracy, computational efficiency and sample complexity. In this paper we introduce a computational framework for examining DNNs in practice, and use it to study their empirical performance with regard to these issues. We examine the performance of DNNs of different widths and depths on a variety of test functions in various dimensions, including smooth and piecewise smooth functions. We also compare DL against best-inclass methods for smooth function approximation based on compressed sensing. Our main conclusion is that there is a crucial gap between the approximation theory of DNNs and their practical performance, with trained DNNs performing relatively poorly on functions for which there are strong approximation results (e.g. smooth functions), yet performing well in comparison to best-in-class methods for other functions. Finally, we present a novel practical existence theorem, which asserts the existence of a DNN architecture and training procedure which offers the same performance as current best-in-class schemes. This result indicates the potential for practical DNN approximation, and the need for future research into practical architecture design and training strategies.},
  archivePrefix = {arXiv},
  eprint = {2001.07523},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/8X6WBPZK/Adcock and Dexter - 2020 - The gap between theory and practice in function ap.pdf},
  journal = {arXiv:2001.07523 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{adeli19_BiasResilientNeuralNetwork,
  title = {Bias-{{Resilient Neural Network}}},
  author = {Adeli, Ehsan and Zhao, Qingyu and Pfefferbaum, Adolf and Sullivan, Edith V. and {Fei-Fei}, Li and Niebles, Juan Carlos and Pohl, Kilian M.},
  year = {2019},
  month = oct,
  abstract = {Presence of bias and confounding effects is inarguably one of the most critical challenges in machine learning applications that has alluded to pivotal debates in the recent years. Such challenges range from spurious associations of confounding variables in medical studies to the bias of race in gender or face recognition systems. One solution is to enhance datasets and organize them such that they do not reflect biases, which is a cumbersome and intensive task. The alternative is to make use of available data and build models considering these biases. Traditional statistical methods apply straightforward techniques such as residualization or stratification to precomputed features to account for confounding variables. However, these techniques are generally not suitable for end-to-end deep learning methods. In this paper, we propose a method based on the adversarial training strategy to learn discriminative features unbiased and invariant to the confounder(s). This is enabled by incorporating a new adversarial loss function that encourages a vanished correlation between the bias and learned features. We apply our method to synthetic data, medical images, and a gender classification (Gender Shades Pilot Parliaments Benchmark) dataset. Our results show that the learned features by our method not only result in superior prediction performance but also are uncorrelated with the bias or confounder variables. The code is available at http://github.com/QingyuZhao/BR-Net/.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.03676},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/adeli et al_2019_bias-resilient neural network.pdf;/home/trung/Zotero/storage/RW99GRVV/1910.html},
  journal = {arXiv:1910.03676 [cs]},
  keywords = {bias,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{adler18_DeepBayesianInversion,
  title = {Deep {{Bayesian Inversion}}},
  author = {Adler, Jonas and {\"O}ktem, Ozan},
  year = {2018},
  month = nov,
  abstract = {Characterizing statistical properties of solutions of inverse problems is essential for decision making. Bayesian inversion offers a tractable framework for this purpose, but current approaches are computationally unfeasible for most realistic imaging applications in the clinic. We introduce two novel deep learning based methods for solving large-scale inverse problems using Bayesian inversion: a sampling based method using a WGAN with a novel mini-discriminator and a direct approach that trains a neural network using a novel loss function. The performance of both methods is demonstrated on image reconstruction in ultra low dose 3D helical CT. We compute the posterior mean and standard deviation of the 3D images followed by a hypothesis test to assess whether a "dark spot" in the liver of a cancer stricken patient is present. Both methods are computationally efficient and our evaluation shows very promising performance that clearly supports the claim that Bayesian inversion is usable for 3D imaging in time critical applications.},
  archivePrefix = {arXiv},
  eprint = {1811.05910},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/adler et al_2018_deep bayesian inversion.pdf;/home/trung/Zotero/storage/TTP5QWYN/1811.html},
  journal = {arXiv:1811.05910 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning,variational},
  primaryClass = {cs, math, stat}
}

@article{advani13_Statisticalmechanicscomplex,
  title = {Statistical Mechanics of Complex Neural Systems and High Dimensional Data},
  author = {Advani, Madhu and Lahiri, Subhaneil and Ganguli, Surya},
  year = {2013},
  month = mar,
  volume = {2013},
  pages = {P03014},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2013/03/P03014},
  abstract = {Recent experimental advances in neuroscience have opened new vistas into the immense complexity of neuronal networks. This proliferation of data challenges us on two parallel fronts. First, how can we form adequate theoretical frameworks for understanding how dynamical network processes cooperate across widely disparate spatiotemporal scales to solve important computational problems? And second, how can we extract meaningful models of neuronal systems from high dimensional datasets? To aid in these challenges, we give a pedagogical review of a collection of ideas and theoretical methods arising at the intersection of statistical physics, computer science and neurobiology. We introduce the interrelated replica and cavity methods, which originated in statistical physics as powerful ways to quantitatively analyze large highly heterogeneous systems of many interacting degrees of freedom. We also introduce the closely related notion of message passing in graphical models, which originated in computer science as a distributed algorithm capable of solving large inference and optimization problems involving many coupled variables. We then show how both the statistical physics and computer science perspectives can be applied in a wide diversity of contexts to problems arising in theoretical neuroscience and data analysis. Along the way we discuss spin glasses, learning theory, illusions of structure in noise, random matrices, dimensionality reduction, and compressed sensing, all within the unified formalism of the replica method. Moreover, we review recent conceptual connections between message passing in graphical models, and neural computation and learning. Overall, these ideas illustrate how statistical physics and computer science might provide a lens through which we can uncover emergent computational functions buried deep within the dynamical complexities of neuronal networks.},
  annotation = {ZSCC: 0000047},
  archivePrefix = {arXiv},
  eprint = {1301.7115},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/T46VMD9C/Advani et al. - 2013 - Statistical mechanics of complex neural systems an.pdf},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  language = {en},
  number = {03}
}

@incollection{agakov04_AuxiliaryVariationalMethod,
  title = {An {{Auxiliary Variational Method}}},
  booktitle = {Neural {{Information Processing}}},
  author = {Agakov, Felix V. and Barber, David},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Dough and Vardi, Moshe Y. and Weikum, Gerhard and Pal, Nikhil Ranjan and Kasabov, Nik and Mudi, Rajani K. and Pal, Srimanta and Parui, Swapan Kumar},
  year = {2004},
  volume = {3316},
  pages = {561--566},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-540-30499-9_86},
  abstract = {An attractive feature of variational methods used in the context of approximate inference in undirected graphical models is a rigorous lower bound on the normalization constants. Here we explore the idea of using augmented variable spaces to improve on the standard mean-field bounds. Our approach forms a more powerful class of approximations than any structured mean field technique. Moreover, the existing variational mixture models may be seen as computationally expensive special cases of our method. A byproduct of our work is an efficient way to calculate a set of mixture coefficients for any set of tractable distributions that principally improves on a flat combination.},
  file = {/home/trung/GoogleDrive/Zotero/agakov et al_2004_an auxiliary variational method.pdf},
  isbn = {978-3-540-23931-4 978-3-540-30499-9},
  language = {en}
}

@article{agarwal20_EstimatingExampleDifficulty,
  title = {Estimating {{Example Difficulty}} Using {{Variance}} of {{Gradients}}},
  author = {Agarwal, Chirag and Hooker, Sara},
  year = {2020},
  month = aug,
  abstract = {In machine learning, a question of great interest is understanding what examples are challenging for a model to classify. Identifying atypical examples helps inform safe deployment of models, isolates examples that require further human inspection, and provides interpretability into model behavior. In this work, we propose Variance of Gradients (VOG) as a proxy metric for detecting outliers in the data distribution. We provide quantitative and qualitative support that VOG is a meaningful way to rank data by difficulty and to surface a tractable subset of the most challenging examples for human-in-the-loop auditing. Data points with high VOG scores are more difficult for the model to classify and over-index on examples that require memorization.},
  archivePrefix = {arXiv},
  eprint = {2008.11600},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/agarwal et al_2020_estimating example difficulty using variance of gradients.pdf},
  journal = {arXiv:2008.11600 [cs]},
  primaryClass = {cs}
}

@article{agarwala20_Temperaturechecktheory,
  title = {Temperature Check: Theory and Practice for Training Models with Softmax-Cross-Entropy Losses},
  shorttitle = {Temperature Check},
  author = {Agarwala, Atish and Pennington, Jeffrey and Dauphin, Yann and Schoenholz, Sam},
  year = {2020},
  month = oct,
  abstract = {The softmax function combined with a cross-entropy loss is a principled approach to modeling probability distributions that has become ubiquitous in deep learning. The softmax function is defined by a lone hyperparameter, the temperature, that is commonly set to one or regarded as a way to tune model confidence after training; however, less is known about how the temperature impacts training dynamics or generalization performance. In this work we develop a theory of early learning for models trained with softmax-cross-entropy loss and show that the learning dynamics depend crucially on the inverse-temperature \$\textbackslash beta\$ as well as the magnitude of the logits at initialization, \$||\textbackslash beta\{\textbackslash bf z\}||\_\{2\}\$. We follow up these analytic results with a large-scale empirical study of a variety of model architectures trained on CIFAR10, ImageNet, and IMDB sentiment analysis. We find that generalization performance depends strongly on the temperature, but only weakly on the initial logit magnitude. We provide evidence that the dependence of generalization on \$\textbackslash beta\$ is not due to changes in model confidence, but is a dynamical phenomenon. It follows that the addition of \$\textbackslash beta\$ as a tunable hyperparameter is key to maximizing model performance. Although we find the optimal \$\textbackslash beta\$ to be sensitive to the architecture, our results suggest that tuning \$\textbackslash beta\$ over the range \$10\^\{-2\}\$ to \$10\^1\$ improves performance over all architectures studied. We find that smaller \$\textbackslash beta\$ may lead to better peak performance at the cost of learning stability.},
  archivePrefix = {arXiv},
  eprint = {2010.07344},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/agarwala et al_2020_temperature check.pdf},
  journal = {arXiv:2010.07344 [cs]},
  primaryClass = {cs}
}

@article{aguiar13_NeuropharmacologicalReviewNootropic,
  title = {Neuropharmacological {{Review}} of the {{Nootropic Herb Bacopa}} Monnieri},
  author = {Aguiar, Sebastian and Borowski, Thomas},
  year = {2013},
  month = aug,
  volume = {16},
  pages = {313--326},
  issn = {1549-1684},
  doi = {10.1089/rej.2013.1431},
  abstract = {This review synthesizes behavioral research with neuromolecular mechanisms putatively involved with the low-toxicity cognitive enhancing action of Bacopa monnieri (BM), a medicinal Ayurvedic herb. BM is traditionally used for various ailments, but is best known as a neural tonic and memory enhancer. Numerous animal and in vitro studies have been conducted, with many evidencing potential medicinal properties. Several randomized, double-blind, placebo-controlled trials have substantiated BM's nootropic utility in humans. There is also evidence for potential attenuation of dementia, Parkinson's disease, and epilepsy. Current evidence suggests BM acts via the following mechanisms\textemdash anti-oxidant neuroprotection (via redox and enzyme induction), acetylcholinesterase inhibition and/or choline acetyltransferase activation, {$\beta$}-amyloid reduction, increased cerebral blood flow, and neurotransmitter modulation (acetylcholine [ACh], 5-hydroxytryptamine [5-HT], dopamine [DA]). BM appears to exhibit low toxicity in model organisms and humans; however, long-term studies of toxicity in humans have yet to be conducted. This review will integrate molecular neuroscience with behavioral research.},
  file = {/home/trung/GoogleDrive/Zotero/aguiar et al_2013_neuropharmacological review of the nootropic herb bacopa monnieri.pdf},
  journal = {Rejuvenation Research},
  number = {4},
  pmcid = {PMC3746283},
  pmid = {23772955}
}

@article{ahn17_NeuralKnowledgeLanguage,
  title = {A {{Neural Knowledge Language Model}}},
  author = {Ahn, Sungjin and Choi, Heeyoul and P{\"a}rnamaa, Tanel and Bengio, Yoshua},
  year = {2017},
  month = mar,
  abstract = {Current language models have a significant limitation in the ability to encode and decode factual knowledge. This is mainly because they acquire such knowledge from statistical co-occurrences although most of the knowledge words are rarely observed. In this paper, we propose a Neural Knowledge Language Model (NKLM) which combines symbolic knowledge provided by the knowledge graph with the RNN language model. By predicting whether the word to generate has an underlying fact or not, the model can generate such knowledge-related words by copying from the description of the predicted fact. In experiments, we show that the NKLM significantly improves the performance while generating a much smaller number of unknown words.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1608.00318},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ahn et al_2017_a neural knowledge language model.pdf;/home/trung/Zotero/storage/PS8UG77F/1608.html},
  journal = {arXiv:1608.00318 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,graph,knowledge,relational},
  primaryClass = {cs}
}

@article{ahn19_VariationalInformationDistillation,
  title = {Variational {{Information Distillation}} for {{Knowledge Transfer}}},
  author = {Ahn, Sungsoo and Hu, Shell Xu and Damianou, Andreas and Lawrence, Neil D and Dai, Zhenwen},
  year = {2019},
  pages = {9},
  abstract = {Transferring knowledge from a teacher neural network pretrained on the same or a similar task to a student neural network can significantly improve the performance of the student neural network. Existing knowledge transfer approaches match the activations or the corresponding handcrafted features of the teacher and the student networks. We propose an information-theoretic framework for knowledge transfer which formulates knowledge transfer as maximizing the mutual information between the teacher and the student networks. We compare our method with existing knowledge transfer methods on both knowledge distillation and transfer learning tasks and show that our method consistently outperforms existing methods. We further demonstrate the strength of our method on knowledge transfer across heterogeneous network architectures by transferring knowledge from a convolutional neural network (CNN) to a multi-layer perceptron (MLP) on CIFAR-10. The resulting MLP significantly outperforms the-state-of-the-art methods and it achieves similar performance to the CNN with a single convolutional layer.},
  file = {/home/trung/GoogleDrive/Zotero/ahn et al_2019_variational information distillation for knowledge transfer.pdf},
  keywords = {information,transfer,variational},
  language = {en}
}

@article{ahuja20_InvariantRiskMinimization,
  title = {Invariant {{Risk Minimization Games}}},
  author = {Ahuja, Kartik and Shanmugam, Karthikeyan and Varshney, Kush R. and Dhurandhar, Amit},
  year = {2020},
  month = mar,
  abstract = {The standard risk minimization paradigm of machine learning is brittle when operating in environments whose test distributions are different from the training distribution due to spurious correlations. Training on data from many environments and finding invariant predictors reduces the effect of spurious features by concentrating models on features that have a causal relationship with the outcome. In this work, we pose such invariant risk minimization as finding the Nash equilibrium of an ensemble game among several environments. By doing so, we develop a simple training algorithm that uses best response dynamics and, in our experiments, yields similar or better empirical accuracy with much lower variance than the challenging bi-level optimization problem of [1]. One key theoretical contribution is showing that the set of Nash equilibria for the proposed game are equivalent to the set of invariant predictors for any finite number of environments, even with nonlinear classifiers and transformations. As a result, our method also retains the generalization guarantees to a large set of environments shown in [1]. The proposed algorithm adds to the collection of successful game-theoretic machine learning algorithms such as generative adversarial networks.},
  archivePrefix = {arXiv},
  eprint = {2002.04692},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ahuja et al_2020_invariant risk minimization games.pdf},
  journal = {arXiv:2002.04692 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{ai19_HackPPLuniversalprobabilistic,
  title = {{{HackPPL}}: A Universal Probabilistic Programming Language},
  shorttitle = {{{HackPPL}}},
  booktitle = {Proceedings of the 3rd {{ACM SIGPLAN International Workshop}} on {{Machine Learning}} and {{Programming Languages}}  - {{MAPL}} 2019},
  author = {Ai, Jessica and Arora, Nimar S. and Dong, Ning and Gokkaya, Beliz and Jiang, Thomas and Kubendran, Anitha and Kumar, Arun and Tingley, Michael and Torabi, Narjes},
  year = {2019},
  pages = {20--28},
  publisher = {{ACM Press}},
  address = {{Phoenix, AZ, USA}},
  doi = {10.1145/3315508.3329974},
  abstract = {HackPPL is a probabilistic programming language (PPL) built within the Hack programming language. Its universal inference engine allows developers to perform inference across a diverse set of models expressible in arbitrary Hack code. Through language-level extensions and direct integration with developer tools, HackPPL aims to bridge the gap between domain-specific and embedded PPLs. This paper overviews the design and implementation choices for the HackPPL toolchain and presents findings by applying it to a representative problem faced by social media companies.},
  file = {/home/trung/GoogleDrive/Zotero/ai et al_2019_hackppl.pdf},
  isbn = {978-1-4503-6719-6},
  language = {en}
}

@article{aicale18_Overuseinjuriessport,
  title = {Overuse Injuries in Sport: A Comprehensive Overview},
  shorttitle = {Overuse Injuries in Sport},
  author = {Aicale, R. and Tarantino, D. and Maffulli, N.},
  year = {2018},
  month = dec,
  volume = {13},
  issn = {1749-799X},
  doi = {10.1186/s13018-018-1017-5},
  abstract = {Background The absence of a single, identifiable traumatic cause has been traditionally used as a definition for a causative factor of overuse injury. Excessive loading, insufficient recovery, and underpreparedness can increase injury risk by exposing athletes to relatively large changes in load. The musculoskeletal system, if subjected to excessive stress, can suffer from various types of overuse injuries which may affect the bone, muscles, tendons, and ligaments. Methods We performed a search (up to March 2018) in the PubMed and Scopus electronic databases to identify the available scientific articles about the pathophysiology and the incidence of overuse sport injuries. For the purposes of our review, we used several combinations of the following keywords: overuse, injury, tendon, tendinopathy, stress fracture, stress reaction, and juvenile osteochondritis dissecans. Results Overuse tendinopathy induces in the tendon pain and swelling with associated decreased tolerance to exercise and various types of tendon degeneration. Poor training technique and a variety of risk factors may predispose athletes to stress reactions that may be interpreted as possible precursors of stress fractures. A frequent cause of pain in adolescents is juvenile osteochondritis dissecans (JOCD), which is characterized by delamination and localized necrosis of the subchondral bone, with or without the involvement of articular cartilage. The purpose of this compressive review is to give an overview of overuse injuries in sport by describing the theoretical foundations of these conditions that may predispose to the development of tendinopathy, stress fractures, stress reactions, and juvenile osteochondritis dissecans and the implication that these pathologies may have in their management. Conclusions Further research is required to improve our knowledge on tendon and bone healing, enabling specific treatment strategies to be developed for the management of overuse injuries.},
  file = {/home/trung/GoogleDrive/Zotero/aicale et al_2018_overuse injuries in sport.pdf},
  journal = {Journal of Orthopaedic Surgery and Research},
  pmcid = {PMC6282309},
  pmid = {30518382}
}

@article{ainslie20_ETCEncodingLong,
  title = {{{ETC}}: {{Encoding Long}} and {{Structured Inputs}} in {{Transformers}}},
  shorttitle = {{{ETC}}},
  author = {Ainslie, Joshua and Ontanon, Santiago and Alberti, Chris and Cvicek, Vaclav and Fisher, Zachary and Pham, Philip and Ravula, Anirudh and Sanghai, Sumit and Wang, Qifan and Yang, Li},
  year = {2020},
  month = sep,
  abstract = {Transformer models have advanced the state of the art in many Natural Language Processing (NLP) tasks. In this paper, we present a new Transformer architecture, Extended Transformer Construction (ETC), that addresses two key challenges of standard Transformer architectures, namely scaling input length and encoding structured inputs. To scale attention to longer inputs, we introduce a novel global-local attention mechanism between global tokens and regular input tokens. We also show that combining global-local attention with relative position encodings and a Contrastive Predictive Coding (CPC) pre-training objective allows ETC to encode structured inputs. We achieve state-of-the-art results on four natural language datasets requiring long and/or structured inputs.},
  archivePrefix = {arXiv},
  eprint = {2004.08483},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ainslie et al_2020_etc.pdf},
  journal = {arXiv:2004.08483 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{ainsworth18_oiVAEOutputinterpretable,
  title = {Oi-{{VAE}}: {{Output}} Interpretable {{VAEs}} for Nonlinear Group Factor Analysis},
  author = {Ainsworth, Samuel K. and Foti, Nicholas J. and Lee, Adrian K. C. and Fox, Emily B.},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {119--128},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm Sweden}},
  abstract = {Deep generative models have recently yielded encouraging results in producing subjectively realistic samples of complex data. Far less attention has been paid to making these generative models interpretable. In many scenarios, ranging from scientific applications to finance, the observed variables have a natural grouping. It is often of interest to understand systems of interaction amongst these groups, and latent factor models (LFMs) are an attractive approach. However, traditional LFMs are limited by assuming a linear correlation structure. We present an output interpretable VAE (oi-VAE) for grouped data that models complex, nonlinear latent-to-observed relationships. We combine a structured VAE comprised of group-specific generators with a sparsity-inducing prior. We demonstrate that oi-VAE yields meaningful notions of interpretability in the analysis of motion capture and MEG data. We further show that in these situations, the regularization inherent to oi-VAE can actually lead to improved generalization and learned generative processes.},
  file = {/home/trung/GoogleDrive/Zotero/ainsworth et al_2018_oi-vae.pdf},
  keywords = {_tablet},
  pdf = {http://proceedings.mlr.press/v80/ainsworth18a/ainsworth18a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{akhundov19_VariationalTrackingPrediction,
  title = {Variational {{Tracking}} and {{Prediction}} with {{Generative Disentangled State}}-{{Space Models}}},
  author = {Akhundov, Adnan and Soelch, Maximilian and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2019},
  month = oct,
  abstract = {We address tracking and prediction of multiple moving objects in visual data streams as inference and sampling in a disentangled latent state-space model. By encoding objects separately and including explicit position information in the latent state space, we perform tracking via amortized variational Bayesian inference of the respective latent positions. Inference is implemented in a modular neural framework tailored towards our disentangled latent space. Generative and inference model are jointly learned from observations only. Comparing to related prior work, we empirically show that our Markovian state-space assumption enables faithful and much improved long-term prediction well beyond the training horizon. Further, our inference model correctly decomposes frames into objects, even in the presence of occlusions. Tracking performance is increased significantly over prior art.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.06205},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/UP4UHMUW/Akhundov et al. - 2019 - Variational Tracking and Prediction with Generativ.pdf},
  journal = {arXiv:1910.06205 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{akiba19_OptunaNextgenerationHyperparameter,
  title = {Optuna: {{A Next}}-Generation {{Hyperparameter Optimization Framework}}},
  shorttitle = {Optuna},
  author = {Akiba, Takuya and Sano, Shotaro and Yanase, Toshihiko and Ohta, Takeru and Koyama, Masanori},
  year = {2019},
  month = jul,
  abstract = {The purpose of this study is to introduce new design-criteria for next-generation hyperparameter optimization software. The criteria we propose include (1) define-by-run API that allows users to construct the parameter search space dynamically, (2) efficient implementation of both searching and pruning strategies, and (3) easy-to-setup, versatile architecture that can be deployed for various purposes, ranging from scalable distributed computing to light-weight experiment conducted via interactive interface. In order to prove our point, we will introduce Optuna, an optimization software which is a culmination of our effort in the development of a next generation optimization software. As an optimization software designed with define-by-run principle, Optuna is particularly the first of its kind. We will present the design-techniques that became necessary in the development of the software that meets the above criteria, and demonstrate the power of our new design through experimental results and real world applications. Our software is available under the MIT license (https://github.com/pfnet/optuna/).},
  archivePrefix = {arXiv},
  eprint = {1907.10902},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/akiba et al_2019_optuna.pdf;/home/trung/Zotero/storage/KB7A5QFJ/1907.html},
  journal = {arXiv:1907.10902 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{akinola19_ProgressiveDisentanglementUsing,
  title = {Progressive {{Disentanglement Using Relevant Factor VAE}}},
  author = {Akinola, Iretiayo and Fobi, Simone},
  year = {2019},
  pages = {4},
  abstract = {Unsupervised learning of meaningful disentanglement remains an open challenge. This problem has roughly two perpendicular objectives: dimensionality reduction of high-dimensional data (such as images) to a low-dimensional latent space, and enforcing a disentanglement structure on the obtained latent space. It has been shown that improved performance in one can potentially hurt the other i.e. increased disentanglement can reduce reconstruction quality. Previous works have developed various reformulations to better decouple these objectives but there is always still a connection which often requires hyperparameter search / tuning to find the right trade-off. In this work, we propose a systematic approach that automatically adapts the relative weights of both components to obtain the right trade-off. Based on the Factor VAE approach, our method can adaptively increase or decrease the weight of disentanglement objective as a function of the discriminator performance. This makes the unsupervised learning process insensitive to the initial choice of hyperparameters and dataset-agnostic. Our approach also enables a learning curriculum that initially places focus on the reconstruction and adaptively shifts emphasis to learning disentanglements.},
  file = {/home/trung/GoogleDrive/Zotero/akinola et al_2019_progressive disentanglement using relevant factor vae.pdf},
  keywords = {_tablet,disentanglement},
  language = {en}
}

@misc{akiyama00_ASCIIArtSynthesis,
  title = {{{ASCII Art Synthesis}} with {{Convolutional Networks}}},
  author = {Akiyama, Osamu},
  file = {/home/trung/GoogleDrive/Zotero/akiyama_ascii art synthesis with convolutional networks.pdf},
  howpublished = {https://nips2017creativity.github.io/doc/ASCII\_Art\_Synthesis.pdf}
}

@article{aktay20_GoogleCOVID19Community,
  title = {Google {{COVID}}-19 {{Community Mobility Reports}}: {{Anonymization Process Description}} (Version 1.0)},
  shorttitle = {Google {{COVID}}-19 {{Community Mobility Reports}}},
  author = {Aktay, Ahmet and Bavadekar, Shailesh and Cossoul, Gwen and Davis, John and Desfontaines, Damien and Fabrikant, Alex and Gabrilovich, Evgeniy and Gadepalli, Krishna and Gipson, Bryant and Guevara, Miguel and Kamath, Chaitanya and Kansal, Mansi and Lange, Ali and Mandayam, Chinmoy and Oplinger, Andrew and Pluntke, Christopher and Roessler, Thomas and Schlosberg, Arran and Shekel, Tomer and Vispute, Swapnil and Vu, Mia and Wellenius, Gregory and Williams, Brian and Wilson, Royce J.},
  year = {2020},
  month = apr,
  abstract = {This document describes the aggregation and anonymization process applied to the initial version of Google COVID-19 Community Mobility Reports (published at http://google.com/covid19/mobility on April 2, 2020), a publicly available resource intended to help public health authorities understand what has changed in response to work-from-home, shelter-in-place, and other recommended policies aimed at flattening the curve of the COVID-19 pandemic. Our anonymization process is designed to ensure that no personal data, including an individual's location, movement, or contacts, can be derived from the resulting metrics. The high-level description of the procedure is as follows: we first generate a set of anonymized metrics from the data of Google users who opted in to Location History. Then, we compute percentage changes of these metrics from a baseline based on the historical part of the anonymized metrics. We then discard a subset which does not meet our bar for statistical reliability, and release the rest publicly in a format that compares the result to the private baseline.},
  archivePrefix = {arXiv},
  eprint = {2004.04145},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/aktay et al_2020_google covid-19 community mobility reports.pdf},
  journal = {arXiv:2004.04145 [cs]},
  primaryClass = {cs}
}

@article{akujuobi19_RecurrentAttentionWalk,
  title = {Recurrent {{Attention Walk}} for {{Semi}}-Supervised {{Classification}}},
  author = {Akujuobi, Uchenna and Zhang, Qiannan and Yufei, Han and Zhang, Xiangliang},
  year = {2019},
  month = oct,
  abstract = {In this paper, we study the graph-based semi-supervised learning for classifying nodes in attributed networks, where the nodes and edges possess content information. Recent approaches like graph convolution networks and attention mechanisms have been proposed to ensemble the first-order neighbors and incorporate the relevant neighbors. However, it is costly (especially in memory) to consider all neighbors without a prior differentiation. We propose to explore the neighborhood in a reinforcement learning setting and find a walk path well-tuned for classifying the unlabelled target nodes. We let an agent (of node classification task) walk over the graph and decide where to direct to maximize classification accuracy. We define the graph walk as a partially observable Markov decision process (POMDP). The proposed method is flexible for working in both transductive and inductive setting. Extensive experiments on four datasets demonstrate that our proposed method outperforms several state-of-the-art methods. Several case studies also illustrate the meaningful movement trajectory made by the agent.},
  archivePrefix = {arXiv},
  eprint = {1910.10266},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/akujuobi et al_2019_recurrent attention walk for semi-supervised classification.pdf},
  journal = {arXiv:1910.10266 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Computer Science - Social and Information Networks,information,recurrent,semi supervised,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{alaa20_ahmedmalaadeeplearninguncertainty,
  title = {Ahmedmalaa/Deep-Learning-Uncertainty},
  author = {Alaa, Ahmed M.},
  year = {2020},
  month = oct,
  abstract = {Literature survey, paper reviews, experimental setups and a collection of implementations for baselines methods for predictive uncertainty estimation in deep learning models.}
}

@article{alam19_SurveyDeepNeural,
  title = {Survey on {{Deep Neural Networks}} in {{Speech}} and {{Vision Systems}}},
  author = {Alam, Mahbubul and Samad, Manar D. and Vidyaratne, Lasitha and Glandon, Alexander and Iftekharuddin, Khan M.},
  year = {2019},
  month = nov,
  abstract = {This survey presents a review of state-of-the-art deep neural network architectures, algorithms, and systems in vision and speech applications. Recent advances in deep artificial neural network algorithms and architectures have spurred rapid innovation and development of intelligent vision and speech systems. With availability of vast amounts of sensor data and cloud computing for processing and training of deep neural networks, and with increased sophistication in mobile and embedded technology, the next-generation intelligent systems are poised to revolutionize personal and commercial computing. This survey begins by providing background and evolution of some of the most successful deep learning models for intelligent vision and speech systems to date. An overview of large-scale industrial research and development efforts is provided to emphasize future trends and prospects of intelligent vision and speech systems. Robust and efficient intelligent systems demand low-latency and high fidelity in resource-constrained hardware platforms such as mobile devices, robots, and automobiles. Therefore, this survey also provides a summary of key challenges and recent successes in running deep neural networks on hardware-restricted platforms, i.e. within limited memory, battery life, and processing capabilities. Finally, emerging applications of vision and speech across disciplines such as affective computing, intelligent transportation, and precision medicine are discussed. To our knowledge, this paper provides one of the most comprehensive surveys on the latest developments in intelligent vision and speech applications from the perspectives of both software and hardware systems. Many of these emerging technologies using deep neural networks show tremendous promise to revolutionize research and development for future vision and speech systems.},
  archivePrefix = {arXiv},
  eprint = {1908.07656},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alam et al_2019_survey on deep neural networks in speech and vision systems.pdf},
  journal = {arXiv:1908.07656 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{albalawi20_UsingTopicModeling,
  title = {Using {{Topic Modeling Methods}} for {{Short}}-{{Text Data}}: {{A Comparative Analysis}}},
  shorttitle = {Using {{Topic Modeling Methods}} for {{Short}}-{{Text Data}}},
  author = {Albalawi, Rania and Yeap, Tet Hin and Benyoucef, Morad},
  year = {2020},
  month = jul,
  volume = {3},
  pages = {42},
  issn = {2624-8212},
  doi = {10.3389/frai.2020.00042},
  abstract = {With the growth of online social network platforms and applications, large amounts of textual user-generated content are created daily in the form of comments, reviews, and short-text messages. As a result, users often find it challenging to discover useful information or more on the topic being discussed from such content. Machine learning and natural language processing algorithms are used to analyze the massive amount of textual social media data available online, including topic modeling techniques that have gained popularity in recent years. This paper investigates the topic modeling subject and its common application areas, methods, and tools. Also, we examine and compare five frequently used topic modeling methods, as applied to short textual social data, to show their benefits practically in detecting important topics. These methods are latent semantic analysis, latent Dirichlet allocation, non-negative matrix factorization, random projection, and principal component analysis. Two textual datasets were selected to evaluate the performance of included topic modeling methods based on the topic quality and some standard statistical evaluation metrics, like recall, precision, F-score, and topic coherence. As a result, latent Dirichlet allocation and non-negative matrix factorization methods delivered more meaningful extracted topics and obtained good results. The paper sheds light on some common topic modeling methods in a short-text context and provides direction for researchers who seek to apply these methods.},
  file = {/home/trung/GoogleDrive/Zotero/albalawi et al_2020_using topic modeling methods for short-text data.pdf},
  journal = {Frontiers in Artificial Intelligence},
  language = {en}
}

@article{alemi17_FixingBrokenELBO,
  title = {Fixing a {{Broken ELBO}}},
  author = {Alemi, Alexander A. and Poole, Ben and Fischer, Ian and Dillon, Joshua V. and Saurous, Rif A. and Murphy, Kevin},
  year = {2017},
  month = nov,
  abstract = {Recent work in unsupervised representation learning has focused on learning deep directed latent-variable models. Fitting these models by maximizing the marginal likelihood or evidence is typically intractable, thus a common approximation is to maximize the evidence lower bound (ELBO) instead. However, maximum likelihood training (whether exact or approximate) does not necessarily result in a good latent representation, as we demonstrate both theoretically and empirically. In particular, we derive variational lower and upper bounds on the mutual information between the input and the latent variable, and use these bounds to derive a rate-distortion curve that characterizes the tradeoff between compression and reconstruction accuracy. Using this framework, we demonstrate that there is a family of models with identical ELBO, but different quantitative and qualitative characteristics. Our framework also suggests a simple new method to ensure that latent variable models with powerful stochastic decoders do not ignore their latent code.},
  archivePrefix = {arXiv},
  eprint = {1711.00464},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alemi et al_2017_fixing a broken elbo.pdf;/home/trung/Zotero/storage/WK9YJJ4Z/1711.html},
  journal = {arXiv:1711.00464 [cs, stat]},
  keywords = {Computer Science - Machine Learning,elbo broken,favorite,Statistics - Machine Learning,vae_issues,variational},
  primaryClass = {cs, stat}
}

@article{alemi18_UncertaintyVariationalInformation,
  title = {Uncertainty in the {{Variational Information Bottleneck}}},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V.},
  year = {2018},
  month = jul,
  abstract = {We present a simple case study, demonstrating that Variational Information Bottleneck (VIB) can improve a network's classification calibration as well as its ability to detect out-of-distribution data. Without explicitly being designed to do so, VIB gives two natural metrics for handling and quantifying uncertainty.},
  archivePrefix = {arXiv},
  eprint = {1807.00906},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alemi et al_2018_uncertainty in the variational information bottleneck.pdf},
  journal = {arXiv:1807.00906 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{alemi19_DeepVariationalInformation,
  title = {Deep {{Variational Information Bottleneck}}},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  year = {2019},
  month = oct,
  abstract = {We present a variational approximation to the information bottleneck of Tishby et al. (1999). This variational approach allows us to parameterize the information bottleneck model using a neural network and leverage the reparameterization trick for efficient training. We call this method "Deep Variational Information Bottleneck", or Deep VIB. We show that models trained with the VIB objective outperform those that are trained with other forms of regularization, in terms of generalization performance and robustness to adversarial attack.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1612.00410},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alemi et al_2019_deep variational information bottleneck.pdf;/home/trung/Zotero/storage/4QMJFUEU/1612.html},
  journal = {arXiv:1612.00410 [cs, math]},
  keywords = {information},
  primaryClass = {cs, math}
}

@article{alemi19_GILBOOneMetric,
  title = {{{GILBO}}: {{One Metric}} to {{Measure Them All}}},
  shorttitle = {{{GILBO}}},
  author = {Alemi, Alexander A. and Fischer, Ian},
  year = {2019},
  month = jan,
  abstract = {We propose a simple, tractable lower bound on the mutual information contained in the joint generative density of any latent variable generative model: the GILBO (Generative Information Lower BOund). It offers a data-independent measure of the complexity of the learned latent variable description, giving the log of the effective description length. It is well-defined for both VAEs and GANs. We compute the GILBO for 800 GANs and VAEs each trained on four datasets (MNIST, FashionMNIST, CIFAR-10 and CelebA) and discuss the results.},
  archivePrefix = {arXiv},
  eprint = {1802.04874},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alemi et al_2019_gilbo.pdf},
  journal = {arXiv:1802.04874 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{alemi19_VariationalPredictiveInformation,
  title = {Variational {{Predictive Information Bottleneck}}},
  author = {Alemi, Alexander A.},
  year = {2019},
  month = oct,
  abstract = {In classic papers, Zellner demonstrated that Bayesian inference could be derived as the solution to an information theoretic functional. Below we derive a generalized form of this functional as a variational lower bound of a predictive information bottleneck objective. This generalized functional encompasses most modern inference procedures and suggests novel ones.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1910.10831},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alemi_2019_variational predictive information bottleneck.pdf;/home/trung/Zotero/storage/IFB5HFND/1910.html},
  journal = {arXiv:1910.10831 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,information,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{alemi19_VariationalPredictiveInformationa,
  title = {Variational {{Predictive Information Bottleneck}}},
  author = {Alemi, Alexander A.},
  year = {2019},
  month = oct,
  abstract = {In classic papers, Zellner demonstrated that Bayesian inference could be derived as the solution to an information theoretic functional. Below we derive a generalized form of this functional as a variational lower bound of a predictive information bottleneck objective. This generalized functional encompasses most modern inference procedures and suggests novel ones.},
  archivePrefix = {arXiv},
  eprint = {1910.10831},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/false;/home/trung/GoogleDrive/Zotero/alemi_2019_variational predictive information bottleneck2.pdf},
  journal = {arXiv:1910.10831 [cs, math, stat]},
  keywords = {information},
  primaryClass = {cs, math, stat}
}

@article{alet20_Tailoringencodinginductive,
  title = {Tailoring: Encoding Inductive Biases by Optimizing Unsupervised Objectives at Prediction Time},
  shorttitle = {Tailoring},
  author = {Alet, Ferran and Kawaguchi, Kenji and {Lozano-Perez}, Tomas and Kaelbling, Leslie Pack},
  year = {2020},
  month = sep,
  abstract = {From CNNs to attention mechanisms, encoding inductive biases into neural networks has been a fruitful source of improvement in machine learning. Auxiliary losses are a general way of encoding biases in order to help networks learn better representations by adding extra terms to the loss function. However, since they are minimized on the training data, they suffer from the same generalization gap as regular task losses. Moreover, by changing the loss function, the network is optimizing a different objective than the one we care about. In this work we solve both problems: first, we take inspiration from transductive learning and note that, after receiving an input but before making a prediction, we can fine-tune our models on any unsupervised objective. We call this process tailoring, because we customize the model to each input. Second, we formulate a nested optimization (similar to those in meta-learning) and train our models to perform well on the task loss after adapting to the tailoring loss. The advantages of tailoring and meta-tailoring are discussed theoretically and demonstrated empirically on several diverse examples: encoding inductive conservation laws from physics to improve predictions, improving local smoothness to increase robustness to adversarial examples, and using contrastive losses on the query image to improve generalization.},
  archivePrefix = {arXiv},
  eprint = {2009.10623},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alet et al_2020_tailoring.pdf},
  journal = {arXiv:2009.10623 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{aliakbarian19_SamplingGoodLatent,
  title = {Sampling {{Good Latent Variables}} via {{CPP}}-{{VAEs}}: {{VAEs}} with {{Condition Posterior}} as {{Prior}}},
  shorttitle = {Sampling {{Good Latent Variables}} via {{CPP}}-{{VAEs}}},
  author = {Aliakbarian, Sadegh and Saleh, Fatemeh Sadat and Salzmann, Mathieu and Petersson, Lars and Gould, Stephen},
  year = {2019},
  month = dec,
  abstract = {In practice, conditional variational autoencoders (CVAEs) perform conditioning by combining two sources of information which are computed completely independently; CVAEs first compute the condition, then sample the latent variable, and finally concatenate these two sources of information. However, these two processes should be tied together such that the model samples a latent variable given the conditioning signal. In this paper, we directly address this by conditioning the sampling of the latent variable on the CVAE condition, thus encouraging it to carry relevant information. We study this specifically for tasks that leverage with strong conditioning signals and where the generative models have highly expressive decoders able to generate a sample based on the information contained in the condition solely. In particular, we experiments with the two challenging tasks of diverse human motion generation and diverse image captioning, for which our results suggest that unifying latent variable sampling and conditioning not only yields samples of higher quality, but also helps the model to avoid the posterior collapse, a known problem of VAEs with expressive decoders.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.08521},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/aliakbarian et al_2019_sampling good latent variables via cpp-vaes.pdf;/home/trung/Zotero/storage/K4R3KDYH/1912.html},
  journal = {arXiv:1912.08521 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{aliakbarian20_SemanticallyPlausibleDiverse,
  title = {Semantically {{Plausible}} and {{Diverse 3D Human Motion Prediction}}},
  author = {Aliakbarian, Sadegh and Saleh, Fatemeh Sadat and Salzmann, Mathieu and Petersson, Lars and Gould, Stephen},
  year = {2020},
  month = jul,
  abstract = {We tackle the task of diverse 3D human motion prediction, that is, forecasting multiple plausible future 3D poses given a sequence of observed 3D poses. In this context, a popular approach consists of using Conditional Variational Autoencoders (CVAEs). Existing approaches to generate diverse motions either fail to capture the diversity in human motion or fail to generate diverse but semantically plausible continuations of an observed motion. In this paper, we address both of these problems by developing a new variational framework that accounts for both diversity and semantic of the generated future motion. Unlike existing approaches that sampling the latent variable is independent of the conditioning, our approach conditions the sampling process of the latent variable on the representation of the past observation, thus encouraging it to carry relevant information. Our experiments demonstrate that our approach not only yields motions of higher quality while retaining diversity, but also generates motions that preserve semantic information contained in the observed 3D pose sequence.},
  archivePrefix = {arXiv},
  eprint = {1912.08521},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/aliakbarian et al_2020_semantically plausible and diverse 3d human motion prediction.pdf},
  journal = {arXiv:1912.08521 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{aliasparthgoyal17_VariationalWalkbackLearning,
  title = {Variational {{Walkback}}: {{Learning}} a {{Transition Operator}} as a {{Stochastic Recurrent Net}}},
  shorttitle = {Variational {{Walkback}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {ALIAS PARTH GOYAL, Anirudh Goyal and Ke, Nan Rosemary and Ganguli, Surya and Bengio, Yoshua},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4392--4402},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/alias parth goyal et al_2017_variational walkback.pdf;/home/trung/Zotero/storage/P7S2MI43/7026-variational-walkback-learning-a-transition-operator-as-a-stochastic-recurrent-net.html},
  keywords = {variational}
}

@article{allen-zhu19_ConvergenceTheoryDeep,
  title = {A {{Convergence Theory}} for {{Deep Learning}} via {{Over}}-{{Parameterization}}},
  author = {{Allen-Zhu}, Zeyuan and Li, Yuanzhi and Song, Zhao},
  year = {2019},
  month = jun,
  abstract = {Deep neural networks (DNNs) have demonstrated dominating performance in many fields; since AlexNet, networks used in practice are going wider and deeper. On the theoretical side, a long line of works has been focusing on training neural networks with one hidden layer. The theory of multi-layer networks remains largely unsettled. In this work, we prove why stochastic gradient descent (SGD) can find \$\textbackslash textit\{global minima\}\$ on the training objective of DNNs in \$\textbackslash textit\{polynomial time\}\$. We only make two assumptions: the inputs are non-degenerate and the network is over-parameterized. The latter means the network width is sufficiently large: \$\textbackslash textit\{polynomial\}\$ in \$L\$, the number of layers and in \$n\$, the number of samples. Our key technique is to derive that, in a sufficiently large neighborhood of the random initialization, the optimization landscape is almost-convex and semi-smooth even with ReLU activations. This implies an equivalence between over-parameterized neural networks and neural tangent kernel (NTK) in the finite (and polynomial) width setting. As concrete examples, starting from randomly initialized weights, we prove that SGD can attain 100\% training accuracy in classification tasks, or minimize regression loss in linear convergence speed, with running time polynomial in \$n,L\$. Our theory applies to the widely-used but non-smooth ReLU activation, and to any smooth and possibly non-convex loss functions. In terms of network architectures, our theory at least applies to fully-connected neural networks, convolutional neural networks (CNN), and residual neural networks (ResNet).},
  archivePrefix = {arXiv},
  eprint = {1811.03962},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/allen-zhu et al_2019_a convergence theory for deep learning via over-parameterization.pdf},
  journal = {arXiv:1811.03962 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{allen-zhu20_BackwardFeatureCorrection,
  title = {Backward {{Feature Correction}}: {{How Deep Learning Performs Deep Learning}}},
  shorttitle = {Backward {{Feature Correction}}},
  author = {{Allen-Zhu}, Zeyuan and Li, Yuanzhi},
  year = {2020},
  month = jan,
  abstract = {How does a 110-layer ResNet learn a high-complexity classifier using relatively few training examples and short training time? We present a theory towards explaining this in terms of hierarchical learning. We refer hierarchical learning as the learner learns to represent a complicated target function by decomposing it into a sequence of simpler functions to reduce sample and time complexity. This paper formally analyzes how multi-layer neural networks can perform such hierarchical learning efficiently and automatically simply by applying stochastic gradient descent (SGD) to the training objective.},
  archivePrefix = {arXiv},
  eprint = {2001.04413},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/JN69TI26/Allen-Zhu and Li - 2020 - Backward Feature Correction How Deep Learning Per.pdf},
  journal = {arXiv:2001.04413 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{allen19_AnalogiesExplainedUnderstanding,
  title = {Analogies {{Explained}}: {{Towards Understanding Word Embeddings}}},
  shorttitle = {Analogies {{Explained}}},
  author = {Allen, Carl and Hospedales, Timothy},
  year = {2019},
  month = jan,
  abstract = {Word embeddings generated by neural network methods such as word2vec (W2V) are well known to exhibit seemingly linear behaviour, e.g. the embeddings of analogy "woman is to queen as man is to king" approximately describe a parallelogram. This property is particularly intriguing since the embeddings are not trained to achieve it. Several explanations have been proposed, but each introduces assumptions that do not hold in practice. We derive a probabilistically grounded definition of paraphrasing that we re-interpret as word transformation, a mathematical description of "\$w\_x\$ is to \$w\_y\$". From these concepts we prove existence of linear relationships between W2V-type embeddings that underlie the analogical phenomenon, identifying explicit error terms.},
  archivePrefix = {arXiv},
  eprint = {1901.09813},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/allen et al_2019_analogies explained.pdf;/home/trung/Zotero/storage/QXGP69VG/1901.html},
  journal = {arXiv:1901.09813 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{alperstein19_AllSMILESVariational,
  title = {All {{SMILES Variational Autoencoder}}},
  author = {Alperstein, Zaccary and Cherkasov, Artem and Rolfe, Jason Tyler},
  year = {2019},
  month = jun,
  abstract = {Variational autoencoders (VAEs) defined over SMILES string and graph-based representations of molecules promise to improve the optimization of molecular properties, thereby revolutionizing the pharmaceuticals and materials industries. However, these VAEs are hindered by the non-unique nature of SMILES strings and the computational cost of graph convolutions. To efficiently pass messages along all paths through the molecular graph, we encode multiple SMILES strings of a single molecule using a set of stacked recurrent neural networks, pooling hidden representations of each atom between SMILES representations, and use attentional pooling to build a final fixed-length latent representation. By then decoding to a disjoint set of SMILES strings of the molecule, our All SMILES VAE learns an almost bijective mapping between molecules and latent representations near the high-probability-mass subspace of the prior. Our SMILES-derived but molecule-based latent representations significantly surpass the state-of-the-art in a variety of fully- and semi-supervised property regression and molecular property optimization tasks.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1905.13343},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/alperstein et al_2019_all smiles variational autoencoder.pdf;/home/trung/Zotero/storage/MNATB4I7/1905.html},
  journal = {arXiv:1905.13343 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{alvarenga20_SafetyEffectivenessCopaiba,
  title = {Safety and {{Effectiveness}} of {{Copaiba Oleoresin}} ({{C}}. Reticulata {{Ducke}}) on {{Inflammation}} and {{Tissue Repair}} of {{Oral Wounds}} in {{Rats}}},
  author = {Alvarenga, Mar{\'i}a Olimpia Paz and Bittencourt, Leonardo Oliveira and Mendes, Paulo Fernando Santos and Ribeiro, Julia Turra and Lameira, Osmar Alves and Monteiro, Marta Chagas and Barboza, Carlos Augusto Galv{\~a}o and Martins, Manoela Domingues and Lima, Rafael Rodrigues},
  year = {2020},
  month = may,
  volume = {21},
  issn = {1422-0067},
  doi = {10.3390/ijms21103568},
  abstract = {In traditional communities of the Brazilian Amazon, the copaiba oleoresin (C. reticulata Ducke) is widely known for its therapeutic activity, especially its wound healing and anti-inflammatory actions. Our study aimed to evaluate these effects in oral lesions and the safety of the dosage proposed. A punch biopsy wound was induced on the ventral surface of the tongue of forty-five male Wistar rats under anesthesia. Animals were randomly allocated to one of three groups based on the treatment: control, corticoid and copaiba. A daily dose of each treatment and vehicle was administrated by oral gavage for three consecutive days. Sample collections took place on the third, seventh and 15th days post-wounding for clinical and histopathological analyses. Blood was collected on the third and seventh days for kidneys and liver function tests. Semi-quantitative analyses were performed based on scores of inflammation and reepithelization. Tissue collagen deposition was detected by PicroSirius red staining. Copaiba-treated wounds revealed a smaller wound area, decreased of acute inflammatory reaction and enhanced reepithelization. The levels of kidney and liver function tests did not reveal presence of damage post-treatments. Our findings suggest that copaiba oleoresin is a safe and effective alternative therapy for inflammation and tissue repair of oral wounds in this animal model.},
  file = {/home/trung/GoogleDrive/Zotero/alvarenga et al_2020_safety and effectiveness of copaiba oleoresin (c.pdf},
  journal = {International Journal of Molecular Sciences},
  number = {10},
  pmcid = {PMC7278981},
  pmid = {32443593}
}

@article{ambrogioni20_Automaticstructuredvariational,
  title = {Automatic Structured Variational Inference},
  author = {Ambrogioni, Luca and Hinne, Max and {van Gerven}, Marcel},
  year = {2020},
  month = feb,
  abstract = {The aim of probabilistic programming is to automatize every aspect of probabilistic inference in arbitrary probabilistic models (programs) so that the user can focus her attention on modeling, without dealing with ad-hoc inference methods. Gradient based automatic differentiation stochastic variational inference offers an attractive option as the default method for (differentiable) probabilistic programming as it combines high performance with high computational efficiency. However, the performance of any (parametric) variational approach depends on the choice of an appropriate variational family. Here, we introduced a fully automatic method for constructing structured variational families inspired to the closed-form update in conjugate models. These pseudo-conjugate families incorporate the forward pass of the input probabilistic program and can capture complex statistical dependencies. Pseudo-conjugate families have the same space and time complexity of the input probabilistic program and are therefore tractable in a very large class of models. We validate our automatic variational method on a wide range of high dimensional inference problems including deep learning components.},
  archivePrefix = {arXiv},
  eprint = {2002.00643},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/EZ55K59Y/Ambrogioni et al. - 2020 - Automatic structured variational inference.pdf},
  journal = {arXiv:2002.00643 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{amezquita19_Orchestratingsinglecellanalysis,
  title = {Orchestrating Single-Cell Analysis with {{Bioconductor}}},
  author = {Amezquita, Robert A. and Lun, Aaron T. L. and Becht, Etienne and Carey, Vince J. and Carpp, Lindsay N. and Geistlinger, Ludwig and Marini, Federico and {Rue-Albrecht}, Kevin and Risso, Davide and Soneson, Charlotte and Waldron, Levi and Pag{\`e}s, Herv{\'e} and Smith, Mike L. and Huber, Wolfgang and Morgan, Martin and Gottardo, Raphael and Hicks, Stephanie C.},
  year = {2019},
  month = dec,
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0654-x},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/R67I9PI9/Amezquita et al. - 2019 - Orchestrating single-cell analysis with Bioconduct.pdf},
  journal = {Nature Methods},
  language = {en}
}

@inproceedings{amini18_VariationalAutoencoderEndtoEnd,
  title = {Variational {{Autoencoder}} for {{End}}-to-{{End Control}} of {{Autonomous Driving}} with {{Novelty Detection}} and {{Training De}}-Biasing},
  booktitle = {2018 {{IEEE}}/{{RSJ International Conference}} on {{Intelligent Robots}} and {{Systems}} ({{IROS}})},
  author = {Amini, Alexander and Schwarting, Wilko and Rosman, Guy and Araki, Brandon and Karaman, Sertac and Rus, Daniela},
  year = {2018},
  month = oct,
  pages = {568--575},
  publisher = {{IEEE}},
  address = {{Madrid}},
  doi = {10.1109/IROS.2018.8594386},
  abstract = {This paper introduces a new method for end-toend training of deep neural networks (DNNs) and evaluates it in the context of autonomous driving. DNN training has been shown to result in high accuracy for perception to action learning given sufficient training data. However, the trained models may fail without warning in situations with insufficient or biased training data. In this paper, we propose and evaluate a novel architecture for self-supervised learning of latent variables to detect the insufficiently trained situations. Our method also addresses training data imbalance, by learning a set of underlying latent variables that characterize the training data and evaluate potential biases. We show how these latent distributions can be leveraged to adapt and accelerate the training pipeline by training on only a fraction of the total dataset. We evaluate our approach on a challenging dataset for driving. The data is collected from a full-scale autonomous vehicle. Our method provides qualitative explanation for the latent variables learned in the model. Finally, we show how our model can be additionally trained as an end-to-end controller, directly outputting a steering control command for an autonomous vehicle.},
  annotation = {ZSCC: 0000014},
  file = {/home/trung/GoogleDrive/Zotero/amini et al_2018_variational autoencoder for end-to-end control of autonomous driving with novelty detection and training de-biasing.pdf},
  isbn = {978-1-5386-8094-0},
  language = {en}
}

@inproceedings{amini19_UncoveringMitigatingAlgorithmic,
  title = {Uncovering and {{Mitigating Algorithmic Bias}} through {{Learned Latent Structure}}},
  booktitle = {Proceedings of the 2019 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Amini, Alexander and Soleimany, Ava P. and Schwarting, Wilko and Bhatia, Sangeeta N. and Rus, Daniela},
  year = {2019},
  month = jan,
  pages = {289--295},
  publisher = {{ACM}},
  address = {{Honolulu HI USA}},
  doi = {10.1145/3306618.3314243},
  abstract = {Recent research has highlighted the vulnerabilities of modern machine learning based systems to bias, especially towards segments of society that are under-represented in training data. In this work, we develop a novel, tunable algorithm for mitigating the hidden, and potentially unknown, biases within training data. Our algorithm fuses the original learning task with a variational autoencoder to learn the latent structure within the dataset and then adaptively uses the learned latent distributions to re-weight the importance of certain data points while training. While our method is generalizable across various data modalities and learning tasks, in this work we use our algorithm to address the issue of racial and gender bias in facial detection systems. We evaluate our algorithm on the Pilot Parliaments Benchmark (PPB), a dataset specifically designed to evaluate biases in computer vision systems, and demonstrate increased overall performance as well as decreased categorical bias with our debiasing approach.},
  annotation = {ZSCC: 0000017},
  file = {/home/trung/GoogleDrive/Zotero/amini et al_2019_uncovering and mitigating algorithmic bias through learned latent structure.pdf},
  isbn = {978-1-4503-6324-2},
  language = {en}
}

@article{amini19_VariationalEndtoEndNavigation,
  title = {Variational {{End}}-to-{{End Navigation}} and {{Localization}}},
  author = {Amini, Alexander and Rosman, Guy and Karaman, Sertac and Rus, Daniela},
  year = {2019},
  month = jun,
  abstract = {Deep learning has revolutionized the ability to learn ``end-to-end'' autonomous vehicle control directly from raw sensory data. While there have been recent extensions to handle forms of navigation instruction, these works are unable to capture the full distribution of possible actions that could be taken and to reason about localization of the robot within the environment. In this paper, we extend end-to-end driving networks with the ability to perform point-to-point navigation as well as probabilistic localization using only noisy GPS data. We define a novel variational network capable of learning from raw camera data of the environment as well as higher level roadmaps to predict (1) a full probability distribution over the possible control commands; and (2) a deterministic control command capable of navigating on the route specified within the map. Additionally, we formulate how our model can be used to localize the robot according to correspondences between the map and the observed visual road topology, inspired by the rough localization that human drivers can perform. We test our algorithms on real-world driving data that the vehicle has never driven through before, and integrate our point-topoint navigation algorithms onboard a full-scale autonomous vehicle for real-time performance. Our localization algorithm is also evaluated over a new set of roads and intersections to demonstrates rough pose localization even in situations without any GPS prior.},
  annotation = {ZSCC: 0000024},
  archivePrefix = {arXiv},
  eprint = {1811.10119},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/amini et al_2019_variational end-to-end navigation and localization.pdf},
  journal = {arXiv:1811.10119 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{amirinezhad20_ActiveLearningCausal,
  title = {Active {{Learning}} of {{Causal Structures}} with {{Deep Reinforcement Learning}}},
  author = {Amirinezhad, Amir and Salehkaleybar, Saber and Hashemi, Matin},
  year = {2020},
  month = sep,
  abstract = {We study the problem of experiment design to learn causal structures from interventional data. We consider an active learning setting in which the experimenter decides to intervene on one of the variables in the system in each step and uses the results of the intervention to recover further causal relationships among the variables. The goal is to fully identify the causal structures with minimum number of interventions. We present the first deep reinforcement learning based solution for the problem of experiment design. In the proposed method, we embed input graphs to vectors using a graph neural network and feed them to another neural network which outputs a variable for performing intervention in each step. Both networks are trained jointly via a Q-iteration algorithm. Experimental results show that the proposed method achieves competitive performance in recovering causal structures with respect to previous works, while significantly reducing execution time in dense graphs.},
  archivePrefix = {arXiv},
  eprint = {2009.03009},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/amirinezhad et al_2020_active learning of causal structures with deep reinforcement learning.pdf},
  journal = {arXiv:2009.03009 [cs]},
  keywords = {causal},
  primaryClass = {cs}
}

@article{amodio19_SAUCIEExploringSingleCell,
  title = {{{SAUCIE}}: {{Exploring Single}}-{{Cell Data}} with {{Deep Multitasking Neural Networks}}},
  author = {Amodio, Matthew and {van Dijk}, David and Srinivasan, Krishnan and Chen, William S and Mohsen, Hussein and Moon, Kevin R and Campbell, Allison and Zhao, Yujiao and Wang, Xiaomei and Venkataswamy, Manjunatha and Desai, Anita and V., Ravi and Kumar, Priti and Montgomery, Ruth and Wolf, Guy and Krishnaswamy, Smita},
  year = {2019},
  month = jan,
  doi = {10.1101/237065},
  abstract = {Biomedical researchers are generating  high-throughput, high-dimensional single-cell data at a staggering rate. As costs of data generation decrease, experimental design is moving towards measurement of many different single-cell samples in the same dataset. These samples can correspond to different patients, conditions, or treatments. While scalability of methods to datasets of these sizes is a challenge on its own, dealing with large-scale experimental design presents a whole new set of problems, including batch effects and sample comparison issues. Currently, there are no computational tools that can both handle large amounts of data in a scalable manner (many cells) and at the same time deal with many samples (many patients or conditions). Moreover, data analysis currently involves the use of different tools that each operate on their own data representation, not guaranteeing a synchronized analysis pipeline. For instance, data visualization methods can be disjoint and mismatched with the clustering method. For this purpose, we present SAUCIE, a deep neural network that leverages the high degree of parallelization and scalability offered by neural networks, as well as the deep representation of data that can be learned by them to perform many single-cell data analysis tasks, all on a unified representation. A well-known limitation of neural networks is their interpretability. Our key contribution here are newly formulated regularizations (penalties) that render features learned in hidden layers of the neural network interpretable. When large multi-patient datasets are fed into SAUCIE, the various hidden layers contain denoised and batch-corrected data, a low dimensional visualization, unsupervised clustering, as well as other information that can be used to explore the data. We show this capability by analyzing a newly generated 180-sample dataset consisting of T cells from dengue patients in India, measured with mass cytometry. We show that SAUCIE, for the first time, can batch correct and process this 11-million cell data to identify cluster-based signatures of acute dengue infection and create a patient manifold, stratifying immune response to dengue on the basis of single-cell measurements.},
  file = {/home/trung/GoogleDrive/Zotero/amodio et al_2019_saucie.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{amos19_OptNetDifferentiableOptimization,
  title = {{{OptNet}}: {{Differentiable Optimization}} as a {{Layer}} in {{Neural Networks}}},
  shorttitle = {{{OptNet}}},
  author = {Amos, Brandon and Kolter, J. Zico},
  year = {2019},
  month = oct,
  abstract = {This paper presents OptNet, a network architecture that integrates optimization problems (here, specifically in the form of quadratic programs) as individual layers in larger end-to-end trainable deep networks. These layers encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected layers often cannot capture. In this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast GPUbased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems. In one notable example, we show that the method is capable of learning to play mini-Sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than other neural architectures.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1703.00443},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/PKUCDUWQ/Amos and Kolter - 2019 - OptNet Differentiable Optimization as a Layer in .pdf},
  journal = {arXiv:1703.00443 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{an20_AEOTGANTrainingGANs,
  title = {{{AE}}-{{OT}}-{{GAN}}: {{Training GANs}} from Data Specific Latent Distribution},
  shorttitle = {{{AE}}-{{OT}}-{{GAN}}},
  author = {An, Dongsheng and Guo, Yang and Zhang, Min and Qi, Xin and Lei, Na and Yau, Shing-Tung and Gu, Xianfeng},
  year = {2020},
  month = jan,
  abstract = {Though generative adversarial networks (GANs) are prominent models to generate realistic and crisp images, they are unstable to train and suffer from the mode collapse/mixture. The problems of GANs come from approximating the intrinsic discontinuous distribution transform map with continuous DNNs. The recently proposed AE-OT model addresses the discontinuity problem by explicitly computing the discontinuous optimal transform map in the latent space of the autoencoder. Though have no mode collapse/mixture, the generated images by AE-OT are blurry. In this paper, we propose the AE-OT-GAN model to utilize the advantages of the both models: generate high quality images and at the same time overcome the mode collapse/mixture problems. Specifically, we firstly embed the low dimensional image manifold into the latent space by training an autoencoder (AE). Then the extended semi-discrete optimal transport (SDOT) map from the uniform distribution to the empirical latent distribution is used to generate new latent codes. Finally, our GAN model is trained to generate high quality images from the latent distribution induced by the extended SDOT map. The distribution transform map from this dataset related latent distribution to the data distribution will be continuous, and thus can be well approximated by the continuous DNNs. Additionally, the paired data between the latent codes and the real images gives us further restriction about the generator and stabilizes the training process. Experiments on simple MNIST dataset and complex datasets like CIFAR10 and CelebA show the advantages of the proposed method.},
  archivePrefix = {arXiv},
  eprint = {2001.03698},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/Y6N4QZPZ/An et al. - 2020 - AE-OT-GAN Training GANs from data specific latent.pdf},
  journal = {arXiv:2001.03698 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{anantrasirichai20_ArtificialIntelligenceCreative,
  title = {Artificial {{Intelligence}} in the {{Creative Industries}}: {{A Review}}},
  shorttitle = {Artificial {{Intelligence}} in the {{Creative Industries}}},
  author = {Anantrasirichai, Nantheera and Bull, David},
  year = {2020},
  month = sep,
  abstract = {This paper reviews the current state of the art in Artificial Intelligence (AI) technologies and applications in the context of the creative industries. A brief background of AI, and specifically Machine Learning (ML) algorithms, is provided including Convolutional Neural Network (CNNs), Generative Adversarial Networks (GANs), Recurrent Neural Networks (RNNs) and Deep Reinforcement Learning (DRL). We categorise creative applications into five groups related to how AI technologies are used: i) content creation, ii) information analysis, iii) content enhancement and post production workflows, iv) information extraction and enhancement, and v) data compression. We critically examine the successes and limitations of this rapidly advancing technology in each of these areas. We further differentiate between the use of AI as a creative tool and its potential as a creator in its own right. We foresee that, in the near future, machine learning-based AI will be adopted widely as a tool or collaborative assistant for creativity. In contrast, we observe that the successes of machine learning in domains with fewer constraints, where AI is the `creator', remain modest. The potential of AI (or its developers) to win awards for its original creations in competition with human creatives is also limited, based on contemporary technologies. We therefore conclude that, in the context of creative industries, maximum benefit from AI will be derived where its focus is human centric -- where it is designed to augment, rather than replace, human creativity.},
  archivePrefix = {arXiv},
  eprint = {2007.12391},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/anantrasirichai et al_2020_artificial intelligence in the creative industries.pdf},
  journal = {arXiv:2007.12391 [cs]},
  primaryClass = {cs}
}

@article{andreini20_DeepDynamicFactor,
  title = {Deep {{Dynamic Factor Models}}},
  author = {Andreini, Paolo and Izzo, Cosimo and Ricco, Giovanni},
  year = {2020},
  month = jul,
  abstract = {We propose a novel deep neural net framework - that we refer to as Deep Dynamic Factor Model (D2FM) -, to encode the information available, from hundreds of macroeconomic and financial time-series into a handful of unobserved latent states. While similar in spirit to traditional dynamic factor models (DFMs), differently from those, this new class of models allows for nonlinearities between factors and observables due to the deep neural net structure. However, by design, the latent states of the model can still be interpreted as in a standard factor model. In an empirical application to the forecast and nowcast of economic conditions in the US, we show the potential of this framework in dealing with high dimensional, mixed frequencies and asynchronously published time series data. In a fully real-time out-of-sample exercise with US data, the D2FM improves over the performances of a state-of-the-art DFM.},
  archivePrefix = {arXiv},
  eprint = {2007.11887},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/andreini et al_2020_deep dynamic factor models.pdf},
  journal = {arXiv:2007.11887 [cs, econ]},
  primaryClass = {cs, econ}
}

@incollection{andrychowicz16_Learninglearngradient,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Andrychowicz, Marcin and Denil, Misha and G{\'o}mez, Sergio and Hoffman, Matthew W and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3981--3989},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/andrychowicz et al_2016_learning to learn by gradient descent by gradient descent.pdf;/home/trung/Zotero/storage/RNS4E5JL/6461-learning-to-learn-by-gradient-descent-by-gradient-descent.html}
}

@article{aneja20_NCPVAEVariationalAutoencoders,
  title = {{{NCP}}-{{VAE}}: {{Variational Autoencoders}} with {{Noise Contrastive Priors}}},
  shorttitle = {{{NCP}}-{{VAE}}},
  author = {Aneja, Jyoti and Schwing, Alexander and Kautz, Jan and Vahdat, Arash},
  year = {2020},
  month = oct,
  abstract = {Variational autoencoders (VAEs) are one of the powerful likelihood-based generative models with applications in various domains. However, they struggle to generate high-quality images, especially when samples are obtained from the prior without any tempering. One explanation for VAEs' poor generative quality is the prior hole problem: the prior distribution fails to match the aggregate approximate posterior. Due to this mismatch, there exist areas in the latent space with high density under the prior that do not correspond to any encoded image. Samples from those areas are decoded to corrupted images. To tackle this issue, we propose an energy-based prior defined by the product of a base prior distribution and a reweighting factor, designed to bring the base closer to the aggregate posterior. We train the reweighting factor by noise contrastive estimation, and we generalize it to hierarchical VAEs with many latent variable groups. Our experiments confirm that the proposed noise contrastive priors improve the generative performance of state-of-the-art VAEs by a large margin on the MNIST, CIFAR-10, CelebA 64, and CelebA HQ 256 datasets.},
  archivePrefix = {arXiv},
  eprint = {2010.02917},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/aneja et al_2020_ncp-vae.pdf},
  journal = {arXiv:2010.02917 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{anonymous20_ImprovingZeroShotVoice,
  title = {Improving {{Zero}}-{{Shot Voice Style Transfer}} via {{Disentangled Representation Learning}}},
  author = {{anonymous}},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/anonymous_2020_improving zero-shot voice style transfer via disentangled representation learning.pdf},
  howpublished = {https://openreview.net/pdf?id=TgSVWXw22FQ}
}

@inproceedings{anonymous2021an,
  title = {An Attention Free Transformer},
  booktitle = {Submitted to International Conference on Learning Representations},
  author = {{Anonymous}},
  year = {2021},
  file = {/home/trung/GoogleDrive/Zotero/anonymous_2021_an attention free transformer.pdf}
}

@inproceedings{anonymous21_imageworth16x16,
  title = {An Image Is Worth 16x16 Words: {{Transformers}} for Image Recognition at Scale},
  booktitle = {Submitted to International Conference on Learning Representations},
  author = {{Anonymous}},
  year = {2021},
  file = {/home/trung/GoogleDrive/Zotero/anonymous_2021_an image is worth 16x16 words.pdf},
  keywords = {favorite}
}

@inproceedings{anonymous21_VerydeepVAEs,
  title = {Very Deep \{\vphantom\}{{VAE}}\vphantom\{\}s Generalize Autoregressive Models and Can Outperform Them on Images},
  booktitle = {Submitted to International Conference on Learning Representations},
  author = {{Anonymous}},
  year = {2021},
  file = {/home/trung/GoogleDrive/Zotero/anonymous_2021_very deep vae s generalize autoregressive models and can outperform them on images.pdf},
  keywords = {_tablet,favorite}
}

@article{ansari19_HyperpriorInducedUnsupervised,
  title = {Hyperprior {{Induced Unsupervised Disentanglement}} of {{Latent Representations}}},
  author = {Ansari, Abdul Fatir and Soh, Harold},
  year = {2019},
  month = jan,
  abstract = {We address the problem of unsupervised disentanglement of latent representations learnt via deep generative models. In contrast to current approaches that operate on the evidence lower bound (ELBO), we argue that statistical independence in the latent space of VAEs can be enforced in a principled hierarchical Bayesian manner. To this effect, we augment the standard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the latent code. By tuning the IW parameters, we are able to encourage (or discourage) independence in the learnt latent dimensions. Extensive experimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and CelebA) show our approach to outperform the \$\textbackslash beta\$-VAE and is competitive with the state-of-the-art FactorVAE. Our approach achieves significantly better disentanglement and reconstruction on a new dataset (CorrelatedEllipses) which introduces correlations between the factors of variation.},
  archivePrefix = {arXiv},
  eprint = {1809.04497},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ansari et al_2019_hyperprior induced unsupervised disentanglement of latent representations.pdf},
  journal = {arXiv:1809.04497 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{ansari19_HyperpriorInducedUnsuperviseda,
  title = {Hyperprior {{Induced Unsupervised Disentanglement}} of {{Latent Representations}}},
  author = {Ansari, Abdul Fatir and Soh, Harold},
  year = {2019},
  month = jan,
  abstract = {We address the problem of unsupervised disentanglement of latent representations learnt via deep generative models. In contrast to current approaches that operate on the evidence lower bound (ELBO), we argue that statistical independence in the latent space of VAEs can be enforced in a principled hierarchical Bayesian manner. To this effect, we augment the standard VAE with an inverse-Wishart (IW) prior on the covariance matrix of the latent code. By tuning the IW parameters, we are able to encourage (or discourage) independence in the learnt latent dimensions. Extensive experimental results on a range of datasets (2DShapes, 3DChairs, 3DFaces and CelebA) show our approach to outperform the \$\textbackslash beta\$-VAE and is competitive with the state-of-the-art FactorVAE. Our approach achieves significantly better disentanglement and reconstruction on a new dataset (CorrelatedEllipses) which introduces correlations between the factors of variation.},
  archivePrefix = {arXiv},
  eprint = {1809.04497},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ansari et al_2019_hyperprior induced unsupervised disentanglement of latent representations2.pdf},
  journal = {arXiv:1809.04497 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{antoran19_DisentanglingLearningRobust,
  title = {Disentangling and {{Learning Robust Representations}} with {{Natural Clustering}}},
  author = {Antoran, Javier and Miguel, Antonio},
  year = {2019},
  month = dec,
  pages = {694--699},
  doi = {10.1109/ICMLA.2019.00125},
  abstract = {Learning representations that disentangle the underlying factors of variability in data is an intuitive way to achieve generalization in deep models. In this work, we address the scenario where generative factors present a multimodal distribution due to the existence of class distinction in the data. We propose N-VAE, a model which is capable of separating factors of variation which are exclusive to certain classes from factors that are shared among classes. This model implements an explicitly compositional latent variable structure by defining a class-conditioned latent space and a shared latent space. We show its usefulness for detecting and disentangling class-dependent generative factors as well as its capacity to generate artificial samples which contain characteristics unseen in the training data.},
  archivePrefix = {arXiv},
  eprint = {1901.09415},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/antoran et al_2019_disentangling and learning robust representations with natural clustering.pdf},
  journal = {2019 18th IEEE International Conference On Machine Learning And Applications (ICMLA)},
  keywords = {disentanglement}
}

@article{antoran20_DepthUncertaintyNeural,
  title = {Depth {{Uncertainty}} in {{Neural Networks}}},
  author = {Antor{\'a}n, Javier and Allingham, James Urquhart and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2020},
  month = jun,
  abstract = {Existing methods for estimating uncertainty in deep learning tend to require multiple forward passes, making them unsuitable for applications where computational resources are limited. To solve this, we perform probabilistic reasoning over the depth of neural networks. Different depths correspond to subnetworks which share weights and whose predictions are combined via marginalisation, yielding model uncertainty. By exploiting the sequential structure of feed-forward networks, we are able to both evaluate our training objective and make predictions with a single forward pass. We validate our approach on real-world regression and image classification tasks. Our approach provides uncertainty calibration, robustness to dataset shift, and accuracies competitive with more computationally expensive baselines.},
  archivePrefix = {arXiv},
  eprint = {2006.08437},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/antorán et al_2020_depth uncertainty in neural networks.pdf},
  journal = {arXiv:2006.08437 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{antoran20_DepthUncertaintyNeurala,
  title = {Depth {{Uncertainty}} in {{Neural Networks}}},
  author = {Antor{\'a}n, Javier and Allingham, James Urquhart and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2020},
  month = oct,
  abstract = {Existing methods for estimating uncertainty in deep learning tend to require multiple forward passes, making them unsuitable for applications where computational resources are limited. To solve this, we perform probabilistic reasoning over the depth of neural networks. Different depths correspond to subnetworks which share weights and whose predictions are combined via marginalisation, yielding model uncertainty. By exploiting the sequential structure of feed-forward networks, we are able to both evaluate our training objective and make predictions with a single forward pass. We validate our approach on real-world regression and image classification tasks. Our approach provides uncertainty calibration, robustness to dataset shift, and accuracies competitive with more computationally expensive baselines.},
  archivePrefix = {arXiv},
  eprint = {2006.08437},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/false},
  journal = {arXiv:2006.08437 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{antoran20_GettingCLUEMethod,
  title = {Getting a {{CLUE}}: {{A Method}} for {{Explaining Uncertainty Estimates}}},
  shorttitle = {Getting a {{CLUE}}},
  author = {Antor{\'a}n, Javier and Bhatt, Umang and Adel, Tameem and Weller, Adrian and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2020},
  month = jun,
  abstract = {Both uncertainty estimation and interpretability are important factors for trustworthy machine learning systems. However, there is little work at the intersection of these two areas. We address this gap by proposing a novel method for interpreting uncertainty estimates from differentiable probabilistic models, like Bayesian Neural Networks (BNNs). Our method, Counterfactual Latent Uncertainty Explanations (CLUE), indicates how to change an input, while keeping it on the data manifold, such that a BNN becomes more confident about the input's prediction. We validate CLUE through 1) a novel framework for evaluating counterfactual explanations of uncertainty, 2) a series of ablation experiments, and 3) a user study. Our experiments show that CLUE outperforms baselines and enables practitioners to better understand which input patterns are responsible for predictive uncertainty.},
  archivePrefix = {arXiv},
  eprint = {2006.06848},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/antorán et al_2020_getting a clue.pdf},
  journal = {arXiv:2006.06848 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{aparicio20_RandomMatrixTheory,
  title = {A {{Random Matrix Theory Approach}} to {{Denoise Single}}-{{Cell Data}}},
  author = {Aparicio, Luis and Bordyuh, Mykola and Blumberg, Andrew J. and Rabadan, Raul},
  year = {2020},
  month = jun,
  volume = {1},
  pages = {100035},
  issn = {26663899},
  doi = {10.1016/j.patter.2020.100035},
  abstract = {Single-cell technologies provide the opportunity to identify new cellular states. However, a major obstacle to the identification of biological signals is noise in single-cell data. In addition, single-cell data are very sparse. We propose a new method based on random matrix theory to analyze and denoise single-cell sequencing data. The method uses the universal distributions predicted by random matrix theory for the eigenvalues and eigenvectors of random covariance/Wishart matrices to distinguish noise from signal. In addition, we explain how sparsity can cause spurious eigenvector localization, falsely identifying meaningful directions in the data. We show that roughly 95\% of the information in single-cell data is compatible with the predictions of random matrix theory, about 3\% is spurious signal induced by sparsity, and only the last 2\% reflects true biological signal. We demonstrate the effectiveness of our approach by comparing with alternative techniques in a variety of examples with marked cell populations.},
  file = {/home/trung/GoogleDrive/Zotero/aparicio et al_2020_a random matrix theory approach to denoise single-cell data.pdf},
  journal = {Patterns},
  language = {en},
  number = {3}
}

@article{apostolopoulou20_SelfReflectiveVariationalAutoencoder,
  title = {Self-{{Reflective Variational Autoencoder}}},
  author = {Apostolopoulou, Ifigeneia and Rosenfeld, Elan and Dubrawski, Artur},
  year = {2020},
  month = jul,
  abstract = {The Variational Autoencoder (VAE) is a powerful framework for learning probabilistic latent variable generative models. However, typical assumptions on the approximate posterior distribution of the encoder and/or the prior, seriously restrict its capacity for inference and generative modeling. Variational inference based on neural autoregressive models respects the conditional dependencies of the exact posterior, but this flexibility comes at a cost: such models are expensive to train in high-dimensional regimes and can be slow to produce samples. In this work, we introduce an orthogonal solution, which we call self-reflective inference. By redesigning the hierarchical structure of existing VAE architectures, self-reflection ensures that the stochastic flow preserves the factorization of the exact posterior, sequentially updating the latent codes in a recurrent manner consistent with the generative model. We empirically demonstrate the clear advantages of matching the variational posterior to the exact posterior\textemdash on binarized MNIST, self-reflective inference achieves state-of-the art performance without resorting to complex, computationally expensive components such as autoregressive layers. Moreover, we design a variational normalizing flow that employs the proposed architecture, yielding predictive benefits compared to its purely generative counterpart. Our proposed modification is quite general and complements the existing literature; self-reflective inference can naturally leverage advances in distribution estimation and generative modeling to improve the capacity of each layer in the hierarchy.},
  archivePrefix = {arXiv},
  eprint = {2007.05166},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/apostolopoulou et al_2020_self-reflective variational autoencoder.pdf},
  journal = {arXiv:2007.05166 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{arandjelovic15_NetVLADCNNarchitecture,
  title = {{{NetVLAD}}: {{CNN}} Architecture for Weakly Supervised Place Recognition},
  shorttitle = {{{NetVLAD}}},
  author = {Arandjelovi{\'c}, Relja and Gronat, Petr and Torii, Akihiko and Pajdla, Tomas and Sivic, Josef},
  year = {2015},
  month = nov,
  abstract = {We tackle the problem of large scale visual place recognition, where the task is to quickly and accurately recognize the location of a given query photograph. We present the following three principal contributions. First, we develop a convolutional neural network (CNN) architecture that is trainable in an end-to-end manner directly for the place recognition task. The main component of this architecture, NetVLAD, is a new generalized VLAD layer, inspired by the "Vector of Locally Aggregated Descriptors" image representation commonly used in image retrieval. The layer is readily pluggable into any CNN architecture and amenable to training via backpropagation. Second, we develop a training procedure, based on a new weakly supervised ranking loss, to learn parameters of the architecture in an end-to-end manner from images depicting the same places over time downloaded from Google Street View Time Machine. Finally, we show that the proposed architecture significantly outperforms non-learnt image representations and off-the-shelf CNN descriptors on two challenging place recognition benchmarks, and improves over current state-of-the-art compact image representations on standard image retrieval benchmarks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1511.07247},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arandjelović et al_2015_netvlad.pdf;/home/trung/Zotero/storage/DP3C2ZTL/1511.html},
  journal = {arXiv:1511.07247 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{ardila19_CommonVoiceMassivelyMultilingual,
  title = {Common {{Voice}}: {{A Massively}}-{{Multilingual Speech Corpus}}},
  shorttitle = {Common {{Voice}}},
  author = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Henretty, Michael and Kohler, Michael and Meyer, Josh and Morais, Reuben and Saunders, Lindsay and Tyers, Francis M. and Weber, Gregor},
  year = {2019},
  month = dec,
  abstract = {The Common Voice corpus is a massively-multilingual collection of transcribed speech intended for speech technology research and development. Common Voice is designed for Automatic Speech Recognition purposes but can be useful in other domains (e.g. language identification). To achieve scale and sustainability, the Common Voice project employs crowdsourcing for both data collection and data validation. The most recent release includes 29 languages, and as of November 2019 there are a total of 38 languages collecting data. Over 50,000 individuals have participated so far, resulting in 2,500 hours of collected audio. To our knowledge this is the largest audio corpus in the public domain for speech recognition, both in terms of number of hours and number of languages. As an example use case for Common Voice, we present speech recognition experiments using Mozilla's DeepSpeech Speech-to-Text toolkit. By applying transfer learning from a source English model, we find an average Character Error Rate improvement of 5.99 +/- 5.48 for twelve target languages (German, French, Italian, Turkish, Catalan, Slovenian, Welsh, Irish, Breton, Tatar, Chuvash, and Kabyle). For most of these languages, these are the first ever published results on end-to-end Automatic Speech Recognition.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1912.06670},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ardila et al_2019_common voice.pdf;/home/trung/Zotero/storage/JWFYW683/1912.html},
  journal = {arXiv:1912.06670 [cs]},
  primaryClass = {cs}
}

@article{arganda17_Parsinglifeshorteningeffects,
  title = {Parsing the Life-Shortening Effects of Dietary Protein: Effects of Individual Amino Acids},
  shorttitle = {Parsing the Life-Shortening Effects of Dietary Protein},
  author = {Arganda, Sara and Bouchebti, Sofia and Bazazi, Sepideh and Le Hesran, Sophie and Puga, Camille and Latil, G{\'e}rard and Simpson, Stephen J. and Dussutour, Audrey},
  year = {2017},
  month = jan,
  volume = {284},
  issn = {0962-8452},
  doi = {10.1098/rspb.2016.2052},
  abstract = {High-protein diets shorten lifespan in many organisms. Is it because protein digestion is energetically costly or because the final products (the amino acids) are harmful? To answer this question while circumventing the life-history trade-off between reproduction and longevity, we fed sterile ant workers on diets based on whole proteins or free amino acids. We found that (i) free amino acids shortened lifespan even more than proteins; (ii) the higher the amino acid-to-carbohydrate ratio, the shorter ants lived and the lower their lipid reserves; (iii) for the same amino acid-to-carbohydrate ratio, ants eating free amino acids had more lipid reserves than those eating whole proteins; and (iv) on whole protein diets, ants seem to regulate food intake by prioritizing sugar, while on free amino acid diets, they seem to prioritize amino acids. To test the effect of the amino acid profile, we tested diets containing proportions of each amino acid that matched the ant's exome; surprisingly, longevity was unaffected by this change. We further tested diets with all amino acids under-represented except one, finding that methionine, serine, threonine and phenylalanine are especially harmful. All together, our results show certain amino acids are key elements behind the high-protein diet reduction in lifespan.},
  file = {/home/trung/GoogleDrive/Zotero/arganda et al_2017_parsing the life-shortening effects of dietary protein.pdf},
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  number = {1846},
  pmcid = {PMC5247493},
  pmid = {28053059}
}

@article{arisdakessian19_DeepImputeaccuratefast,
  title = {{{DeepImpute}}: An Accurate, Fast, and Scalable Deep Neural Network Method to Impute Single-Cell {{RNA}}-Seq Data},
  shorttitle = {{{DeepImpute}}},
  author = {Arisdakessian, C{\'e}dric and Poirion, Olivier and Yunits, Breck and Zhu, Xun and Garmire, Lana X.},
  year = {2019},
  month = dec,
  volume = {20},
  pages = {211},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1837-6},
  abstract = {Single-cell RNA sequencing (scRNA-seq) offers new opportunities to study gene expression of tens of thousands of single cells simultaneously. We present DeepImpute, a deep neural network-based imputation algorithm that uses dropout layers and loss functions to learn patterns in the data, allowing for accurate imputation. Overall, DeepImpute yields better accuracy than other six publicly available scRNA-seq imputation methods on experimental data, as measured by the mean squared error or Pearson's correlation coefficient. DeepImpute is an accurate, fast, and scalable imputation tool that is suited to handle the ever-increasing volume of scRNA-seq data, and is freely available at https://github.com/lanagarmire/DeepImpute.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/GoogleDrive/Zotero/arisdakessian et al_2019_deepimpute.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{arjovsky19_InvariantRiskMinimization,
  title = {Invariant {{Risk Minimization}}},
  author = {Arjovsky, Martin and Bottou, L{\'e}on and Gulrajani, Ishaan and {Lopez-Paz}, David},
  year = {2019},
  month = jul,
  abstract = {We introduce Invariant Risk Minimization (IRM), a learning paradigm to estimate invariant correlations across multiple training distributions. To achieve this goal, IRM learns a data representation such that the optimal classifier, on top of that data representation, matches for all training distributions. Through theory and experiments, we show how the invariances learned by IRM relate to the causal structures governing the data and enable out-of-distribution generalization.},
  archivePrefix = {arXiv},
  eprint = {1907.02893},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arjovsky et al_2019_invariant risk minimization.pdf},
  journal = {arXiv:1907.02893 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,invariant,risk minimiazation,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{armenta20_RepresentationTheoryNeural,
  title = {The {{Representation Theory}} of {{Neural Networks}}},
  author = {Armenta, Marco Antonio and Jodoin, Pierre-Marc},
  year = {2020},
  month = jul,
  abstract = {In this work, we show that neural networks can be represented via the mathematical theory of quiver representations. More specifically, we prove that a neural network is a quiver representation with activation functions, a mathematical object that we represent using a network quiver. Also, we show that network quivers gently adapt to common neural network concepts such as fully-connected layers, convolution operations, residual connections, batch normalization, and pooling operations. We show that this mathematical representation is by no means an approximation of what neural networks are as it exactly matches reality. This interpretation is algebraic and can be studied with algebraic methods.},
  archivePrefix = {arXiv},
  eprint = {2007.12213},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/armenta et al_2020_the representation theory of neural networks.pdf},
  journal = {arXiv:2007.12213 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{arnez20_ComparisonUncertaintyEstimation,
  title = {A {{Comparison}} of {{Uncertainty Estimation Approaches}} in {{Deep Learning Components}} for {{Autonomous Vehicle Applications}}},
  author = {Arnez, Fabio and Espinoza, Huascar and Radermacher, Ansgar and Terrier, Fran{\c c}ois},
  year = {2020},
  month = jul,
  abstract = {A key factor for ensuring safety in Autonomous Vehicles (AVs) is to avoid any abnormal behaviors under undesirable and unpredicted circumstances. As AVs increasingly rely on Deep Neural Networks (DNNs) to perform safety-critical tasks, different methods for uncertainty quantification have recently been proposed to measure the inevitable source of errors in data and models. However, uncertainty quantification in DNNs is still a challenging task. These methods require a higher computational load, a higher memory footprint, and introduce extra latency, which can be prohibitive in safety-critical applications. In this paper, we provide a brief and comparative survey of methods for uncertainty quantification in DNNs along with existing metrics to evaluate uncertainty predictions. We are particularly interested in understanding the advantages and downsides of each method for specific AV tasks and types of uncertainty sources.},
  archivePrefix = {arXiv},
  eprint = {2006.15172},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arnez et al_2020_a comparison of uncertainty estimation approaches in deep learning components for autonomous vehicle applications.pdf},
  journal = {arXiv:2006.15172 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{arora17_GANsactuallylearn,
  title = {Do {{GANs}} Actually Learn the Distribution? {{An}} Empirical Study},
  shorttitle = {Do {{GANs}} Actually Learn the Distribution?},
  author = {Arora, Sanjeev and Zhang, Yi},
  year = {2017},
  month = jun,
  abstract = {Do GANS (Generative Adversarial Nets) actually learn the target distribution? The foundational paper of (Goodfellow et al 2014) suggested they do, if they were given ``sufficiently large'' deep nets, sample size, and computation time. A recent theoretical analysis in Arora et al (to appear at ICML 2017) raised doubts whether the same holds when discriminator has finite size. It showed that the training objective can approach its optimum value even if the generated distribution has very low support \textemdash in other words, the training objective is unable to prevent mode collapse.},
  archivePrefix = {arXiv},
  eprint = {1706.08224},
  eprinttype = {arxiv},
  journal = {arXiv:1706.08224 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{arora18_GANslearndistribution,
  title = {Do {{GANs}} Learn the Distribution? {{Some}} Theory and Empirics},
  booktitle = {International Conference on Learning Representations},
  author = {Arora, Sanjeev and Risteski, Andrej and Zhang, Yi},
  year = {2018},
  file = {/home/trung/GoogleDrive/Zotero/arora et al_2018_do gans learn the distribution.pdf}
}

@incollection{arora19_ImplicitRegularizationDeep,
  title = {Implicit {{Regularization}} in {{Deep Matrix Factorization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Arora, Sanjeev and Cohen, Nadav and Hu, Wei and Luo, Yuping},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {7413--7424},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/arora et al_2019_implicit regularization in deep matrix factorization.pdf}
}

@article{arora19_TheoreticalAnalysisContrastive,
  title = {A {{Theoretical Analysis}} of {{Contrastive Unsupervised Representation Learning}}},
  author = {Arora, Sanjeev and Khandeparkar, Hrishikesh and Khodak, Mikhail and Plevrakis, Orestis and Saunshi, Nikunj},
  year = {2019},
  month = feb,
  abstract = {Recent empirical works have successfully used unlabeled data to learn feature representations that are broadly useful in downstream classification tasks. Several of these methods are reminiscent of the well-known word2vec embedding algorithm: leveraging availability of pairs of semantically "similar" data points and "negative samples," the learner forces the inner product of representations of similar pairs with each other to be higher on average than with negative samples. The current paper uses the term contrastive learning for such algorithms and presents a theoretical framework for analyzing them by introducing latent classes and hypothesizing that semantically similar points are sampled from the same latent class. This framework allows us to show provable guarantees on the performance of the learned representations on the average classification task that is comprised of a subset of the same set of latent classes. Our generalization bound also shows that learned representations can reduce (labeled) sample complexity on downstream tasks. We conduct controlled experiments in both the text and image domains to support the theory.},
  annotation = {ZSCC: 0000026},
  archivePrefix = {arXiv},
  eprint = {1902.09229},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arora et al_2019_a theoretical analysis of contrastive unsupervised representation learning.pdf;/home/trung/Zotero/storage/PRHR7CI6/1902.html},
  journal = {arXiv:1902.09229 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{arpit17_CloserLookMemorization,
  title = {A {{Closer Look}} at {{Memorization}} in {{Deep Networks}}},
  author = {Arpit, Devansh and Jastrz{\k{e}}bski, Stanis{\l}aw and Ballas, Nicolas and Krueger, David and Bengio, Emmanuel and Kanwal, Maxinder S. and Maharaj, Tegan and Fischer, Asja and Courville, Aaron and Bengio, Yoshua and {Lacoste-Julien}, Simon},
  year = {2017},
  month = jul,
  abstract = {We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.},
  annotation = {ZSCC: 0000196},
  archivePrefix = {arXiv},
  eprint = {1706.05394},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arpit et al_2017_a closer look at memorization in deep networks.pdf;/home/trung/Zotero/storage/N3CK4JBQ/1706.html},
  journal = {arXiv:1706.05394 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{arpit20_NeuralBayesGeneric,
  title = {Neural {{Bayes}}: {{A Generic Parameterization Method}} for {{Unsupervised Representation Learning}}},
  shorttitle = {Neural {{Bayes}}},
  author = {Arpit, Devansh and Wang, Huan and Xiong, Caiming and Socher, Richard and Bengio, Yoshua},
  year = {2020},
  month = feb,
  abstract = {We introduce a parameterization method called Neural Bayes which allows computing statistical quantities that are in general difficult to compute and opens avenues for formulating new objectives for unsupervised representation learning. Specifically, given an observed random variable \$\textbackslash mathbf\{x\}\$ and a latent discrete variable \$z\$, we can express \$p(\textbackslash mathbf\{x\}|z)\$, \$p(z|\textbackslash mathbf\{x\})\$ and \$p(z)\$ in closed form in terms of a sufficiently expressive function (Eg. neural network) using our parameterization without restricting the class of these distributions. To demonstrate its usefulness, we develop two independent use cases for this parameterization: 1. Mutual Information Maximization (MIM): MIM has become a popular means for self-supervised representation learning. Neural Bayes allows us to compute mutual information between observed random variables \$\textbackslash mathbf\{x\}\$ and latent discrete random variables \$z\$ in closed form. We use this for learning image representations and show its usefulness on downstream classification tasks. 2. Disjoint Manifold Labeling: Neural Bayes allows us to formulate an objective which can optimally label samples from disjoint manifolds present in the support of a continuous distribution. This can be seen as a specific form of clustering where each disjoint manifold in the support is a separate cluster. We design clustering tasks that obey this formulation and empirically show that the model optimally labels the disjoint manifolds. Our code is available at \textbackslash url\{https://github.com/salesforce/NeuralBayes\}},
  archivePrefix = {arXiv},
  eprint = {2002.09046},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arpit et al_2020_neural bayes.pdf},
  journal = {arXiv:2002.09046 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{arulkumaran17_BriefSurveyDeep,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  volume = {34},
  pages = {26--38},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policy-based methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep \$Q\$-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archivePrefix = {arXiv},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arulkumaran et al_2017_a brief survey of deep reinforcement learning.pdf},
  journal = {IEEE Signal Processing Magazine},
  number = {6}
}

@article{arvanitidis20_GeometricallyEnrichedLatent,
  title = {Geometrically {{Enriched Latent Spaces}}},
  author = {Arvanitidis, Georgios and Hauberg, S{\o}ren and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = aug,
  abstract = {A common assumption in generative models is that the generator immerses the latent space into a Euclidean ambient space. Instead, we consider the ambient space to be a Riemannian manifold, which allows for encoding domain knowledge through the associated Riemannian metric. Shortest paths can then be defined accordingly in the latent space to both follow the learned manifold and respect the ambient geometry. Through careful design of the ambient metric we can ensure that shortest paths are well-behaved even for deterministic generators that otherwise would exhibit a misleading bias. Experimentally we show that our approach improves interpretability of learned representations both using stochastic and deterministic generators.},
  archivePrefix = {arXiv},
  eprint = {2008.00565},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/arvanitidis et al_2020_geometrically enriched latent spaces.pdf},
  journal = {arXiv:2008.00565 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ashfaque00_LinearAlgebraDeep,
  title = {Linear {{Algebra}} for {{Deep Learning}}},
  author = {Ashfaque, Johar M},
  pages = {7},
  file = {/home/trung/GoogleDrive/Zotero/ashfaque_linear algebra for deep learning.pdf},
  language = {en}
}

@article{askell19_RoleCooperationResponsible,
  title = {The {{Role}} of {{Cooperation}} in {{Responsible AI Development}}},
  author = {Askell, Amanda and Brundage, Miles and Hadfield, Gillian},
  year = {2019},
  month = jul,
  abstract = {In this paper, we argue that competitive pressures could incentivize AI companies to underinvest in ensuring their systems are safe, secure, and have a positive social impact. Ensuring that AI systems are developed responsibly may therefore require preventing and solving collective action problems between companies. We note that there are several key factors that improve the prospects for cooperation in collective action problems. We use this to identify strategies to improve the prospects for industry cooperation on the responsible development of AI.},
  archivePrefix = {arXiv},
  eprint = {1907.04534},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/askell et al_2019_the role of cooperation in responsible ai development.pdf;/home/trung/Zotero/storage/TNVU72RB/1907.html},
  journal = {arXiv:1907.04534 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  primaryClass = {cs}
}

@article{asokan00_TeachingGANWhat,
  title = {Teaching a {{GAN What Not}} to {{Learn}}},
  author = {Asokan, Siddarth and Seelamantula, Chandra Sekhar},
  pages = {12},
  abstract = {Generative adversarial networks (GANs) were originally envisioned as unsupervised generative models that learn to follow a target distribution. Variants such as conditional GANs, auxiliary-classifier GANs (ACGANs) project GANs on to supervised and semi-supervised learning frameworks by providing labelled data and using multi-class discriminators. In this paper, we approach the supervised GAN problem from a different perspective, one that is motivated by the philosophy of the famous Persian poet Rumi who said, ``The art of knowing is knowing what to ignore.'' In the GAN framework, we not only provide the GAN positive data that it must learn to model, but also present it with so-called negative samples that it must learn to avoid \textemdash{} we call this the Rumi framework. This formulation allows the discriminator to represent the underlying target distribution better by learning to penalize generated samples that are undesirable \textemdash{} we show that this capability accelerates the learning process of the generator. We present a reformulation of the standard GAN (SGAN) and least-squares GAN (LSGAN) within the Rumi setting. The advantage of the reformulation is demonstrated by means of experiments conducted on MNIST, Fashion MNIST, CelebA, and CIFAR-10 datasets. Finally, we consider an application of the proposed formulation to address the important problem of learning an under-represented class in an unbalanced dataset. The Rumi approach results in substantially lower FID scores than the standard GAN frameworks while possessing better generalization capability.},
  file = {/home/trung/GoogleDrive/Zotero/asokan et al_teaching a gan what not to learn.pdf},
  keywords = {_tablet},
  language = {en}
}

@article{asoodeh20_BottleneckProblemsInformation,
  title = {Bottleneck {{Problems}}: {{An Information}} and {{Estimation}}-{{Theoretic View}}},
  shorttitle = {Bottleneck {{Problems}}},
  author = {Asoodeh, Shahab and Calmon, Flavio P.},
  year = {2020},
  month = nov,
  volume = {22},
  pages = {1325},
  issn = {1099-4300},
  doi = {10.3390/e22111325},
  abstract = {Information bottleneck (IB) and privacy funnel (PF) are two closely related optimization problems which have found applications in machine learning, design of privacy algorithms, capacity problems (e.g., Mrs. Gerber's Lemma), and strong data processing inequalities, among others. In this work, we first investigate the functional properties of IB and PF through a unified theoretical framework. We then connect them to three information-theoretic coding problems, namely hypothesis testing against independence, noisy source coding, and dependence dilution. Leveraging these connections, we prove a new cardinality bound on the auxiliary variable in IB, making its computation more tractable for discrete random variables. In the second part, we introduce a general family of optimization problems, termed ``bottleneck problems'', by replacing mutual information in IB and PF with other notions of mutual information, namely f -information and Arimoto's mutual information. We then argue that, unlike IB and PF, these problems lead to easily interpretable guarantees in a variety of inference tasks with statistical constraints on accuracy and privacy. While the underlying optimization problems are non-convex, we develop a technique to evaluate bottleneck problems in closed form by equivalently expressing them in terms of lower convex or upper concave envelope of certain functions. By applying this technique to a binary case, we derive closed form expressions for several bottleneck problems.},
  file = {/home/trung/GoogleDrive/Zotero/asoodeh et al_2020_bottleneck problems.pdf},
  journal = {Entropy},
  keywords = {information},
  language = {en},
  number = {11}
}

@incollection{asperti19_GenerativeAspectsVariational,
  title = {About {{Generative Aspects}} of {{Variational Autoencoders}}},
  booktitle = {Machine {{Learning}}, {{Optimization}}, and {{Data Science}}},
  author = {Asperti, Andrea},
  editor = {Nicosia, Giuseppe and Pardalos, Panos and Umeton, Renato and Giuffrida, Giovanni and Sciacca, Vincenzo},
  year = {2019},
  volume = {11943},
  pages = {71--82},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-37599-7_7},
  file = {/home/trung/GoogleDrive/Zotero/asperti_2019_about generative aspects of variational autoencoders.pdf},
  isbn = {978-3-030-37598-0 978-3-030-37599-7},
  language = {en}
}

@article{asperti19_SparsityVariationalAutoencoders,
  title = {Sparsity in {{Variational Autoencoders}}},
  author = {Asperti, Andrea},
  year = {2019},
  month = feb,
  abstract = {Working in high-dimensional latent spaces, the internal encoding of data in Variational Autoencoders becomes naturally sparse. We discuss this known but controversial phenomenon sometimes refereed to as overpruning, to emphasize the under-use of the model capacity. In fact, it is an important form of self-regularization, with all the typical benefits associated with sparsity: it forces the model to focus on the really important features, highly reducing the risk of overfitting. Especially, it is a major methodological guide for the correct tuning of the model capacity, progressively augmenting it to attain sparsity, or conversely reducing the dimension of the network removing links to zeroed out neurons. The degree of sparsity crucially depends on the network architecture: for instance, convolutional networks typically show less sparsity, likely due to the tighter relation of features to different spatial regions of the input.},
  archivePrefix = {arXiv},
  eprint = {1812.07238},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/asperti_2019_sparsity in variational autoencoders.pdf},
  journal = {arXiv:1812.07238 [cs, stat]},
  keywords = {favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{asperti20_Balancingreconstructionerror,
  title = {Balancing Reconstruction Error and {{Kullback}}-{{Leibler}} Divergence in {{Variational Autoencoders}}},
  author = {Asperti, Andrea and Trentin, Matteo},
  year = {2020},
  month = feb,
  abstract = {In the loss function of Variational Autoencoders there is a well known tension between two components: the reconstruction loss, improving the quality of the resulting images, and the Kullback-Leibler divergence, acting as a regularizer of the latent space. Correctly balancing these two components is a delicate issue, easily resulting in poor generative behaviours. In a recent work [8], a sensible improvement has been obtained by allowing the network to learn the balancing factor during training, according to a suitable loss function. In this article, we show that learning can be replaced by a simple deterministic computation, helping to understand the underlying mechanism, and resulting in a faster and more accurate behaviour. On typical datasets such as Cifar and Celeba, our technique sensibly outperforms all previous VAE architectures.},
  archivePrefix = {arXiv},
  eprint = {2002.07514},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/asperti et al_2020_balancing reconstruction error and kullback-leibler divergence in variational autoencoders.pdf},
  journal = {arXiv:2002.07514 [cs]},
  keywords = {vae_issues},
  language = {en},
  primaryClass = {cs}
}

@article{asperti20_VarianceLossVariational,
  title = {Variance {{Loss}} in {{Variational Autoencoders}}},
  author = {Asperti, Andrea},
  year = {2020},
  month = may,
  abstract = {In this article, we highlight what appears to be major issue of Variational Autoencoders, evinced from an extensive experimentation with different network architectures and datasets: the variance of generated data is significantly lower than that of training data. Since generative models are usually evaluated with metrics such as the Frechet Inception Distance (FID) that compare the distributions of (features of) real versus generated images, the variance loss typically results in degraded scores. This problem is particularly relevant in a two stage setting, where we use a second VAE to sample in the latent space of the first VAE. The minor variance creates a mismatch between the actual distribution of latent variables and those generated by the second VAE, that hinders the beneficial effects of the second stage. Renormalizing the output of the second VAE towards the expected normal spherical distribution, we obtain a sudden burst in the quality of generated samples, as also testified in terms of FID.},
  archivePrefix = {arXiv},
  eprint = {2002.09860},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/asperti_2020_variance loss in variational autoencoders.pdf},
  journal = {arXiv:2002.09860 [cs]},
  keywords = {vae_issues},
  primaryClass = {cs}
}

@incollection{atamna20_PrincipledApproachAnalyze,
  title = {A {{Principled Approach}} to {{Analyze Expressiveness}} and {{Accuracy}} of {{Graph Neural Networks}}},
  booktitle = {Advances in {{Intelligent Data Analysis XVIII}}},
  author = {Atamna, Asma and Sokolovska, Nataliya and Crivello, Jean-Claude},
  editor = {Berthold, Michael R. and Feelders, Ad and Krempl, Georg},
  year = {2020},
  volume = {12080},
  pages = {27--39},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-44584-3_3},
  abstract = {Graph neural networks (GNNs) have known an increasing success recently, with many GNN variants achieving state-of-the-art results on node and graph classification tasks. The proposed GNNs, however, often implement complex node and graph embedding schemes, which makes it challenging to explain their performance. In this paper, we investigate the link between a GNN's expressiveness, that is, its ability to map different graphs to different representations, and its generalization performance in a graph classification setting. In particular, we propose a principled experimental procedure where we (i) define a practical measure for expressiveness, (ii) introduce an expressiveness-based loss function that we use to train a simple yet practical GNN that is permutation-invariant, (iii) illustrate our procedure on benchmark graph classification problems and on an original real-world application. Our results reveal that expressiveness alone does not guarantee a better performance, and that a powerful GNN should be able to produce graph representations that are well separated with respect to the class of the corresponding graphs.},
  file = {/home/trung/GoogleDrive/Zotero/atamna et al_2020_a principled approach to analyze expressiveness and accuracy of graph neural networks.pdf},
  isbn = {978-3-030-44583-6 978-3-030-44584-3},
  language = {en}
}

@article{atanov18_DeepWeightPrior,
  title = {The {{Deep Weight Prior}}},
  author = {Atanov, Andrei and Ashukha, Arsenii and Struminsky, Kirill and Vetrov, Dmitry and Welling, Max},
  year = {2018},
  month = oct,
  abstract = {Bayesian inference is known to provide a general framework for incorporating prior knowledge or specific properties into machine learning models via carefully choosing a prior distribution. In this work, we propose a new type of prior distributions for convolutional neural networks, deep weight prior (DWP), that exploit generative models to encourage a specific structure of trained convolutional filters e.g., spatial correlations of weights. We define DWP in the form of an implicit distribution and propose a method for variational inference with such type of implicit priors. In experiments, we show that DWP improves the performance of Bayesian neural networks when training data are limited, and initialization of weights with samples from DWP accelerates training of conventional convolutional neural networks.},
  archivePrefix = {arXiv},
  eprint = {1810.06943},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/atanov et al_2018_the deep weight prior.pdf;/home/trung/Zotero/storage/2M26TXRB/1810.html},
  journal = {arXiv:1810.06943 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{atanov18_UncertaintyEstimationStochastic,
  title = {Uncertainty {{Estimation}} via {{Stochastic Batch Normalization}}},
  author = {Atanov, Andrei and Ashukha, Arsenii and Molchanov, Dmitry and Neklyudov, Kirill and Vetrov, Dmitry},
  year = {2018},
  month = feb,
  abstract = {In this work, we investigate Batch Normalization technique and propose its probabilistic interpretation. We propose a probabilistic model and show that Batch Normalization maximazes the lower bound of its marginalized log-likelihood. Then, according to the new probabilistic model, we design an algorithm which acts consistently during train and test. However, inference becomes computationally inefficient. To reduce memory and computational cost, we propose Stochastic Batch Normalization -- an efficient approximation of proper inference procedure. This method provides us with a scalable uncertainty estimation technique. We demonstrate the performance of Stochastic Batch Normalization on popular architectures (including deep convolutional architectures: VGG-like and ResNets) for MNIST and CIFAR-10 datasets.},
  archivePrefix = {arXiv},
  eprint = {1802.04893},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/atanov et al_2018_uncertainty estimation via stochastic batch normalization.pdf;/home/trung/Zotero/storage/P3M5TQR2/1802.html},
  journal = {arXiv:1802.04893 [cs, stat]},
  keywords = {Computer Science - Machine Learning,normalization,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{atanov19_SemiConditionalNormalizingFlows,
  title = {Semi-{{Conditional Normalizing Flows}} for {{Semi}}-{{Supervised Learning}}},
  author = {Atanov, Andrei and Volokhova, Alexandra and Ashukha, Arsenii and Sosnovik, Ivan and Vetrov, Dmitry},
  year = {2019},
  month = may,
  abstract = {This paper proposes a semi-conditional normalizing flow model for semi-supervised learning. The model uses both labelled and unlabeled data to learn an explicit model of joint distribution over objects and labels. Semi-conditional architecture of the model allows us to efficiently compute a value and gradients of the marginal likelihood for unlabeled objects. The conditional part of the model is based on a proposed conditional coupling layer. We demonstrate a performance of the model for semi-supervised classification problem on different datasets. The model outperforms the baseline approach based on variational autoencoders on MNIST dataset.},
  annotation = {ZSCC: 0000003},
  archivePrefix = {arXiv},
  eprint = {1905.00505},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/atanov et al_2019_semi-conditional normalizing flows for semi-supervised learning.pdf},
  journal = {arXiv:1905.00505 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{athalye19_anishathalyeneuralstyle,
  title = {Anishathalye/Neural-Style},
  author = {Athalye, Anish},
  year = {2019},
  month = dec,
  abstract = {Neural style in TensorFlow! :art:. Contribute to anishathalye/neural-style development by creating an account on GitHub.},
  copyright = {GPL-3.0}
}

@misc{attias00_VariationalBayesianFramework,
  title = {A {{Variational Bayesian Framework}} for {{Graphical Models}}.Pdf},
  author = {Attias, Hagai},
  file = {/home/trung/GoogleDrive/Zotero/attias_a variational bayesian framework for graphical models.pdf}
}

@article{atzmon20_IsometricAutoencoders,
  title = {Isometric {{Autoencoders}}},
  author = {Atzmon, Matan and Gropp, Amos and Lipman, Yaron},
  year = {2020},
  month = jun,
  abstract = {High dimensional data is often assumed to be concentrated near a low-dimensional manifold. Autoencoders (AE) is a popular technique to learn representations of such data by pushing it through a neural network with a low dimension bottleneck while minimizing a reconstruction error. Using high capacity AE often leads to a large collection of minimizers, many of which represent a low dimensional manifold that fits the data well but generalizes poorly. Two sources of bad generalization are: extrinsic, where the learned manifold possesses extraneous parts that are far from the data; and intrinsic, where the encoder and decoder introduce arbitrary distortion in the low dimensional parameterization. An approach taken to alleviate these issues is to add a regularizer that favors a particular solution; common regularizers promote sparsity, small derivatives, or robustness to noise. In this paper, we advocate an isometry (ie, distance preserving) regularizer. Specifically, our regularizer encourages: (i) the decoder to be an isometry; and (ii) the encoder to be a pseudo-isometry, where pseudo-isometry is an extension of an isometry with an orthogonal projection operator. In a nutshell, (i) preserves all geometric properties of the data such as volume, length, angle, and probability density. It fixes the intrinsic degree of freedom since any two isometric decoders to the same manifold will differ by a rigid motion. (ii) Addresses the extrinsic degree of freedom by minimizing derivatives in orthogonal directions to the manifold and hence disfavoring complicated manifold solutions. Experimenting with the isometry regularizer on dimensionality reduction tasks produces useful low-dimensional data representations, while incorporating it in AE models leads to an improved generalization.},
  archivePrefix = {arXiv},
  eprint = {2006.09289},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/atzmon et al_2020_isometric autoencoders.pdf},
  journal = {arXiv:2006.09289 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{augello16_Artworkcreationcognitive,
  title = {Artwork Creation by a Cognitive Architecture Integrating Computational Creativity and Dual Process Approaches},
  author = {Augello, Agnese and Infantino, Ignazio and Lieto, Antonio and Pilato, Giovanni and Rizzo, Riccardo and Vella, Filippo},
  year = {2016},
  month = jan,
  abstract = {The paper proposes a novel cognitive architecture (CA) for computational creativity based on the Psi model and on the mechanisms inspired by dual process theories of reasoning and rationality. In recent years, many cognitive models have focused on dual process theories to better describe and implement complex cognitive skills in artificial agents, but creativity has been approached only at a descriptive level. In previous works we have described various modules of the cognitive architecture that allows a robot to execute creative paintings. By means of dual process theories we refine some relevant mechanisms to obtain artworks, and in particular we explain details about the resolution level of the CA dealing with different strategies of access to the Long Term Memory (LTM) and managing the interaction between S1 and S2 processes of the dual process theory. The creative process involves both divergent and convergent processes in either implicit or explicit manner. This leads to four activities (exploratory, reflective, tacit, and analytic) that, triggered by urges and motivations, generate creative acts. These creative acts exploit both the LTM and the WM in order to make novel substitutions to a perceived image by properly mixing parts of pictures coming from different domains. The paper highlights the role of the interaction between S1 and S2 processes, modulated by the resolution level, which focuses the attention of the creative agent by broadening or narrowing the exploration of novel solutions, or even drawing the solution from a set of already made associations. An example of artificial painter is described in some experimentations by using a robotic platform.},
  archivePrefix = {arXiv},
  eprint = {1601.00669},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/augello et al_2016_artwork creation by a cognitive architecture integrating computational creativity and dual process approaches.pdf},
  journal = {arXiv:1601.00669 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{avgerinos18_Effectscreatinesupplementation,
  title = {Effects of Creatine Supplementation on Cognitive Function of Healthy Individuals: {{A}} Systematic Review of Randomized Controlled Trials},
  shorttitle = {Effects of Creatine Supplementation on Cognitive Function of Healthy Individuals},
  author = {Avgerinos, Konstantinos I. and Spyrou, Nikolaos and Bougioukas, Konstantinos I. and Kapogiannis, Dimitrios},
  year = {2018},
  month = jul,
  volume = {108},
  pages = {166--173},
  issn = {0531-5565},
  doi = {10.1016/j.exger.2018.04.013},
  abstract = {Background and aims: Creatine is a supplement used by sportsmen to increase athletic performance by improving energy supply to muscle tissues. It is also an essential brain compound and some hypothesize that it aids cognition by improving energy supply and neuroprotection. The aim of this systematic review is to investigate the effects of oral creatine administration on cognitive function in healthy individuals. Methods: A search of multiple electronic databases was performed for the identification of randomized clinical trials (RCTs) examining the cognitive effects of oral creatine supplementation in healthy individuals. Results: Six studies (281 individuals) met our inclusion criteria. Generally, there was evidence that short term memory and intelligence/reasoning may be improved by creatine administration. Regarding other cognitive domains, such as long-term memory, spatial memory, memory scanning, attention, executive function, response inhibition, word fluency, reaction time and mental fatigue, the results were conflicting. Performance on cognitive tasks stayed unchanged in young individuals. Vegetarians responded better than meat-eaters in memory tasks but for other cognitive domains no differences were observed. Conclusions: Oral creatine administration may improve short-term memory and intelligence/reasoning of healthy individuals but its effect on other cognitive domains remains unclear. Findings suggest potential benefit for aging and stressed individuals. Since creatine is safe, future studies should include larger sample sizes. It is imperative that creatine should be tested on patients with dementias or cognitive impairment.},
  file = {/home/trung/GoogleDrive/Zotero/avgerinos et al_2018_effects of creatine supplementation on cognitive function of healthy individuals.pdf},
  journal = {Experimental gerontology},
  pmcid = {PMC6093191},
  pmid = {29704637}
}

@article{awiszus19_LearningDisentangledRepresentations,
  title = {Learning {{Disentangled Representations}} via {{Independent Subspaces}}},
  author = {Awiszus, Maren and Ackermann, Hanno and Rosenhahn, Bodo},
  year = {2019},
  month = aug,
  abstract = {Image generating neural networks are mostly viewed as black boxes, where any change in the input can have a number of globally effective changes on the output. In this work, we propose a method for learning disentangled representations to allow for localized image manipulations. We use face images as our example of choice. Depending on the image region, identity and other facial attributes can be modified. The proposed network can transfer parts of a face such as shape and color of eyes, hair, mouth, etc.\textasciitilde directly between persons while all other parts of the face remain unchanged. The network allows to generate modified images which appear like realistic images. Our model learns disentangled representations by weak supervision. We propose a localized resnet autoencoder optimized using several loss functions including a loss based on the semantic segmentation, which we interpret as masks, and a loss which enforces disentanglement by decomposition of the latent space into statistically independent subspaces. We evaluate the proposed solution w.r.t. disentanglement and generated image quality. Convincing results are demonstrated using the CelebA dataset.},
  archivePrefix = {arXiv},
  eprint = {1908.08989},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/awiszus et al_2019_learning disentangled representations via independent subspaces.pdf},
  journal = {arXiv:1908.08989 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{ba15_LearningWakeSleepRecurrent,
  title = {Learning {{Wake}}-{{Sleep Recurrent Attention Models}}},
  booktitle = {{{NIPS}}},
  author = {Ba, Jimmy and Grosse, Roger B. and Salakhutdinov, Ruslan and Frey, Brendan J.},
  year = {2015},
  abstract = {Despite their success, convolutional neural networks are computationally expensive because they must examine all image locations. Stochastic attention-based models have been shown to improve computational efficiency at test time, but they remain difficult to train because of intractable posterior inference and high variance in the stochastic gradient estimates. Borrowing techniques from the literature on training deep generative models, we present the Wake-Sleep Recurrent Attention Model, a method for training stochastic attention networks which improves posterior inference and which reduces the variability in the stochastic gradients. We show that our method can greatly speed up the training time for stochastic attention networks in the domains of image classification and caption generation.},
  annotation = {ZSCC: 0000047},
  archivePrefix = {arXiv},
  eprint = {1509.06812},
  eprinttype = {arxiv},
  keywords = {Analysis of algorithms,Approximation algorithm,Artificial neural network,attention,Computation,Computer vision,Control variates,Convolutional neural network,Gradient,Random-access memory,Spatial variability,Speedup,Wake-sleep algorithm}
}

@article{ba16_LayerNormalization,
  title = {Layer {{Normalization}}},
  author = {Ba, Jimmy Lei and Kiros, Jamie Ryan and Hinton, Geoffrey E.},
  year = {2016},
  month = jul,
  abstract = {Training state-of-the-art, deep neural networks is computationally expensive. One way to reduce the training time is to normalize the activities of the neurons. A recently introduced technique called batch normalization uses the distribution of the summed input to a neuron over a mini-batch of training cases to compute a mean and variance which are then used to normalize the summed input to that neuron on each training case. This significantly reduces the training time in feed-forward neural networks. However, the effect of batch normalization is dependent on the mini-batch size and it is not obvious how to apply it to recurrent neural networks. In this paper, we transpose batch normalization into layer normalization by computing the mean and variance used for normalization from all of the summed inputs to the neurons in a layer on a single training case. Like batch normalization, we also give each neuron its own adaptive bias and gain which are applied after the normalization but before the non-linearity. Unlike batch normalization, layer normalization performs exactly the same computation at training and test times. It is also straightforward to apply to recurrent neural networks by computing the normalization statistics separately at each time step. Layer normalization is very effective at stabilizing the hidden state dynamics in recurrent networks. Empirically, we show that layer normalization can substantially reduce the training time compared with previously published techniques.},
  archivePrefix = {arXiv},
  eprint = {1607.06450},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ba et al_2016_layer normalization.pdf;/home/trung/Zotero/storage/PZT7Z99M/1607.html},
  journal = {arXiv:1607.06450 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{ba16_UsingFastWeights,
  title = {Using {{Fast Weights}} to {{Attend}} to the {{Recent Past}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Ba, Jimmy and Hinton, Geoffrey E and Mnih, Volodymyr and Leibo, Joel Z and Ionescu, Catalin},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {4331--4339},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/ba et al_2016_using fast weights to attend to the recent past.pdf;/home/trung/Zotero/storage/MQHRU8NW/6057-using-fast-weights-to-attend-to-the-recent-past.html},
  keywords = {fast,recurrent}
}

@article{ba19_Discoveringtopicsneural,
  title = {Discovering Topics with Neural Topic Models Built from {{PLSA}} Assumptions},
  author = {Ba, Sileye 0},
  year = {2019},
  month = nov,
  abstract = {In this paper we present a model for unsupervised topic discovery in texts corpora. The proposed model uses documents, words, and topics lookup table embedding as neural network model parameters to build probabilities of words given topics, and probabilities of topics given documents. These probabilities are used to recover by marginalization probabilities of words given documents. For very large corpora where the number of documents can be in the order of billions, using a neural auto-encoder based document embedding is more scalable then using a lookup table embedding as classically done. We thus extended the lookup based document embedding model to continuous auto-encoder based model. Our models are trained using probabilistic latent semantic analysis (PLSA) assumptions. We evaluated our models on six datasets with a rich variety of contents. Conducted experiments demonstrate that the proposed neural topic models are very effective in capturing relevant topics. Furthermore, considering perplexity metric, conducted evaluation benchmarks show that our topic models outperform latent Dirichlet allocation (LDA) model which is classically used to address topic discovery tasks.},
  archivePrefix = {arXiv},
  eprint = {1911.10924},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ba_2019_discovering topics with neural topic models built from plsa assumptions.pdf},
  journal = {arXiv:1911.10924 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{baan19_UnderstandingMultiHeadAttention,
  title = {Understanding {{Multi}}-{{Head Attention}} in {{Abstractive Summarization}}},
  author = {Baan, Joris and {ter Hoeve}, Maartje and {van der Wees}, Marlies and Schuth, Anne and {de Rijke}, Maarten},
  year = {2019},
  month = nov,
  abstract = {Attention mechanisms in deep learning architectures have often been used as a means of transparency and, as such, to shed light on the inner workings of the architectures. Recently, there has been a growing interest in whether or not this assumption is correct. In this paper we investigate the interpretability of multi-head attention in abstractive summarization, a sequence-to-sequence task for which attention does not have an intuitive alignment role, such as in machine translation. We first introduce three metrics to gain insight in the focus of attention heads and observe that these heads specialize towards relative positions, specific part-of-speech tags, and named entities. However, we also find that ablating and pruning these heads does not lead to a significant drop in performance, indicating redundancy. By replacing the softmax activation functions with sparsemax activation functions, we find that attention heads behave seemingly more transparent: we can ablate fewer heads and heads score higher on our interpretability metrics. However, if we apply pruning to the sparsemax model we find that we can prune even more heads, raising the question whether enforced sparsity actually improves transparency. Finally, we find that relative positions heads seem integral to summarization performance and persistently remain after pruning.},
  archivePrefix = {arXiv},
  eprint = {1911.03898},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/baan et al_2019_understanding multi-head attention in abstractive summarization.pdf},
  journal = {arXiv:1911.03898 [cs]},
  primaryClass = {cs}
}

@article{baar17_MinimizingInjuryMaximizing,
  title = {Minimizing {{Injury}} and {{Maximizing Return}} to {{Play}}: {{Lessons}} from {{Engineered Ligaments}}},
  shorttitle = {Minimizing {{Injury}} and {{Maximizing Return}} to {{Play}}},
  author = {Baar, Keith},
  year = {2017},
  month = mar,
  volume = {47},
  pages = {5--11},
  issn = {1179-2035},
  doi = {10.1007/s40279-017-0719-x},
  abstract = {Musculoskeletal injuries account for more than 70\% of time away from sports. One of the reasons for the high number of injuries and long return to play is that we have only a very basic understanding of how our training alters tendon and ligament (sinew) structure and function. Sinews are highly dense tissues that are difficult to characterize both in vivo and in vitro. Recently, engineered ligaments have been developed in vitro using cells from human anterior cruciate ligaments or hamstring tendons. These three-dimensional tissues can be grown in a laboratory, treated with agents thought to affect sinew physiology, and then mechanically tested to determine their function. Using these tissues, we have learned that sinews, like bone, quickly become refractory to an exercise stimulus, suggesting that short ({$<$}10~min) periods of activity with relatively long (6~h) periods of rest are best to train these tissues. The engineered sinews have also shown how estrogen decreases sinew function and that a factor released following intense exercise increases sinew collagen synthesis and function. Last, engineered sinews are being used to screen possible nutritional interventions that may benefit tendon or ligament function. Using the data derived from these tissue-engineered sinews, new nutritional and training regimes are being designed and tested with the goal of minimizing injury and accelerating return to play.},
  annotation = {ZSCC: 0000017},
  file = {/home/trung/GoogleDrive/Zotero/baar_2017_minimizing injury and maximizing return to play.pdf},
  journal = {Sports Medicine (Auckland, N.Z.)},
  language = {eng},
  number = {Suppl 1},
  pmcid = {PMC5371618},
  pmid = {28332110}
}

@article{babaei19_DataAugmentationAutoEncoders,
  title = {Data {{Augmentation}} by {{AutoEncoders}} for {{Unsupervised Anomaly Detection}}},
  author = {Babaei, Kasra and Chen, ZhiYuan and Maul, Tomas},
  year = {2019},
  month = dec,
  abstract = {This paper proposes an autoencoder (AE) that is used for improving the performance of once-class classifiers for the purpose of detecting anomalies. Traditional one-class classifiers (OCCs) perform poorly under certain conditions such as high-dimensionality and sparsity. Also, the size of the training set plays an important role on the performance of one-class classifiers. Autoencoders have been widely used for obtaining useful latent variables from high-dimensional datasets. In the proposed approach, the AE is capable of deriving meaningful features from highdimensional datasets while doing data augmentation at the same time. The augmented data is used for training the OCC algorithms. The experimental results show that the proposed approach enhance the performance of OCC algorithms and also outperforms other well-known approaches.},
  archivePrefix = {arXiv},
  eprint = {1912.13384},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/JX2LXWEQ/Babaei et al. - 2019 - Data Augmentation by AutoEncoders for Unsupervised.pdf},
  journal = {arXiv:1912.13384 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bacciu19_GentleIntroductionDeep,
  title = {A {{Gentle Introduction}} to {{Deep Learning}} for {{Graphs}}},
  author = {Bacciu, Davide and Errica, Federico and Micheli, Alessio and Podda, Marco},
  year = {2019},
  month = dec,
  abstract = {The adaptive processing of graph data is a long-standing research topic which has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is designed as a tutorial introduction to the field of deep learning for graphs. It favours a consistent and progressive introduction of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view to the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing. It introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. The methodological exposition is complemented by a discussion of interesting research challenges and applications in the field.},
  archivePrefix = {arXiv},
  eprint = {1912.12693},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bacciu et al_2019_a gentle introduction to deep learning for graphs.pdf;/home/trung/Zotero/storage/5LCCXKR2/1912.html},
  journal = {arXiv:1912.12693 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{bachman19_LearningRepresentationsMaximizing,
  title = {Learning {{Representations}} by {{Maximizing Mutual Information Across Views}}},
  author = {Bachman, Philip and Hjelm, R. Devon and Buchwalter, William},
  year = {2019},
  month = jul,
  abstract = {We propose an approach to self-supervised representation learning based on maximizing mutual information between features extracted from multiple views of a shared context. For example, one could produce multiple views of a local spatio-temporal context by observing it from different locations (e.g., camera positions within a scene), and via different modalities (e.g., tactile, auditory, or visual). Or, an ImageNet image could provide a context from which one produces multiple views by repeatedly applying data augmentation. Maximizing mutual information between features extracted from these views requires capturing information about high-level factors whose influence spans multiple views -- e.g., presence of certain objects or occurrence of certain events. Following our proposed approach, we develop a model which learns image representations that significantly outperform prior methods on the tasks we consider. Most notably, using self-supervised learning, our model learns representations which achieve 68.1\% accuracy on ImageNet using standard linear evaluation. This beats prior results by over 12\% and concurrent results by 7\%. When we extend our model to use mixture-based representations, segmentation behaviour emerges as a natural side-effect. Our code is available online: https://github.com/Philip-Bachman/amdim-public.},
  archivePrefix = {arXiv},
  eprint = {1906.00910},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bachman et al_2019_learning representations by maximizing mutual information across views.pdf},
  journal = {arXiv:1906.00910 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@techreport{badsha18_Imputationsinglecellgene,
  title = {Imputation of Single-Cell Gene Expression with an Autoencoder Neural Network},
  author = {Badsha, Md. Bahadur and Li, Rui and Liu, Boxiang and Li, Yang I. and Xian, Min and Banovich, Nicholas E. and Fu, Audrey Qiuyan},
  year = {2018},
  month = dec,
  institution = {{Bioinformatics}},
  doi = {10.1101/504977},
  abstract = {ABSTRACT                        Background             Single-cell RNA-sequencing (scRNA-seq) is a rapidly evolving technology that enables measurement of gene expression levels at an unprecedented resolution. Despite the explosive growth in the number of cells that can be assayed by a single experiment, scRNA-seq still has several limitations, including high rates of dropouts, which result in a large number of genes having zero read count in the scRNA-seq data, and complicate downstream analyses.                                   Methods             To overcome this problem, we treat zeros as missing values and develop nonparametric deep learning methods for imputation. Specifically, our LATE (Learning with AuToEncoder) method trains an autoencoder with random initial values of the parameters, whereas our TRANSLATE (TRANSfer learning with LATE) method further allows for the use of a reference gene expression data set to provide LATE with an initial set of parameter estimates.                                   Results             On both simulated and real data, LATE and TRANSLATE outperform existing scRNA-seq imputation methods, achieving lower mean squared error in most cases, recovering nonlinear gene-gene relationships, and better separating cell types. They are also highly scalable and can efficiently process over 1 million cells in just a few hours on a GPU.                                   Conclusions             We demonstrate that our nonparametric approach to imputation based on autoencoders is powerful and highly efficient.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/badsha et al_2018_imputation of single-cell gene expression with an autoencoder neural network.pdf},
  language = {en},
  type = {Preprint}
}

@article{baek20_SinglecellATACsequencing,
  title = {Single-Cell {{ATAC}} Sequencing Analysis: {{From}} Data Preprocessing to Hypothesis Generation},
  shorttitle = {Single-Cell {{ATAC}} Sequencing Analysis},
  author = {Baek, Seungbyn and Lee, Insuk},
  year = {2020},
  month = jun,
  volume = {18},
  pages = {1429--1439},
  issn = {2001-0370},
  doi = {10.1016/j.csbj.2020.06.012},
  abstract = {Most genetic variations associated with human complex traits are located in non-coding genomic regions. Therefore, understanding the genotype-to-phenotype axis requires a comprehensive catalog of functional non-coding genomic elements, most of which are involved in epigenetic regulation of gene expression. Genome-wide maps of open chromatin regions can facilitate functional analysis of cis- and trans-regulatory elements via their connections with trait-associated sequence variants. Currently, Assay for Transposase Accessible Chromatin with high-throughput sequencing (ATAC-seq) is considered the most accessible and cost-effective strategy for genome-wide profiling of chromatin accessibility. Single-cell ATAC-seq (scATAC-seq) technology has also been developed to study cell type-specific chromatin accessibility in tissue samples containing a heterogeneous cellular population. However, due to the intrinsic nature of scATAC-seq data, which are highly noisy and sparse, accurate extraction of biological signals and devising effective biological hypothesis are difficult. To overcome such limitations in scATAC-seq data analysis, new methods and software tools have been developed over the past few years. Nevertheless, there is no consensus for the best practice of scATAC-seq data analysis yet. In this review, we discuss scATAC-seq technology and data analysis methods, ranging from preprocessing to downstream analysis, along with an up-to-date list of published studies that involved the application of this method. We expect this review will provide a guideline for successful data generation and analysis methods using appropriate software tools and databases for the study of chromatin accessibility at single-cell resolution.},
  file = {/home/trung/GoogleDrive/Zotero/baek et al_2020_single-cell atac sequencing analysis.pdf},
  journal = {Computational and Structural Biotechnology Journal},
  pmcid = {PMC7327298},
  pmid = {32637041}
}

@article{baevski20_wav2vecFrameworkSelfSupervised,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self}}-{{Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  year = {2020},
  month = sep,
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/noisy test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 5.2/8.6 WER on the noisy/clean test sets of Librispeech. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
  archivePrefix = {arXiv},
  eprint = {2006.11477},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/baevski et al_2020_wav2vec 2.pdf},
  journal = {arXiv:2006.11477 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{baevski20_wav2vecFrameworkSelfSuperviseda,
  title = {Wav2vec 2.0: {{A Framework}} for {{Self}}-{{Supervised Learning}} of {{Speech Representations}}},
  shorttitle = {Wav2vec 2.0},
  author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  year = {2020},
  month = oct,
  abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
  archivePrefix = {arXiv},
  eprint = {2006.11477},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/baevski et al_2020_wav2vec 3.pdf},
  journal = {arXiv:2006.11477 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{bagad20_CoughCOVIDEvidence,
  title = {Cough {{Against COVID}}: {{Evidence}} of {{COVID}}-19 {{Signature}} in {{Cough Sounds}}},
  shorttitle = {Cough {{Against COVID}}},
  author = {Bagad, Piyush and Dalmia, Aman and Doshi, Jigar and Nagrani, Arsha and Bhamare, Parag and Mahale, Amrita and Rane, Saurabh and Agarwal, Neeraj and Panicker, Rahul},
  year = {2020},
  month = sep,
  abstract = {Testing capacity for COVID-19 remains a challenge globally due to the lack of adequate supplies, trained personnel, and sample-processing equipment. These problems are even more acute in rural and underdeveloped regions. We demonstrate that solicited-cough sounds collected over a phone, when analysed by our AI model, have statistically significant signal indicative of COVID-19 status (AUC 0.72, t-test,p {$<$}0.01,95\% CI 0.61-0.83). This holds true for asymptomatic patients as well. Towards this, we collect the largest known(to date) dataset of microbiologically confirmed COVID-19 cough sounds from 3,621 individuals. When used in a triaging step within an overall testing protocol, by enabling risk-stratification of individuals before confirmatory tests, our tool can increase the testing capacity of a healthcare system by 43\% at disease prevalence of 5\%, without additional supplies, trained personnel, or physical infrastructure},
  archivePrefix = {arXiv},
  eprint = {2009.08790},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bagad et al_2020_cough against covid.pdf},
  journal = {arXiv:2009.08790 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{bagherinezhad18_LabelRefineryImproving,
  title = {Label {{Refinery}}: {{Improving ImageNet Classification}} through {{Label Progression}}},
  shorttitle = {Label {{Refinery}}},
  author = {Bagherinezhad, Hessam and Horton, Maxwell and Rastegari, Mohammad and Farhadi, Ali},
  year = {2018},
  month = may,
  abstract = {Among the three main components (data, labels, and models) of any supervised learning system, data and models have been the main subjects of active research. However, studying labels and their properties has received very little attention. Current principles and paradigms of labeling impose several challenges to machine learning algorithms. Labels are often incomplete, ambiguous, and redundant. In this paper we study the effects of various properties of labels and introduce the Label Refinery: an iterative procedure that updates the ground truth labels after examining the entire dataset. We show significant gain using refined labels across a wide range of models. Using a Label Refinery improves the state-of-the-art top-1 accuracy of (1) AlexNet from 59.3 to 67.2, (2) MobileNet from 70.6 to 73.39, (3) MobileNet-0.25 from 50.6 to 55.59, (4) VGG19 from 72.7 to 75.46, and (5) Darknet19 from 72.9 to 74.47.},
  archivePrefix = {arXiv},
  eprint = {1805.02641},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bagherinezhad et al_2018_label refinery.pdf},
  journal = {arXiv:1805.02641 [cs]},
  primaryClass = {cs}
}

@article{baglini00_Directcausationnew,
  title = {Direct Causation: {{A}} New Approach to an Old Question},
  author = {Baglini, Rebekah and Siegal, Elitzur A Bar-Asher},
  pages = {10},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/baglini et al_direct causation.pdf},
  keywords = {causal},
  language = {en}
}

@article{bahdanau14_NeuralMachineTranslation,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2014},
  month = sep,
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder\textendash decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder\textendash decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  annotation = {ZSCC: 0008751},
  archivePrefix = {arXiv},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bahdanau et al_2014_neural machine translation by jointly learning to align and translate.pdf},
  journal = {arXiv:1409.0473 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bahdanau15_EndtoEndAttentionbasedLarge,
  title = {End-to-{{End Attention}}-Based {{Large Vocabulary Speech Recognition}}},
  author = {Bahdanau, Dzmitry and Chorowski, Jan and Serdyuk, Dmitriy and Brakel, Philemon and Bengio, Yoshua},
  year = {2015},
  month = aug,
  abstract = {Many of the current state-of-the-art Large Vocabulary Continuous Speech Recognition Systems (LVCSR) are hybrids of neural networks and Hidden Markov Models (HMMs). Most of these systems contain separate components that deal with the acoustic modelling, language modelling and sequence decoding. We investigate a more direct approach in which the HMM is replaced with a Recurrent Neural Network (RNN) that performs sequence prediction directly at the character level. Alignment between the input features and the desired character sequence is learned automatically by an attention mechanism built into the RNN. For each predicted character, the attention mechanism scans the input sequence and chooses relevant frames. We propose two methods to speed up this operation: limiting the scan to a subset of most promising frames and pooling over time the information contained in neighboring frames, thereby reducing source sequence length. Integrating an n-gram language model into the decoding process yields recognition accuracies similar to other HMM-free RNN-based approaches.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1508.04395},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bahdanau et al_2015_end-to-end attention-based large vocabulary speech recognition.pdf},
  journal = {arXiv:1508.04395 [cs]},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,speech recognition},
  language = {en},
  primaryClass = {cs}
}

@article{bahl20_Facecoveringsmask,
  title = {Face Coverings and Mask to Minimise Droplet Dispersion and Aerosolisation: A Video Case Study},
  shorttitle = {Face Coverings and Mask to Minimise Droplet Dispersion and Aerosolisation},
  author = {Bahl, Prateek and Bhattacharjee, Shovon and de Silva, Charitha and Chughtai, Abrar Ahmad and Doolan, Con and MacIntyre, C. Raina},
  year = {2020},
  month = jul,
  publisher = {{BMJ Publishing Group Ltd}},
  issn = {0040-6376, 1468-3296},
  doi = {10.1136/thoraxjnl-2020-215748},
  abstract = {To evaluate the effectiveness of the Centers for Disease Control and Prevention (CDC) recommended one- and two-layer cloth face covering against a three-ply surgical mask, we challenged the cloth covering against speaking, coughing and sneezing. The one-layer covering was made using `quick cut T-shirt face covering (no-sew method)' and the two-layer covering was prepared using the sew method prescribed by CDC.1 To provide visual evidence of the efficacy of face coverings we used a tailored LED lighting system (GS Vitec MultiLED PT) along with a high-speed camera (nac MEMRECAM HX-7s) to capture the light scattered by droplets and aerosols expelled during speaking, coughing and sneezing while wearing different types of masks (figure 1 and online supplementary video). The video for speaking was captured at \ldots},
  chapter = {Chest clinic},
  copyright = {\textcopyright{} Author(s) (or their employer(s)) 2020. No commercial re-use. See rights and permissions. Published by BMJ.},
  file = {/home/trung/GoogleDrive/Zotero/bahl et al_2020_face coverings and mask to minimise droplet dispersion and aerosolisation.pdf},
  journal = {Thorax},
  language = {en},
  pmid = {32709611}
}

@article{bahr18_Effectsmassagelikeessential,
  title = {Effects of a Massage-like Essential Oil Application Procedure Using {{Copaiba}} and {{Deep Blue}} Oils in Individuals with Hand Arthritis},
  author = {Bahr, Tyler and Allred, Kathryn and Martinez, Devin and Rodriguez, Damian and Winterton, Paul},
  year = {2018},
  month = nov,
  volume = {33},
  pages = {170--176},
  issn = {17443881},
  doi = {10.1016/j.ctcp.2018.10.004},
  abstract = {Background and Purpose: Existing research suggests that both massage and essential oils may have analgesic and anti-inflammatory benefits. We investigate the benefits of the AromaTouch Hand Technique\textregistered{} (ATHT), a procedure that combines a moderate pressure touch with the application of essential oils to the hand, in individuals with hand arthritis. Methods and materials: Thirty-six participants with rheumatoid arthritis, osteoarthritis, and/or chronic inflammation received ATHTs with either a 50/50 preparation of Deep Blue\textregistered{} and Copaiba oil or a coconut oil placebo twice daily for 5 consecutive days. Changes in maximum flexion in finger and thumb joints, items from the Arthritis Hand Function Test, and hand pain scores were evaluated. Results: Participants treated with the essential oil preparation required significantly less time to complete dexterity tasks and showed about 50\% decrease in pain scores, increased finger strength, and significantly increased angle of maximum flexion compared to subjects treated with coconut oil. Conclusion: The ATHT with Copaiba and Deep Blue may have ameliorative effects on hand arthritis. 1. Introduction Arthritis is a general term describing a condition in which the joints become painful and inflamed. Rheumatoid arthritis (RA) is an autoimmune disorder characterized by chronic inflammatory vascularization and infiltration of the synovial membrane of diarthrodial joints, while osteoarthritis (OA) is caused by the erosion of joint cartilage, resulting in painful bone-against-bone rubbing. Humoral and cellmediated immunity are both implicated in the pathogenesis of RA. Onset of RA is marked by the atypical presentation of self-antigen by immune cells, which is followed by activation of autoreactive T cells. T cells accumulate in the afflicted joints and secrete proinflammatory cytokines, attracting other cells and aggravating the immune response. B cells release autoantibodies and activate T cells, while both B cells and macrophages secrete proinflammatory cytokines (e.g. TNF-{$\alpha$}, IL-6, IL-8) [1]. In RA, neutrophils are found in very high numbers in the joint synovium [2]. Elevated cytokine levels are thought to play a major role in the induction of neutrophil infiltration to the synovium [3]. Although neutrophils are absent in the synovial fluid in patients with OA, inflammatory cytokines, chemokines, and other inflammatory markers are found in pathogenic concentrations in the synovial fluids. While inflammation is a hallmark of OA, it is not its cause, unlike RA.},
  file = {/home/trung/GoogleDrive/Zotero/bahr et al_2018_effects of a massage-like essential oil application procedure using copaiba and deep blue oils in individuals with hand arthritis.pdf},
  journal = {Complementary Therapies in Clinical Practice},
  language = {en}
}

@article{bai18_EmpiricalEvaluationGeneric,
  title = {An {{Empirical Evaluation}} of {{Generic Convolutional}} and {{Recurrent Networks}} for {{Sequence Modeling}}},
  author = {Bai, Shaojie and Kolter, J. Zico and Koltun, Vladlen},
  year = {2018},
  month = apr,
  abstract = {For most deep learning practitioners, sequence modeling is synonymous with recurrent networks. Yet recent results indicate that convolutional architectures can outperform recurrent networks on tasks such as audio synthesis and machine translation. Given a new sequence modeling task or dataset, which architecture should one use? We conduct a systematic evaluation of generic convolutional and recurrent architectures for sequence modeling. The models are evaluated across a broad range of standard tasks that are commonly used to benchmark recurrent networks. Our results indicate that a simple convolutional architecture outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets, while demonstrating longer effective memory. We conclude that the common association between sequence modeling and recurrent networks should be reconsidered, and convolutional networks should be regarded as a natural starting point for sequence modeling tasks. To assist related work, we have made code available at http://github.com/locuslab/TCN .},
  archivePrefix = {arXiv},
  eprint = {1803.01271},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bai et al_2018_an empirical evaluation of generic convolutional and recurrent networks for sequence modeling.pdf;/home/trung/Zotero/storage/KPQ4XV2P/1803.html},
  journal = {arXiv:1803.01271 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,favorite},
  primaryClass = {cs}
}

@article{bai19_TuningFreeDisentanglementProjection,
  title = {Tuning-{{Free Disentanglement}} via {{Projection}}},
  author = {Bai, Yue and Duan, Leo L.},
  year = {2019},
  month = sep,
  abstract = {In representation learning and non-linear dimension reduction, there is a huge interest to learn the ``disentangled'' latent variables, where each sub-coordinate almost uniquely controls a facet of the observed data. While many regularization approaches have been proposed on variational autoencoders, heuristic tuning is required to balance between disentanglement and loss in reconstruction accuracy \textemdash{} due to the unsupervised nature, there is no principled way to find an optimal weight for regularization. Motivated to completely bypass regularization, we consider a projection strategy: modifying the canonical Gaussian encoder, we add a layer of scaling and rotation to the Gaussian mean, such that the marginal correlations among latent sub-coordinates become exactly zero. This achieves a theoretically maximal disentanglement, as guaranteed by zero cross-correlation between one latent sub-coordinate and the observed varying with the rest. Unlike regularizations, the extra projection layer does not impact the flexibility of the previous encoder layers, leading to almost no loss in expressiveness. This approach is simple to implement in practice. Our numerical experiments demonstrate very good performance, with no tuning required.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1906.11732},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bai et al_2019_tuning-free disentanglement via projection.pdf},
  journal = {arXiv:1906.11732 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bai19_TuningFreeDisentanglementProjectiona,
  title = {Tuning-{{Free Disentanglement}} via {{Projection}}},
  author = {Bai, Yue and Duan, Leo L.},
  year = {2019},
  month = sep,
  abstract = {In representation learning and non-linear dimension reduction, there is a huge interest to learn the ``disentangled'' latent variables, where each sub-coordinate almost uniquely controls a facet of the observed data. While many regularization approaches have been proposed on variational autoencoders, heuristic tuning is required to balance between disentanglement and loss in reconstruction accuracy \textemdash{} due to the unsupervised nature, there is no principled way to find an optimal weight for regularization. Motivated to completely bypass regularization, we consider a projection strategy: modifying the canonical Gaussian encoder, we add a layer of scaling and rotation to the Gaussian mean, such that the marginal correlations among latent sub-coordinates become exactly zero. This achieves a theoretically maximal disentanglement, as guaranteed by zero cross-correlation between one latent sub-coordinate and the observed varying with the rest. Unlike regularizations, the extra projection layer does not impact the flexibility of the previous encoder layers, leading to almost no loss in expressiveness. This approach is simple to implement in practice. Our numerical experiments demonstrate very good performance, with no tuning required.},
  archivePrefix = {arXiv},
  eprint = {1906.11732},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bai et al_2019_tuning-free disentanglement via projection2.pdf},
  journal = {arXiv:1906.11732 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bakhtin20_EnergyBasedModelsText,
  title = {Energy-{{Based Models}} for {{Text}}},
  author = {Bakhtin, Anton and Deng, Yuntian and Gross, Sam and Ott, Myle and Ranzato, Marc'Aurelio and Szlam, Arthur},
  year = {2020},
  month = apr,
  abstract = {Current large-scale auto-regressive language models (Radford et al., 2019a,b) display impressive fluency and can generate convincing text. In this work we start by asking the question: Can the generations of these models be reliably distinguished from real text by statistical discriminators? We find experimentally that the answer is affirmative when we have access to the training data for the model, and guardedly affirmative even if we do not. This suggests that the auto-regressive models can be improved by incorporating the (globally normalized) discriminators into the generative process. We give a formalism for this using the Energy-Based Model framework, and show that it indeed improves the results of the generative models, measured both in terms of perplexity and in terms of human evaluation.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2004.10188},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bakhtin et al_2020_energy-based models for text.pdf},
  journal = {arXiv:2004.10188 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{balandat19_BoTorchProgrammableBayesian,
  title = {{{BoTorch}}: {{Programmable Bayesian Optimization}} in {{PyTorch}}},
  shorttitle = {{{BoTorch}}},
  author = {Balandat, Maximilian and Karrer, Brian and Jiang, Daniel R. and Daulton, Samuel and Letham, Benjamin and Wilson, Andrew Gordon and Bakshy, Eytan},
  year = {2019},
  month = oct,
  abstract = {Bayesian optimization provides sample-efficient global optimization for a broad range of applications, including automatic machine learning, molecular chemistry, and experimental design. We introduce BoTorch, a modern programming framework for Bayesian optimization. Enabled by Monte-Carlo (MC) acquisition functions and auto-differentiation, BoTorch's modular design facilitates flexible specification and optimization of probabilistic models written in PyTorch, radically simplifying implementation of novel acquisition functions. Our MC approach is made practical by a distinctive algorithmic foundation that leverages fast predictive distributions and hardware acceleration. In experiments, we demonstrate the improved sample efficiency of BoTorch relative to other popular libraries. BoTorch is open source and available at https://github.com/pytorch/botorch.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.06403},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/balandat et al_2019_botorch.pdf;/home/trung/Zotero/storage/9RETI2D5/1910.html},
  journal = {arXiv:1910.06403 [cs, math, stat]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{balasubramanian20_PolarizedVAEProximityBased,
  title = {Polarized-{{VAE}}: {{Proximity Based Disentangled Representation Learning}} for {{Text Generation}}},
  shorttitle = {Polarized-{{VAE}}},
  author = {Balasubramanian, Vikash and Kobyzev, Ivan and Bahuleyan, Hareesh and Shapiro, Ilya and Vechtomova, Olga},
  year = {2020},
  month = apr,
  abstract = {Learning disentangled representations of real world data is a challenging open problem. Most previous methods have focused on either fully supervised approaches which use attribute labels or unsupervised approaches that manipulate the factorization in the latent space of models such as the variational autoencoder (VAE), by training with task-specific losses. In this work we propose polarized-VAE, a novel approach that disentangles selected attributes in the latent space based on proximity measures reflecting the similarity between data points with respect to these attributes. We apply our method to disentangle the semantics and syntax of a sentence and carry out transfer experiments. Polarized-VAE significantly outperforms the VAE baseline and is competitive with the state-of-the-art approaches, while being more a general framework that is applicable to other attribute disentanglement tasks.},
  archivePrefix = {arXiv},
  eprint = {2004.10809},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/balasubramanian et al_2020_polarized-vae.pdf},
  journal = {arXiv:2004.10809 [cs]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs}
}

@incollection{balasubramanyan13_TopicModelsSemisupervised,
  title = {From {{Topic Models}} to {{Semi}}-Supervised {{Learning}}: {{Biasing Mixed}}-{{Membership Models}} to {{Exploit Topic}}-{{Indicative Features}} in {{Entity Clustering}}},
  shorttitle = {From {{Topic Models}} to {{Semi}}-Supervised {{Learning}}},
  booktitle = {Advanced {{Information Systems Engineering}}},
  author = {Balasubramanyan, Ramnath and Dalvi, Bhavana and Cohen, William W.},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Salinesi, Camille and Norrie, Moira C. and Pastor, {\'O}scar},
  year = {2013},
  volume = {7908},
  pages = {628--642},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-40991-2_40},
  abstract = {We present methods to introduce different forms of supervision into mixed-membership latent variable models. Firstly, we introduce a technique to bias the models to exploit topic-indicative features, i.e. features which are apriori known to be good indicators of the latent topics that generated them. Next, we present methods to modify the Gibbs sampler used for approximate inference in such models to permit injection of stronger forms of supervision in the form of labels for features and documents, along with a description of the corresponding change in the underlying generative process. This ability allows us to span the range from unsupervised topic models to semi-supervised learning in the same mixed membership model. Experimental results from an entity-clustering task demonstrate that the biasing technique and the introduction of feature and document labels provide a significant increase in clustering performance over baseline mixed-membership methods.},
  file = {/home/trung/GoogleDrive/Zotero/balasubramanyan et al_2013_from topic models to semi-supervised learning.pdf},
  isbn = {978-3-642-38708-1 978-3-642-38709-8},
  language = {en}
}

@article{baldi91_ContrastiveLearningNeural,
  title = {Contrastive {{Learning}} and {{Neural Oscillations}}},
  author = {Baldi, Pierre and Pineda, Fernando},
  year = {1991},
  month = dec,
  volume = {3},
  pages = {526--545},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1991.3.4.526},
  abstract = {The concept of Contrastive Learning (CL) is developed as a family of possible learning algorithms for neural networks. CL is an extension of Deterministic Boltzmann Machines to more general dynamical systems. During learning, the network oscillates between two phases. One phase has a teacher signal and one phase has no teacher signal. The weights are updated using a learning rule that corresponds to gradient descent on a contrast function that measures the discrepancy between the free network and the network with a teacher signal. The CL approach provides a general unified framework for developing new learning algorithms. It also shows that many different types of clamping and teacher signals are possible. Several examples are given and an analysis of the landscape of the contrast function is proposed with some relevant predictions for the CL curves. An approach that may be suitable for collective analog implementations is described. Simulation results and possible extensions are briefly discussed together with a new conjecture regarding the function of certain oscillations in the brain. In the appendix, we also examine two extensions of contrastive learning to time-dependent trajectories.},
  annotation = {ZSCC: 0000042},
  file = {/home/trung/Zotero/storage/VCCFK7HV/Baldi and Pineda - 1991 - Contrastive Learning and Neural Oscillations.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {4}
}

@article{balgi19_NeuralDisentanglementusing,
  title = {Neural {{Disentanglement}} Using {{Mixture Latent Space}} with {{Continuous}} and {{Discrete Variables}}},
  author = {Balgi, Sourabh},
  year = {2019},
  pages = {4},
  abstract = {Recent advances in deep learning techniques have shown extordinary capability of deep neural networks in extracting features required to perform the task at hand. However, the features learnt are relevant only for the initial task at hand. This is due to the fact that the features learnt are usually task specific and do not capture the most general and task agnostic features of the input. On the other hand, humans are excellent task agnositc learners by automatically disentangling the features. Recently variational autoencoders (VAEs) have shown to be the de-facto models to capture the latent variables in a generative sense. As these latent features can be represented as continuous and/or discrete variables, this motivated us to use VAEs with a mixture of continuous and discrete variables for the latent space. We achieve this by performing our experiments using a modified version of JointVAE to learn the disentangled features.},
  file = {/home/trung/GoogleDrive/Zotero/balgi_2019_neural disentanglement using mixture latent space with continuous and discrete variables.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{baltrusaitis17_MultimodalMachineLearning,
  title = {Multimodal {{Machine Learning}}: {{A Survey}} and {{Taxonomy}}},
  shorttitle = {Multimodal {{Machine Learning}}},
  author = {Baltru{\v s}aitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
  year = {2017},
  month = aug,
  abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
  archivePrefix = {arXiv},
  eprint = {1705.09406},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/baltrušaitis et al_2017_multimodal machine learning.pdf},
  journal = {arXiv:1705.09406 [cs]},
  primaryClass = {cs}
}

@article{bao17_CVAEGANFineGrainedImage,
  title = {{{CVAE}}-{{GAN}}: {{Fine}}-{{Grained Image Generation}} through {{Asymmetric Training}}},
  shorttitle = {{{CVAE}}-{{GAN}}},
  author = {Bao, Jianmin and Chen, Dong and Wen, Fang and Li, Houqiang and Hua, Gang},
  year = {2017},
  month = oct,
  abstract = {We present variational generative adversarial networks, a general learning framework that combines a variational auto-encoder with a generative adversarial network, for synthesizing images in fine-grained categories, such as faces of a specific person or objects in a category. Our approach models an image as a composition of label and latent attributes in a probabilistic model. By varying the fine-grained category label fed into the resulting generative model, we can generate images in a specific category with randomly drawn values on a latent attribute vector. Our approach has two novel aspects. First, we adopt a cross entropy loss for the discriminative and classifier network, but a mean discrepancy objective for the generative network. This kind of asymmetric loss function makes the GAN training more stable. Second, we adopt an encoder network to learn the relationship between the latent space and the real image space, and use pairwise feature matching to keep the structure of generated images. We experiment with natural images of faces, flowers, and birds, and demonstrate that the proposed models are capable of generating realistic and diverse samples with fine-grained category labels. We further show that our models can be applied to other tasks, such as image inpainting, super-resolution, and data augmentation for training better face recognition models.},
  annotation = {ZSCC: 0000142},
  archivePrefix = {arXiv},
  eprint = {1703.10155},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bao et al_2017_cvae-gan.pdf;/home/trung/Zotero/storage/HDFEMLTL/1703.html},
  journal = {arXiv:1703.10155 [cs]},
  primaryClass = {cs}
}

@article{baratin20_ImplicitRegularizationDeep,
  title = {Implicit {{Regularization}} in {{Deep Learning}}: {{A View}} from {{Function Space}}},
  shorttitle = {Implicit {{Regularization}} in {{Deep Learning}}},
  author = {Baratin, Aristide and George, Thomas and Laurent, C{\'e}sar and Hjelm, R. Devon and Lajoie, Guillaume and Vincent, Pascal and {Lacoste-Julien}, Simon},
  year = {2020},
  month = aug,
  abstract = {We approach the problem of implicit regularization in deep learning from a geometrical viewpoint. We highlight a possible regularization effect induced by a dynamical alignment of the neural tangent features introduced by Jacot et al, along a small number of task-relevant directions. By extrapolating a new analysis of Rademacher complexity bounds in linear models, we propose and study a new heuristic complexity measure for neural networks which captures this phenomenon, in terms of sequences of tangent kernel classes along in the learning trajectories.},
  archivePrefix = {arXiv},
  eprint = {2008.00938},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/baratin et al_2020_implicit regularization in deep learning.pdf},
  journal = {arXiv:2008.00938 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@book{baratin20_ImplicitRegularizationDeepa,
  title = {Implicit {{Regularization}} in {{Deep Learning}}: {{A View}} from {{Function Space}}},
  shorttitle = {Implicit {{Regularization}} in {{Deep Learning}}},
  author = {Baratin, Aristide and George, Thomas and Laurent, C{\'e}sar and Hjelm, R Devon and Lajoie, Guillaume and Vincent, Pascal and {Lacoste-Julien}, Simon},
  year = {2020},
  month = aug,
  abstract = {We approach the problem of implicit regularization in deep learning from a geometrical viewpoint. We highlight a possible regularization effect induced by a dynamical alignment of the neural tangent features introduced by Jacot et al, along a small number of task-relevant directions. By extrapolating a new analysis of Rademacher complexity bounds in linear models, we propose and study a new heuristic complexity measure for neural networks which captures this phenomenon, in terms of sequences of tangent kernel classes along in the learning trajectories.}
}

@book{barber12_Bayesianreasoningmachine,
  title = {Bayesian Reasoning and Machine Learning},
  author = {Barber, David},
  year = {2012},
  publisher = {{Cambridge University Press}},
  address = {{USA}},
  abstract = {Machine learning methods extract value from vast data sets quickly and with modest resources. They are established tools in a wide range of industrial applications, including search engines, DNA sequencing, stock market analysis, and robot locomotion, and their use is spreading rapidly. People who know the methods have their choice of rewarding jobs. This hands-on text opens these opportunities to computer science students with modest mathematical backgrounds. It is designed for final-year undergraduates and master's students with limited background in linear algebra and calculus. Comprehensive and coherent, it develops everything from basic reasoning to advanced techniques within the framework of graphical models. Students learn more than a menu of techniques, they develop analytical and problem-solving skills that equip them for the real world. Numerous examples and exercises, both computer based and theoretical, are included in every chapter. Resources for students and instructors, including a MATLAB toolbox, are available online.},
  file = {/home/trung/GoogleDrive/Zotero/barber_2012_bayesian reasoning and machine learning.pdf},
  isbn = {0-521-51814-8}
}

@article{barcelo00_LogicalExpressivenessGraph,
  title = {Logical {{Expressiveness}} of {{Graph Neural Networks}}},
  author = {Barcel{\'o}, Pablo and Kostylev, Egor V and Monet, Mika{\"e}l},
  pages = {10},
  file = {/home/trung/GoogleDrive/Zotero/barceló et al_logical expressiveness of graph neural networks.pdf},
  language = {en}
}

@techreport{barello18_SparseCodingVariationalAutoEncoders,
  title = {Sparse-{{Coding Variational Auto}}-{{Encoders}}},
  author = {Barello, Gabriel and Charles, Adam S. and Pillow, Jonathan W.},
  year = {2018},
  month = aug,
  institution = {{Neuroscience}},
  doi = {10.1101/399246},
  abstract = {The sparse coding model posits that the visual system has evolved to efficiently code natural stimuli using a sparse set of features from an overcomplete dictionary. The classic sparse coding model suffers from two key limitations, however: (1) computing the neural response to an image patch requires minimizing a nonlinear objective function, which is not neurally plausible; and (2) fitting the model to data has typically relied on an approximate inference method that does not take into account uncertainty. Here we address these two shortcomings by formulating a variational inference method for the sparse coding model inspired by the variational auto-encoder (VAE) framework. The sparse-coding variational auto-encoder (SVAE) augments the classic sparse coding model with a probabilistic recognition model, parametrized by a deep neural network. This recognition model provides a neurally plausible implementation for the mapping from image patches to neural activities, and enables a principled method for fitting the sparse coding model to data via maximization of the evidence lower bound (ELBO). The SVAE differs from the traditional VAE in three important ways: the generative model is the sparse coding model instead of a deep network; the latent representation is overcomplete, with more latent dimensions than image pixels; and the prior over latent variables is a sparse or heavy-tailed instead of Gaussian. We fit the SVAE to natural image data under different assumed prior distributions, and show that it obtains higher test performance than previous fitting methods. Finally, we examine the response properties of the recognition network and show that it captures important nonlinear properties of neurons in the early visual pathway.},
  file = {/home/trung/GoogleDrive/Zotero/barello et al_2018_sparse-coding variational auto-encoders.pdf},
  language = {en},
  type = {Preprint}
}

@article{baron19_CellTypePurification,
  title = {Cell {{Type Purification}} by {{Single}}-{{Cell Transcriptome}}-{{Trained Sorting}}},
  author = {Baron, Chlo{\'e} S. and Barve, Aditya and Muraro, Mauro J. and {van der Linden}, Reinier and Dharmadhikari, Gitanjali and Lyubimova, Anna and {de Koning}, Eelco J.P. and {van Oudenaarden}, Alexander},
  year = {2019},
  month = oct,
  volume = {179},
  pages = {527-542.e19},
  issn = {00928674},
  doi = {10.1016/j.cell.2019.08.006},
  abstract = {Much of current molecular and cell biology research relies on the ability to purify cell types by fluorescence-activated cell sorting (FACS). FACS typically relies on the ability to label cell types of interest with antibodies or fluorescent transgenic constructs. However, antibody availability is often limited, and genetic manipulation is labor intensive or impossible in the case of primary human tissue. To date, no systematic method exists to enrich for cell types without a priori knowledge of cell-type markers. Here, we propose GateID, a computational method that combines single-cell transcriptomics with FACS index sorting to purify cell types of choice using only native cellular properties such as cell size, granularity, and mitochondrial content. We validate GateID by purifying various cell types from zebrafish kidney marrow and the human pancreas to high purity without resorting to specific antibodies or transgenes.},
  file = {/home/trung/GoogleDrive/Zotero/baron et al_2019_cell type purification by single-cell transcriptome-trained sorting.pdf},
  journal = {Cell},
  language = {en},
  number = {2}
}

@misc{barrat19_robbiebarratartDCGAN,
  title = {Robbiebarrat/Art-{{DCGAN}}},
  author = {Barrat, Robbie},
  year = {2019},
  month = dec,
  abstract = {Modified implementation of DCGAN focused on generative art. Includes pre-trained models for landscapes, nude-portraits, and others.}
}

@article{barrow17_UseDeepLearning,
  title = {The {{Use Of Deep Learning To Solve Invariance Issues In Object Recognition}}},
  author = {Barrow, Erik Jonathan},
  year = {2017},
  pages = {152},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/barrow_2017_the use of deep learning to solve invariance issues in object recognition.pdf},
  language = {en}
}

@article{bartunov19_MetaLearningDeepEnergyBased,
  title = {Meta-{{Learning Deep Energy}}-{{Based Memory Models}}},
  author = {Bartunov, Sergey and Rae, Jack W. and Osindero, Simon and Lillicrap, Timothy P.},
  year = {2019},
  month = oct,
  abstract = {We study the problem of learning associative memory -- a system which is able to retrieve a remembered pattern based on its distorted or incomplete version. Attractor networks provide a sound model of associative memory: patterns are stored as attractors of the network dynamics and associative retrieval is performed by running the dynamics starting from a query pattern until it converges to an attractor. In such models the dynamics are often implemented as an optimization procedure that minimizes an energy function, such as in the classical Hopfield network. In general it is difficult to derive a writing rule for a given dynamics and energy that is both compressive and fast. Thus, most research in energy-based memory has been limited either to tractable energy models not expressive enough to handle complex high-dimensional objects such as natural images, or to models that do not offer fast writing. We present a novel meta-learning approach to energy-based memory models (EBMM) that allows one to use an arbitrary neural architecture as an energy model and quickly store patterns in its weights. We demonstrate experimentally that our EBMM approach can build compressed memories for synthetic and natural data, and is capable of associative retrieval that outperforms existing memory systems in terms of the reconstruction error and compression rate.},
  archivePrefix = {arXiv},
  eprint = {1910.02720},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bartunov et al_2019_meta-learning deep energy-based memory models.pdf;/home/trung/GoogleDrive/Zotero/bartunov et al_2019_meta-learning deep energy-based memory models2.pdf;/home/trung/GoogleDrive/Zotero/bartunov et al_2019_meta-learning deep energy-based memory models3.pdf;/home/trung/Zotero/storage/2D2DZIW5/1910.html;/home/trung/Zotero/storage/N5YAR8C2/1910.html;/home/trung/Zotero/storage/WZBLYHZZ/1910.html},
  journal = {arXiv:1910.02720 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,energy,energy base,energy based,memory,meta learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{bartz20_OneModelReconstruct,
  title = {One {{Model}} to {{Reconstruct Them All}}: {{A Novel Way}} to {{Use}} the {{Stochastic Noise}} in {{StyleGAN}}},
  shorttitle = {One {{Model}} to {{Reconstruct Them All}}},
  author = {Bartz, Christian and Bethge, Joseph and Yang, Haojin and Meinel, Christoph},
  year = {2020},
  month = oct,
  abstract = {Generative Adversarial Networks (GANs) have achieved state-of-the-art performance for several image generation and manipulation tasks. Different works have improved the limited understanding of the latent space of GANs by embedding images into specific GAN architectures to reconstruct the original images. We present a novel StyleGAN-based autoencoder architecture, which can reconstruct images with very high quality across several data domains. We demonstrate a previously unknown grade of generalizablility by training the encoder and decoder independently and on different datasets. Furthermore, we provide new insights about the significance and capabilities of noise inputs of the well-known StyleGAN architecture. Our proposed architecture can handle up to 40 images per second on a single GPU, which is approximately 28x faster than previous approaches. Finally, our model also shows promising results, when compared to the state-of-the-art on the image denoising task, although it was not explicitly designed for this task.},
  archivePrefix = {arXiv},
  eprint = {2010.11113},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bartz et al_2020_one model to reconstruct them all.pdf},
  journal = {arXiv:2010.11113 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{basharat20_overviewalgorithmsassociated,
  title = {An Overview of Algorithms and Associated Applications for Single Cell {{RNA}}-{{Seq}} Data Imputation},
  author = {Basharat, Zarrin and Majeed, Sania and Saleem, Humaira and Khan, Ishtiaq Ahmad and Yasmin, Azra},
  year = {2020},
  month = jul,
  volume = {21},
  issn = {13892029},
  doi = {10.2174/1389202921999200716104916},
  abstract = {Single cell RNA-Seq technology enables the assessment of RNA expression in individual cells. This makes it popular in experimental biology for gleaning specifications of novel cell types as well as inferring heterogeneity. Experimental data conventionally contains zero counts or dropout events for many single cell transcripts. Such missing data hampers the accurate analysis using standard workflows, designed for massive RNA-Seq datasets. Imputation for single cell datasets is done to infer the missing values. This was traditionally done with ad-hoc code but later customized pipelines, workflows and specialized software appeared for this purpose. This made it easy to benchmark and cluster things in an organized manner. In this review, we have assembled a catalog of available RNASeq single cell imputation algorithms/workflows and associated softwares for the scientific community performing single-cell RNA-Seq data analysis. Continued development of imputation methods, especially using deep learning approaches, would be necessary for eradicating associated pitfalls and addressing challenges associated with future large scale and heterogeneous datasets.},
  file = {/home/trung/GoogleDrive/Zotero/basharat et al_2020_an overview of algorithms and associated applications for single cell rna-seq data imputation.pdf},
  journal = {Current Genomics},
  language = {en}
}

@article{battenberg19_LocationRelativeAttentionMechanisms,
  title = {Location-{{Relative Attention Mechanisms For Robust Long}}-{{Form Speech Synthesis}}},
  author = {Battenberg, Eric and {Skerry-Ryan}, R. J. and Mariooryad, Soroosh and Stanton, Daisy and Kao, David and Shannon, Matt and Bagby, Tom},
  year = {2019},
  month = oct,
  abstract = {Despite the ability to produce human-level speech for in-domain text, attention-based end-to-end text-to-speech (TTS) systems suffer from text alignment failures that increase in frequency for out-of-domain text. We show that these failures can be addressed using simple location-relative attention mechanisms that do away with content-based query/key comparisons. We compare two families of attention mechanisms: location-relative GMM-based mechanisms and additive energy-based mechanisms. We suggest simple modifications to GMM-based attention that allow it to align quickly and consistently during training, and introduce a new location-relative attention mechanism to the additive energy-based family, called Dynamic Convolution Attention (DCA). We compare the various mechanisms in terms of alignment speed and consistency during training, naturalness, and ability to generalize to long utterances, and conclude that GMM attention and DCA can generalize to very long utterances, while preserving naturalness for shorter, in-domain utterances.},
  archivePrefix = {arXiv},
  eprint = {1910.10288},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/battenberg et al_2019_location-relative attention mechanisms for robust long-form speech synthesis.pdf;/home/trung/Zotero/storage/4JJ5V68J/1910.html},
  journal = {arXiv:1910.10288 [cs, eess]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{bau20_Understandingroleindividual,
  title = {Understanding the Role of Individual Units in a Deep Neural Network},
  author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Lapedriza, Agata and Zhou, Bolei and Torralba, Antonio},
  year = {2020},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  doi = {10.1073/pnas.1907375117},
  abstract = {Deep neural networks excel at finding hierarchical representations that solve complex tasks over large datasets. How can we humans understand these learned representations? In this work, we present network dissection, an analytic framework to systematically identify the semantics of individual hidden units within image classification and image generation networks. First, we analyze a convolutional neural network (CNN) trained on scene classification and discover units that match a diverse set of object concepts. We find evidence that the network has learned many object classes that play crucial roles in classifying scene classes. Second, we use a similar analytic method to analyze a generative adversarial network (GAN) model trained to generate scenes. By analyzing changes made when small sets of units are activated or deactivated, we find that objects can be added and removed from the output scenes while adapting to the context. Finally, we apply our analytic framework to understanding adversarial attacks and to semantic image editing.The code, trained model weights, and datasets needed to reproduce the results in this paper are public and available to download from GitHub at https://github.com/davidbau/dissect and at the project website at https://dissect.csail.mit.edu/data/.},
  elocation-id = {201907375},
  eprint = {https://www.pnas.org/content/early/2020/08/31/1907375117.full.pdf},
  file = {/home/trung/GoogleDrive/Zotero/bau et al_2020_understanding the role of individual units in a deep neural network.pdf},
  journal = {Proceedings of the National Academy of Sciences}
}

@article{bau20_UnitsGANsExtended,
  title = {On the {{Units}} of {{GANs}} ({{Extended Abstract}})},
  author = {Bau, David and Zhu, Jun-Yan and Strobelt, Hendrik and Zhou, Bolei and Tenenbaum, Joshua B. and Freeman, William T. and Torralba, Antonio},
  year = {2020},
  month = aug,
  abstract = {Generative Adversarial Networks (GANs) have achieved impressive results for many real-world applications. As an active research topic, many GAN variants have emerged with improvements in sample quality and training stability. However, visualization and understanding of GANs is largely missing. How does a GAN represent our visual world internally? What causes the artifacts in GAN results? How do architectural choices affect GAN learning? Answering such questions could enable us to develop new insights and better models. In this work, we present an analytic framework to visualize and understand GANs at the unit-, object-, and scene-level. We first identify a group of interpretable units that are closely related to concepts with a segmentation-based network dissection method. We quantify the causal effect of interpretable units by measuring the ability of interventions to control objects in the output. Finally, we examine the contextual relationship between these units and their surrounding by inserting the discovered concepts into new images. We show several practical applications enabled by our framework, from comparing internal representations across different layers, models, and datasets, to improving GANs by locating and removing artifact-causing units, to interactively manipulating objects in the scene. We will open source our interactive tools to help researchers and practitioners better understand their models.},
  archivePrefix = {arXiv},
  eprint = {1901.09887},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bau et al_2020_on the units of gans (extended abstract).pdf},
  journal = {arXiv:1901.09887 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{baudart18_DeepProbabilisticProgramming,
  title = {Deep {{Probabilistic Programming Languages}}: {{A Qualitative Study}}},
  shorttitle = {Deep {{Probabilistic Programming Languages}}},
  author = {Baudart, Guillaume and Hirzel, Martin and Mandel, Louis},
  year = {2018},
  month = apr,
  abstract = {Deep probabilistic programming languages try to combine the advantages of deep learning with those of probabilistic programming languages. If successful, this would be a big step forward in machine learning and programming languages. Unfortunately, as of now, this new crop of languages is hard to use and understand. This paper addresses this problem directly by explaining deep probabilistic programming languages and indirectly by characterizing their current strengths and weaknesses.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1804.06458},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/baudart et al_2018_deep probabilistic programming languages.pdf;/home/trung/Zotero/storage/YFD5WUGX/1804.html},
  journal = {arXiv:1804.06458 [cs]},
  primaryClass = {cs}
}

@article{bauer19_ResampledPriorsVariational,
  title = {Resampled {{Priors}} for {{Variational Autoencoders}}},
  author = {Bauer, Matthias and Mnih, Andriy},
  year = {2019},
  month = apr,
  abstract = {We propose Learned Accept/Reject Sampling (LARS), a method for constructing richer priors using rejection sampling with a learned acceptance function. This work is motivated by recent analyses of the VAE objective, which pointed out that commonly used simple priors can lead to underfitting. As the distribution induced by LARS involves an intractable normalizing constant, we show how to estimate it and its gradients efficiently. We demonstrate that LARS priors improve VAE performance on several standard datasets both when they are learned jointly with the rest of the model and when they are fitted to a pretrained model. Finally, we show that LARS can be combined with existing methods for defining flexible priors for an additional boost in performance.},
  archivePrefix = {arXiv},
  eprint = {1810.11428},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bauer et al_2019_resampled priors for variational autoencoders.pdf;/home/trung/Zotero/storage/YJYYXLA3/1810.html},
  journal = {arXiv:1810.11428 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{belagiannis18_AdversarialNetworkCompression,
  title = {Adversarial {{Network Compression}}},
  author = {Belagiannis, Vasileios and Farshad, Azade and Galasso, Fabio},
  year = {2018},
  month = nov,
  abstract = {Neural network compression has recently received much attention due to the computational requirements of modern deep models. In this work, our objective is to transfer knowledge from a deep and accurate model to a smaller one. Our contributions are threefold: (i) we propose an adversarial network compression approach to train the small student network to mimic the large teacher, without the need for labels during training; (ii) we introduce a regularization scheme to prevent a trivially-strong discriminator without reducing the network capacity and (iii) our approach generalizes on different teacher-student models.},
  annotation = {ZSCC: 0000019},
  archivePrefix = {arXiv},
  eprint = {1803.10750},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/belagiannis et al_2018_adversarial network compression.pdf;/home/trung/GoogleDrive/Zotero/belagiannis et al_2018_adversarial network compression2.pdf},
  journal = {arXiv:1803.10750 [cs]},
  keywords = {adversarial,compression,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{belghazi18_MutualInformationNeural,
  title = {Mutual {{Information Neural Estimation}}},
  author = {Belghazi, Mohamed Ishmael and Baratin, Aristide and Rajeswar, Sai and Ozair, Sherjil and Bengio, Yoshua and Courville, Aaron and Hjelm, R Devon},
  year = {2018},
  pages = {10},
  abstract = {We argue that the estimation of mutual information between high dimensional continuous random variables can be achieved by gradient descent over neural networks. We present a Mutual Information Neural Estimator (MINE) that is linearly scalable in dimensionality as well as in sample size, trainable through back-prop, and strongly consistent. We present a handful of applications on which MINE can be used to minimize or maximize mutual information. We apply MINE to improve adversarially trained generative models. We also use MINE to implement the Information Bottleneck, applying it to supervised classification; our results demonstrate substantial improvement in flexibility and performance in these settings.},
  annotation = {ZSCC: 0000146},
  file = {/home/trung/GoogleDrive/Zotero/belghazi et al_2018_mutual information neural estimation.pdf;/home/trung/GoogleDrive/Zotero/belghazi et al_2018_mutual information neural estimation2.pdf},
  keywords = {favorite,information},
  language = {en}
}

@article{bell20_AdaptationAlgorithmsSpeech,
  title = {Adaptation {{Algorithms}} for {{Speech Recognition}}: {{An Overview}}},
  shorttitle = {Adaptation {{Algorithms}} for {{Speech Recognition}}},
  author = {Bell, Peter and Fainberg, Joachim and Klejch, Ondrej and Li, Jinyu and Renals, Steve and Swietojanski, Pawel},
  year = {2020},
  month = aug,
  abstract = {We present a structured overview of adaptation algorithms for neural network-based speech recognition, considering both hybrid hidden Markov model / neural network systems and end-to-end neural network systems, with a focus on speaker adaptation, domain adaptation, and accent adaptation. The overview characterizes adaptation algorithms as based on embeddings, model parameter adaptation, or data augmentation. We present a meta-analysis of the performance of speech recognition adaptation algorithms, based on relative error rate reductions as reported in the literature.},
  archivePrefix = {arXiv},
  eprint = {2008.06580},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bell et al_2020_adaptation algorithms for speech recognition.pdf},
  journal = {arXiv:2008.06580 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{bellar15_effectdaysalpha,
  title = {The Effect of 6~Days of Alpha Glycerylphosphorylcholine on Isometric Strength},
  author = {Bellar, David and LeBlanc, Nina R. and Campbell, Brian},
  year = {2015},
  month = nov,
  volume = {12},
  issn = {1550-2783},
  doi = {10.1186/s12970-015-0103-x},
  abstract = {Background Ergogenic aides are widely used by fitness enthusiasts and athletes to increase performance. Alpha glycerylphosphorylcholine (A-GPC) has demonstrated some initial promise in changing explosive performance. The purpose of the present investigation was to determine if 6~days of supplementation with A-GPC would augment isometric force production compared to a placebo. Methods Thirteen college-aged males (Means\,{$\pm$}\,SD; Age: 21.9\,{$\pm$}\,2.2~years, Height: 180.3\,{$\pm$}\,7.7~cm, Weight: 87.6\,{$\pm$}\,15.6~kg; VO2 max: 40.08\,{$\pm$}\,7.23~ml O2*Kg-1*min-1, Body Fat: 17.5\,{$\pm$}\,4.6~\%) gave written informed consent to participate in the study. The study was a double blind, placebo controlled, cross-over design. The participants reported to the lab for an initial visit where they were familiarized with the isometric mid thigh pull in a custom squat cage on a force platform and upper body isometric test against a high frequency load cell, and baseline measurements were taken for both. The participant then consumed either 600~mg per day of A-GPC or placebo and at the end of 6~days performed isometric mid thigh pulls and an upper body isometric test. A one-week washout period was used before the participants' baseline was re-measured and crossed over to the other treatment. Results The A-GPC treatment resulted in significantly greater isometric mid thigh pull peak force change from baseline (t\,=\,1.76, p\,=\,0.044) compared with placebo (A-GPC: 98.8.\,{$\pm$}\,236.9~N vs Placebo: -39.0\,{$\pm$}\,170.9~N). For the upper body test the A-GPC treatment trended towards greater change from baseline force production (A-GPC: 50.9\,{$\pm$}\,167.2~N Placebo: -14.9\,{$\pm$}\,114.9~N) but failed to obtain statistical significance (t\,=\,1.16, p\,=\,0.127). Conclusions A-GPC is effective at increasing lower body force production after 6~days of supplementation. Sport performance coaches can consider adding A-GPC to the diet of speed and power athletes to enhance muscle performance.},
  file = {/home/trung/GoogleDrive/Zotero/bellar et al_2015_the effect of 6 days of alpha glycerylphosphorylcholine on isometric strength.pdf},
  journal = {Journal of the International Society of Sports Nutrition},
  pmcid = {PMC4650143},
  pmid = {26582972}
}

@article{bellot20_GeneralizationInvariancesPresence,
  title = {Generalization and {{Invariances}} in the {{Presence}} of {{Unobserved Confounding}}},
  author = {Bellot, Alexis and {van der Schaar}, Mihaela},
  year = {2020},
  month = jul,
  abstract = {The ability to extrapolate, or generalize, from observed to new related environments is central to any form of reliable machine learning, yet most methods fail when moving beyond i.i.d data. In some cases, the reason lies in a misappreciation of the causal structure that governs the observed data. But, in others, it is unobserved data, such as hidden confounders, that drive changes in observed distributions and distort observed correlations. In this paper, we argue that generalization must be defined with respect to a broader class of distribution shifts, irrespective of their origin (arising from changes in observed, unobserved or target variables). We propose a new learning principle from which we may expect an explicit notion of generalization to certain new environments, even in the presence of hidden confounding. This principle leads us to formulate a general objective that may be paired with any gradient-based learning algorithm; algorithms that have a causal interpretation in some cases and enjoy notions of predictive stability in others. We demonstrate the empirical performance of our approach on healthcare data from different modalities, including image and speech data.},
  archivePrefix = {arXiv},
  eprint = {2007.10653},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bellot et al_2020_generalization and invariances in the presence of unobserved confounding.pdf},
  journal = {arXiv:2007.10653 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ben-david00_DoesUnlabeledData,
  title = {Does {{Unlabeled Data Provably Help}}? {{Worst}}-Case {{Analysis}} of the {{Sample Complexity}} of {{Semi}}-{{Supervised Learning}}},
  author = {{Ben-David}, Shai and Lu, Tyler and Pal, David},
  pages = {12},
  abstract = {We study the potential benefits to classification prediction that arise from having access to unlabeled samples. We compare learning in the semi-supervised model to the standard, supervised PAC (distribution free) model, considering both the realizable and the unrealizable (agnostic) settings.},
  file = {/home/trung/GoogleDrive/Zotero/ben-david et al_does unlabeled data provably help.pdf},
  language = {en}
}

@article{ben-david00_ImpossibilityTheoremsDomain,
  title = {Impossibility {{Theorems}} for {{Domain Adaptation}}},
  author = {{Ben-David}, Shai and Luu, Teresa and Lu, Tyler and Pal, David},
  pages = {8},
  abstract = {The domain adaptation problem in machine learning occurs when the test data generating distribution differs from the one that generates the training data. It is clear that the success of learning under such circumstances depends on similarities between the two data distributions. We study assumptions about the relationship between the two distributions that one needed for domain adaptation learning to succeed. We analyze the assumptions in an agnostic PAC-style learning model for a the setting in which the learner can access a labeled training data sample and an unlabeled sample generated by the test data distribution. We focus on three assumptions: (i) similarity between the unlabeled distributions, (ii) existence of a classifier in the hypothesis class with low error on both training and testing distributions, and (iii) the covariate shift assumption. I.e., the assumption that the conditioned label distribution (for each data point) is the same for both the training and test distributions. We show that without either assumption (i) or (ii), the combination of the remaining assumptions is not sufficient to guarantee successful learning. Our negative results hold with respect to any domain adaptation learning algorithm, as long as it does not have access to target labeled examples. In particular, we provide formal proofs that the popular covariate shift assumption is rather weak and does not relieve the necessity of the other assumptions.},
  file = {/home/trung/GoogleDrive/Zotero/ben-david et al_impossibility theorems for domain adaptation.pdf},
  language = {en}
}

@article{ben-david14_Domainadaptationcan,
  title = {Domain Adaptation\textendash Can Quantity Compensate for Quality?},
  author = {{Ben-David}, Shai and Urner, Ruth},
  year = {2014},
  month = mar,
  volume = {70},
  pages = {185--202},
  issn = {1012-2443, 1573-7470},
  doi = {10.1007/s10472-013-9371-9},
  abstract = {The Domain Adaptation problem in machine learning occurs when the distribution generating the test data differs from the one that generates the training data. A common approach to this issue is to train a standard learner for the learning task with the available training sample (generated by a distribution that is different from the test distribution). In this work we address this approach, investigating whether there exist successful learning methods for which learning of a target task can be achieved by substituting the standard target-distribution generated sample by a (possibly larger) sample generated by a different distribution without worsening the error guarantee on the learned classifier. We give a positive answer, showing that this is possible when using a Nearest Neighbor algorithm. We show this under the assumptions of covariate shift as well as a bound on the ratio of the probability weights between the source (training) and target (test) distribution. We further show that these assumptions are not always sufficient to allow such a replacement of the training sample: For proper learning, where the output classifier has to come from a predefined class, we prove that any learner needs access to data generated from the target distribution.},
  file = {/home/trung/GoogleDrive/Zotero/ben-david et al_2014_domain adaptation–can quantity compensate for quality.pdf},
  journal = {Annals of Mathematics and Artificial Intelligence},
  language = {en},
  number = {3}
}

@inproceedings{bender20_CanWeightSharing,
  title = {Can {{Weight Sharing Outperform Random Architecture Search}}? {{An Investigation With TuNAS}}},
  shorttitle = {Can {{Weight Sharing Outperform Random Architecture Search}}?},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Bender, Gabriel and Liu, Hanxiao and Chen, Bo and Chu, Grace and Cheng, Shuyang and Kindermans, Pieter-Jan and Le, Quoc V.},
  year = {2020},
  month = jun,
  pages = {14311--14320},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01433},
  abstract = {Efficient Neural Architecture Search methods based on weight sharing have shown good promise in democratizing Neural Architecture Search for computer vision models. There is, however, an ongoing debate whether these efficient methods are significantly better than random search. Here we perform a thorough comparison between efficient and random search methods on a family of progressively larger and more challenging search spaces for image classification and detection on ImageNet and COCO. While the efficacies of both methods are problem-dependent, our experiments demonstrate that there are large, realistic tasks where efficient search methods can provide substantial gains over random search. In addition, we propose and evaluate techniques which improve the quality of searched architectures and reduce the need for manual hyper-parameter tuning.},
  file = {/home/trung/GoogleDrive/Zotero/bender et al_2020_can weight sharing outperform random architecture search.pdf},
  isbn = {978-1-72817-168-5},
  language = {en}
}

@article{bengio00_DeepGenerativeStochastic,
  title = {Deep {{Generative Stochastic Networks Trainable}} by {{Backprop}}},
  author = {Bengio, Yoshua and {Thibodeau-Laufer}, {\'E}ric and Alain, Guillaume and Yosinski, Jason},
  pages = {9},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. The transition distribution of the Markov chain is conditional on the previous state, generally involving a small move, so this conditional distribution has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn because it is easier to approximate its partition function, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. We provide theorems that generalize recent work on the probabilistic interpretation of denoising autoencoders and obtain along the way an interesting justification for dependency networks and generalized pseudolikelihood, along with a definition of an appropriate joint distribution and sampling mechanism even when the conditionals are not consistent. GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. We validate these theoretical results with experiments on two image datasets using an architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with simple backprop, without the need for layerwise pretraining.},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_deep generative stochastic networks trainable by backprop.pdf},
  language = {en}
}

@article{bengio00_ScalingLearningAlgorithms,
  title = {Scaling {{Learning Algorithms}} towards {{AI}}},
  author = {Bengio, Yoshua and LeCun, Yann},
  pages = {41},
  abstract = {One long-term goal of machine learning research is to produce methods that are applicable to highly complex tasks, such as perception (vision, audition), reasoning, intelligent control, and other artificially intelligent behaviors. We argue that in order to progress toward this goal, the Machine Learning community must endeavor to discover algorithms that can learn highly complex functions, with minimal need for prior knowledge, and with minimal human intervention. We present mathematical and empirical evidence suggesting that many popular approaches to non-parametric learning, particularly kernel methods, are fundamentally limited in their ability to learn complex high-dimensional functions. Our analysis focuses on two problems. First, kernel machines are shallow architectures, in which one large layer of simple template matchers is followed by a single layer of trainable coefficients. We argue that shallow architectures can be very inefficient in terms of required number of computational elements and examples. Second, we analyze a limitation of kernel machines with a local kernel, linked to the curse of dimensionality, that applies to supervised, unsupervised (manifold learning) and semi-supervised kernel machines. Using empirical results on invariant image recognition tasks, kernel methods are compared with deep architectures, in which lower-level features or concepts are progressively combined into more abstract and higher-level representations. We argue that deep architectures have the potential to generalize in non-local ways, i.e., beyond immediate neighbors, and that this is crucial in order to make progress on the kind of complex tasks required for artificial intelligence.},
  annotation = {ZSCC: 0001099},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_scaling learning algorithms towards ai.pdf},
  language = {en}
}

@inproceedings{bengio09_Curriculumlearning,
  title = {Curriculum Learning},
  booktitle = {Proceedings of the 26th {{Annual International Conference}} on {{Machine Learning}} - {{ICML}} '09},
  author = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
  year = {2009},
  pages = {1--8},
  publisher = {{ACM Press}},
  address = {{Montreal, Quebec, Canada}},
  doi = {10.1145/1553374.1553380},
  abstract = {Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ``curriculum learning''. In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).},
  annotation = {ZSCC: 0001534},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_2009_curriculum learning.pdf},
  isbn = {978-1-60558-516-1},
  language = {en}
}

@inproceedings{bengio13_DeepLearningRepresentations,
  title = {Deep {{Learning}} of {{Representations}}: {{Looking Forward}}},
  booktitle = {Statistical {{Language}} and {{Speech Processing}}},
  author = {Bengio, Yoshua},
  editor = {Dediu, Adrian-Horia and {Mart{\'i}n-Vide}, Carlos and Mitkov, Ruslan and Truthe, Bianca},
  year = {2013},
  pages = {1--37},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  abstract = {Deep learning research aims at discovering learning algorithms that discover multiple levels of distributed representations, with higher levels representing more abstract concepts. Although the study of deep learning has already led to impressive theoretical results, learning algorithms and breakthrough experiments, several challenges lie ahead. This paper proposes to examine some of these challenges, centering on the questions of scaling deep learning algorithms to much larger models and datasets, reducing optimization difficulties due to ill-conditioning or local minima, designing more efficient and powerful inference and sampling procedures, and learning to disentangle the factors of variation underlying the observed data. It also proposes a few forward-looking research directions aimed at overcoming these challenges.},
  file = {/home/trung/GoogleDrive/Zotero/bengio_2013_deep learning of representations.pdf},
  isbn = {978-3-642-39593-2}
}

@article{bengio13_EstimatingPropagatingGradients,
  title = {Estimating or {{Propagating Gradients Through Stochastic Neurons}} for {{Conditional Computation}}},
  author = {Bengio, Yoshua and L{\'e}onard, Nicholas and Courville, Aaron},
  year = {2013},
  month = aug,
  abstract = {Stochastic neurons and hard non-linearities can be useful for a number of reasons in deep learning models, but in many cases they pose a challenging problem: how to estimate the gradient of a loss function with respect to the input of such stochastic or non-smooth neurons? I.e., can we ``back-propagate'' through these stochastic neurons? We examine this question, existing approaches, and compare four families of solutions, applicable in different settings. One of them is the minimum variance unbiased gradient estimator for stochatic binary neurons (a special case of the REINFORCE algorithm). A second approach, introduced here, decomposes the operation of a binary stochastic neuron into a stochastic binary part and a smooth differentiable part, which approximates the expected effect of the pure stochatic binary neuron to first order. A third approach involves the injection of additive or multiplicative noise in a computational graph that is otherwise differentiable. A fourth approach heuristically copies the gradient with respect to the stochastic output directly as an estimator of the gradient with respect to the sigmoid argument (we call this the straight-through estimator). To explore a context where these estimators are useful, we consider a small-scale version of conditional computation, where sparse stochastic units form a distributed representation of gaters that can turn off in combinatorially many ways large chunks of the computation performed in the rest of the neural network. In this case, it is important that the gating units produce an actual 0 most of the time. The resulting sparsity can be potentially be exploited to greatly reduce the computational cost of large deep networks for which conditional computation would be useful.},
  annotation = {ZSCC: 0000574},
  archivePrefix = {arXiv},
  eprint = {1308.3432},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_2013_estimating or propagating gradients through stochastic neurons for conditional computation.pdf},
  journal = {arXiv:1308.3432 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{bengio13_RepresentationLearningReview,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  shorttitle = {Representation {{Learning}}},
  author = {Bengio, Y. and Courville, A. and Vincent, P.},
  year = {2013},
  month = aug,
  volume = {35},
  pages = {1798--1828},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2013.50},
  abstract = {Learning robust representations can help uncover underlying biological variation in scRNA-seq data. Disentangled representation learning is one approach to obtain such informative as well as interpretable representations. Here, we learn disentangled representations of scRNA-seq data using {$\beta$} variational autoencoder ({$\beta$}-VAE) and apply the model for out-of-distribution (OOD) prediction. We demonstrate accurate gene expression predictions for cell-types absent from training in a perturbation and a developmental dataset. We further show that {$\beta$}-VAE outperforms a state-ofthe-art disentanglement method for scRNA-seq in OOD prediction while achieving better disentanglement performance.},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_2013_representation learning.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {8}
}

@article{bengio14_RepresentationLearningReview,
  title = {Representation {{Learning}}: {{A Review}} and {{New Perspectives}}},
  author = {Bengio, Y. and Courville, A. and Vincent, P.},
  year = {2014},
  volume = {35},
  pages = {1798--1828},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_2014_representation learning.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  number = {8}
}

@article{bengio17_ConsciousnessPrior,
  title = {The {{Consciousness Prior}}},
  author = {Bengio, Yoshua},
  year = {2017},
  month = sep,
  abstract = {A new prior is proposed for representation learning, which can be combined with other priors in order to help disentangling abstract factors from each other. It is inspired by the phenomenon of consciousness seen as the formation of a low-dimensional combination of a few concepts constituting a conscious thought, i.e., consciousness as awareness at a particular time instant. This provides a powerful constraint on the representation in that such low-dimensional thought vectors can correspond to statements about reality which are true, highly probable, or very useful for taking decisions. The fact that a few elements of the current state can be combined into such a predictive or useful statement is a strong constraint and deviates considerably from the maximum likelihood approaches to modelling data and how states unfold in the future based on an agent's actions. Instead of making predictions in the sensory (e.g. pixel) space, the consciousness prior allows the agent to make predictions in the abstract space, with only a few dimensions of that space being involved in each of these predictions. The consciousness prior also makes it natural to map conscious states to natural language utterances or to express classical AI knowledge in the form of facts and rules, although the conscious states may be richer than what can be expressed easily in the form of a sentence, a fact or a rule.},
  archivePrefix = {arXiv},
  eprint = {1709.08568},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bengio_2017_the consciousness prior.pdf},
  journal = {arXiv:1709.08568 [cs, stat]},
  keywords = {classical,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,deep learning,favorite,prior,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{bengio19_MetaTransferObjectiveLearning,
  title = {A {{Meta}}-{{Transfer Objective}} for {{Learning}} to {{Disentangle Causal Mechanisms}}},
  author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, S{\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  year = {2019},
  month = jan,
  abstract = {We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.},
  archivePrefix = {arXiv},
  eprint = {1901.10912},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_2019_a meta-transfer objective for learning to disentangle causal mechanisms.pdf;/home/trung/Zotero/storage/AQ3PW7QZ/1901.html},
  journal = {arXiv:1901.10912 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning,transfer learning},
  primaryClass = {cs, stat}
}

@article{bengio19_MetaTransferObjectiveLearninga,
  title = {A {{Meta}}-{{Transfer Objective}} for {{Learning}} to {{Disentangle Causal Mechanisms}}},
  author = {Bengio, Yoshua and Deleu, Tristan and Rahaman, Nasim and Ke, Rosemary and Lachapelle, S{\'e}bastien and Bilaniuk, Olexa and Goyal, Anirudh and Pal, Christopher},
  year = {2019},
  month = feb,
  abstract = {We propose to meta-learn causal structures based on how fast a learner adapts to new distributions arising from sparse distributional changes, e.g. due to interventions, actions of agents and other sources of non-stationarities. We show that under this assumption, the correct causal structural choices lead to faster adaptation to modified distributions because the changes are concentrated in one or just a few mechanisms when the learned knowledge is modularized appropriately. This leads to sparse expected gradients and a lower effective number of degrees of freedom needing to be relearned while adapting to the change. It motivates using the speed of adaptation to a modified distribution as a meta-learning objective. We demonstrate how this can be used to determine the cause-effect relationship between two observed variables. The distributional changes do not need to correspond to standard interventions (clamping a variable), and the learner has no direct knowledge of these interventions. We show that causal structures can be parameterized via continuous variables and learned end-to-end. We then explore how these ideas could be used to also learn an encoder that would map low-level observed variables to unobserved causal variables leading to faster adaptation out-of-distribution, learning a representation space where one can satisfy the assumptions of independent mechanisms and of small and sparse changes in these mechanisms due to actions and non-stationarities.},
  archivePrefix = {arXiv},
  eprint = {1901.10912},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_2019_a meta-transfer objective for learning to disentangle causal mechanisms2.pdf},
  journal = {arXiv:1901.10912 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bengio19_SystemDeepLearning,
  title = {From {{System}} 1 {{Deep Learning}} to {{System}} 2 {{Deep Learning}}},
  author = {Bengio, Yoshua},
  year = {2019},
  pages = {36},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/bengio_2019_from system 1 deep learning to system 2 deep learning.pdf},
  language = {en}
}

@article{bengio20_MachineLearningCombinatorial,
  title = {Machine {{Learning}} for {{Combinatorial Optimization}}: A {{Methodological Tour}} d'{{Horizon}}},
  shorttitle = {Machine {{Learning}} for {{Combinatorial Optimization}}},
  author = {Bengio, Yoshua and Lodi, Andrea and Prouvost, Antoine},
  year = {2020},
  month = mar,
  abstract = {This paper surveys the recent attempts, both from the machine learning and operations research communities, at leveraging machine learning to solve combinatorial optimization problems. Given the hard nature of these problems, state-of-the-art algorithms rely on handcrafted heuristics for making decisions that are otherwise too expensive to compute or mathematically not well defined. Thus, machine learning looks like a natural candidate to make such decisions in a more principled and optimized way. We advocate for pushing further the integration of machine learning and combinatorial optimization and detail a methodology to do so. A main point of the paper is seeing generic optimization problems as data points and inquiring what is the relevant distribution of problems to use for learning on a given task.},
  archivePrefix = {arXiv},
  eprint = {1811.06128},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bengio et al_2020_machine learning for combinatorial optimization.pdf},
  journal = {arXiv:1811.06128 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{benito-vicente18_FamilialHypercholesterolemiaMost,
  title = {Familial {{Hypercholesterolemia}}: {{The Most Frequent Cholesterol Metabolism Disorder Caused Disease}}},
  shorttitle = {Familial {{Hypercholesterolemia}}},
  author = {{Benito-Vicente}, Asier and Uribe, Kepa B. and Jebari, Shifa and {Galicia-Garcia}, Unai and Ostolaza, Helena and Martin, Cesar},
  year = {2018},
  month = nov,
  volume = {19},
  issn = {1422-0067},
  doi = {10.3390/ijms19113426},
  abstract = {Cholesterol is an essential component of cell barrier formation and signaling transduction involved in many essential physiologic processes. For this reason, cholesterol metabolism must be tightly controlled. Cell cholesterol is mainly acquired from two sources: Dietary cholesterol, which is absorbed in the intestine and, intracellularly synthesized cholesterol that is mainly synthesized in the liver. Once acquired, both are delivered to peripheral tissues in a lipoprotein dependent mechanism. Malfunctioning of cholesterol metabolism is caused by multiple hereditary diseases, including Familial Hypercholesterolemia, Sitosterolemia Type C and Niemann-Pick Type C1. Of these, familial hypercholesterolemia (FH) is a common inherited autosomal co-dominant disorder characterized by high plasma cholesterol levels. Its frequency is estimated to be 1:200 and, if untreated, increases the risk of premature cardiovascular disease. This review aims to summarize the current knowledge on cholesterol metabolism and the relation of FH to cholesterol homeostasis with special focus on the genetics, diagnosis and treatment.},
  annotation = {ZSCC: 0000007},
  file = {/home/trung/GoogleDrive/Zotero/benito-vicente et al_2018_familial hypercholesterolemia.pdf},
  journal = {International Journal of Molecular Sciences},
  number = {11},
  pmcid = {PMC6275065},
  pmid = {30388787}
}

@article{benton11_influencecreatinesupplementation,
  title = {The Influence of Creatine Supplementation on the Cognitive Functioning of Vegetarians and Omnivores},
  author = {Benton, David and Donohoe, Rachel},
  year = {2011},
  month = apr,
  volume = {105},
  pages = {1100--1105},
  issn = {0007-1145, 1475-2662},
  doi = {10.1017/S0007114510004733},
  abstract = {Creatine when combined with P forms phosphocreatine that acts as a reserve of high-energy phosphate. Creatine is found mostly in meat, fish and other animal products, and the levels of muscle creatine are known to be lower in vegetarians. Creatine supplementation influences brain functioning as indicated by imaging studies and the measurement of oxygenated Hb. Given the key role played by creatine in the provision of energy, the influence of its supplementation on cognitive functioning was examined, contrasting the effect in omnivores and vegetarians. Young adult females (n 128) were separated into those who were and were not vegetarian. Randomly and under a double-blind procedure, subjects consumed either a placebo or 20 g of creatine supplement for 5 d. Creatine supplementation did not influence measures of verbal fluency and vigilance. However, in vegetarians rather than in those who consume meat, creatine supplementation resulted in better memory. Irrespective of dietary style, the supplementation of creatine decreased the variability in the responses to a choice reaction-time task.},
  file = {/home/trung/GoogleDrive/Zotero/benton et al_2011_the influence of creatine supplementation on the cognitive functioning of vegetarians and omnivores.pdf},
  journal = {British Journal of Nutrition},
  language = {en},
  number = {7}
}

@article{berner00_DotaLargeScale,
  title = {Dota 2 with {{Large Scale Deep Reinforcement Learning}}},
  author = {Berner, Christopher and Brockman, Greg and Chan, Brooke and Cheung, Vicki and Dennison, Christy and Farhi, David and Fischer, Quirin and Hashme, Shariq and Hesse, Chris and J{\'o}zefowicz, Rafal and Gray, Scott and Olsson, Catherine and Pachocki, Jakub and Petrov, Michael and Salimans, Tim and Schlatter, Jeremy and Schneider, Jonas and Sidor, Szymon and Sutskever, Ilya and Tang, Jie and Wolski, Filip and Zhang, Susan},
  pages = {66},
  abstract = {On April 13th, 2019, OpenAI Five became the first AI system to defeat the world champions at an esports game. The game of Dota 2 presents novel challenges for AI systems such as long time horizons, imperfect information, and complex, continuous state-action spaces, all challenges which will become increasingly central to more capable AI systems. OpenAI Five leveraged existing reinforcement learning techniques, scaled to learn from batches of approximately 2 million frames every 2 seconds. We developed a distributed training system and tools for continual training which allowed us to train OpenAI Five for 10 months. By defeating the Dota 2 world champion (Team OG), OpenAI Five demonstrates that self-play reinforcement learning can achieve superhuman performance on a difficult task.},
  file = {/home/trung/GoogleDrive/Zotero/berner et al_dota 2 with large scale deep reinforcement learning.pdf},
  language = {en}
}

@techreport{bernstein19_Solodoubletidentification,
  title = {Solo: Doublet Identification via Semi-Supervised Deep Learning},
  shorttitle = {Solo},
  author = {Bernstein, Nicholas and Fong, Nicole and Lam, Irene and Roy, Margaret and Hendrickson, David G. and Kelley, David R.},
  year = {2019},
  month = nov,
  institution = {{Bioinformatics}},
  doi = {10.1101/841981},
  abstract = {A             bstract                      Single cell RNA-seq (scRNA-seq) measurements of gene expression enable an unprecedented high-resolution view into cellular state. However, current methods often result in two or more cells that share the same cell-identifying barcode; these ``doublets'' violate the fundamental premise of single cell technology and can lead to incorrect inferences. Here, we describe Solo, a semi-supervised deep learning approach that identifies doublets with greater accuracy than existing methods. Solo can be applied in combination with experimental doublet detection methods to further purify scRNA-seq data to true single cells beyond any previous approach.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/bernstein et al_2019_solo.pdf},
  language = {en},
  type = {Preprint}
}

@misc{berry19_Researchersdiscoverwhy,
  title = {Researchers Discover Why High Protein Diets Reduce Health and Lifespan},
  author = {Berry, Sarah},
  year = {2019},
  month = feb,
  abstract = {High protein diets reduce healthspan and lifespan and researchers now understand why.},
  chapter = {Health \& wellness},
  howpublished = {https://www.smh.com.au/lifestyle/health-and-wellness/researchers-discover-why-high-protein-diets-reduce-health-and-lifespan-20190225-p51046.html},
  journal = {The Sydney Morning Herald},
  language = {en}
}

@article{berthelot18_UnderstandingImprovingInterpolation,
  title = {Understanding and {{Improving Interpolation}} in {{Autoencoders}} via an {{Adversarial Regularizer}}},
  author = {Berthelot, David and Raffel, Colin and Roy, Aurko and Goodfellow, Ian},
  year = {2018},
  month = jul,
  abstract = {Autoencoders provide a powerful framework for learning compressed representations by encoding all of the information needed to reconstruct a data point in a latent code. In some cases, autoencoders can ``interpolate'': By decoding the convex combination of the latent codes for two datapoints, the autoencoder can produce an output which semantically mixes characteristics from the datapoints. In this paper, we propose a regularization procedure which encourages interpolated outputs to appear more realistic by fooling a critic network which has been trained to recover the mixing coefficient from interpolated data. We then develop a simple benchmark task where we can quantitatively measure the extent to which various autoencoders can interpolate and show that our regularizer dramatically improves interpolation in this setting. We also demonstrate empirically that our regularizer produces latent codes which are more effective on downstream tasks, suggesting a possible link between interpolation abilities and learning useful representations.},
  archivePrefix = {arXiv},
  eprint = {1807.07543},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/berthelot et al_2018_understanding and improving interpolation in autoencoders via an adversarial regularizer.pdf},
  journal = {arXiv:1807.07543 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{besserve18_Counterfactualsuncovermodular,
  title = {Counterfactuals Uncover the Modular Structure of Deep Generative Models},
  author = {Besserve, Michel and Sun, R{\'e}my and Sch{\"o}lkopf, Bernhard},
  year = {2018},
  month = dec,
  abstract = {Deep generative models such as Generative Adversarial Networks (GANs) and Variational Auto-Encoders (VAEs) are important tools to capture and investigate the properties of complex empirical data. However, the complexity of their inner elements makes their functioning challenging to assess and modify. In this respect, these architectures behave as black box models. In order to better understand the function of such networks, we analyze their modularity based on the counterfactual manipulation of their internal variables. Experiments with face images support that modularity between groups of channels is achieved to some degree within convolutional layers of vanilla VAE and GAN generators. This helps understand the functional organization of these systems and allows designing meaningful transformations of the generated images without further training.},
  archivePrefix = {arXiv},
  eprint = {1812.03253},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/besserve et al_2018_counterfactuals uncover the modular structure of deep generative models.pdf},
  journal = {arXiv:1812.03253 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{bhadury16_ScalingDynamicTopic,
  title = {Scaling up {{Dynamic Topic Models}}},
  booktitle = {Proceedings of the 25th {{International Conference}} on {{World Wide Web}} - {{WWW}} '16},
  author = {Bhadury, Arnab and Chen, Jianfei and Zhu, Jun and Liu, Shixia},
  year = {2016},
  pages = {381--390},
  publisher = {{ACM Press}},
  address = {{Montr\&\#233;al, Qu\&\#233;bec, Canada}},
  doi = {10.1145/2872427.2883046},
  abstract = {Dynamic topic models (DTMs) are very effective in discovering topics and capturing their evolution trends in time series data. To do posterior inference of DTMs, existing methods are all batch algorithms that scan the full dataset before each update of the model and make inexact variational approximations with mean-field assumptions. Due to a lack of a more scalable inference algorithm, despite the usefulness, DTMs have not captured large topic dynamics.},
  file = {/home/trung/GoogleDrive/Zotero/bhadury et al_2016_scaling up dynamic topic models.pdf},
  isbn = {978-1-4503-4143-1},
  language = {en}
}

@article{bhagat20_DisentanglingRepresentationsusing,
  title = {Disentangling {{Representations}} Using {{Gaussian Processes}} in {{Variational Autoencoders}} for {{Video Prediction}}},
  author = {Bhagat, Sarthak and Uppal, Shagun and Yin, Vivian and Lim, Nengli},
  year = {2020},
  month = jan,
  abstract = {We introduce MGP-VAE, a variational autoencoder which uses Gaussian processes (GP) to model the latent space distribution. We employ MGP-VAE for the unsupervised learning of video sequences to obtain disentangled representations. Previous work in this area has mainly been confined to separating dynamic information from static content. We improve on previous results by establishing a framework by which multiple features, static or dynamic, can be disentangled. Specifically we use fractional Brownian motions (fBM) and Brownian bridges (BB) to enforce an inter-frame correlation structure in each independent channel. We show that varying this correlation structure enables one to capture different aspects of variation in the data. We demonstrate the quality of our disentangled representations on numerous experiments on three publicly available datasets, and also perform quantitative tests on a video prediction task. In addition, we introduce a novel geodesic loss function which takes into account the curvature of the data manifold to improve learning in the prediction task. Our experiments show quantitatively that the combination of our improved disentangled representations with the novel loss function enable MGP-VAE to outperform the state-of-the-art in video prediction.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.02408},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bhagat et al_2020_disentangling representations using gaussian processes in variational autoencoders for video prediction.pdf;/home/trung/Zotero/storage/H563SCEF/2001.html},
  journal = {arXiv:2001.02408 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{bhatt20_MachineLearningExplainability,
  title = {Machine {{Learning Explainability}} for {{External Stakeholders}}},
  author = {Bhatt, Umang and Andrus, McKane and Weller, Adrian and Xiang, Alice},
  year = {2020},
  month = jul,
  abstract = {As machine learning is increasingly deployed in high-stakes contexts affecting people's livelihoods, there have been growing calls to open the black box and to make machine learning algorithms more explainable. Providing useful explanations requires careful consideration of the needs of stakeholders, including end-users, regulators, and domain experts. Despite this need, little work has been done to facilitate inter-stakeholder conversation around explainable machine learning. To help address this gap, we conducted a closed-door, day-long workshop between academics, industry experts, legal scholars, and policymakers to develop a shared language around explainability and to understand the current shortcomings of and potential solutions for deploying explainable machine learning in service of transparency goals. We also asked participants to share case studies in deploying explainable machine learning at scale. In this paper, we provide a short summary of various case studies of explainable machine learning, lessons from those studies, and discuss open challenges.},
  archivePrefix = {arXiv},
  eprint = {2007.05408},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bhatt et al_2020_machine learning explainability for external stakeholders.pdf},
  journal = {arXiv:2007.05408 [cs]},
  primaryClass = {cs}
}

@inproceedings{bhattacharjee19_DirectionalAttentionbased,
  title = {Directional {{Attention}} Based {{Video Frame Prediction}} Using {{Graph Convolutional Networks}}},
  booktitle = {2019 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Bhattacharjee, Prateep and Das, Sukhendu},
  year = {2019},
  month = jul,
  pages = {1--10},
  publisher = {{IEEE}},
  address = {{Budapest, Hungary}},
  doi = {10.1109/IJCNN.2019.8852090},
  abstract = {This paper proposes a novel network architecture for video frame prediction based on Graph Convolutional Neural Networks (GCNN). Most recent methods often fail in situations where multiple close-by objects at different scales move in random directions with variable speeds. We overcome this by modeling the scene as a space-time graph with intermediate features from the pixels (or a local region) as vertices and the relationships among them as edges. Our main contribution lies within posing the frame generation problem with our proposed space-time graph, which enables the network to learn the spatial as well as temporal inter-pixel relationships independent of each other, thus making the system invariant to velocity differences among the moving objects present in the scene. Moreover, we also propose a novel directional attention mechanism for the graph based model to efficiently learn a significance score based on directional relationship between pixels in the original scene. We also show that the proposed model generalizes better on the much more challenging task of predicting semantic scene segmentation of future scenes, even without access to any raw RGB frames. We perform several proxy tasks such as comparison of the quality of the semantic segmentation produced on the generated frames and comparing the accuracies for the task of recognizing actions in case of the dataset consisting of human actions. We use the popular Cityscapes traffic scene segmentation dataset as well as UCF-101 and Penn Action containing human actions to quantitatively and qualitatively evaluate the proposed framework over the recent state-of-the-art.},
  file = {/home/trung/GoogleDrive/Zotero/bhattacharjee et al_2019_directional attention based video frame prediction using graph convolutional networks.pdf},
  isbn = {978-1-72811-985-4},
  language = {en}
}

@techreport{bi20_EpidemiologyTransmissionCOVID19,
  title = {Epidemiology and {{Transmission}} of {{COVID}}-19 in {{Shenzhen China}}: {{Analysis}} of 391 Cases and 1,286 of Their Close Contacts},
  shorttitle = {Epidemiology and {{Transmission}} of {{COVID}}-19 in {{Shenzhen China}}},
  author = {Bi, Qifang and Wu, Yongsheng and Mei, Shujiang and Ye, Chenfei and Zou, Xuan and Zhang, Zhen and Liu, Xiaojian and Wei, Lan and Truelove, Shaun A and Zhang, Tong and Gao, Wei and Cheng, Cong and Tang, Xiujuan and Wu, Xiaoliang and Wu, Yu and Sun, Binbin and Huang, Suli and Sun, Yu and Zhang, Juncen and Ma, Ting and Lessler, Justin and Feng, Teijian},
  year = {2020},
  month = mar,
  institution = {{Infectious Diseases (except HIV/AIDS)}},
  doi = {10.1101/2020.03.03.20028423},
  abstract = {Background Rapid spread of SARS-CoV-2 in Wuhan prompted heightened surveillance in Shenzhen and elsewhere in China. The resulting data provide a rare opportunity to measure key metrics of disease course, transmission, and the impact of control.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/bi et al_2020_epidemiology and transmission of covid-19 in shenzhen china.pdf},
  language = {en},
  type = {Preprint}
}

@inproceedings{bian19_SelfAttentionNetworksTextIndependent,
  title = {Self-{{Attention Networks}} for {{Text}}-{{Independent Speaker Verification}}},
  booktitle = {2019 {{Chinese Control And Decision Conference}} ({{CCDC}})},
  author = {Bian, Tengyue and Chen, Fangzhou and Xu, Li},
  year = {2019},
  month = jun,
  pages = {3955--3960},
  issn = {1948-9447, 1948-9439},
  doi = {10.1109/CCDC.2019.8833466},
  abstract = {In this paper, we present a self-attention based model for text-independent speaker verification task and a novel variant of the triplet loss. Conventional convolutional neural networks (CNNs) used in speaker verification tasks need very deep layers to realize considerable performance. In our proposed model, the self-attention mechanism could easily capture long-range dependencies, thus achieves better representational capability with fewer parameters. Based on triplet loss, we propose a novel triplet selection method, which makes the training more efficient and achieves significant performance enhancement. Text-independent speaker verification experiments on AISHELL-2 corpus shows that the proposed model with the improved loss function decreases the verification equal error rate (EER) by 16.81\% relatively compared with the state-of-the-art ResNet-like model with common triplet loss, while the proposed model has fewer parameters and requires lower computational cost.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/bian et al_2019_self-attention networks for text-independent speaker verification.pdf;/home/trung/Zotero/storage/6HEERU8W/8833466.html},
  keywords = {Computational modeling,Computer architecture,conventional convolutional neural networks,deep layers,Feature extraction,improved loss function,long-range dependencies,Mathematical model,neural nets,Neural networks,performance enhancement,ResNet-like model,Self-Attention,self-attention mechanism,speaker recognition,Speaker Verification,speaker verification tasks,Task analysis,text-independent speaker verification task,Training,triplet loss,Triplet Loss,triplet selection method,verification equal error rate}
}

@article{biesner20_GenerativeDeepLearning,
  title = {Generative {{Deep Learning Techniques}} for {{Password Generation}}},
  author = {Biesner, David and Cvejoski, Kostadin and Georgiev, Bogdan and Sifa, Rafet and Krupicka, Erik},
  year = {2020},
  month = dec,
  abstract = {Password guessing approaches via deep learning have recently been investigated with significant breakthroughs in their ability to generate novel, realistic password candidates. In the present work we study a broad collection of deep learning and probabilistic based models in the light of password guessing: attention-based deep neural networks, autoencoding mechanisms and generative adversarial networks. We provide novel generative deep-learning models in terms of variational autoencoders exhibiting state-of-art sampling performance, yielding additional latent-space features such as interpolations and targeted sampling. Lastly, we perform a thorough empirical analysis in a unified controlled framework over well-known datasets (RockYou, LinkedIn, Youku, Zomato, Pwnd). Our results not only identify the most promising schemes driven by deep neural networks, but also illustrate the strengths of each approach in terms of generation variability and sample uniqueness.},
  archivePrefix = {arXiv},
  eprint = {2012.05685},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/biesner et al_2020_generative deep learning techniques for password generation.pdf},
  journal = {arXiv:2012.05685 [cs]},
  keywords = {favorite,representation},
  primaryClass = {cs}
}

@book{bishop06_Patternrecognitionmachine,
  title = {Pattern Recognition and Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2006},
  publisher = {{Springer}},
  address = {{New York}},
  file = {/home/trung/GoogleDrive/Zotero/bishop_2006_pattern recognition and machine learning.pdf},
  isbn = {978-0-387-31073-2},
  keywords = {favorite,Machine learning,Pattern perception},
  language = {en},
  lccn = {Q327 .B52 2006},
  series = {Information Science and Statistics}
}

@article{bishop13_Modelbasedmachinelearninga,
  title = {Model-Based Machine Learning},
  author = {Bishop, Christopher M.},
  year = {2013},
  month = feb,
  volume = {371},
  pages = {20120222},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2012.0222},
  file = {/home/trung/GoogleDrive/Zotero/bishop_2013_model-based machine learning.pdf},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number = {1984}
}

@article{bishop13_VariationalRelevanceVector,
  title = {Variational {{Relevance Vector Machines}}},
  author = {Bishop, Christopher M. and Tipping, Michael},
  year = {2013},
  month = jan,
  abstract = {The Support Vector Machine (SVM) of Vapnik (1998) has become widely established as one of the leading approaches to pattern recognition and machine learning. It expresses predictions in terms of a linear combination of kernel functions centred on a subset of the training data, known as support vectors. Despite its widespread success, the SVM suffers from some important limitations, one of the most significant being that it makes point predictions rather than generating predictive distributions. Recently Tipping (1999) has formulated the Relevance Vector Machine (RVM), a probabilistic model whose functional form is equivalent to the SVM. It achieves comparable recognition accuracy to the SVM, yet provides a full predictive distribution, and also requires substantially fewer kernel functions. The original treatment of the RVM relied on the use of type II maximum likelihood (the `evidence framework') to provide point estimates of the hyperparameters which govern model sparsity. In this paper we show how the RVM can be formulated and solved within a completely Bayesian paradigm through the use of variational inference, thereby giving a posterior distribution over both parameters and hyperparameters. We demonstrate the practicality and performance of the variational RVM using both synthetic and real world examples.},
  archivePrefix = {arXiv},
  eprint = {1301.3838},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bishop et al_2013_variational relevance vector machines.pdf},
  journal = {arXiv:1301.3838 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{bishop94_MixtureDensityNetwork,
  title = {Mixture {{Density Network}}},
  author = {Bishop, M. Christopher},
  year = {1994},
  file = {/home/trung/GoogleDrive/Zotero/bishop_1994_mixture density network.pdf},
  howpublished = {https://publications.aston.ac.uk/id/eprint/373/1/NCRG\_94\_004.pdf}
}

@article{bitton19_AssistedSoundSample,
  title = {Assisted {{Sound Sample Generation}} with {{Musical Conditioning}} in {{Adversarial Auto}}-{{Encoders}}},
  author = {Bitton, Adrien and Esling, Philippe and Caillon, Antoine and Fouilleul, Martin},
  year = {2019},
  month = jun,
  abstract = {Generative models have thrived in computer vision, enabling unprecedented image processes. Yet the results in audio remain less advanced. Our project targets real-time sound synthesis from a reduced set of high-level parameters, including semantic controls that can be adapted to different sound libraries and specific tags. These generative variables should allow expressive modulations of target musical qualities and continuously mix into new styles. To this extent we train AEs on an orchestral database of individual note samples, along with their intrinsic attributes: note class, timbre domain and extended playing techniques. We condition the decoder for control over the rendered note attributes and use latent adversarial training for learning expressive style parameters that can ultimately be mixed. We evaluate both generative performances and latent representation. Our ablation study demonstrates the effectiveness of the musical conditioning mechanisms. The proposed model generates notes as magnitude spectrograms from any probabilistic latent code samples, with expressive control of orchestral timbres and playing styles. Its training data subsets can directly be visualized in the 3D latent representation. Waveform rendering can be done offline with GLA. In order to allow real-time interactions, we fine-tune the decoder with a pretrained MCNN and embed the full waveform generation pipeline in a plugin. Moreover the encoder could be used to process new input samples, after manipulating their latent attribute representation, the decoder can generate sample variations as an audio effect would. Our solution remains rather fast to train, it can directly be applied to other sound domains, including an user's libraries with custom sound tags that could be mapped to specific generative controls. As a result, it fosters creativity and intuitive audio style experimentations.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1904.06215},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bitton et al_2019_assisted sound sample generation with musical conditioning in adversarial auto-encoders.pdf;/home/trung/Zotero/storage/A8YS727P/1904.html},
  journal = {arXiv:1904.06215 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{biza20_Learningdiscretestate,
  title = {Learning Discrete State Abstractions with Deep Variational Inference},
  author = {Biza, Ondrej and Platt, Robert and {van de Meent}, Jan-Willem and Wong, Lawson L. S.},
  year = {2020},
  month = mar,
  abstract = {Abstraction is crucial for effective sequential decision making in domains with large state spaces. In this work, we propose a variational information bottleneck method for learning approximate bisimulations, a type of state abstraction. We use a deep neural net encoder to map states onto continuous embeddings. The continuous latent space is then compressed into a discrete representation using an action-conditioned hidden Markov model, which is trained end-to-end with the neural network. Our method is suited for environments with high-dimensional states and learns from a stream of experience collected by an agent acting in a Markov decision process. Through a learned discrete abstract model, we can efficiently plan for unseen goals in a multi-goal Reinforcement Learning setting. We test our method in simplified robotic manipulation domains with image states. We also compare it against previous model-based approaches to finding bisimulations in discrete grid-world-like environments.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2003.04300},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/92RVKSMX/Biza et al. - 2020 - Learning discrete state abstractions with deep var.pdf},
  journal = {arXiv:2003.04300 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{blei03_LatentDirichletAllocation,
  title = {Latent {{Dirichlet Allocation}}},
  author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
  year = {2003},
  month = mar,
  volume = {3},
  pages = {993--1022},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
  file = {/home/trung/GoogleDrive/Zotero/blei et al_2003_latent dirichlet allocation.pdf},
  journal = {J. Mach. Learn. Res.},
  keywords = {favorite,representation},
  number = {null}
}

@inproceedings{blei06_Dynamictopicmodels,
  title = {Dynamic Topic Models},
  booktitle = {Proceedings of the 23rd International Conference on {{Machine}} Learning  - {{ICML}} '06},
  author = {Blei, David M. and Lafferty, John D.},
  year = {2006},
  pages = {113--120},
  publisher = {{ACM Press}},
  address = {{Pittsburgh, Pennsylvania}},
  doi = {10.1145/1143844.1143859},
  abstract = {A family of probabilistic time series models is developed to analyze the time evolution of topics in large document collections. The approach is to use state space models on the natural parameters of the multinomial distributions that represent the topics. Variational approximations based on Kalman filters and nonparametric wavelet regression are developed to carry out approximate posterior inference over the latent topics. In addition to giving quantitative, predictive models of a sequential corpus, dynamic topic models provide a qualitative window into the contents of a large document collection. The models are demonstrated by analyzing the OCR'ed archives of the journal Science from 1880 through 2000.},
  file = {/home/trung/GoogleDrive/Zotero/blei et al_2006_dynamic topic models.pdf},
  isbn = {978-1-59593-383-6},
  language = {en}
}

@article{blei12_Probabilistictopicmodels,
  title = {Probabilistic Topic Models},
  author = {Blei, David M.},
  year = {2012},
  month = apr,
  volume = {55},
  pages = {77},
  issn = {00010782},
  doi = {10.1145/2133806.2133826},
  file = {/home/trung/GoogleDrive/Zotero/blei_2012_probabilistic topic models2.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {4}
}

@article{blei12_Probabilistictopicmodelsa,
  title = {Probabilistic Topic Models},
  author = {Blei, David M.},
  year = {2012},
  month = apr,
  volume = {55},
  pages = {77--84},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  issn = {0001-0782},
  doi = {10.1145/2133806.2133826},
  abstract = {Surveying a suite of algorithms that offer a solution to managing large document archives.},
  file = {/home/trung/GoogleDrive/Zotero/blei_2012_probabilistic topic models.pdf},
  issue_date = {April 2012},
  journal = {Communications of the ACM},
  keywords = {favorite},
  number = {4}
}

@article{blei14_BuildComputeCritique,
  title = {Build, {{Compute}}, {{Critique}}, {{Repeat}}: {{Data Analysis}} with {{Latent Variable Models}}},
  shorttitle = {Build, {{Compute}}, {{Critique}}, {{Repeat}}},
  author = {Blei, David M.},
  year = {2014},
  month = jan,
  volume = {1},
  pages = {203--232},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-022513-115657},
  abstract = {We survey how to use latent variable models to solve data analysis problems. A latent variable model is a probabilistic model of hidden and observed variables, where the hidden variables encode hidden patterns in our data. We uncover these patterns through the posterior, the conditional distribution of the hidden variables given the observations, which we use to explore, summarize, and form predictions about the data. Latent variable models have had a major impact on numerous applied fields\textemdash computational biology, natural language processing, social network analysis, computer vision, and many others. Here, we take the perspective\textemdash inspired by the work of George Box\textemdash that models are developed in an iterative process: we formulate a model, use it to analyze data, assess how the analysis succeeds and fails, revise the model, and repeat. With this view, we describe how new research in statistics and machine learning has transformed each of these essential activities. First, we discuss the types of patterns that latent variable models capture and how to formulate complex models by combining simpler ones. We describe probabilistic graphical models as a language for formulating latent variable models. Next we discuss algorithms for using models to analyze data, algorithms that approximate the conditional distribution of the hidden structure given the observations. We describe mean field variational inference, an approach that transforms this probabilistic computation into an optimization problem. Finally we discuss how we use our analyses to solve real-world problems: exploring the data, forming predictions about new data, and pointing us in the direction of improved models. We discuss predictive sample re-use and posterior predictive checks, two methods that step outside of the model's assumptions to scrutinize its explanation of the observed data.},
  file = {/home/trung/GoogleDrive/Zotero/blei_2014_build, compute, critique, repeat.pdf},
  journal = {Annual Review of Statistics and Its Application},
  language = {en},
  number = {1}
}

@article{blei17_VariationalInferenceReview,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  volume = {112},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  file = {/home/trung/GoogleDrive/Zotero/blei et al_2017_variational inference.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {518}
}

@article{blier18_DescriptionLengthDeep,
  title = {The {{Description Length}} of {{Deep Learning Models}}},
  author = {Blier, L{\'e}onard and Ollivier, Yann},
  year = {2018},
  month = nov,
  abstract = {Solomonoff's general theory of inference (Solomonoff, 1964) and the Minimum Description Length principle (Gr\"unwald, 2007; Rissanen, 2007) formalize Occam's razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded.},
  archivePrefix = {arXiv},
  eprint = {1802.07044},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/CS6W8RHV/Blier and Ollivier - 2018 - The Description Length of Deep Learning Models.pdf},
  journal = {arXiv:1802.07044 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{blier18_DescriptionLengthDeepa,
  title = {The {{Description Length}} of {{Deep Learning Models}}},
  author = {Blier, L{\'e}onard and Ollivier, Yann},
  year = {2018},
  month = nov,
  abstract = {Solomonoff's general theory of inference and the Minimum Description Length principle formalize Occam's razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks. Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff's approach.},
  archivePrefix = {arXiv},
  eprint = {1802.07044},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/blier et al_2018_the description length of deep learning models.pdf},
  journal = {arXiv:1802.07044 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{bogdan19_Maximumlikelihoodestimation,
  title = {Maximum Likelihood Estimation for Discrete Exponential Families and Random Graphs},
  author = {Bogdan, Krzysztof and Bosy, Micha{\l} and Skalski, Tomasz},
  year = {2019},
  month = nov,
  abstract = {We characterize the existence of maximum likelihood estimators for discrete exponential families and give applications to random graph models.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.13143},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bogdan et al_2019_maximum likelihood estimation for discrete exponential families and random graphs.pdf;/home/trung/Zotero/storage/KJXYHJ5X/1911.html},
  journal = {arXiv:1911.13143 [math, stat]},
  keywords = {Mathematics - Combinatorics,Mathematics - Probability,Mathematics - Statistics Theory},
  primaryClass = {math, stat}
}

@article{bojarski20_NVIDIAPilotNetExperiments,
  title = {The {{NVIDIA PilotNet Experiments}}},
  author = {Bojarski, Mariusz and Chen, Chenyi and Daw, Joyjit and De{\u g}irmenci, Alperen and Deri, Joya and Firner, Bernhard and Flepp, Beat and Gogri, Sachin and Hong, Jesse and Jackel, Lawrence and Jia, Zhenhua and Lee, B. J. and Liu, Bo and Liu, Fei and Muller, Urs and Payne, Samuel and Prasad, Nischal Kota Nagendra and Provodin, Artem and Roach, John and Rvachov, Timur and Tadimeti, Neha and {van Engelen}, Jesper and Wen, Haiguang and Yang, Eric and Yang, Zongyi},
  year = {2020},
  month = oct,
  abstract = {Four years ago, an experimental system known as PilotNet became the first NVIDIA system to steer an autonomous car along a roadway. This system represents a departure from the classical approach for self-driving in which the process is manually decomposed into a series of modules, each performing a different task. In PilotNet, on the other hand, a single deep neural network (DNN) takes pixels as input and produces a desired vehicle trajectory as output; there are no distinct internal modules connected by human-designed interfaces. We believe that handcrafted interfaces ultimately limit performance by restricting information flow through the system and that a learned approach, in combination with other artificial intelligence systems that add redundancy, will lead to better overall performing systems. We continue to conduct research toward that goal. This document describes the PilotNet lane-keeping effort, carried out over the past five years by our NVIDIA PilotNet group in Holmdel, New Jersey. Here we present a snapshot of system status in mid-2020 and highlight some of the work done by the PilotNet group.},
  archivePrefix = {arXiv},
  eprint = {2010.08776},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bojarski et al_2020_the nvidia pilotnet experiments.pdf},
  journal = {arXiv:2010.08776 [cs]},
  primaryClass = {cs}
}

@book{bonaccorso20_MasteringMachineLearning,
  title = {Mastering {{Machine Learning Algorithms}}},
  author = {Bonaccorso, Guiseppe},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/bonaccorso_2020_mastering machine learning algorithms.pdf}
}

@article{booch20_ThinkingFastSlow,
  title = {Thinking {{Fast}} and {{Slow}} in {{AI}}},
  author = {Booch, Grady and Fabiano, Francesco and Horesh, Lior and Kate, Kiran and Lenchner, Jon and Linck, Nick and Loreggia, Andrea and Murugesan, Keerthiram and Mattei, Nicholas and Rossi, Francesca and Srivastava, Biplav},
  year = {2020},
  month = oct,
  abstract = {This paper proposes a research direction to advance AI which draws inspiration from cognitive theories of human decision making. The premise is that if we gain insights about the causes of some human capabilities that are still lacking in AI (for instance, adaptability, generalizability, common sense, and causal reasoning), we may obtain similar capabilities in an AI system by embedding these causal components. We hope that the high-level description of our vision included in this paper, as well as the several research questions that we propose to consider, can stimulate the AI research community to define, try and evaluate new methodologies, frameworks, and evaluation metrics, in the spirit of achieving a better understanding of both human and machine intelligence.},
  archivePrefix = {arXiv},
  eprint = {2010.06002},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/booch et al_2020_thinking fast and slow in ai.pdf},
  journal = {arXiv:2010.06002 [cs]},
  keywords = {favorite},
  primaryClass = {cs}
}

@article{bosc20_sequencetosequenceVAEslearn,
  title = {Do Sequence-to-Sequence {{VAEs}} Learn Global Features of Sentences?},
  author = {Bosc, Tom and Vincent, Pascal},
  year = {2020},
  month = apr,
  abstract = {A longstanding goal in NLP is to compute global sentence representations. Such representations would be useful for sample-efficient semi-supervised learning and controllable text generation. To learn to represent global and local information separately, Bowman et al. (2016) proposed to train a sequenceto-sequence model with the variational autoencoder (VAE) objective. What precisely is encoded in these latent variables expected to capture global features? We measure which words benefit most from the latent information by decomposing the reconstruction loss per position in the sentence. Using this method, we see that VAEs are prone to memorizing the first words and the sentence length, drastically limiting their usefulness. To alleviate this, we propose variants based on bag-of-words assumptions and language model pretraining. These variants learn latents that are more global: they are more predictive of topic or sentiment labels, and their reconstructions are more faithful to the labels of the original documents.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2004.07683},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bosc et al_2020_do sequence-to-sequence vaes learn global features of sentences.pdf},
  journal = {arXiv:2004.07683 [cs]},
  keywords = {_tablet,disentanglement},
  language = {en},
  primaryClass = {cs}
}

@article{bose20_LatentVariableModelling,
  title = {Latent {{Variable Modelling}} with {{Hyperbolic Normalizing Flows}}},
  author = {Bose, Avishek Joey and Smofsky, Ariella and Liao, Renjie and Panangaden, Prakash and Hamilton, William L.},
  year = {2020},
  month = aug,
  abstract = {The choice of approximate posterior distributions plays a central role in stochastic variational inference (SVI). One effective solution is the use of normalizing flows to construct flexible posterior distributions. However, one key limitation of existing normalizing flows is that they are restricted to the Euclidean space and are ill-equipped to model data with an underlying hierarchical structure. To address this fundamental limitation, we present the first extension of normalizing flows to hyperbolic spaces. We first elevate normalizing flows to hyperbolic spaces using coupling transforms defined on the tangent bundle, termed Tangent Coupling (T C). We further introduce Wrapped Hyperboloid Coupling (WHC), a fully invertible and learnable transformation that explicitly utilizes the geometric structure of hyperbolic spaces, allowing for expressive posteriors while being efficient to sample from. We demonstrate the efficacy of our novel normalizing flow over hyperbolic VAEs and Euclidean normalizing flows. Our approach achieves improved performance on density estimation, as well as reconstruction of real-world graph data, which exhibit a hierarchical structure. Finally, we show that our approach can be used to power a generative model over hierarchical data using hyperbolic latent variables.},
  archivePrefix = {arXiv},
  eprint = {2002.06336},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bose et al_2020_latent variable modelling with hyperbolic normalizing flows.pdf},
  journal = {arXiv:2002.06336 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{bouchacourt17_MultiLevelVariationalAutoencoder,
  title = {Multi-{{Level Variational Autoencoder}}: {{Learning Disentangled Representations}} from {{Grouped Observations}}},
  shorttitle = {Multi-{{Level Variational Autoencoder}}},
  author = {Bouchacourt, Diane and Tomioka, Ryota and Nowozin, Sebastian},
  year = {2017},
  month = may,
  abstract = {We would like to learn a representation of the data which decomposes an observation into factors of variation which we can independently control. Specifically, we want to use minimal supervision to learn a latent representation that reflects the semantics behind a specific grouping of the data, where within a group the samples share a common factor of variation. For example, consider a collection of face images grouped by identity. We wish to anchor the semantics of the grouping into a relevant and disentangled representation that we can easily exploit. However, existing deep probabilistic models often assume that the observations are independent and identically distributed. We present the Multi-Level Variational Autoencoder (ML-VAE), a new deep probabilistic model for learning a disentangled representation of a set of grouped observations. The ML-VAE separates the latent representation into semantically meaningful parts by working both at the group level and the observation level, while retaining efficient test-time inference. Quantitative and qualitative evaluations show that the ML-VAE model (i) learns a semantically meaningful disentanglement of grouped data, (ii) enables manipulation of the latent representation, and (iii) generalises to unseen groups.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1705.08841},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bouchacourt et al_2017_multi-level variational autoencoder.pdf;/home/trung/GoogleDrive/Zotero/bouchacourt et al_2017_multi-level variational autoencoder2.pdf},
  journal = {arXiv:1705.08841 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{boucher20_MontrealaiVIP,
  title = {Montreal.Ai {{VIP AI}} 101 {{CHEATSHEET}}},
  author = {Boucher, Vincent},
  year = {2020},
  pages = {27},
  abstract = {For the purpose of entrusting all sentient beings with powerful AI tools to learn, deploy and scale AI in order to enhance their prosperity, to settle planetary-scale problems and to inspire those who, with AI, will shape the 21st Century, MONTR\'EAL.AI introduces this VIP AI 101 CheatSheet for All.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/boucher_2020_montreal.pdf},
  language = {en}
}

@article{boucher20_VIPAI101,
  title = {{{VIP AI}} 101 {{CHEATSHEET}}},
  author = {Boucher, Vincent},
  year = {2020},
  pages = {32},
  abstract = {For the purpose of entrusting all sentient beings with powerful AI tools to learn, deploy and scale AI in order to enhance their prosperity, to settle planetary-scale problems and to inspire those who, with AI, will shape the 21st Century, MONTR\'EAL.AI introduces this VIP AI 101 CheatSheet for All.},
  file = {/home/trung/Zotero/storage/VKVBRVRQ/Boucher - 2020 - VIP AI 101 CHEATSHEET.pdf},
  language = {en}
}

@book{boucheron13_ConcentrationInequalitiesNonasymptotic,
  title = {Concentration {{Inequalities}}: {{A Nonasymptotic Theory}} of {{Independence}}},
  shorttitle = {Concentration {{Inequalities}}},
  author = {Boucheron, St{\'e}phane and Lugosi, G{\'a}bor and Massart, Pascal},
  year = {2013},
  month = feb,
  publisher = {{Oxford University Press}},
  doi = {10.1093/acprof:oso/9780199535255.001.0001},
  file = {/home/trung/GoogleDrive/Zotero/boucheron et al_2013_concentration inequalities.pdf},
  isbn = {978-0-19-953525-5},
  language = {en}
}

@article{bourached19_RaidersLostArt,
  title = {Raiders of the {{Lost Art}}},
  author = {Bourached, Anthony and Cann, George},
  year = {2019},
  month = sep,
  abstract = {Neural style transfer, first proposed by Gatys et al. (2015), can be used to create novel artistic work through rendering a content image in the form of a style image. We present a novel method of reconstructing lost artwork, by applying neural style transfer to x-radiographs of artwork with secondary interior artwork beneath a primary exterior, so as to reconstruct lost artwork. Finally we reflect on AI art exhibitions and discuss the social, cultural, ethical, and philosophical impact of these technical innovations.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1909.05677},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/bourached et al_2019_raiders of the lost art.pdf;/home/trung/Zotero/storage/63J34TIC/1909.html},
  journal = {arXiv:1909.05677 [cs, stat]},
  keywords = {art,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,generative,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{bousquet04_Advancedlecturesmachine,
  title = {Advanced Lectures on Machine Learning: {{ML Summer Schools}} 2003, {{Canberra}}, {{Australia}}, {{February}} 2-14, 2003 [and] {{T\"ubingen}}, {{Germany}}, {{August}} 4-16, 2003: Revised Lectures},
  shorttitle = {Advanced Lectures on Machine Learning},
  editor = {Bousquet, Olivier and von Luxburg, Ulrike and R{\"a}tsch, Gunnar},
  year = {2004},
  publisher = {{Springer}},
  address = {{Berlin ; New York}},
  annotation = {OCLC: ocm56492380},
  file = {/home/trung/GoogleDrive/Zotero/bousquet et al_2004_advanced lectures on machine learning.pdf},
  isbn = {978-3-540-23122-6},
  keywords = {Congresses,Machine learning},
  language = {en},
  lccn = {Q325.5 .M344 2003},
  number = {3176},
  series = {Lecture Notes in Computer Science, {{Lecture}} Notes in Artificial Intelligence}
}

@phdthesis{bouza12_Hypothesisbasedcollaborativefiltering,
  title = {Hypothesis-Based Collaborative Filtering: Retrieving like-Minded Individuals Based on the Comparison of Hypothesized Preferences},
  shorttitle = {Hypothesis-Based Collaborative Filtering},
  author = {Bouza, Amancio and Bernstein, Abraham},
  year = {2012},
  address = {{Erscheinungsort nicht ermittelbar}},
  annotation = {OCLC: 797113745},
  file = {/home/trung/GoogleDrive/Zotero/bouza et al_2012_hypothesis-based collaborative filtering.pdf},
  isbn = {9781105585081},
  language = {en},
  school = {[Verlag nicht ermittelbar]}
}

@article{bowman16_GeneratingSentencesContinuous,
  title = {Generating {{Sentences}} from a {{Continuous Space}}},
  author = {Bowman, Samuel R. and Vilnis, Luke and Vinyals, Oriol and Dai, Andrew M. and Jozefowicz, Rafal and Bengio, Samy},
  year = {2016},
  month = may,
  abstract = {The standard recurrent neural network language model (rnnlm) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an rnn-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1511.06349},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/28ANBFKD/Bowman et al. - 2016 - Generating Sentences from a Continuous Space.pdf},
  journal = {arXiv:1511.06349 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{boxma05_anaerobicmitochondrionthat,
  title = {An Anaerobic Mitochondrion That Produces Hydrogen},
  author = {Boxma, Brigitte and {de Graaf}, Rob M. and {van der Staay}, Georg W. M. and {van Alen}, Theo A. and Ricard, Guenola and Gabald{\'o}n, Toni and {van Hoek}, Angela H. A. M. and {Moon-van der Staay}, Seung Yeo and Koopman, Werner J. H. and {van Hellemond}, Jaap J. and Tielens, Aloysius G. M. and Friedrich, Thorsten and Veenhuis, Marten and Huynen, Martijn A. and Hackstein, Johannes H. P.},
  year = {2005},
  month = mar,
  volume = {434},
  pages = {74--79},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature03343},
  abstract = {Mitochondria arose once in evolution, and their origin entailed an endosymbiosis accompanied by gene transfers from the endosymbiont to the host. Anaerobic mitochondria pose a puzzle for traditional views on mitochondrial origins but fit nicely in newer theories on mitochondrial evolution that were formulated specifically to take the common ancestry of mitochondria and hydrogenosomes into account. The presence of mitochondria in the eukaryote common ancestor continues to change the way we look at eukaryote origins, with endosymbiosis playing a more central role in considerations on the matter now than it did twenty years ago. The integral part that mitochondria play in many aspects of eukaryote biology might well reflect their role in the origin of eukaryotes themselves.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/boxma et al_2005_an anaerobic mitochondrion that produces hydrogen.pdf},
  journal = {Nature},
  language = {en},
  number = {7029}
}

@techreport{boyeau19_DeepGenerativeModels,
  title = {Deep {{Generative Models}} for {{Detecting Differential Expression}} in {{Single Cells}}},
  author = {Boyeau, Pierre and Lopez, Romain and Regier, Jeffrey and Gayoso, Adam and Jordan, Michael I. and Yosef, Nir},
  year = {2019},
  month = oct,
  institution = {{Cell Biology}},
  doi = {10.1101/794289},
  abstract = {Detecting differentially expressed genes is important for characterizing subpopulations of cells. However, in scRNA-seq data, nuisance variation due to technical factors like sequencing depth and RNA capture efficiency obscures the underlying biological signal. First, we show that deep generative models, which combined Bayesian statistics and deep neural networks, better estimate the log-fold-change in gene expression levels between subpopulations of cells. Second, we use Bayesian decision theory to detect differentially expressed genes while controlling the false discovery rate. Our experiments on simulated and real datasets show that our approach outperforms state-of-the-art DE frameworks. Finally, we introduce a technique for improving the posterior approximation, and show that it also improves differential expression performance.},
  file = {/home/trung/GoogleDrive/Zotero/boyeau et al_2019_deep generative models for detecting differential expression in single cells2.pdf},
  keywords = {variational},
  language = {en},
  type = {Preprint}
}

@article{boyeau19_DeepGenerativeModelsa,
  title = {Deep {{Generative Models}} for {{Detecting Differential Expression}} in {{Single Cells}}},
  author = {Boyeau, Pierre and Lopez, Romain and Regier, Jeffrey and Gayoso, Adam and Jordan, Michael I and Yosef, Nir},
  year = {2019},
  pages = {8},
  abstract = {Detecting differentially expressed genes is important for characterizing subpopulations of cells. However, in scRNA-seq data, nuisance variation due to technical factors like sequencing depth and RNA capture efficiency obscures the underlying biological signal. First, we show that deep generative models, which combined Bayesian statistics and deep neural networks, better estimate the log-fold-change in gene expression levels between subpopulations of cells. Second, we use Bayesian decision theory to detect differentially expressed genes while controlling the false discovery rate. Our experiments on simulated and real datasets show that our approach outperforms state-of-the-art DE frameworks. Finally, we introduce a technique for improving the posterior approximation, and show that it also improves differential expression performance.},
  file = {/home/trung/GoogleDrive/Zotero/boyeau et al_2019_deep generative models for detecting differential expression in single cells.pdf},
  language = {en}
}

@article{braithwaite20_VarianceConstrainedAutoencoding,
  title = {Variance {{Constrained Autoencoding}}},
  author = {Braithwaite, D. T. and O'Connor, M. and Kleijn, W. B.},
  year = {2020},
  month = may,
  abstract = {Recent state-of-the-art autoencoder based generative models have an encoder-decoder structure and learn a latent representation with a pre-defined distribution that can be sampled from. Implementing the encoder networks of these models in a stochastic manner provides a natural and common approach to avoid overfitting and enforce a smooth decoder function. However, we show that for stochastic encoders, simultaneously attempting to enforce a distribution constraint and minimising an output distortion leads to a reduction in generative and reconstruction quality. In addition, attempting to enforce a latent distribution constraint is not reasonable when performing disentanglement. Hence, we propose the variance-constrained autoencoder (VCAE), which only enforces a variance constraint on the latent distribution. Our experiments show that VCAE improves upon Wasserstein Autoencoder and the Variational Autoencoder in both reconstruction and generative quality on MNIST and CelebA. Moreover, we show that VCAE equipped with a total correlation penalty term performs equivalently to FactorVAE at learning disentangled representations on 3D-Shapes while being a more principled approach.},
  archivePrefix = {arXiv},
  eprint = {2005.03807},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/braithwaite et al_2020_variance constrained autoencoding.pdf},
  journal = {arXiv:2005.03807 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{brakel17_LearningIndependentFeatures,
  title = {Learning {{Independent Features}} with {{Adversarial Nets}} for {{Non}}-Linear {{ICA}}},
  author = {Brakel, Philemon and Bengio, Yoshua},
  year = {2017},
  month = oct,
  abstract = {Reliable measures of statistical dependence could be useful tools for learning independent features and performing tasks like source separation using Independent Component Analysis (ICA). Unfortunately, many of such measures, like the mutual information, are hard to estimate and optimize directly. We propose to learn independent features with adversarial objectives which optimize such measures implicitly. These objectives compare samples from the joint distribution and the product of the marginals without the need to compute any probability densities. We also propose two methods for obtaining samples from the product of the marginals using either a simple resampling trick or a separate parametric distribution. Our experiments show that this strategy can easily be applied to different types of model architectures and solve both linear and non-linear ICA problems.},
  annotation = {ZSCC: 0000031},
  archivePrefix = {arXiv},
  eprint = {1710.05050},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/brakel et al_2017_learning independent features with adversarial nets for non-linear ica.pdf},
  journal = {arXiv:1710.05050 [stat]},
  language = {en},
  primaryClass = {stat}
}

@book{brath15_Graphanalysisvisualization,
  title = {Graph Analysis and Visualization: Discovering Business Opportunity in Linked Data},
  shorttitle = {Graph Analysis and Visualization},
  author = {Brath, Richard},
  year = {2015},
  publisher = {{Wiley}},
  address = {{Indianapolis, IN}},
  file = {/home/trung/GoogleDrive/Zotero/brath_2015_graph analysis and visualization.pdf},
  isbn = {978-1-118-84584-4 978-1-118-84569-1 978-1-118-84587-5},
  keywords = {Business,Data processing,Graph theory,Network analysis (Planning)},
  language = {en},
  lccn = {MLCM 2018/46409 (T)}
}

@article{bravogonzalez-blas19_cisTopiccisregulatorytopic,
  title = {{{cisTopic}}: Cis-Regulatory Topic Modeling on Single-Cell {{ATAC}}-Seq Data},
  shorttitle = {{{cisTopic}}},
  author = {{Bravo Gonz{\'a}lez-Blas}, Carmen and Minnoye, Liesbeth and Papasokrati, Dafni and Aibar, Sara and Hulselmans, Gert and Christiaens, Valerie and Davie, Kristofer and Wouters, Jasper and Aerts, Stein},
  year = {2019},
  month = may,
  volume = {16},
  pages = {397--400},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0367-1},
  abstract = {We present cisTopic, a probabilistic framework to simultaneously discover co-accessible enhancers and stable cell states from sparse single-cell epigenomics data (http://github.com/ aertslab/cistopic). On a compendium of single-cell ATAC-seq datasets from differentiating hematopoietic cells, brain, and transcription-factor perturbations, we demonstrate that topic modelling can be exploited for a robust identification of cell types, enhancers, and relevant transcription factors. cisTopic provides insight into the mechanisms underlying regulatory heterogeneity within cell populations.},
  file = {/home/trung/GoogleDrive/Zotero/bravo gonzález-blas et al_2019_cistopic.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {5}
}

@article{brehmer20_Flowssimultaneousmanifold,
  title = {Flows for Simultaneous Manifold Learning and Density Estimation},
  author = {Brehmer, Johann and Cranmer, Kyle},
  year = {2020},
  month = jun,
  abstract = {We introduce manifold-learning flows (M-flows), a new class of generative models that simultaneously learn the data manifold as well as a tractable probability density on that manifold. Combining aspects of normalizing flows, GANs, autoencoders, and energy-based models, they have the potential to represent datasets with a manifold structure more faithfully and provide handles on dimensionality reduction, denoising, and out-of-distribution detection. We argue why such models should not be trained by maximum likelihood alone and present a new training algorithm that separates manifold and density updates. In a range of experiments we demonstrate how M-flows learn the data manifold and allow for better inference than standard flows in the ambient data space.},
  archivePrefix = {arXiv},
  eprint = {2003.13913},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/brehmer et al_2020_flows for simultaneous manifold learning and density estimation.pdf},
  journal = {arXiv:2003.13913 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{brekelmans19_ExactRateDistortionAutoencoders,
  title = {Exact {{Rate}}-{{Distortion}} in {{Autoencoders}} via {{Echo Noise}}},
  author = {Brekelmans, Rob and Moyer, Daniel and Galstyan, Aram and Steeg, Greg Ver},
  year = {2019},
  month = nov,
  abstract = {Compression is at the heart of effective representation learning. However, lossy compression is typically achieved through simple parametric models like Gaussian noise to preserve analytic tractability, and the limitations this imposes on learning are largely unexplored. Further, the Gaussian prior assumptions in models such as variational autoencoders (VAEs) provide only an upper bound on the compression rate in general. We introduce a new noise channel, \textbackslash emph\{Echo noise\}, that admits a simple, exact expression for mutual information for arbitrary input distributions. The noise is constructed in a data-driven fashion that does not require restrictive distributional assumptions. With its complex encoding mechanism and exact rate regularization, Echo leads to improved bounds on log-likelihood and dominates \$\textbackslash beta\$-VAEs across the achievable range of rate-distortion trade-offs. Further, we show that Echo noise can outperform flow-based methods without the need to train additional distributional transformations.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1904.07199},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/brekelmans et al_2019_exact rate-distortion in autoencoders via echo noise.pdf;/home/trung/Zotero/storage/NEP3MXQ3/1904.html},
  journal = {arXiv:1904.07199 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@misc{bresson19_CE7454DeepLearning,
  title = {{{CE7454}}: {{Deep Learning}} for {{Data Science}} 2018/19},
  author = {Bresson, Xavier},
  year = {2019},
  month = nov,
  abstract = {Deep learning course CE7454, 2018. Contribute to xbresson/CE7454\_2018 development by creating an account on GitHub.},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {MIT}
}

@article{britz17_MassiveExplorationNeural,
  title = {Massive {{Exploration}} of {{Neural Machine Translation Architectures}}},
  author = {Britz, Denny and Goldie, Anna and Luong, Minh-Thang and Le, Quoc},
  year = {2017},
  month = mar,
  abstract = {Neural Machine Translation (NMT) has shown remarkable progress over the past few years with production systems now being deployed to end-users. One major drawback of current architectures is that they are expensive to train, typically requiring days to weeks of GPU time to converge. This makes exhaustive hyperparameter search, as is commonly done with other neural network architectures, prohibitively expensive. In this work, we present the first large-scale analysis of NMT architecture hyperparameters. We report empirical results and variance numbers for several hundred experimental runs, corresponding to over 250,000 GPU hours on the standard WMT English to German translation task. Our experiments lead to novel insights and practical advice for building and extending NMT architectures. As part of this contribution, we release an open-source NMT framework that enables researchers to easily experiment with novel techniques and reproduce state of the art results.},
  annotation = {ZSCC: 0000189},
  archivePrefix = {arXiv},
  eprint = {1703.03906},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/britz et al_2017_massive exploration of neural machine translation architectures.pdf;/home/trung/Zotero/storage/8SIEDBPE/1703.html},
  journal = {arXiv:1703.03906 [cs]},
  keywords = {attention,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{brock19_LargeScaleGAN,
  title = {Large {{Scale GAN Training}} for {{High Fidelity Natural Image Synthesis}}},
  author = {Brock, Andrew and Donahue, Jeff and Simonyan, Karen},
  year = {2019},
  month = feb,
  abstract = {Despite recent progress in generative image modeling, successfully generating high-resolution, diverse samples from complex datasets such as ImageNet remains an elusive goal. To this end, we train Generative Adversarial Networks at the largest scale yet attempted, and study the instabilities specific to such scale. We find that applying orthogonal regularization to the generator renders it amenable to a simple "truncation trick," allowing fine control over the trade-off between sample fidelity and variety by reducing the variance of the Generator's input. Our modifications lead to models which set the new state of the art in class-conditional image synthesis. When trained on ImageNet at 128x128 resolution, our models (BigGANs) achieve an Inception Score (IS) of 166.5 and Frechet Inception Distance (FID) of 7.4, improving over the previous best IS of 52.52 and FID of 18.6.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1809.11096},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/brock et al_2019_large scale gan training for high fidelity natural image synthesis.pdf;/home/trung/Zotero/storage/B55GILVR/1809.html},
  journal = {arXiv:1809.11096 [cs, stat]},
  keywords = {adversarial,Computer Science - Machine Learning,gan,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{brosnan06_SulfurContainingAminoAcids,
  title = {The {{Sulfur}}-{{Containing Amino Acids}}: {{An Overview}}},
  shorttitle = {The {{Sulfur}}-{{Containing Amino Acids}}},
  author = {Brosnan, John T. and Brosnan, Margaret E.},
  year = {2006},
  month = jun,
  volume = {136},
  pages = {1636S-1640S},
  issn = {0022-3166, 1541-6100},
  doi = {10.1093/jn/136.6.1636S},
  abstract = {Methionine, cysteine, homocysteine, and taurine are the 4 common sulfur-containing amino acids, but only the first 2 are incorporated into proteins. Sulfur belongs to the same group in the periodic table as oxygen but is much less electronegative. This difference accounts for some of the distinctive properties of the sulfur-containing amino acids. Methionine is the initiating amino acid in the synthesis of virtually all eukaryotic proteins; Nformylmethionine serves the same function in prokaryotes. Within proteins, many of the methionine residues are buried in the hydrophobic core, but some, which are exposed, are susceptible to oxidative damage. Cysteine, by virtue of its ability to form disulfide bonds, plays a crucial role in protein structure and in protein-folding pathways. Methionine metabolism begins with its activation to S-adenosylmethionine. This is a cofactor of extraordinary versatility, playing roles in methyl group transfer, 59-deoxyadenosyl group transfer, polyamine synthesis, ethylene synthesis in plants, and many others. In animals, the great bulk of S-adenosylmethionine is used in methylation reactions. S-Adenosylhomocysteine, which is a product of these methyltransferases, gives rise to homocysteine. Homocysteine may be remethylated to methionine or converted to cysteine by the transsulfuration pathway. Methionine may also be metabolized by a transamination pathway. This pathway, which is significant only at high methionine concentrations, produces a number of toxic endproducts. Cysteine may be converted to such important products as glutathione and taurine. Taurine is present in many tissues at higher concentrations than any of the other amino acids. It is an essential nutrient for cats. J. Nutr. 136: 1636S\textendash 1640S, 2006.},
  annotation = {ZSCC: 0000783},
  file = {/home/trung/Zotero/storage/6YD2FEIT/Brosnan and Brosnan - 2006 - The Sulfur-Containing Amino Acids An Overview.pdf},
  journal = {The Journal of Nutrition},
  language = {en},
  number = {6}
}

@phdthesis{brown20_EquilibriumFindingLarge,
  title = {Equilibrium {{Finding}} for {{Large Adversarial Imperfect}}-{{Information Games}}},
  author = {Brown, Noam},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/brown_2020_equilibrium finding for large adversarial imperfect-information games.pdf}
}

@article{brown20_LanguageModelsare,
  title = {Language {{Models}} Are {{Few}}-{{Shot Learners}}},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  year = {2020},
  month = jul,
  abstract = {Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions - something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3's few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.},
  archivePrefix = {arXiv},
  eprint = {2005.14165},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/brown et al_2020_language models are few-shot learners.pdf},
  journal = {arXiv:2005.14165 [cs]},
  primaryClass = {cs}
}

@article{browne12_SurveyMonteCarlo,
  title = {A {{Survey}} of {{Monte Carlo Tree Search Methods}}},
  author = {Browne, Cameron and Powley, Edward and Whitehouse, Daniel and Lucas, Simon and Tavener, Stephen and Perez, Diego and Samothrakis, Spyridon and Colton, Simon},
  year = {2012},
  volume = {4},
  pages = {49},
  abstract = {Monte Carlo Tree Search (MCTS) is a recently proposed search method that combines the precision of tree search with the generality of random sampling. It has received considerable interest due to its spectacular success in the difficult problem of computer Go, but has also proved beneficial in a range of other domains. This paper is a survey of the literature to date, intended to provide a snapshot of the state of the art after the first five years of MCTS research. We outline the core algorithm's derivation, impart some structure on the many variations and enhancements that have been proposed, and summarise the results from the key game and non-game domains to which MCTS methods have been applied. A number of open research questions indicate that the field is ripe for future work.},
  file = {/home/trung/GoogleDrive/Zotero/browne et al_2012_a survey of monte carlo tree search methods.pdf},
  journal = {IEEE TRANSACTIONS ON COMPUTATIONAL INTELLIGENCE AND AI IN GAMES},
  language = {en},
  number = {1}
}

@article{brundage20_TrustworthyAIDevelopment,
  title = {Toward {{Trustworthy AI Development}}: {{Mechanisms}} for {{Supporting Verifiable Claims}}},
  shorttitle = {Toward {{Trustworthy AI Development}}},
  author = {Brundage, Miles and Avin, Shahar and Wang, Jasmine and Belfield, Haydn and Krueger, Gretchen and Hadfield, Gillian and Khlaaf, Heidy and Yang, Jingying and Toner, Helen and Fong, Ruth and Maharaj, Tegan and Koh, Pang Wei and Hooker, Sara and Leung, Jade and Trask, Andrew and Bluemke, Emma and Lebensold, Jonathan and O'Keefe, Cullen and Koren, Mark and Ryffel, Th{\'e}o and Rubinovitz, J. B. and Besiroglu, Tamay and Carugati, Federica and Clark, Jack and Eckersley, Peter and {de Haas}, Sarah and Johnson, Maritza and Laurie, Ben and Ingerman, Alex and Krawczuk, Igor and Askell, Amanda and Cammarota, Rosario and Lohn, Andrew and Krueger, David and Stix, Charlotte and Henderson, Peter and Graham, Logan and Prunkl, Carina and Martin, Bianca and Seger, Elizabeth and Zilberman, Noa and {h{\'E}igeartaigh}, Se{\'a}n {\'O} and Kroeger, Frens and Sastry, Girish and Kagan, Rebecca and Weller, Adrian and Tse, Brian and Barnes, Elizabeth and Dafoe, Allan and Scharre, Paul and {Herbert-Voss}, Ariel and Rasser, Martijn and Sodhani, Shagun and Flynn, Carrick and Gilbert, Thomas Krendl and Dyer, Lisa and Khan, Saif and Bengio, Yoshua and Anderljung, Markus},
  year = {2020},
  month = apr,
  abstract = {With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.},
  archivePrefix = {arXiv},
  eprint = {2004.07213},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/brundage et al_2020_toward trustworthy ai development.pdf},
  journal = {arXiv:2004.07213 [cs]},
  primaryClass = {cs}
}

@article{buda18_systematicstudyclass,
  title = {A Systematic Study of the Class Imbalance Problem in Convolutional Neural Networks},
  author = {Buda, Mateusz and Maki, Atsuto and Mazurowski, Maciej A.},
  year = {2018},
  month = oct,
  volume = {106},
  pages = {249--259},
  issn = {08936080},
  doi = {10.1016/j.neunet.2018.07.011},
  abstract = {In this study, we systematically investigate the impact of class imbalance on classification performance of convolutional neural networks (CNNs) and compare frequently used methods to address the issue. Class imbalance is a common problem that has been comprehensively studied in classical machine learning, yet very limited systematic research is available in the context of deep learning. In our study, we use three benchmark datasets of increasing complexity, MNIST, CIFAR-10 and ImageNet, to investigate the effects of imbalance on classification and perform an extensive comparison of several methods to address the issue: oversampling, undersampling, two-phase training, and thresholding that compensates for prior class probabilities. Our main evaluation metric is area under the receiver operating characteristic curve (ROC AUC) adjusted to multi-class tasks since overall accuracy metric is associated with notable difficulties in the context of imbalanced data. Based on results from our experiments we conclude that (i) the effect of class imbalance on classification performance is detrimental; (ii) the method of addressing class imbalance that emerged as dominant in almost all analyzed scenarios was oversampling; (iii) oversampling should be applied to the level that completely eliminates the imbalance, whereas the optimal undersampling ratio depends on the extent of imbalance; (iv) as opposed to some classical machine learning models, oversampling does not cause overfitting of CNNs; (v) thresholding should be applied to compensate for prior class probabilities when overall number of properly classified cases is of interest.},
  archivePrefix = {arXiv},
  eprint = {1710.05381},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/buda et al_2018_a systematic study of the class imbalance problem in convolutional neural networks.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{budish13_DesigningRandomAllocation,
  title = {Designing {{Random Allocation Mechanisms}}: {{Theory}} and {{Applications}}},
  shorttitle = {Designing {{Random Allocation Mechanisms}}},
  author = {Budish, Eric and Che, Yeon-Koo and Kojima, Fuhito and Milgrom, Paul},
  year = {2013},
  month = apr,
  volume = {103},
  pages = {585--623},
  issn = {0002-8282},
  doi = {10.1257/aer.103.2.585},
  abstract = {Randomization is commonplace in everyday resource allocation. We generalize the theory of randomized assignment to accommodate multi-unit allocations and various real-world constraints, such as group-specific quotas (``controlled choice'') in school choice and house allocation, and scheduling and curriculum constraints in course allocation. We develop new mechanisms that are ex ante efficient and fair in these environments, and that incorporate certain non-additive substitutable preferences. We also develop a ``utility guarantee'' technique that limits ex post unfairness in random allocations, supplementing the ex ante fairness promoted by randomization. This can be applied to multi-unit assignment problems and certain two-sided matching problems. (JEL C78, D82)},
  file = {/home/trung/GoogleDrive/Zotero/budish et al_2013_designing random allocation mechanisms.pdf},
  journal = {American Economic Review},
  language = {en},
  number = {2}
}

@article{bulinski19_StatisticalEstimationShannon,
  title = {Statistical {{Estimation}} of the {{Shannon Entropy}}},
  author = {Bulinski, Alexander and Dimitrov, Denis},
  year = {2019},
  month = jan,
  volume = {35},
  pages = {17--46},
  issn = {1439-7617},
  doi = {10.1007/s10114-018-7440-z},
  abstract = {The behavior of the Kozachenko\textendash Leonenko estimates for the (differential) Shannon entropy is studied when the number of i.i.d. vector-valued observations tends to infinity. The asymptotic unbiasedness and L2-consistency of the estimates are established. The conditions employed involve the analogues of the Hardy\textendash Littlewood maximal function. It is shown that the results are valid in particular for the entropy estimation of any nondegenerate Gaussian vector.},
  file = {/home/trung/GoogleDrive/Zotero/bulinski et al_2019_statistical estimation of the shannon entropy.pdf},
  journal = {Acta Mathematica Sinica, English Series},
  keywords = {information},
  number = {1}
}

@article{burda15_ImportanceWeightedAutoencoders,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2015},
  month = sep,
  abstract = {The variational autoencoder (VAE; Kingma \& Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1509.00519},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/burda et al_2015_importance weighted autoencoders.pdf},
  journal = {arXiv:1509.00519 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{burgess18_Understandingdisentanglingbeta,
  title = {Understanding Disentangling in \$\textbackslash beta\$-{{VAE}}},
  author = {Burgess, Christopher P. and Higgins, Irina and Pal, Arka and Matthey, Loic and Watters, Nick and Desjardins, Guillaume and Lerchner, Alexander},
  year = {2018},
  month = apr,
  abstract = {We present new intuitions and theoretical assessments of the emergence of disentangled representation in variational autoencoders. Taking a rate-distortion theory perspective, we show the circumstances under which representations aligned with the underlying generative factors of variation of data emerge when optimising the modified ELBO bound in {$\beta$}-VAE, as training progresses. From these insights, we propose a modification to the training regime of {$\beta$}-VAE, that progressively increases the information capacity of the latent code during training. This modification facilitates the robust learning of disentangled representations in {$\beta$}-VAE, without the previous trade-off in reconstruction accuracy.},
  annotation = {ZSCC: 0000154},
  archivePrefix = {arXiv},
  eprint = {1804.03599},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2018/false},
  journal = {arXiv:1804.03599 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{burgess19_MONetUnsupervisedScene,
  title = {{{MONet}}: {{Unsupervised Scene Decomposition}} and {{Representation}}},
  shorttitle = {{{MONet}}},
  author = {Burgess, Christopher P. and Matthey, Loic and Watters, Nicholas and Kabra, Rishabh and Higgins, Irina and Botvinick, Matt and Lerchner, Alexander},
  year = {2019},
  month = jan,
  abstract = {The ability to decompose scenes in terms of abstract building blocks is crucial for general intelligence. Where those basic building blocks share meaningful properties, interactions and other regularities across scenes, such decompositions can simplify reasoning and facilitate imagination of novel scenarios. In particular, representing perceptual observations in terms of entities should improve data efficiency and transfer performance on a wide range of tasks. Thus we need models capable of discovering useful decompositions of scenes by identifying units with such regularities and representing them in a common format. To address this problem, we have developed the Multi-Object Network (MONet). In this model, a VAE is trained end-to-end together with a recurrent attention network -- in a purely unsupervised manner -- to provide attention masks around, and reconstructions of, regions of images. We show that this model is capable of learning to decompose and represent challenging 3D scenes into semantically meaningful components, such as objects and background elements.},
  annotation = {ZSCC: 0000023},
  archivePrefix = {arXiv},
  eprint = {1901.11390},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/burgess et al_2019_monet.pdf;/home/trung/Zotero/storage/IBEWHV4E/1901.html},
  journal = {arXiv:1901.11390 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{burkhardt19_DecouplingSparsitySmoothness,
  title = {Decoupling {{Sparsity}} and {{Smoothness}} in the {{Dirichlet Variational Autoencoder Topic Model}}},
  author = {Burkhardt, Sophie and Kramer, Stefan},
  year = {2019},
  pages = {27},
  abstract = {Recent work on variational autoencoders (VAEs) has enabled the development of generative topic models using neural networks. Topic models based on latent Dirichlet allocation (LDA) successfully use the Dirichlet distribution as a prior for the topic and word distributions to enforce sparseness. However, there is a trade-off between sparsity and smoothness in Dirichlet distributions. Sparsity is important for a low reconstruction error during training of the autoencoder, whereas smoothness enables generalization and leads to a better loglikelihood of the test data. Both of these properties are encoded in the Dirichlet parameter vector. By rewriting this parameter vector into a product of a sparse binary vector and a smoothness vector, we decouple the two properties, leading to a model that features both a competitive topic coherence and a high log-likelihood. Efficient training is enabled using rejection sampling variational inference for the reparameterization of the Dirichlet distribution. Our experiments show that our method is competitive with other recent VAE topic models.},
  file = {/home/trung/GoogleDrive/Zotero/burkhardt et al_2019_decoupling sparsity and smoothness in the dirichlet variational autoencoder topic model.pdf},
  language = {en}
}

@article{burkhardt19_SurveyMultiLabelTopic,
  title = {A {{Survey}} of {{Multi}}-{{Label Topic Models}}},
  author = {Burkhardt, Sophie and Kramer, Stefan},
  year = {2019},
  month = nov,
  volume = {21},
  pages = {61--79},
  issn = {1931-0145, 1931-0153},
  doi = {10.1145/3373464.3373474},
  abstract = {Every day, an enormous amount of text data is produced. Sources of text data include news, social media, emails, text messages, medical reports, scientific publications and fiction. To keep track of this data, there are categories, key words, tags or labels that are assigned to each text. Automatically predicting such labels is the task of multi-label text classification. Often however, we are interested in more than just the pure classification: rather, we would like to understand which parts of a text belong to the label, which words are important for the label or which labels occur together. Because of this, topic models may be used for multi-label classification as an interpretable model that is flexible and easily extensible. This survey demonstrates the manifold possibilities and flexibility of the topic model framework for the complex setting of multi-label text classification by categorizing different variants of models.},
  file = {/home/trung/GoogleDrive/Zotero/burkhardt et al_2019_a survey of multi-label topic models.pdf},
  journal = {ACM SIGKDD Explorations Newsletter},
  language = {en},
  number = {2}
}

@article{butawan17_MethylsulfonylmethaneApplicationsSafety,
  title = {Methylsulfonylmethane: {{Applications}} and {{Safety}} of a {{Novel Dietary Supplement}}},
  shorttitle = {Methylsulfonylmethane},
  author = {Butawan, Matthew and Benjamin, Rodney L. and Bloomer, Richard J.},
  year = {2017},
  month = mar,
  volume = {9},
  issn = {2072-6643},
  doi = {10.3390/nu9030290},
  abstract = {Methylsulfonylmethane (MSM) has become a popular dietary supplement used for a variety of purposes, including its most common use as an anti-inflammatory agent. It has been well-investigated in animal models, as well as in human clinical trials and experiments. A variety of health-specific outcome measures are improved with MSM supplementation, including inflammation, joint/muscle pain, oxidative stress, and antioxidant capacity. Initial evidence is available regarding the dose of MSM needed to provide benefit, although additional work is underway to determine the precise dose and time course of treatment needed to provide optimal benefits. As a Generally Recognized As Safe (GRAS) approved substance, MSM is well-tolerated by most individuals at dosages of up to four grams daily, with few known and mild side effects. This review provides an overview of MSM, with details regarding its common uses and applications as a dietary supplement, as well as its safety for consumption.},
  annotation = {ZSCC: 0000033},
  file = {/home/trung/GoogleDrive/Zotero/butawan et al_2017_methylsulfonylmethane.pdf},
  journal = {Nutrients},
  language = {eng},
  number = {3},
  pmcid = {PMC5372953},
  pmid = {28300758}
}

@article{butawan17_MethylsulfonylmethaneApplicationsSafetya,
  title = {Methylsulfonylmethane: {{Applications}} and {{Safety}} of a {{Novel Dietary Supplement}}},
  shorttitle = {Methylsulfonylmethane},
  author = {Butawan, Matthew and Benjamin, Rodney L. and Bloomer, Richard J.},
  year = {2017},
  month = mar,
  volume = {9},
  issn = {2072-6643},
  doi = {10.3390/nu9030290},
  abstract = {Methylsulfonylmethane (MSM) has become a popular dietary supplement used for a variety of purposes, including its most common use as an anti-inflammatory agent. It has been well-investigated in animal models, as well as in human clinical trials and experiments. A variety of health-specific outcome measures are improved with MSM supplementation, including inflammation, joint/muscle pain, oxidative stress, and antioxidant capacity. Initial evidence is available regarding the dose of MSM needed to provide benefit, although additional work is underway to determine the precise dose and time course of treatment needed to provide optimal benefits. As a Generally Recognized As Safe (GRAS) approved substance, MSM is well-tolerated by most individuals at dosages of up to four grams daily, with few known and mild side effects. This review provides an overview of MSM, with details regarding its common uses and applications as a dietary supplement, as well as its safety for consumption.},
  file = {/home/trung/GoogleDrive/Zotero/butawan et al_2017_methylsulfonylmethane2.pdf},
  journal = {Nutrients},
  number = {3},
  pmcid = {PMC5372953},
  pmid = {28300758}
}

@article{butler18_Integratingsinglecelltranscriptomic,
  title = {Integrating Single-Cell Transcriptomic Data across Different Conditions, Technologies, and Species},
  author = {Butler, Andrew and Hoffman, Paul and Smibert, Peter and Papalexi, Efthymia and Satija, Rahul},
  year = {2018},
  month = may,
  volume = {36},
  pages = {411--420},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/nbt.4096},
  file = {/home/trung/GoogleDrive/Zotero/butler et al_2018_integrating single-cell transcriptomic data across different conditions, technologies, and species.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {5}
}

@article{butler20_impactnutritionCOVID19,
  title = {The Impact of Nutrition on {{COVID}}-19 Susceptibility and Long-Term Consequences},
  author = {Butler, Michael J. and Barrientos, Ruth M.},
  year = {2020},
  month = apr,
  issn = {0889-1591},
  doi = {10.1016/j.bbi.2020.04.040},
  abstract = {While all groups are affected by the COVID-19 pandemic, the elderly, underrepresented minorities, and those with underlying medical conditions are at the greatest risk. The high rate of consumption of diets high in saturated fats, sugars, and refined carbohydrates (collectively called Western diet, WD) worldwide, contribute to the prevalence of obesity and type 2 diabetes, and could place these populations at an increased risk for severe COVID-19 pathology and mortality. WD consumption activates the innate immune system and impairs adaptive immunity, leading to chronic inflammation and impaired host defense against viruses. Furthermore, peripheral inflammation caused by COVID-19 may have long-term consequences in those that recover, leading to chronic medical conditions such as dementia and neurodegenerative disease, likely through neuroinflammatory mechanisms that can be compounded by an unhealthy diet. Thus, now more than ever, wider access to healthy foods should be a top priority and individuals should be mindful of healthy eating habits to reduce susceptibility to and long-term complications from COVID-19.},
  file = {/home/trung/GoogleDrive/Zotero/butler et al_2020_the impact of nutrition on covid-19 susceptibility and long-term consequences.pdf;/home/trung/Zotero/storage/F9CIKQFW/S0889159120305377.html},
  journal = {Brain, Behavior, and Immunity},
  language = {en}
}

@article{cai18_ExploringEncodingLayer,
  title = {Exploring the {{Encoding Layer}} and {{Loss Function}} in {{End}}-to-{{End Speaker}} and {{Language Recognition System}}},
  author = {Cai, Weicheng and Chen, Jinkun and Li, Ming},
  year = {2018},
  month = apr,
  abstract = {In this paper, we explore the encoding/pooling layer and loss function in the end-to-end speaker and language recognition system. First, a unified and interpretable end-to-end system for both speaker and language recognition is developed. It accepts variable-length input and produces an utterance level result. In the end-to-end system, the encoding layer plays a role in aggregating the variable-length input sequence into an utterance level representation. Besides the basic temporal average pooling, we introduce a self-attentive pooling layer and a learnable dictionary encoding layer to get the utterance level representation. In terms of loss function for open-set speaker verification, to get more discriminative speaker embedding, center loss and angular softmax loss is introduced in the end-to-end system. Experimental results on Voxceleb and NIST LRE 07 datasets show that the performance of end-to-end learning system could be significantly improved by the proposed encoding layer and loss function.},
  archivePrefix = {arXiv},
  eprint = {1804.05160},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cai et al_2018_exploring the encoding layer and loss function in end-to-end speaker and language recognition system.pdf;/home/trung/GoogleDrive/Zotero/cai et al_2018_exploring the encoding layer and loss function in end-to-end speaker and language recognition system2.pdf;/home/trung/Zotero/storage/8ERH39K2/1804.html;/home/trung/Zotero/storage/SH2YH2XU/1804.html},
  journal = {arXiv:1804.05160 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{cai18_NovelLearnableDictionary,
  title = {A {{Novel Learnable Dictionary Encoding Layer}} for {{End}}-to-{{End Language Identification}}},
  author = {Cai, Weicheng and Cai, Zexin and Zhang, Xiang and Wang, Xiaoqi and Li, Ming},
  year = {2018},
  month = apr,
  abstract = {A novel learnable dictionary encoding layer is proposed in this paper for end-to-end language identification. It is inline with the conventional GMM i-vector approach both theoretically and practically. We imitate the mechanism of traditional GMM training and Supervector encoding procedure on the top of CNN. The proposed layer can accumulate high-order statistics from variable-length input sequence and generate an utterance level fixed-dimensional vector representation. Unlike the conventional methods, our new approach provides an end-to-end learning framework, where the inherent dictionary are learned directly from the loss function. The dictionaries and the encoding representation for the classifier are learned jointly. The representation is orderless and therefore appropriate for language identification. We conducted a preliminary experiment on NIST LRE07 closed-set task, and the results reveal that our proposed dictionary encoding layer achieves significant error reduction comparing with the simple average pooling.},
  archivePrefix = {arXiv},
  eprint = {1804.00385},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cai et al_2018_a novel learnable dictionary encoding layer for end-to-end language identification.pdf;/home/trung/Zotero/storage/B253FER4/1804.html},
  journal = {arXiv:1804.00385 [cs, eess]},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{cai19_DKUSMIIPSystemNIST,
  title = {The {{DKU}}-{{SMIIP System}} for {{NIST}} 2018 {{Speaker Recognition Evaluation}}},
  author = {Cai, Danwei and Cai, Weicheng and Li, Ming},
  year = {2019},
  month = jul,
  abstract = {In this paper, we present the system submission for the NIST 2018 Speaker Recognition Evaluation by DKU Speech and Multi-Modal Intelligent Information Processing (SMIIP) Lab. We explore various kinds of state-of-the-art front-end extractors as well as back-end modeling for text-independent speaker verifications. Our submitted primary systems employ multiple state-of-the-art front-end extractors, including the MFCC i-vector, the DNN tandem i-vector, the TDNN x-vector, and the deep ResNet. After speaker embedding is extracted, we exploit several kinds of back-end modeling to perform variability compensation and domain adaptation for mismatch training and testing conditions. The final submitted system on the fixed condition obtains actual detection cost of 0.392 and 0.494 on CMN2 and VAST evaluation data respectively. After the official evaluation, we further extend our experiments by investigating multiple encoding layer designs and loss functions for the deep ResNet system.},
  archivePrefix = {arXiv},
  eprint = {1907.02191},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cai et al_2019_the dku-smiip system for nist 2018 speaker recognition evaluation.pdf;/home/trung/Zotero/storage/DW9KY26S/1907.html},
  journal = {arXiv:1907.02191 [eess]},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {eess}
}

@article{cai19_GroupRepresentationTheory,
  title = {Group {{Representation Theory}} for {{Knowledge Graph Embedding}}},
  author = {Cai, Chen},
  year = {2019},
  month = sep,
  abstract = {Knowledge graph embedding has recently become a popular way to model relations and infer missing links. In this paper, we present a group theoretical perspective of knowledge graph embedding, connecting previous methods with different group actions. Furthermore, by utilizing Schur's lemma from group representation theory, we show that the state of the art embedding method RotatE can model relations from any finite Abelian group.},
  archivePrefix = {arXiv},
  eprint = {1909.05100},
  eprinttype = {arxiv},
  journal = {arXiv:1909.05100 [cs, math]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Representation Theory},
  primaryClass = {cs, math}
}

@article{calder20_OptimalNutritionalStatus,
  title = {Optimal {{Nutritional Status}} for a {{Well}}-{{Functioning Immune System Is}} an {{Important Factor}} to {{Protect}} against {{Viral Infections}}},
  author = {Calder, Philip C. and Carr, Anitra C. and Gombart, Adrian F. and Eggersdorfer, Manfred},
  year = {2020},
  month = apr,
  volume = {12},
  pages = {1181},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  doi = {10.3390/nu12041181},
  abstract = {Public health practices including handwashing and vaccinations help reduce the spread and impact of infections. Nevertheless, the global burden of infection is high, and additional measures are necessary. Acute respiratory tract infections, for example, were responsible for approximately 2.38 million deaths worldwide in 2016. The role nutrition plays in supporting the immune system is well-established. A wealth of mechanistic and clinical data show that vitamins, including vitamins A, B6, B12, C, D, E, and folate; trace elements, including zinc, iron, selenium, magnesium, and copper; and the omega-3 fatty acids eicosapentaenoic acid and docosahexaenoic acid play important and complementary roles in supporting the immune system. Inadequate intake and status of these nutrients are widespread, leading to a decrease in resistance to infections and as a consequence an increase in disease burden. Against this background the following conclusions are made: (1) supplementation with the above micronutrients and omega-3 fatty acids is a safe, effective, and low-cost strategy to help support optimal immune function; (2) supplementation above the Recommended Dietary Allowance (RDA), but within recommended upper safety limits, for specific nutrients such as vitamins C and D is warranted; and (3) public health officials are encouraged to include nutritional strategies in their recommendations to improve public health.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  file = {/home/trung/GoogleDrive/Zotero/calder et al_2020_optimal nutritional status for a well-functioning immune system is an important factor to protect against viral infections.pdf;/home/trung/Zotero/storage/MGYBZGYJ/htm.html},
  journal = {Nutrients},
  language = {en},
  number = {4}
}

@article{callaway20_racecoronavirusvaccines,
  title = {The Race for Coronavirus Vaccines: A Graphical Guide},
  shorttitle = {The Race for Coronavirus Vaccines},
  author = {Callaway, Ewen},
  year = {2020},
  month = apr,
  volume = {580},
  pages = {576--577},
  publisher = {{Nature Publishing Group}},
  doi = {10.1038/d41586-020-01221-y},
  abstract = {Eight ways in which scientists hope to provide immunity to SARS-CoV-2 .},
  annotation = {ZSCC: 0000000},
  copyright = {2020 Nature},
  file = {/home/trung/GoogleDrive/Zotero/callaway_2020_the race for coronavirus vaccines.pdf;/home/trung/Zotero/storage/X2S2TKQY/d41586-020-01221-y.html},
  journal = {Nature},
  language = {en},
  number = {7805}
}

@article{campbell19_UniversalBoostingVariational,
  title = {Universal {{Boosting Variational Inference}}},
  author = {Campbell, Trevor and Li, Xinglong},
  year = {2019},
  month = oct,
  abstract = {Boosting variational inference (BVI) approximates an intractable probability density by iteratively building up a mixture of simple component distributions one at a time, using techniques from sparse convex optimization to provide both computational scalability and approximation error guarantees. But the guarantees have strong conditions that do not often hold in practice, resulting in degenerate component optimization problems; and we show that the ad-hoc regularization used to prevent degeneracy in practice can cause BVI to fail in unintuitive ways. We thus develop universal boosting variational inference (UBVI), a BVI scheme that exploits the simple geometry of probability densities under the Hellinger metric to prevent the degeneracy of other gradient-based BVI methods, avoid difficult joint optimizations of both component and weight, and simplify fully-corrective weight optimizations. We show that for any target density and any mixture component family, the output of UBVI converges to the best possible approximation in the mixture family, even when the mixture family is misspecified. We develop a scalable implementation based on exponential family mixture components and standard stochastic optimization techniques. Finally, we discuss statistical benefits of the Hellinger distance as a variational objective through bounds on posterior probability, moment, and importance sampling errors. Experiments on multiple datasets and models show that UBVI provides reliable, accurate posterior approximations.},
  archivePrefix = {arXiv},
  eprint = {1906.01235},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/campbell et al_2019_universal boosting variational inference.pdf},
  journal = {arXiv:1906.01235 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{campos16_SmokingCognition,
  title = {Smoking and {{Cognition}}},
  author = {Campos, Marcela Waisman and Serebrisky, Debora and {Castaldelli-Maia}, Joao Mauricio},
  year = {2016},
  volume = {9},
  pages = {76--79},
  issn = {1874-4745},
  doi = {10.2174/1874473709666160803101633},
  abstract = {Given the large availability of nicotinic acetylcholine receptors (nAChRs) throughout the brain, and the wide range of neurotransmitter systems affected (norepinephrine, serotonin and dopamine), nicotine influences a wide variety of cognitive domains such as sensorial, motor, attention, executive function, learning and memory. This article reviews current state of the art research on the effects of nicotine upon cognition. There are different neurobiological mechanisms involved in acute/chronic smoking and nicotine abstinence. Smoking reinforcement could be due to the initial cognitive improvement, that is, individuals can learn that smoking temporarily increases cognitive functioning (improving some components of attention and memory). These acute nicotine effects improve (i) cognitive performance above smokers' normal levels, and (ii) cognitive disruption resulting from nicotine abstinence. Both neurobiological effects act as reinforcers to nicotine use, greatly contributing to the development of nicotine dependence. However, heavy smoking is associated with cognitive impairment and cognitive decline in middle age. Future clinical research should investigate the role of positive and negative cognitive effects of nicotine in smoking cessation treatment. This is clearly an important scientific issue, with insufficient current data from which to draw definitive conclusions.},
  journal = {Current Drug Abuse Reviews},
  language = {eng},
  number = {2},
  pmid = {27492358}
}

@article{campos18_SKIPRNNLEARNING,
  title = {{{SKIP RNN}}: {{LEARNING TO SKIP STATE UPDATES IN RECURRENT NEURAL NETWORKS}}},
  author = {Campos, V{\i}ctor and Jou, Brendan and {Giro-i-Nieto}, Xavier and Torres, Jordi and Chang, Shih-Fu},
  year = {2018},
  pages = {17},
  abstract = {Recurrent Neural Networks (RNNs) continue to show outstanding performance in sequence modeling tasks. However, training RNNs on long sequences often face challenges like slow inference, vanishing gradients and difficulty in capturing long term dependencies. In backpropagation through time settings, these issues are tightly coupled with the large, sequential computational graph resulting from unfolding the RNN in time. We introduce the Skip RNN model which extends existing RNN models by learning to skip state updates and shortens the effective size of the computational graph. This model can also be encouraged to perform fewer state updates through a budget constraint. We evaluate the proposed model on various tasks and show how it can reduce the number of required RNN updates while preserving, and sometimes even improving, the performance of the baseline RNN models. Source code is publicly available at https://imatge-upc. github.io/skiprnn-2017-telecombcn/.},
  file = {/home/trung/GoogleDrive/Zotero/campos et al_2018_skip rnn.pdf},
  keywords = {fast,recurrent,skip},
  language = {en}
}

@article{camuto20_TheoreticalUnderstandingRobustness,
  title = {Towards a {{Theoretical Understanding}} of the {{Robustness}} of {{Variational Autoencoders}}},
  author = {Camuto, Alexander and Willetts, Matthew and Roberts, Stephen and Holmes, Chris and Rainforth, Tom},
  year = {2020},
  month = jul,
  abstract = {We make inroads into understanding the robustness of Variational Autoencoders (VAEs) to adversarial attacks and other input perturbations. While previous work has developed algorithmic approaches to attacking and defending VAEs, there remains a lack of formalization for what it means for a VAE to be robust. To address this, we develop a novel criterion for robustness in probabilistic models: r-robustness. We then use this to construct the first theoretical results for the robustness of VAEs, deriving margins in the input space for which we can provide guarantees about the resulting reconstruction. Informally, we are able to define a region within which any perturbation will produce a reconstruction that is similar to the original reconstruction. To support our analysis, we show that VAEs trained using disentangling methods not only score well under our robustness metrics, but that the reasons for this can be interpreted through our theoretical results.},
  archivePrefix = {arXiv},
  eprint = {2007.07365},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/camuto et al_2020_towards a theoretical understanding of the robustness of variational autoencoders.pdf},
  journal = {arXiv:2007.07365 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{cao18_Jointprofilingchromatin,
  title = {Joint Profiling of Chromatin Accessibility and Gene Expression in Thousands of Single Cells},
  author = {Cao, Junyue and Cusanovich, Darren A. and Ramani, Vijay and Aghamirzaie, Delasa and Pliner, Hannah A. and Hill, Andrew J. and Daza, Riza M. and {McFaline-Figueroa}, Jose L. and Packer, Jonathan S. and Christiansen, Lena and Steemers, Frank J. and Adey, Andrew C. and Trapnell, Cole and Shendure, Jay},
  year = {2018},
  month = sep,
  volume = {361},
  pages = {1380--1385},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aau0730},
  abstract = {Although we can increasingly measure transcription, chromatin, methylation, and other aspects of molecular biology at single-cell resolution, most assays survey only one aspect of cellular biology. Here we describe sci-CAR, a combinatorial indexing\textendash based coassay that jointly profiles chromatin accessibility and mRNA (CAR) in each of thousands of single cells. As a proof of concept, we apply sci-CAR to 4825 cells, including a time series of dexamethasone treatment, as well as to 11,296 cells from the adult mouse kidney. With the resulting data, we compare the pseudotemporal dynamics of chromatin accessibility and gene expression, reconstruct the chromatin accessibility profiles of cell types defined by RNA profiles, and link cis-regulatory sites to their target genes on the basis of the covariance of chromatin accessibility and transcription across large numbers of single cells.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/DALMVMXT/Cao et al. - 2018 - Joint profiling of chromatin accessibility and gen.pdf},
  journal = {Science},
  language = {en},
  number = {6409}
}

@article{cao19_UnderstandingSpectralBias,
  title = {Towards {{Understanding}} the {{Spectral Bias}} of {{Deep Learning}}},
  author = {Cao, Yuan and Fang, Zhiying and Wu, Yue and Zhou, Ding-Xuan and Gu, Quanquan},
  year = {2019},
  month = dec,
  abstract = {An intriguing phenomenon observed during training neural networks is the spectral bias, where neural networks are biased towards learning less complex functions. The priority of learning functions with low complexity might be at the core of explaining generalization ability of neural network, and certain efforts have been made to provide theoretical explanation for spectral bias. However, there is still no satisfying theoretical result justifying the underlying mechanism of spectral bias. In this paper, we give a comprehensive and rigorous explanation for spectral bias and relate it with the neural tangent kernel function proposed in recent work. We prove that the training process of neural networks can be decomposed along different directions defined by the eigenfunctions of the neural tangent kernel, where each direction has its own convergence rate and the rate is determined by the corresponding eigenvalue. We then provide a case study when the input data is uniformly distributed over the unit sphere, and show that lower degree spherical harmonics are easier to be learned by over-parameterized neural networks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1912.01198},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cao et al_2019_towards understanding the spectral bias of deep learning.pdf;/home/trung/Zotero/storage/XQ92ZQIA/1912.html},
  journal = {arXiv:1912.01198 [cs, stat]},
  keywords = {Computer Science - Machine Learning,spectral bias,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{cao20_ComprehensiveSurveyGeometric,
  title = {A {{Comprehensive Survey}} on {{Geometric Deep Learning}}},
  author = {Cao, Wenming and Yan, Zhiyue and He, Zhiquan and He, Zhihai},
  year = {2020},
  pages = {1--1},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2975067},
  abstract = {Deep learning methods have achieved great success in analyzing traditional data such as texts, sounds, images and videos. More and more research works are carrying out to extend standard deep learning technologies to geometric data such as point cloud or voxel grid of 3D objects, real life networks such as social and citation network. Many methods have been proposed in the research area. In this work, we aim to provide a comprehensive survey of geometric deep learning and related methods. First, we introduce the relevant knowledge and history of geometric deep learning field as well as the theoretical background. In the method part, we review different graph network models for graphs and manifold data. Besides, practical applications of these methods, datasets currently available in different research area and the problems and challenges are also summarized.},
  file = {/home/trung/GoogleDrive/Zotero/cao et al_2020_a comprehensive survey on geometric deep learning.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{cardon18_NEURONSSPIKEBACK,
  title = {{{NEURONS SPIKE BACK The Invention}} of {{Inductive Machines}} and the {{Artificial Intelligence Controversy}}},
  shorttitle = {La Revanche Des Neurones},
  author = {Cardon, Dominique and Cointet, Jean-Philippe and Mazi{\`e}res, Antoine},
  year = {2018},
  volume = {n\textdegree{} 211},
  pages = {173},
  issn = {0751-7971, 1777-5809},
  doi = {10.3917/res.211.0173},
  abstract = {Since 2010, machine learning based predictive techniques, and more specifically deep learning neural networks, have achieved spectacular performances in the fields of image recognition or automatic translation, under the umbrella term of ``Artificial Intelligence''. But their filiation to this field of research is not straightforward. In the tumultuous history of AI, learning techniques using so-called "connectionist" neural networks have long been mocked and ostracized by the "symbolic" movement. This article retraces the history of artificial intelligence through the lens of the tension between symbolic and connectionist approaches. From a social history of science and technology perspective, it seeks to highlight how researchers, relying on the availability of massive data and the multiplication of computing power have undertaken to reformulate the symbolic AI project by reviving the spirit of adaptive and inductive machines dating back from the era of cybernetics.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/cardon et al_2018_neurons spike back the invention of inductive machines and the artificial intelligence controversy.pdf},
  journal = {R\'eseaux},
  keywords = {connectionist,history,symbolic},
  language = {en},
  number = {5}
}

@misc{carlzimmer19_ScienceWritingGuidelines,
  title = {Science {{Writing}}: {{Guidelines}} and {{Guidance}}},
  shorttitle = {Science {{Writing}}},
  author = {{carlzimmer}},
  year = {2019},
  month = oct,
  abstract = {These are notes for a class called ``Writing about Science, Medicine, and the Environment,'' which I have taught for several years at Yale\ldots},
  file = {/home/trung/Zotero/storage/T2RVMTL9/science-writing-guidelines-and-guidance-8c6a6bc37d75.html},
  howpublished = {https://medium.com/swlh/science-writing-guidelines-and-guidance-8c6a6bc37d75},
  journal = {Medium},
  language = {en}
}

@techreport{caron19_Singlecellanalysischildhood,
  title = {Single-Cell Analysis of Childhood Leukemia Reveals a Link between Developmental States and Ribosomal Protein Expression as a Source of Intra-Individual Heterogeneity},
  author = {Caron, Maxime and {St-Onge}, Pascal and Sontag, Thomas and Wang, Yu Chang and Richer, Chantal and Ragoussis, Ioannis and Sinnett, Daniel and Bourque, Guillaume},
  year = {2019},
  month = jun,
  institution = {{Cancer Biology}},
  doi = {10.1101/683854},
  abstract = {Childhood acute lymphoblastic leukemia (cALL) is the most common pediatric cancer. It is characterized by bone marrow lymphoid precursors that acquire genetic alterations, resulting in disrupted maturation and uncontrollable proliferation. More than a dozen molecular subtypes of variable severity can be used to classify cALL cases. Modern therapy protocols currently cure 85-90\% of cases, but other patients are refractory or will relapse and eventually succumb to their disease. To better understand these difficult cases, we investigated the nature and extent of intra-individual transcriptional heterogeneity of cALL at the cellular level by sequencing the transcriptomes of 39,375 individual cells in eight patients (six pre-B and two pre-T) and three healthy pediatric controls. We observed intra-individual transcriptional clusters in five out of the eight patients. Using pseudotime maturation trajectories of healthy B and T cells, we obtained the predicted developmental state of each leukemia cell and observed distribution shifts within patients. We showed that the predicted developmental states of these cancer cells are inversely correlated with ribosomal protein expression levels, which could be a common contributor to intra-individual heterogeneity in cALL patients.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/caron et al_2019_single-cell analysis of childhood leukemia reveals a link between developmental states and ribosomal protein expression as a source of intra-individual heterogeneity.pdf},
  language = {en},
  type = {Preprint}
}

@article{caron20_UnsupervisedLearningVisual,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year = {2020},
  month = jul,
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archivePrefix = {arXiv},
  eprint = {2006.09882},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/caron et al_2020_unsupervised learning of visual features by contrasting cluster assignments.pdf},
  journal = {arXiv:2006.09882 [cs]},
  keywords = {self-supervised},
  primaryClass = {cs}
}

@article{casale18_GaussianProcessPrior,
  title = {Gaussian {{Process Prior Variational Autoencoders}}},
  author = {Casale, Francesco Paolo and Dalca, Adrian V. and Saglietti, Luca and Listgarten, Jennifer and Fusi, Nicolo},
  year = {2018},
  month = nov,
  abstract = {Variational autoencoders (VAE) are a powerful and widely-used class of models to learn complex data distributions in an unsupervised fashion. One important limitation of VAEs is the prior assumption that latent sample representations are independent and identically distributed. However, for many important datasets, such as time-series of images, this assumption is too strong: accounting for covariances between samples, such as those in time, can yield to a more appropriate model specification and improve performance in downstream tasks. In this work, we introduce a new model, the Gaussian Process (GP) Prior Variational Autoencoder (GPPVAE), to specifically address this issue. The GPPVAE aims to combine the power of VAEs with the ability to model correlations afforded by GP priors. To achieve efficient inference in this new class of models, we leverage structure in the covariance matrix, and introduce a new stochastic backpropagation strategy that allows for computing stochastic gradients in a distributed and low-memory fashion. We show that our method outperforms conditional VAEs (CVAEs) and an adaptation of standard VAEs in two image data applications.},
  annotation = {ZSCC: 0000014},
  archivePrefix = {arXiv},
  eprint = {1810.11738},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/casale et al_2018_gaussian process prior variational autoencoders.pdf},
  journal = {arXiv:1810.11738 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{cemgil20_AdversariallyRobustRepresentations,
  title = {Adversarially {{Robust Representations}} with {{Smooth Encoders}}},
  author = {Cemgil, A Taylan and Ghaisas, Sumedh and Dvijotham, Krishnamurthy and Kohli, Pushmeet},
  year = {2020},
  pages = {20},
  abstract = {This paper studies the undesired phenomena of over-sensitivity of representations learned by deep networks to semantically-irrelevant changes in data. We identify a cause for this shortcoming in the classical Variational Auto-encoder (VAE) objective, the evidence lower bound (ELBO). We show that the ELBO fails to control the behaviour of the encoder out of the support of the empirical data distribution and this behaviour of the VAE can lead to extreme errors in the learned representation. This is a key hurdle in the effective use of representations for data-efficient learning and transfer. To address this problem, we propose to augment the data with specifications that enforce insensitivity of the representation with respect to families of transformations. To incorporate these specifications, we propose a regularization method that is based on a selection mechanism that creates a fictive data point by explicitly perturbing an observed true data point. For certain choices of parameters, our formulation naturally leads to the minimization of the entropy regularized Wasserstein distance between representations. We illustrate our approach on standard datasets and experimentally show that significant improvements in the downstream adversarial accuracy can be achieved by learning robust representations completely in an unsupervised manner, without a reference to a particular downstream task and without a costly supervised adversarial training procedure.},
  file = {/home/trung/GoogleDrive/Zotero/cemgil et al_2020_adversarially robust representations with smooth encoders.pdf},
  keywords = {_tablet,favorite},
  language = {en}
}

@article{chalupka17_Causalfeaturelearning,
  title = {Causal Feature Learning: An Overview},
  shorttitle = {Causal Feature Learning},
  author = {Chalupka, Krzysztof and Eberhardt, Frederick and Perona, Pietro},
  year = {2017},
  month = jan,
  volume = {44},
  pages = {137--164},
  issn = {0385-7417, 1349-6964},
  doi = {10.1007/s41237-016-0008-2},
  abstract = {Causal feature learning (CFL) (Chalupka et al., Proceedings of the Thirty-First Conference on Uncertainty in Artificial Intelligence. AUAI Press, Edinburgh, pp 181\textendash 190, 2015) is a causal inference framework rooted in the language of causal graphical models (Pearl J, Reasoning and inference. Cambridge University Press, Cambridge, 2009; Spirtes et al., Causation, Prediction, and Search. Massachusetts Institute of Technology, Massachusetts, 2000), and computational mechanics (Shalizi, PhD thesis, University of Wisconsin at Madison, 2001). CFL is aimed at discovering high-level causal relations from low-level data, and at reducing the experimental effort to understand confounding among the high-level variables. We first review the scientific motivation for CFL, then present a detailed introduction to the framework, laying out the definitions and algorithmic steps. A simple example illustrates the techniques involved in the learning steps and provides visual intuition. Finally, we discuss the limitations of the current framework and list a number of open problems.},
  file = {/home/trung/GoogleDrive/Zotero/chalupka et al_2017_causal feature learning.pdf},
  journal = {Behaviormetrika},
  keywords = {causal},
  language = {en},
  number = {1}
}

@article{chambliss21_MundanityExcellenceEthnographic,
  title = {The {{Mundanity}} of {{Excellence}}: {{An Ethnographic Report}} on {{Stratification}} and {{Olympic Swimmers}}},
  shorttitle = {The {{Mundanity}} of {{Excellence}}},
  author = {Chambliss, Daniel F.},
  year = {21},
  volume = {7},
  pages = {70},
  issn = {07352751},
  doi = {10.2307/202063},
  file = {/home/trung/GoogleDrive/Zotero/chambliss_1989_the mundanity of excellence.pdf},
  journal = {Sociological Theory},
  language = {en},
  number = {1}
}

@article{chami20_MachineLearningGraphs,
  title = {Machine {{Learning}} on {{Graphs}}: {{A Model}} and {{Comprehensive Taxonomy}}},
  shorttitle = {Machine {{Learning}} on {{Graphs}}},
  author = {Chami, Ines and {Abu-El-Haija}, Sami and Perozzi, Bryan and R{\'e}, Christopher and Murphy, Kevin},
  year = {2020},
  month = may,
  abstract = {There has been a surge of recent interest in learning representations for graph-structured data. Graph representation learning methods have generally fallen into three main categories, based on the availability of labeled data. The first, network embedding (such as shallow graph embedding or graph auto-encoders), focuses on learning unsupervised representations of relational structure. The second, graph regularized neural networks, leverages graphs to augment neural network losses with a regularization objective for semi-supervised learning. The third, graph neural networks, aims to learn differentiable functions over discrete topologies with arbitrary structure. However, despite the popularity of these areas there has been surprisingly little work on unifying the three paradigms. Here, we aim to bridge the gap between graph neural networks, network embedding and graph regularization models. We propose a comprehensive taxonomy of representation learning methods for graph-structured data, aiming to unify several disparate bodies of work. Specifically, we propose a Graph Encoder Decoder Model (GRAPHEDM), which generalizes popular algorithms for semi-supervised learning on graphs (e.g. GraphSage, Graph Convolutional Networks, Graph Attention Networks), and unsupervised learning of graph representations (e.g. DeepWalk, node2vec, etc) into a single consistent approach. To illustrate the generality of this approach, we fit over thirty existing methods into this framework. We believe that this unifying view both provides a solid foundation for understanding the intuition behind these methods, and enables future research in the area.},
  archivePrefix = {arXiv},
  eprint = {2005.03675},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chami et al_2020_machine learning on graphs.pdf},
  journal = {arXiv:2005.03675 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chan00_ExchangeableVariationalAutoencoders,
  title = {Exchangeable {{Variational Autoencoders}} with {{Applications}} to {{Genomic Data}}},
  author = {Chan, Jeffrey and Spence, Jeffrey P and Song, Yun S},
  pages = {6},
  abstract = {Exchangeable-structured datapoints (datapoints which contain permutationinvariant symmetries) are ubiquitous in statistical problems ranging from point clouds to graphs to sets. Particularly in biological settings, where multiple experiments derived from a noisy scientific process attempt to measure a latent variable of interest, experimental datapoints are often exchangeable-structured demanding the development of methods which can exploit this structure. Modern machine learning approaches to scalable Bayesian inference typically use autoencoding variational Bayes \textendash{} marrying ideas from deep learning and probabilistic modeling to achieve practical inference for expressive models. Current VAE-based approaches do not naturally handle exchangeable (but non-i.i.d.) datapoints. Often exchangeable-structured datapoints may contain heterogeneity in datapoint dimensions precluding a staightforward application of the vanilla VAE framework. In this work, we develop the Exchangeable Variational Autoencoder which provides inferential and computational benefits while enabling varying set size data to be robustly handled in the VAE framework. We then demonstrate its efficacy in two settings: (1) on the well-studied Latent Dirichlet Allocation model and (2) on the bootstrapped, isoform-level uncertainty estimates of single-cell RNA-seq data.},
  file = {/home/trung/GoogleDrive/Zotero/chan et al_exchangeable variational autoencoders with applications to genomic data.pdf},
  language = {en}
}

@article{chan00_ShowDivideNeural,
  title = {Show, {{Divide}} and {{Neural}}: {{Weighted Style Transfer}}},
  author = {Chan, Ethan and Bhargava, Rishabh},
  pages = {6},
  abstract = {The neural style algorithm has been very successful in obtaining the style of famous pieces of art. There have been attempts to overlay this style on images, to varying degrees of success. We believe that we can improve the vanilla Neural Style by performing image segmentation, and applying neural style only on the background, or only on our main object or both. This leads to our object remaining distinct, while the new image still manages to take an artsy look.},
  file = {/home/trung/GoogleDrive/Zotero/chan et al_show, divide and neural.pdf},
  language = {en}
}

@article{chan15_ListenAttendSpell,
  title = {Listen, {{Attend}} and {{Spell}}},
  author = {Chan, William and Jaitly, Navdeep and Le, Quoc V. and Vinyals, Oriol},
  year = {2015},
  month = aug,
  abstract = {We present Listen, Attend and Spell (LAS), a neural network that learns to transcribe speech utterances to characters. Unlike traditional DNN-HMM models, this model learns all the components of a speech recognizer jointly. Our system has two components: a listener and a speller. The listener is a pyramidal recurrent network encoder that accepts filter bank spectra as inputs. The speller is an attention-based recurrent network decoder that emits characters as outputs. The network produces character sequences without making any independence assumptions between the characters. This is the key improvement of LAS over previous end-to-end CTC models. On a subset of the Google voice search task, LAS achieves a word error rate (WER) of 14.1\% without a dictionary or a language model, and 10.3\% with language model rescoring over the top 32 beams. By comparison, the state-of-the-art CLDNN-HMM model achieves a WER of 8.0\%.},
  annotation = {ZSCC: 0000227},
  archivePrefix = {arXiv},
  eprint = {1508.01211},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chan et al_2015_listen, attend and spell.pdf;/home/trung/Zotero/storage/Q35W4Z4Z/1508.html},
  journal = {arXiv:1508.01211 [cs, stat]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,speech recognition,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chang17_DilatedRecurrentNeural,
  title = {Dilated {{Recurrent Neural Networks}}},
  author = {Chang, Shiyu and Zhang, Yang and Han, Wei and Yu, Mo and Guo, Xiaoxiao and Tan, Wei and Cui, Xiaodong and Witbrock, Michael and {Hasegawa-Johnson}, Mark and Huang, Thomas S.},
  year = {2017},
  month = oct,
  abstract = {Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task. There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DilatedRNN, which simultaneously tackles all of these challenges. The proposed architecture is characterized by multi-resolution dilated recurrent skip connections and can be combined flexibly with diverse RNN cells. Moreover, the DilatedRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies. To provide a theory-based quantification of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures. We rigorously prove the advantages of the DilatedRNN over other recurrent neural architectures. The code for our method is publicly available at https://github.com/code-terminator/DilatedRNN},
  archivePrefix = {arXiv},
  eprint = {1710.02224},
  eprinttype = {arxiv},
  journal = {arXiv:1710.02224 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{chang18_DropoutFeatureRanking,
  title = {Dropout {{Feature Ranking}} for {{Deep Learning Models}}},
  author = {Chang, Chun-Hao and Rampasek, Ladislav and Goldenberg, Anna},
  year = {2018},
  month = mar,
  abstract = {Deep neural networks (DNNs) achieve state-of-the-art results in a variety of domains. Unfortunately, DNNs are notorious for their non-interpretability, and thus limit their applicability in hypothesis-driven domains such as biology and healthcare. Moreover, in the resource-constraint setting, it is critical to design tests relying on fewer more informative features leading to high accuracy performance within reasonable budget. We aim to close this gap by proposing a new general feature ranking method for deep learning. We show that our simple yet effective method performs on par or compares favorably to eight strawman, classical and deep-learning feature ranking methods in two simulations and five very different datasets on tasks ranging from classification to regression, in both static and time series scenarios. We also illustrate the use of our method on a drug response dataset and show that it identifies genes relevant to the drug-response.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1712.08645},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chang et al_2018_dropout feature ranking for deep learning models.pdf;/home/trung/Zotero/storage/66UP636E/1712.html},
  journal = {arXiv:1712.08645 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chang18_LatentVariableModeling,
  title = {Latent {{Variable Modeling}} for {{Generative Concept Representations}} and {{Deep Generative Models}}},
  author = {Chang, Daniel T.},
  year = {2018},
  month = dec,
  abstract = {Latent representations are the essence of deep generative models and determine their usefulness and power. For latent representations to be useful as generative concept representations, their latent space must support latent space interpolation, attribute vectors and concept vectors, among other things. We investigate and discuss latent variable modeling, including latent variable models, latent representations and latent spaces, particularly hierarchical latent representations and latent space vectors and geometry. Our focus is on that used in variational autoencoders and generative adversarial networks.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1812.11856},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chang_2018_latent variable modeling for generative concept representations and deep generative models.pdf;/home/trung/Zotero/storage/LKEMCF5P/1812.html},
  journal = {arXiv:1812.11856 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chang19_AutomaticallyComposingRepresentation,
  title = {Automatically {{Composing Representation Transformations}} as a {{Means}} for {{Generalization}}},
  author = {Chang, Michael B. and Gupta, Abhishek and Levine, Sergey and Griffiths, Thomas L.},
  year = {2019},
  month = may,
  abstract = {A generally intelligent learner should generalize to more complex tasks than it has previously encountered, but the two common paradigms in machine learning -- either training a separate learner per task or training a single learner for all tasks -- both have difficulty with such generalization because they do not leverage the compositional structure of the task distribution. This paper introduces the compositional problem graph as a broadly applicable formalism to relate tasks of different complexity in terms of problems with shared subproblems. We propose the compositional generalization problem for measuring how readily old knowledge can be reused and hence built upon. As a first step for tackling compositional generalization, we introduce the compositional recursive learner, a domain-general framework for learning algorithmic procedures for composing representation transformations, producing a learner that reasons about what computation to execute by making analogies to previously seen problems. We show on a symbolic and a high-dimensional domain that our compositional approach can generalize to more complex problems than the learner has previously encountered, whereas baselines that are not explicitly compositional do not.},
  archivePrefix = {arXiv},
  eprint = {1807.04640},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chang et al_2019_automatically composing representation transformations as a means for generalization.pdf},
  journal = {arXiv:1807.04640 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chang19_InformationClosureTheory,
  title = {Information {{Closure Theory}} of {{Consciousness}}},
  author = {Chang, Acer Y. C. and Biehl, Martin and Yu, Yen and Kanai, Ryota},
  year = {2019},
  month = sep,
  abstract = {Information processing in neural systems can be described and analysed at multiple spatiotemporal scales. Generally, information at lower levels is more fine-grained and can be coarse-grained in higher levels. However, information processed only at specific levels seems to be available for conscious awareness. We do not have direct experience of information available at the level of individual neurons, which is noisy and highly stochastic. Neither do we have experience of more macro-level interactions such as interpersonal communications. Neurophysiological evidence suggests that conscious experiences co-vary with information encoded in coarse-grained neural states such as the firing pattern of a population of neurons. In this article, we introduce a new informational theory of consciousness: Information Closure Theory of Consciousness (ICT). We hypothesise that conscious processes are processes which form non-trivial informational closure (NTIC) with respect to the environment at certain coarse-grained levels. This hypothesis implies that conscious experience is confined due to informational closure from conscious processing to other coarse-grained levels. ICT proposes new quantitative definitions of both conscious content and conscious level. With the parsimonious definitions and a hypothesise, ICT provides explanations and predictions of various phenomena associated with consciousness. The implications of ICT naturally reconciles issues in many existing theories of consciousness and provides explanations for many of our intuitions about consciousness. Most importantly, ICT demonstrates that information can be the common language between consciousness and physical reality.},
  archivePrefix = {arXiv},
  eprint = {1909.13045},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chang et al_2019_information closure theory of consciousness.pdf;/home/trung/Zotero/storage/H4W4UILC/1909.html},
  journal = {arXiv:1909.13045 [q-bio]},
  keywords = {favorite,Quantitative Biology - Neurons and Cognition},
  primaryClass = {q-bio}
}

@article{chang19_ProbabilisticGenerativeDeep,
  title = {Probabilistic {{Generative Deep Learning}} for {{Molecular Design}}},
  author = {Chang, Daniel T.},
  year = {2019},
  month = feb,
  abstract = {Probabilistic generative deep learning for molecular design involves the discovery and design of new molecules and analysis of their structure, properties and activities by probabilistic generative models using the deep learning approach. It leverages the existing huge databases and publications of experimental results, and quantum-mechanical calculations, to learn and explore molecular structure, properties and activities. We discuss the major components of probabilistic generative deep learning for molecular design, which include molecular structure, molecular representations, deep generative models, molecular latent representations and latent space, molecular structure-property and structure-activity relationships, molecular similarity and molecular design. We highlight significant recent work using or applicable to this new approach.},
  annotation = {ZSCC: 0000004},
  archivePrefix = {arXiv},
  eprint = {1902.05148},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chang_2019_probabilistic generative deep learning for molecular design.pdf;/home/trung/Zotero/storage/Z8JIPJEM/1902.html},
  journal = {arXiv:1902.05148 [cs]},
  primaryClass = {cs}
}

@article{chang19_TieredLatentRepresentations,
  title = {Tiered {{Latent Representations}} and {{Latent Spaces}} for {{Molecular Graphs}}},
  author = {Chang, Daniel T.},
  year = {2019},
  month = mar,
  abstract = {Molecular graphs generally contain subgraphs (known as groups) that are identifiable and significant in composition, functionality, geometry, etc. Flat latent representations (node embeddings or graph embeddings) fail to represent, and support the use of, groups. Fully hierarchical latent representations, on the other hand, are difficult to learn and, even if learned, may be too complex to use or interpret. We propose tiered latent representations and latent spaces for molecular graphs as a simple way to explicitly represent and utilize groups, which consist of the atom (node) tier, the group tier and the molecule (graph) tier. Specifically, we propose an architecture for learning tiered latent representations and latent spaces using graph autoencoders, graph neural networks, differentiable group pooling and the membership matrix. We discuss its various components, major challenges and related work, for both a deterministic and a probabilistic model. We also briefly discuss the usage and exploration of tiered latent spaces. The tiered approach is applicable to other types of structured graphs similar in nature to molecular graphs.},
  annotation = {ZSCC: 0000003},
  archivePrefix = {arXiv},
  eprint = {1904.02653},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chang_2019_tiered latent representations and latent spaces for molecular graphs.pdf;/home/trung/Zotero/storage/XZMQFEK6/1904.html},
  journal = {arXiv:1904.02653 [cs]},
  primaryClass = {cs}
}

@article{chao20_RevisitingMetaLearningSupervised,
  title = {Revisiting {{Meta}}-{{Learning}} as {{Supervised Learning}}},
  author = {Chao, Wei-Lun and Ye, Han-Jia and Zhan, De-Chuan and Campbell, Mark and Weinberger, Kilian Q.},
  year = {2020},
  month = feb,
  abstract = {Recent years have witnessed an abundance of new publications and approaches on metalearning. This community-wide enthusiasm has sparked great insights but has also created a plethora of seemingly different frameworks, which can be hard to compare and evaluate. In this paper, we aim to provide a principled, unifying framework by revisiting and strengthening the connection between meta-learning and traditional supervised learning. By treating pairs of task-specific data sets and target models as (feature, label) samples, we can reduce many meta-learning algorithms to instances of supervised learning. This view not only unifies meta-learning into an intuitive and practical framework but also allows us to transfer insights from supervised learning directly to improve meta-learning. For example, we obtain a better understanding of generalization properties, and we can readily transfer well-understood techniques, such as model ensemble, pre-training, joint training, data augmentation, and even nearest neighbor based methods. We provide an intuitive analogy of these methods in the context of meta-learning and show that they give rise to significant improvements in model performance on few-shot learning.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2002.00573},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/9UTCKIAD/Chao et al. - 2020 - Revisiting Meta-Learning as Supervised Learning.pdf},
  journal = {arXiv:2002.00573 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chartsias18_Factorisedspatialrepresentation,
  title = {Factorised Spatial Representation Learning: Application in Semi-Supervised Myocardial Segmentation},
  shorttitle = {Factorised Spatial Representation Learning},
  author = {Chartsias, Agisilaos and Joyce, Thomas and Papanastasiou, Giorgos and Semple, Scott and Williams, Michelle and Newby, David and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
  year = {2018},
  month = nov,
  abstract = {The success and generalisation of deep learning algorithms heavily depend on learning good feature representations. In medical imaging this entails representing anatomical information, as well as properties related to the specific imaging setting. Anatomical information is required to perform further analysis, whereas imaging information is key to disentangle scanner variability and potential artefacts. The ability to factorise these would allow for training algorithms only on the relevant information according to the task. To date, such factorisation has not been attempted. In this paper, we propose a methodology of latent space factorisation relying on the cycle-consistency principle. As an example application, we consider cardiac MR segmentation, where we separate information related to the myocardium from other features related to imaging and surrounding substructures. We demonstrate the proposed method's utility in a semi-supervised setting: we use very few labelled images together with many unlabelled images to train a myocardium segmentation neural network. Specifically, we achieve comparable performance to fully supervised networks using a fraction of labelled images in experiments on ACDC and a dataset from Edinburgh Imaging Facility QMRI. Code will be made available at https://github.com/agis85/spatial\_factorisation.},
  archivePrefix = {arXiv},
  eprint = {1803.07031},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chartsias et al_2018_factorised spatial representation learning.pdf},
  journal = {arXiv:1803.07031 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{chartsias20_Disentanglealignfuse,
  title = {Disentangle, Align and Fuse for Multimodal and Zero-Shot Image Segmentation},
  author = {Chartsias, Agisilaos and Papanastasiou, Giorgos and Wang, Chengjia and Semple, Scott and Newby, David E. and Dharmakumar, Rohan and Tsaftaris, Sotirios A.},
  year = {2020},
  month = apr,
  abstract = {Magnetic resonance (MR) protocols rely on several sequences to properly assess pathology and organ status. Yet, despite advances in image analysis we tend to treat each sequence, here termed modality, in isolation. Taking advantage of the information shared between modalities (largely an organ's anatomy) is beneficial for multi-modality multi-input processing and learning. However, we must overcome inherent anatomical misregistrations and disparities in signal intensity across the modalities to claim this benefit. We present a method that offers improved segmentation accuracy of the modality of interest (over a single input model), by learning to leverage information present in other modalities, even if few (semi-supervised) or no annotations (zero-shot) are available for this specific modality. Core to our method is learning a disentangled decomposition into anatomical and imaging factors. Shared anatomical factors from the different inputs are jointly processed and fused to extract more accurate segmentation masks. Image misregistrations are corrected with a Spatial Transformer Network, that non-linearly aligns the anatomical factors. The imaging factor captures signal intensity characteristics across different modality data, and is used for image reconstruction, enabling semi-supervised learning. Temporal and slice pairing between inputs are learned dynamically. We demonstrate applications in Late Gadolinium Enhanced (LGE) and Blood Oxygenation Level Dependent (BOLD) cardiac segmentation, as well as in T2 abdominal segmentation.},
  archivePrefix = {arXiv},
  eprint = {1911.04417},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chartsias et al_2020_disentangle, align and fuse for multimodal and zero-shot image segmentation.pdf},
  journal = {arXiv:1911.04417 [cs]},
  primaryClass = {cs}
}

@article{chatzou16_Multiplesequencealignment,
  title = {Multiple Sequence Alignment Modeling: Methods and Applications},
  shorttitle = {Multiple Sequence Alignment Modeling},
  author = {Chatzou, Maria and Magis, Cedrik and Chang, Jia-Ming and Kemena, Carsten and Bussotti, Giovanni and Erb, Ionas and Notredame, Cedric},
  year = {2016},
  month = nov,
  volume = {17},
  pages = {1009--1023},
  issn = {1467-5463, 1477-4054},
  doi = {10.1093/bib/bbv099},
  abstract = {This review provides an overview on the development of Multiple sequence alignment (MSA) methods and their main applications. It is focused on progress made over the past decade. The three first sections review recent algorithmic developments for protein, RNA/DNA and genomic alignments. The fourth section deals with benchmarks and explores the relationship between empirical and simulated data, along with the impact on method developments. The last part of the review gives an overview on available MSA local reliability estimators and their dependence on various algorithmic properties of available methods.},
  file = {/home/trung/GoogleDrive/Zotero/chatzou et al_2016_multiple sequence alignment modeling.pdf},
  journal = {Briefings in Bioinformatics},
  language = {en},
  number = {6}
}

@article{chaudhari18_Stochasticgradientdescent,
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  year = {2018},
  month = jan,
  abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such ``out-of-equilibrium'' behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
  archivePrefix = {arXiv},
  eprint = {1710.11029},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/G42W9KNP/Chaudhari and Soatto - 2018 - Stochastic gradient descent performs variational i.pdf},
  journal = {arXiv:1710.11029 [cond-mat, stat]},
  language = {en},
  primaryClass = {cond-mat, stat}
}

@article{chaudhari19_AttentiveSurveyAttention,
  title = {An {{Attentive Survey}} of {{Attention Models}}},
  author = {Chaudhari, Sneha and Polatkan, Gungor and Ramanath, Rohan and Mithal, Varun},
  year = {2019},
  month = apr,
  abstract = {Attention Model has now become an important concept in neural networks that has been researched within diverse application domains. This survey provides a structured and comprehensive overview of the developments in modeling attention. In particular, we propose a taxonomy which groups existing techniques into coherent categories. We review the different neural architectures in which attention has been incorporated, and also show how attention improves interpretability of neural models. Finally, we discuss some applications in which modeling attention has a significant impact. We hope this survey will provide a succinct introduction to attention models and guide practitioners while developing approaches for their applications.},
  annotation = {ZSCC: 0000010},
  archivePrefix = {arXiv},
  eprint = {1904.02874},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chaudhari et al_2019_an attentive survey of attention models.pdf;/home/trung/Zotero/storage/NLE5LQPT/1904.html},
  journal = {arXiv:1904.02874 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,global attention,hard attention,hierarchical,local attention,soft attention,Statistics - Machine Learning,survey},
  primaryClass = {cs, stat}
}

@misc{chaudhary20_IllustratedSimCLRFramework,
  title = {The {{Illustrated SimCLR Framework}}},
  author = {Chaudhary, Amit},
  year = {2020},
  month = mar,
  abstract = {A visual guide to the SimCLR framework for contrastive learning of visual~representations.},
  annotation = {ZSCC: NoCitationData[s0]},
  chapter = {illustration},
  file = {/home/trung/Zotero/storage/PQY8ZPZ7/illustrated-simclr.html},
  howpublished = {https://amitness.com/2020/03/illustrated-simclr/},
  journal = {Amit Chaudhary},
  language = {en}
}

@misc{chaudhary20_SemiSupervisedLearningComputer,
  title = {Semi-{{Supervised Learning}} in {{Computer Vision}}},
  author = {Chaudhary, Amit},
  year = {2020},
  month = jul,
  abstract = {A comprehensive overview of recent semi-supervised learning methods in Computer Vision},
  howpublished = {http://amitness.com/2020/07/semi-supervised-learning/},
  journal = {Amit Chaudhary},
  language = {en}
}

@article{chazan19_DeepClusteringBased,
  title = {Deep {{Clustering Based}} on a {{Mixture}} of {{Autoencoders}}},
  author = {Chazan, Shlomo E. and Gannot, Sharon and Goldberger, Jacob},
  year = {2019},
  month = mar,
  abstract = {In this paper we propose a Deep Autoencoder MIxture Clustering (DAMIC) algorithm based on a mixture of deep autoencoders where each cluster is represented by an autoencoder. A clustering network transforms the data into another space and then selects one of the clusters. Next, the autoencoder associated with this cluster is used to reconstruct the data-point. The clustering algorithm jointly learns the nonlinear data representation and the set of autoencoders. The optimal clustering is found by minimizing the reconstruction loss of the mixture of autoencoder network. Unlike other deep clustering algorithms, no regularization term is needed to avoid data collapsing to a single point. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {1812.06535},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chazan et al_2019_deep clustering based on a mixture of autoencoders.pdf},
  journal = {arXiv:1812.06535 [cs, stat]},
  keywords = {self-supervised},
  primaryClass = {cs, stat}
}

@article{che20_YourGANSecretly,
  title = {Your {{GAN}} Is {{Secretly}} an {{Energy}}-Based {{Model}} and {{You Should}} Use {{Discriminator Driven Latent Sampling}}},
  author = {Che, Tong and Zhang, Ruixiang and {Sohl-Dickstein}, Jascha and Larochelle, Hugo and Paull, Liam and Cao, Yuan and Bengio, Yoshua},
  year = {2020},
  month = mar,
  abstract = {The sum of the implicit generator log-density log pg of a GAN with the logit score of the discriminator defines an energy function which yields the true data density when the generator is imperfect but the discriminator is optimal. This makes it possible to improve on the typical generator (with implicit density pg). We show that samples can be generated from this modified density by sampling in latent space according to an energy-based model induced by the sum of the latent prior log-density and the discriminator output score. We call this process of running Markov Chain Monte Carlo in the latent space, and then applying the generator function, Discriminator Driven Latent Sampling (DDLS). We show that DDLS is highly efficient compared to previous methods which work in the high-dimensional pixel space, and can be applied to improve on previously trained GANs of many types. We evaluate DDLS on both synthetic and real-world datasets qualitatively and quantitatively. On CIFAR-10, DDLS substantially improves the Inception Score of an off-the-shelf pre-trained SN-GAN (Miyato et al., 2018) from 8.22 to 9.09 which is comparable to the class-conditional BigGAN (Brock et al., 2019) model. This achieves a new state-of-theart in the unconditional image synthesis setting without introducing extra parameters or additional training.},
  archivePrefix = {arXiv},
  eprint = {2003.06060},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/che et al_2020_your gan is secretly an energy-based model and you should use discriminator driven latent sampling.pdf},
  journal = {arXiv:2003.06060 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chen16_InfoGANInterpretableRepresentation,
  title = {{{InfoGAN}}: {{Interpretable Representation Learning}} by {{Information Maximizing Generative Adversarial Nets}}},
  author = {Chen, Xi and Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2016},
  pages = {9},
  abstract = {This paper describes InfoGAN, an information-theoretic extension to the Generative Adversarial Network that is able to learn disentangled representations in a completely unsupervised manner. InfoGAN is a generative adversarial network that also maximizes the mutual information between a small subset of the latent variables and the observation. We derive a lower bound of the mutual information objective that can be optimized efficiently. Specifically, InfoGAN successfully disentangles writing styles from digit shapes on the MNIST dataset, pose from lighting of 3D rendered images, and background digits from the central digit on the SVHN dataset. It also discovers visual concepts that include hair styles, presence/absence of eyeglasses, and emotions on the CelebA face dataset. Experiments show that InfoGAN learns interpretable representations that are competitive with representations learned by existing supervised methods. For an up-to-date version of this paper, please see https://arxiv.org/abs/1606.03657.},
  annotation = {ZSCC: 0001581},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2016_infogan.pdf},
  keywords = {_tablet,disentanglement,favorite,information},
  language = {en}
}

@article{chen17_VariationalLossyAutoencoder,
  title = {Variational {{Lossy Autoencoder}}},
  author = {Chen, Xi and Kingma, Diederik P. and Salimans, Tim and Duan, Yan and Dhariwal, Prafulla and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  year = {2017},
  month = mar,
  abstract = {Representation learning seeks to expose certain aspects of observed data in a learned representation that's amenable to downstream tasks like classification. For instance, a good representation for 2D images might be one that describes only global structure and discards information about detailed texture. In this paper, we present a simple but principled method to learn such global representations by combining Variational Autoencoder (VAE) with neural autoregressive models such as RNN, MADE and PixelRNN/CNN. Our proposed VAE model allows us to have control over what the global latent code can learn and , by designing the architecture accordingly, we can force the global latent code to discard irrelevant information such as texture in 2D images, and hence the VAE only "autoencodes" data in a lossy fashion. In addition, by leveraging autoregressive models as both prior distribution \$p(z)\$ and decoding distribution \$p(x|z)\$, we can greatly improve generative modeling performance of VAEs, achieving new state-of-the-art results on MNIST, OMNIGLOT and Caltech-101 Silhouettes density estimation tasks.},
  archivePrefix = {arXiv},
  eprint = {1611.02731},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2017_variational lossy autoencoder.pdf;/home/trung/Zotero/storage/QKUPUGHZ/1611.html},
  journal = {arXiv:1611.02731 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chen18_NeuralOrdinaryDifferential,
  title = {Neural {{Ordinary Differential Equations}}},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David},
  year = {2018},
  month = jun,
  abstract = {We introduce a new family of deep neural network models. Instead of specifying a discrete sequence of hidden layers, we parameterize the derivative of the hidden state using a neural network. The output of the network is computed using a black-box differential equation solver. These continuous-depth models have constant memory cost, adapt their evaluation strategy to each input, and can explicitly trade numerical precision for speed. We demonstrate these properties in continuous-depth residual networks and continuous-time latent variable models. We also construct continuous normalizing flows, a generative model that can train by maximum likelihood, without partitioning or ordering the data dimensions. For training, we show how to scalably backpropagate through any ODE solver, without access to its internal operations. This allows end-to-end training of ODEs within larger models.},
  archivePrefix = {arXiv},
  eprint = {1806.07366},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2018_neural ordinary differential equations.pdf;/home/trung/Zotero/storage/MFSXCC4D/1806.html},
  journal = {arXiv:1806.07366 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,ode,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{chen18_rapidrobustmethod,
  title = {A Rapid and Robust Method for Single Cell Chromatin Accessibility Profiling},
  author = {Chen, Xi and Miragaia, Ricardo J. and Natarajan, Kedar Nath and Teichmann, Sarah A.},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {5345},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-07771-0},
  annotation = {ZSCC: 0000033},
  file = {/home/trung/Zotero/storage/VZ4EKMFF/Chen et al. - 2018 - A rapid and robust method for single cell chromati.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{chen18_risedeeplearning,
  title = {The Rise of Deep Learning in Drug Discovery},
  author = {Chen, Hongming},
  year = {2018},
  volume = {23},
  pages = {10},
  file = {/home/trung/Zotero/storage/5CRWDXDT/Chen - 2018 - The rise of deep learning in drug discovery.pdf},
  journal = {Drug Discovery Today},
  language = {en},
  number = {6}
}

@article{chen18_SelfSupervisedGANsAuxiliary,
  title = {Self-{{Supervised GANs}} via {{Auxiliary Rotation Loss}}},
  author = {Chen, Ting and Zhai, Xiaohua and Ritter, Marvin and Lucic, Mario and Houlsby, Neil},
  year = {2018},
  month = nov,
  abstract = {Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.},
  archivePrefix = {arXiv},
  eprint = {1811.11212},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2018_self-supervised gans via auxiliary rotation loss.pdf},
  journal = {arXiv:1811.11212 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{chen18_VariationalSequentialLabelers,
  title = {Variational {{Sequential Labelers}} for {{Semi}}-{{Supervised Learning}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Chen, Mingda and Tang, Qingming and Livescu, Karen and Gimpel, Kevin},
  year = {2018},
  month = oct,
  pages = {215--226},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1020},
  abstract = {We introduce a family of multitask variational methods for semi-supervised sequence labeling. Our model family consists of a latent-variable generative model and a discriminative labeler. The generative models use latent variables to define the conditional probability of a word given its context, drawing inspiration from word prediction objectives commonly used in learning word embeddings. The labeler helps inject discriminative information into the latent space. We explore several latent variable configurations, including ones with hierarchical structure, which enables the model to account for both label-specific and word-specific information. Our models consistently outperform standard sequential baselines on 8 sequence labeling datasets, and improve further with unlabeled data.},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2018_variational sequential labelers for semi-supervised learning.pdf},
  keywords = {semi-supervised,sequential,variational}
}

@article{chen19_AIPNetGenerativeAdversarial,
  title = {{{AIPNet}}: {{Generative Adversarial Pre}}-Training of {{Accent}}-Invariant {{Networks}} for {{End}}-to-End {{Speech Recognition}}},
  shorttitle = {{{AIPNet}}},
  author = {Chen, Yi-Chen and Yang, Zhaojun and Yeh, Ching-Feng and Jain, Mahaveer and Seltzer, Michael L.},
  year = {2019},
  month = nov,
  abstract = {As one of the major sources in speech variability, accents have posed a grand challenge to the robustness of speech recognition systems. In this paper, our goal is to build a unified end-to-end speech recognition system that generalizes well across accents. For this purpose, we propose a novel pre-training framework AIPNet based on generative adversarial nets (GAN) for accent-invariant representation learning: Accent Invariant Pre-training Networks. We pre-train AIPNet to disentangle accent-invariant and accent-specific characteristics from acoustic features through adversarial training on accented data for which transcriptions are not necessarily available. We further fine-tune AIPNet by connecting the accent-invariant module with an attention-based encoder-decoder model for multi-accent speech recognition. In the experiments, our approach is compared against four baselines including both accent-dependent and accent-independent models. Experimental results on 9 English accents show that the proposed approach outperforms all the baselines by 2.3 \textbackslash sim 4.5\% relative reduction on average WER when transcriptions are available in all accents and by 1.6 \textbackslash sim 6.1\% relative reduction when transcriptions are only available in US accent.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1911.11935},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2019_aipnet.pdf;/home/trung/Zotero/storage/2PB5FH2G/1911.html},
  journal = {arXiv:1911.11935 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{chen19_Assessmentcomputationalmethods,
  title = {Assessment of Computational Methods for the Analysis of Single-Cell {{ATAC}}-Seq Data},
  author = {Chen, Huidong and Lareau, Caleb and Andreani, Tommaso and Vinyard, Michael E. and Garcia, Sara P. and Clement, Kendell and {Andrade-Navarro}, Miguel A. and Buenrostro, Jason D. and Pinello, Luca},
  year = {2019},
  month = dec,
  volume = {20},
  pages = {241},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1854-5},
  abstract = {Background: Recent innovations in single-cell Assay for Transposase Accessible Chromatin using sequencing (scATAC-seq) enable profiling of the epigenetic landscape of thousands of individual cells. scATAC-seq data analysis presents unique methodological challenges. scATAC-seq experiments sample DNA, which, due to low copy numbers (diploid in humans), lead to inherent data sparsity (1\textendash 10\% of peaks detected per cell) compared to transcriptomic (scRNA-seq) data (10\textendash 45\% of expressed genes detected per cell). Such challenges in data generation emphasize the need for informative features to assess cell heterogeneity at the chromatin level. Results: We present a benchmarking framework that is applied to 10 computational methods for scATAC-seq on 13 synthetic and real datasets from different assays, profiling cell types from diverse tissues and organisms. Methods for processing and featurizing scATAC-seq data were compared by their ability to discriminate cell types when combined with common unsupervised clustering approaches. We rank evaluated methods and discuss computational challenges associated with scATAC-seq analysis including inherently sparse data, determination of features, peak calling, the effects of sequencing coverage and noise, and clustering performance. Running times and memory requirements are also discussed. Conclusions: This reference summary of scATAC-seq methods offers recommendations for best practices with consideration for both the non-expert user and the methods developer. Despite variation across methods and datasets, SnapATAC, Cusanovich2018, and cisTopic outperform other methods in separating cell populations of different coverages and noise levels in both synthetic and real datasets. Notably, SnapATAC is the only method able to analyze a large dataset ({$>$} 80,000 cells).},
  annotation = {ZSCC: 0000010},
  file = {/home/trung/Zotero/storage/DNW5H9M3/Chen et al. - 2019 - Assessment of computational methods for the analys.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{chen19_Assessmentcomputationalmethodsa,
  title = {Assessment of Computational Methods for the Analysis of Single-Cell {{ATAC}}-Seq Data},
  author = {Chen, Huidong and Lareau, Caleb and Andreani, Tommaso and Vinyard, Michael E. and Garcia, Sara P. and Clement, Kendell and {Andrade-Navarro}, Miguel A. and Buenrostro, Jason D. and Pinello, Luca},
  year = {2019},
  month = dec,
  volume = {20},
  pages = {241},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1854-5},
  abstract = {Background: Recent innovations in single-cell Assay for Transposase Accessible Chromatin using sequencing (scATAC-seq) enable profiling of the epigenetic landscape of thousands of individual cells. scATAC-seq data analysis presents unique methodological challenges. scATAC-seq experiments sample DNA, which, due to low copy numbers (diploid in humans), lead to inherent data sparsity (1\textendash 10\% of peaks detected per cell) compared to transcriptomic (scRNA-seq) data (10\textendash 45\% of expressed genes detected per cell). Such challenges in data generation emphasize the need for informative features to assess cell heterogeneity at the chromatin level. Results: We present a benchmarking framework that is applied to 10 computational methods for scATAC-seq on 13 synthetic and real datasets from different assays, profiling cell types from diverse tissues and organisms. Methods for processing and featurizing scATAC-seq data were compared by their ability to discriminate cell types when combined with common unsupervised clustering approaches. We rank evaluated methods and discuss computational challenges associated with scATAC-seq analysis including inherently sparse data, determination of features, peak calling, the effects of sequencing coverage and noise, and clustering performance. Running times and memory requirements are also discussed. Conclusions: This reference summary of scATAC-seq methods offers recommendations for best practices with consideration for both the non-expert user and the methods developer. Despite variation across methods and datasets, SnapATAC, Cusanovich2018, and cisTopic outperform other methods in separating cell populations of different coverages and noise levels in both synthetic and real datasets. Notably, SnapATAC is the only method able to analyze a large dataset ({$>$} 80,000 cells).},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2019_assessment of computational methods for the analysis of single-cell atac-seq data.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{chen19_Imagegenerationlatent,
  title = {Image Generation via Latent Space Learning Using Improved Combination},
  author = {Chen, Yanxiang and Wu, Guang and Zhou, Jie and Qi, Guojun},
  year = {2019},
  month = may,
  volume = {340},
  pages = {8--18},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.02.031},
  abstract = {Many researches have brought progress in learning a good generative model by combing the advantages of GAN and VAE, where latent space learning is always important for generating high-quality images. But these existing works mainly seek to impose the latent space a given distribution in advance or to obey a Gaussian distribution with KL divergence penalty, which leads to the difficulties of deciding a suitable prior distribution corresponding to different datasets. Thus in this paper we develop a two-stage combining method of AE and GAN under unsupervised and supervised conditions respectively, each stage performed to improve the effect of modeling latent distribution. In the first stage, an adversarial procedure achieves to match the latent distribution with real data distribution determined by arbitrary dataset without having access to a pre-set prior distribution. In the second stage, besides one adversarial procedure trained for outputting images, the other adversarial procedure is designed to attain the goal of optimizing the latent distribution of the first stage via back-propagation. Therefore, loop optimization of the network parameters during training will at last allow the framework to map an input noise to a high quality image. Extensive experiments are conducted to verify the performance of representing latent space and generating images on different datasets including MNIST, fashion-MNIST, CIFAR-10, and CelebA. The code and tutorials already release at https://github.com/TwistedW/CAE-CGAN.},
  file = {/home/trung/Zotero/storage/Z4ELZ2GM/Chen et al. - 2019 - Image generation via latent space learning using i.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{chen19_IsolatingSourcesDisentanglement,
  title = {Isolating {{Sources}} of {{Disentanglement}} in {{Variational Autoencoders}}},
  author = {Chen, Ricky T. Q. and Li, Xuechen and Grosse, Roger and Duvenaud, David},
  year = {2019},
  month = apr,
  abstract = {We decompose the evidence lower bound to show the existence of a term measuring the total correlation between latent variables. We use this to motivate our \$\textbackslash beta\$-TCVAE (Total Correlation Variational Autoencoder), a refinement of the state-of-the-art \$\textbackslash beta\$-VAE objective for learning disentangled representations, requiring no additional hyperparameters during training. We further propose a principled classifier-free measure of disentanglement called the mutual information gap (MIG). We perform extensive quantitative and qualitative experiments, in both restricted and non-restricted settings, and show a strong relation between total correlation and disentanglement, when the latent variables model is trained using our framework.},
  archivePrefix = {arXiv},
  eprint = {1802.04942},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2019_isolating sources of disentanglement in variational autoencoders.pdf},
  journal = {arXiv:1802.04942 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{chen19_RethinkingUsageBatch,
  title = {Rethinking the {{Usage}} of {{Batch Normalization}} and {{Dropout}} in the {{Training}} of {{Deep Neural Networks}}},
  author = {Chen, Guangyong and Chen, Pengfei and Shi, Yujun and Hsieh, Chang-Yu and Liao, Benben and Zhang, Shengyu},
  year = {2019},
  month = may,
  abstract = {In this work, we propose a novel technique to boost training efficiency of a neural network. Our work is based on an excellent idea that whitening the inputs of neural networks can achieve a fast convergence speed. Given the well-known fact that independent components must be whitened, we introduce a novel Independent-Component (IC) layer before each weight layer, whose inputs would be made more independent. However, determining independent components is a computationally intensive task. To overcome this challenge, we propose to implement an IC layer by combining two popular techniques, Batch Normalization and Dropout, in a new manner that we can rigorously prove that Dropout can quadratically reduce the mutual information and linearly reduce the correlation between any pair of neurons with respect to the dropout layer parameter \$p\$. As demonstrated experimentally, the IC layer consistently outperforms the baseline approaches with more stable training process, faster convergence speed and better convergence limit on CIFAR10/100 and ILSVRC2012 datasets. The implementation of our IC layer makes us rethink the common practices in the design of neural networks. For example, we should not place Batch Normalization before ReLU since the non-negative responses of ReLU will make the weight layer updated in a suboptimal way, and we can achieve better performance by combining Batch Normalization and Dropout together as an IC layer.},
  archivePrefix = {arXiv},
  eprint = {1905.05928},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2019_rethinking the usage of batch normalization and dropout in the training of deep neural networks.pdf},
  journal = {arXiv:1905.05928 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chen19_RobustOrdinalVAE,
  title = {Robust {{Ordinal VAE}}: {{Employing Noisy Pairwise Comparisons}} for {{Disentanglement}}},
  shorttitle = {Robust {{Ordinal VAE}}},
  author = {Chen, Junxiang and Batmanghelich, Kayhan},
  year = {2019},
  month = oct,
  abstract = {Recent work by Locatello et al. (2018) has shown that an inductive bias is required to disentangle factors of interest in Variational Autoencoder (VAE). Motivated by a real-world problem, we propose a setting where such bias is introduced by providing pairwise ordinal comparisons between instances, based on the desired factor to be disentangled. For example, a doctor compares pairs of patients based on the level of severity of their illnesses, and the desired factor is a quantitive level of the disease severity. In a real-world application, the pairwise comparisons are usually noisy. Our method, Robust Ordinal VAE (ROVAE), incorporates the noisy pairwise ordinal comparisons in the disentanglement task. We introduce non-negative random variables in ROVAE, such that it can automatically determine whether each pairwise ordinal comparison is trustworthy and ignore the noisy comparisons. Experimental results demonstrate that ROVAE outperforms existing methods and is more robust to noisy pairwise comparisons in both benchmark datasets and a real-world application.},
  archivePrefix = {arXiv},
  eprint = {1910.05898},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2019_robust ordinal vae.pdf},
  journal = {arXiv:1910.05898 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{chen19_SelfSupervisedGANsAuxiliary,
  title = {Self-{{Supervised GANs}} via {{Auxiliary Rotation Loss}}},
  author = {Chen, Ting and Zhai, Xiaohua and Ritter, Marvin and Lucic, Mario and Houlsby, Neil},
  year = {2019},
  month = apr,
  abstract = {Conditional GANs are at the forefront of natural image synthesis. The main drawback of such models is the necessity for labeled data. In this work we exploit two popular unsupervised learning techniques, adversarial training and self-supervision, and take a step towards bridging the gap between conditional and unconditional GANs. In particular, we allow the networks to collaborate on the task of representation learning, while being adversarial with respect to the classic GAN game. The role of self-supervision is to encourage the discriminator to learn meaningful feature representations which are not forgotten during training. We test empirically both the quality of the learned image representations, and the quality of the synthesized images. Under the same conditions, the self-supervised GAN attains a similar performance to state-of-the-art conditional counterparts. Finally, we show that this approach to fully unsupervised learning can be scaled to attain an FID of 23.4 on unconditional ImageNet generation.},
  annotation = {ZSCC: 0000031},
  archivePrefix = {arXiv},
  eprint = {1811.11212},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2019_self-supervised gans via auxiliary rotation loss.pdf;/home/trung/Zotero/storage/GUTVS2FS/1811.html},
  journal = {arXiv:1811.11212 [cs, stat]},
  keywords = {self-supervised},
  primaryClass = {cs, stat}
}

@article{chen20_ArePowerfulGraph,
  title = {Are {{Powerful Graph Neural Nets Necessary}}? {{A Dissection}} on {{Graph Classification}}},
  shorttitle = {Are {{Powerful Graph Neural Nets Necessary}}?},
  author = {Chen, Ting and Bian, Song and Sun, Yizhou},
  year = {2020},
  month = jun,
  abstract = {Graph Neural Nets (GNNs) have received increasing attentions, partially due to their superior performance in many node and graph classification tasks. However, there is a lack of understanding on what they are learning and how sophisticated the learned graph functions are. In this work, we propose a dissection of GNNs on graph classification into two parts: 1) the graph filtering, where graph-based neighbor aggregations are performed, and 2) the set function, where a set of hidden node features are composed for prediction. To study the importance of both parts, we propose to linearize them separately. We first linearize the graph filtering function, resulting Graph Feature Network (GFN), which is a simple lightweight neural net defined on a \textbackslash textit\{set\} of graph augmented features. Further linearization of GFN's set function results in Graph Linear Network (GLN), which is a linear function. Empirically we perform evaluations on common graph classification benchmarks. To our surprise, we find that, despite the simplification, GFN could match or exceed the best accuracies produced by recently proposed GNNs (with a fraction of computation cost), while GLN underperforms significantly. Our results demonstrate the importance of non-linear set function, and suggest that linear graph filtering with non-linear set function is an efficient and powerful scheme for modeling existing graph classification benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1905.04579},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_are powerful graph neural nets necessary.pdf},
  journal = {arXiv:1905.04579 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chen20_DecoupledVariationalEmbedding,
  title = {Decoupled {{Variational Embedding}} for {{Signed Directed Networks}}},
  author = {Chen, Xu and Yao, Jiangchao and Li, Maosen and {zhang}, Ya and Wang, Yanfeng},
  year = {2020},
  month = aug,
  abstract = {Node representation learning for signed directed networks has received considerable attention in many real-world applications such as link sign prediction, node classification and node recommendation. The challenge lies in how to adequately encode the complex topological information of the networks. Recent studies mainly focus on preserving the first-order network topology which indicates the closeness relationships of nodes. However, these methods generally fail to capture the high-order topology which indicates the local structures of nodes and serves as an essential characteristic of the network topology. In addition, for the first-order topology, the additional value of non-existent links is largely ignored. In this paper, we propose to learn more representative node embeddings by simultaneously capturing the first-order and high-order topology in signed directed networks. In particular, we reformulate the representation learning problem on signed directed networks from a variational auto-encoding perspective and further develop a decoupled variational embedding (DVE) method. DVE leverages a specially designed auto-encoder structure to capture both the first-order and high-order topology of signed directed networks, and thus learns more representative node embedding. Extensive experiments are conducted on three widely used real-world datasets. Comprehensive results on both link sign prediction and node recommendation task demonstrate the effectiveness of DVE. Qualitative results and analysis are also given to provide a better understanding of DVE.},
  archivePrefix = {arXiv},
  eprint = {2008.12450},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_decoupled variational embedding for signed directed networks.pdf},
  journal = {arXiv:2008.12450 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chen20_DeepsoftKmeans,
  title = {Deep Soft {{K}}-Means Clustering with Self-Training for Single-Cell {{RNA}} Sequence Data},
  author = {Chen, Liang and Wang, Weinan and Zhai, Yuyao and Deng, Minghua},
  year = {2020},
  month = jun,
  volume = {2},
  pages = {lqaa039},
  issn = {2631-9268},
  doi = {10.1093/nargab/lqaa039},
  abstract = {Single-cell RNA sequencing (scRNA-seq) allows researchers to study cell heterogeneity at the cellular level. A crucial step in analyzing scRNA-seq data is to cluster cells into subpopulations to facilitate subsequent downstream analysis. However, frequent dropout events and increasing size of scRNA-seq data make clustering such high-dimensional, sparse and massive transcriptional expression profiles challenging. Although some existing deep learningbased clustering algorithms for single cells combine dimensionality reduction with clustering, they either ignore the distance and affinity constraints between similar cells or make some additional latent space assumptions like mixture Gaussian distribution, failing to learn cluster-friendly low-dimensional space. Therefore, in this paper, we combine the deep learning technique with the use of a denoising autoencoder to characterize scRNA-seq data while propose a soft self-training K-means algorithm to cluster the cell population in the learned latent space. The self-training procedure can effectively aggregate the similar cells and pursue more cluster-friendly latent space. Our method, called `scziDesk', alternately performs data compression, data reconstruction and soft clustering iteratively, and the results exhibit excellent compatibility and robustness in both simulated and real data. Moreover, our proposed method has perfect scalability in line with cell size on largescale datasets.},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_deep soft k-means clustering with self-training for single-cell rna sequence data.pdf},
  journal = {NAR Genomics and Bioinformatics},
  language = {en},
  number = {2}
}

@article{chen20_Dirichletprocessbitermbased,
  title = {A {{Dirichlet}} Process Biterm-Based Mixture Model for Short Text Stream Clustering},
  author = {Chen, Junyang and Gong, Zhiguo and Liu, Weiwen},
  year = {2020},
  month = may,
  volume = {50},
  pages = {1609--1619},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-019-01606-1},
  abstract = {Short text stream clustering has become an important problem for mining textual data in diverse social media platforms (e.g., Twitter). However, most of the existing clustering methods (e.g., LDA and PLSA) are developed based on the assumption of a static corpus of long texts, while little attention has been given to short text streams. Different from the long texts, the clustering of short texts is more challenging since their word co-occurrence pattern easily suffers from a sparsity problem. In this paper, we propose a Dirichlet process biterm-based mixture model (DP-BMM), which can deal with the topic drift problem and the sparsity problem in short text stream clustering. The major advantages of DP-BMM include (1) DP-BMM explicitly exploits the word-pairs constructed from each document to enhance the word co-occurrence pattern in short texts; (2) DP-BMM can deal with the topic drift problem of short text streams naturally. Moreover, we further propose an improved algorithm of DP-BMM with forgetting property called DP-BMM-FP, which can efficiently delete biterms of outdated documents by deleting clusters of outdated batches. To perform inference, we adopt an online Gibbs sampling method for parameter estimation. Our extensive experimental results on real-world datasets show that DP-BMM and DP-BMM-FP can achieve a better performance than the state-of-the-art methods in terms of NMI metrics.},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_a dirichlet process biterm-based mixture model for short text stream clustering.pdf},
  journal = {Applied Intelligence},
  language = {en},
  number = {5}
}

@misc{chen20_DNCDifferentialNeural,
  title = {{{DNC}}: {{Differential Neural Network}}},
  shorttitle = {{{DNC}}},
  author = {Chen, Sherwin},
  year = {2020},
  month = jun,
  abstract = {A detailed walk-through of DNC},
  howpublished = {https://medium.com/towards-artificial-intelligence/dnc-differential-neural-network-3cfd82d0d99e},
  journal = {Medium},
  language = {en}
}

@article{chen20_DynamicalCentralLimit,
  title = {A {{Dynamical Central Limit Theorem}} for {{Shallow Neural Networks}}},
  author = {Chen, Zhengdao and Rotskoff, Grant M. and Bruna, Joan and {Vanden-Eijnden}, Eric},
  year = {2020},
  month = aug,
  abstract = {Recent theoretical work has characterized the dynamics of wide shallow neural networks trained via gradient descent in an asymptotic regime called the mean-field limit as the number of parameters tends towards infinity. At initialization, the randomly sampled parameters lead to a deviation from the mean-field limit that is dictated by the classical Central Limit Theorem (CLT). However, the dynamics of training introduces correlations among the parameters, raising the question of how the fluctuations evolve during training. Here, we analyze the mean-field dynamics as a Wasserstein gradient flow and prove that the deviations from the mean-field limit scaled by the width, in the width-asymptotic limit, remain bounded throughout training. In particular, they eventually vanish in the CLT scaling if the mean-field dynamics converges to a measure that interpolates the training data. This observation has implications for both the approximation rate and the generalization: the upper bound we obtain is given by a MonteCarlo type resampling error, which does not depend explicitly on the dimension. This bound motivates a regularizaton term on the 2-norm of the underlying measure, which is also connected to generalization via the variation-norm function spaces.},
  archivePrefix = {arXiv},
  eprint = {2008.09623},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_a dynamical central limit theorem for shallow neural networks.pdf},
  journal = {arXiv:2008.09623 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{chen20_GraphNeuralNetworks,
  title = {On {{Graph Neural Networks}} versus {{Graph}}-{{Augmented MLPs}}},
  author = {Chen, Lei and Chen, Zhengdao and Bruna, Joan},
  year = {2020},
  month = oct,
  abstract = {From the perspective of expressive power, this work compares multi-layer Graph Neural Networks (GNNs) with a simplified alternative that we call Graph-Augmented Multi-Layer Perceptrons (GA-MLPs), which first augments node features with certain multi-hop operators on the graph and then applies an MLP in a node-wise fashion. From the perspective of graph isomorphism testing, we show both theoretically and numerically that GA-MLPs with suitable operators can distinguish almost all non-isomorphic graphs, just like the Weifeiler-Lehman (WL) test. However, by viewing them as node-level functions and examining the equivalence classes they induce on rooted graphs, we prove a separation in expressive power between GA-MLPs and GNNs that grows exponentially in depth. In particular, unlike GNNs, GA-MLPs are unable to count the number of attributed walks. We also demonstrate via community detection experiments that GA-MLPs can be limited by their choice of operator family, as compared to GNNs with higher flexibility in learning.},
  archivePrefix = {arXiv},
  eprint = {2010.15116},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_on graph neural networks versus graph-augmented mlps.pdf},
  journal = {arXiv:2010.15116 [cs, math, stat]},
  keywords = {_tablet,favorite},
  primaryClass = {cs, math, stat}
}

@article{chen20_IntegratingDeepSupervised,
  title = {Integrating {{Deep Supervised}}, {{Self}}-{{Supervised}} and {{Unsupervised Learning}} for {{Single}}-{{Cell RNA}}-Seq {{Clustering}} and {{Annotation}}},
  author = {Chen, Liang and Zhai, Yuyao and He, Qiuyan and Wang, Weinan and Deng, Minghua},
  year = {2020},
  month = jul,
  volume = {11},
  pages = {792},
  issn = {2073-4425},
  doi = {10.3390/genes11070792},
  abstract = {As single-cell RNA sequencing technologies mature, massive gene expression profiles can be obtained. Consequently, cell clustering and annotation become two crucial and fundamental procedures affecting other specific downstream analyses. Most existing single-cell RNA-seq (scRNA-seq) data clustering algorithms do not take into account the available cell annotation results on the same tissues or organisms from other laboratories. Nonetheless, such data could assist and guide the clustering process on the target dataset. Identifying marker genes through differential expression analysis to manually annotate large amounts of cells also costs labor and resources. Therefore, in this paper, we propose a novel end-to-end cell supervised clustering and annotation framework called scAnCluster, which fully utilizes the cell type labels available from reference data to facilitate the cell clustering and annotation on the unlabeled target data. Our algorithm integrates deep supervised learning, self-supervised learning and unsupervised learning techniques together, and it outperforms other customized scRNA-seq supervised clustering methods in both simulation and real data. It is particularly worth noting that our method performs well on the challenging task of discovering novel cell types that are absent in the reference data.},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_integrating deep supervised, self-supervised and unsupervised learning for single-cell rna-seq clustering and annotation.pdf},
  journal = {Genes},
  language = {en},
  number = {7}
}

@article{chen20_LearningFlatLatent,
  title = {Learning {{Flat Latent Manifolds}} with {{VAEs}}},
  author = {Chen, Nutan and Klushyn, Alexej and Ferroni, Francesco and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2020},
  month = aug,
  abstract = {Measuring the similarity between data points often requires domain knowledge, which can in parts be compensated by relying on unsupervised methods such as latent-variable models, where similarity/distance is estimated in a more compact latent space. Prevalent is the use of the Euclidean metric, which has the drawback of ignoring information about similarity of data stored in the decoder, as captured by the framework of Riemannian geometry. We propose an extension to the framework of variational auto-encoders allows learning flat latent manifolds, where the Euclidean metric is a proxy for the similarity between data points. This is achieved by defining the latent space as a Riemannian manifold and by regularising the metric tensor to be a scaled identity matrix. Additionally, we replace the compact prior typically used in variational auto-encoders with a recently presented, more expressive hierarchical one---and formulate the learning problem as a constrained optimisation problem. We evaluate our method on a range of data-sets, including a video-tracking benchmark, where the performance of our unsupervised approach nears that of state-of-the-art supervised approaches, while retaining the computational efficiency of straight-line-based approaches.},
  archivePrefix = {arXiv},
  eprint = {2002.04881},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_learning flat latent manifolds with vaes.pdf},
  journal = {arXiv:2002.04881 [cs, stat]},
  keywords = {_tablet,disentanglement},
  primaryClass = {cs, stat}
}

@article{chen20_ProbabilisticMachineLearning,
  title = {Probabilistic {{Machine Learning}} for {{Healthcare}}},
  author = {Chen, Irene Y. and Joshi, Shalmali and Ghassemi, Marzyeh and Ranganath, Rajesh},
  year = {2020},
  month = sep,
  abstract = {Machine learning can be used to make sense of healthcare data. Probabilistic machine learning models help provide a complete picture of observed data in healthcare. In this review, we examine how probabilistic machine learning can advance healthcare. We consider challenges in the predictive model building pipeline where probabilistic models can be beneficial including calibration and missing data. Beyond predictive models, we also investigate the utility of probabilistic machine learning models in phenotyping, in generative models for clinical use cases, and in reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {2009.11087},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_probabilistic machine learning for healthcare.pdf},
  journal = {arXiv:2009.11087 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chen20_SimpleFrameworkContrastive,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = feb,
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive selfsupervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-ofthe-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100\texttimes{} fewer labels.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/IDVHH33D/Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf},
  journal = {arXiv:2002.05709 [cs, stat]},
  keywords = {favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{chen20_VAEPPVariationalAutoencoder,
  title = {{{VAEPP}}: {{Variational Autoencoder}} with a {{Pull}}-Back {{Prior}}},
  author = {Chen, Wenxiao and Liu, Wenda and Cai, Zhenting and Xu, Haowen and Pei, Dan},
  year = {2020},
  pages = {14},
  abstract = {Many approaches to training generative models by distinct training objectives have been proposed in the past. Variational Autoencoder (VAE) is an outstanding model of them based on log-likelihood. In this paper, we propose a novel learnable prior, Pull-back Prior, for VAEs by adjusting the density of the prior through a discriminator that can assess the quality of data. It involves the discriminator from the theory of GANs to enrich the prior in VAEs. Based on it, we propose a more general framework, VAE with a Pull-back Prior (VAEPP), which uses existing techniques of VAEs and WGANs, to improve the log-likelihood, quality of sampling and stability of training. In MNIST and CIFAR-10, the log-likelihood of VAEPP outperforms models without autoregressive components and is comparable to autoregressive models. In MNIST, Fashion-MNIST, CIFAR-10 and CelebA, the FID of VAEPP is comparable to GANs and SOTA of VAEs.},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_vaepp.pdf},
  language = {en}
}

@article{chen20_WeaklySupervisedDisentanglement,
  title = {Weakly {{Supervised Disentanglement}} by {{Pairwise Similarities}}},
  author = {Chen, Junxiang and Batmanghelich, Kayhan},
  year = {2020},
  month = mar,
  abstract = {Recently, researches related to unsupervised disentanglement learning with deep generative models have gained substantial popularity. However, without introducing supervision, there is no guarantee that the factors of interest can be successfully recovered. Motivated by a real-world problem, we propose a setting where the user introduces weak supervision by providing similarities between instances based on a factor to be disentangled. The similarity is provided as either a binary (yes/no) or a real-valued label describing whether a pair of instances are similar or not. We propose a new method for weakly supervised disentanglement of latent variables within the framework of Variational Autoencoder. Experimental results demonstrate that utilizing weak supervision improves the performance of the disentanglement method substantially.},
  archivePrefix = {arXiv},
  eprint = {1906.01044},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chen et al_2020_weakly supervised disentanglement by pairwise similarities.pdf},
  journal = {arXiv:1906.01044 [cs, stat]},
  keywords = {_tablet,disentanglement,favorite},
  primaryClass = {cs, stat}
}

@article{cheng16_LongShortTermMemoryNetworks,
  title = {Long {{Short}}-{{Term Memory}}-{{Networks}} for {{Machine Reading}}},
  author = {Cheng, Jianpeng and Dong, Li and Lapata, Mirella},
  year = {2016},
  month = sep,
  abstract = {In this paper we address the question of how to render sequence-level networks better at handling structured input. We propose a machine reading simulator which processes text incrementally from left to right and performs shallow reasoning with memory and attention. The reader extends the Long Short-Term Memory architecture with a memory network in place of a single memory cell. This enables adaptive memory usage during recurrence with neural attention, offering a way to weakly induce relations among tokens. The system is initially designed to process a single sequence but we also demonstrate how to integrate it with an encoder-decoder architecture. Experiments on language modeling, sentiment analysis, and natural language inference show that our model matches or outperforms the state of the art.},
  annotation = {ZSCC: 0000315},
  archivePrefix = {arXiv},
  eprint = {1601.06733},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cheng et al_2016_long short-term memory-networks for machine reading.pdf;/home/trung/Zotero/storage/L7EDG9DS/1601.html},
  journal = {arXiv:1601.06733 [cs]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,deep attention,inter attetion,intra attention,self attention},
  primaryClass = {cs}
}

@article{cheng20_ImprovingDisentangledText,
  title = {Improving {{Disentangled Text Representation Learning}} with {{Information}}-{{Theoretic Guidance}}},
  author = {Cheng, Pengyu and Min, Martin Renqiang and Shen, Dinghan and Malon, Christopher and Zhang, Yizhe and Li, Yitong and Carin, Lawrence},
  year = {2020},
  month = jun,
  abstract = {Learning disentangled representations of natural language is essential for many NLP tasks, e.g., conditional text generation, style transfer, personalized dialogue systems, etc. Similar problems have been studied extensively for other forms of data, such as images and videos. However, the discrete nature of natural language makes the disentangling of textual representations more challenging (e.g., the manipulation over the data space cannot be easily achieved). Inspired by information theory, we propose a novel method that effectively manifests disentangled representations of text, without any supervision on semantics. A new mutual information upper bound is derived and leveraged to measure dependence between style and content. By minimizing this upper bound, the proposed method induces style and content embeddings into two independent low-dimensional spaces. Experiments on both conditional text generation and text-style transfer demonstrate the high quality of our disentangled representation in terms of content and style preservation.},
  archivePrefix = {arXiv},
  eprint = {2006.00693},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cheng et al_2020_improving disentangled text representation learning with information-theoretic guidance.pdf},
  journal = {arXiv:2006.00693 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{cheng20_RevisitingFactorizingAggregated,
  title = {Revisiting {{Factorizing Aggregated Posterior}} in {{Learning Disentangled Representations}}},
  author = {Cheng, Ze and Li, Juncheng and Wang, Chenxu and Gu, Jixuan and Xu, Hao and Li, Xinjian and Metze, Florian},
  year = {2020},
  month = sep,
  abstract = {In the problem of learning disentangled representations, one of the promising methods is to factorize aggregated posterior by penalizing the total correlation of sampled latent variables. However, this well-motivated strategy has a blind spot: there is a disparity between the sampled latent representation and its corresponding mean representation. In this paper, we provide a theoretical explanation that low total correlation of sampled representation cannot guarantee low total correlation of the mean representation. Indeed, we prove that for the multivariate normal distributions, the mean representation with arbitrarily high total correlation can have a corresponding sampled representation with bounded total correlation. We also propose a method to eliminate this disparity. Experiments show that our model can learn a mean representation with much lower total correlation, hence a factorized mean representation. Moreover, we offer a detailed explanation of the limitations of factorizing aggregated posterior -- factor disintegration. Our work indicates a potential direction for future research of disentangled learning.},
  archivePrefix = {arXiv},
  eprint = {2009.05739},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cheng et al_2020_revisiting factorizing aggregated posterior in learning disentangled representations.pdf},
  journal = {arXiv:2009.05739 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{cherief-abdellatif19_ConvergenceRatesVariational,
  title = {Convergence {{Rates}} of {{Variational Inference}} in {{Sparse Deep Learning}}},
  author = {{Ch{\'e}rief-Abdellatif}, Badr-Eddine},
  year = {2019},
  month = sep,
  abstract = {Variational inference is becoming more and more popular for approximating intractable posterior distributions in Bayesian statistics and machine learning. Meanwhile, a few recent works have provided theoretical justification and new insights on deep neural networks for estimating smooth functions in usual settings such as nonparametric regression. In this paper, we show that variational inference for sparse deep learning retains the same generalization properties than exact Bayesian inference. In particular, we highlight the connection between estimation and approximation theories via the classical bias-variance trade-off and show that it leads to near-minimax rates of convergence for H\textbackslash "older smooth functions. Additionally, we show that the model selection framework over the neural network architecture via ELBO maximization does not overfit and adaptively achieves the optimal rate of convergence.},
  archivePrefix = {arXiv},
  eprint = {1908.04847},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chérief-abdellatif_2019_convergence rates of variational inference in sparse deep learning.pdf},
  journal = {arXiv:1908.04847 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{chevalier-boisvert18_BabyAIFirstSteps,
  title = {{{BabyAI}}: {{First Steps Towards Grounded Language Learning With}} a {{Human In}} the {{Loop}}},
  shorttitle = {{{BabyAI}}},
  author = {{Chevalier-Boisvert}, Maxime and Bahdanau, Dzmitry and Lahlou, Salem and Willems, Lucas and Saharia, Chitwan and Nguyen, Thien Huu and Bengio, Yoshua},
  year = {2018},
  month = oct,
  abstract = {Allowing humans to interactively train artificial agents to understand language instructions is desirable for both practical and scientific reasons, but given the poor data efficiency of the current learning methods, this goal may require substantial research efforts. Here, we introduce the BabyAI research platform to support investigations towards including humans in the loop for grounded language learning. The BabyAI platform comprises an extensible suite of 19 levels of increasing difficulty. The levels gradually lead the agent towards acquiring a combinatorially rich synthetic language which is a proper subset of English. The platform also provides a heuristic expert agent for the purpose of simulating a human teacher. We report baseline results and estimate the amount of human involvement that would be required to train a neural network-based agent on some of the BabyAI levels. We put forward strong evidence that current deep learning methods are not yet sufficiently sample efficient when it comes to learning a language with compositional properties.},
  archivePrefix = {arXiv},
  eprint = {1810.08272},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chevalier-boisvert et al_2018_babyai.pdf},
  journal = {arXiv:1810.08272 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  language = {en},
  primaryClass = {cs}
}

@article{chi20_SparsityPenalizedStackedDenoising,
  title = {Sparsity-{{Penalized Stacked Denoising Autoencoders}} for {{Imputing Single}}-{{Cell RNA}}-Seq {{Data}}},
  author = {Chi, Weilai and Deng, Minghua},
  year = {2020},
  month = may,
  volume = {11},
  pages = {532},
  issn = {2073-4425},
  doi = {10.3390/genes11050532},
  abstract = {Single-cell RNA-seq (scRNA-seq) is quite prevalent in studying transcriptomes, but it suffers from excessive zeros, some of which are true, but others are false. False zeros, which can be seen as missing data, obstruct the downstream analysis of single-cell RNA-seq data. How to distinguish true zeros from false ones is the key point of this problem. Here, we propose sparsity-penalized stacked denoising autoencoders (scSDAEs) to impute scRNA-seq data. scSDAEs adopt stacked denoising autoencoders with a sparsity penalty, as well as a layer-wise pretraining procedure to improve model fitting. scSDAEs can capture nonlinear relationships among the data and incorporate information about the observed zeros. We tested the imputation efficiency of scSDAEs on recovering the true values of gene expression and helping downstream analysis. First, we show that scSDAE can recover the true values and the sample\textendash sample correlations of bulk sequencing data with simulated noise. Next, we demonstrate that scSDAEs accurately impute RNA mixture dataset with different dilutions, spike-in RNA concentrations affected by technical zeros, and improves the consistency of RNA and protein levels in CITE-seq data. Finally, we show that scSDAEs can help downstream clustering analysis. In this study, we develop a deep learning-based method, scSDAE, to impute single-cell RNA-seq affected by technical zeros. Furthermore, we show that scSDAEs can recover the true values, to some extent, and help downstream analysis.},
  file = {/home/trung/GoogleDrive/Zotero/chi et al_2020_sparsity-penalized stacked denoising autoencoders for imputing single-cell rna-seq data.pdf},
  journal = {Genes},
  language = {en},
  number = {5}
}

@article{chiappa19_CausalBayesianNetworks,
  title = {A {{Causal Bayesian Networks Viewpoint}} on {{Fairness}}},
  author = {Chiappa, Silvia and Isaac, William S.},
  year = {2019},
  volume = {547},
  pages = {3--20},
  doi = {10.1007/978-3-030-16744-8_1},
  abstract = {We offer a graphical interpretation of unfairness in a dataset as the presence of an unfair causal path in the causal Bayesian network representing the data-generation mechanism. We use this viewpoint to revisit the recent debate surrounding the COMPAS pretrial risk assessment tool and, more generally, to point out that fairness evaluation on a model requires careful considerations on the patterns of unfairness underlying the training data. We show that causal Bayesian networks provide us with a powerful tool to measure unfairness in a dataset and to design fair models in complex unfairness scenarios.},
  archivePrefix = {arXiv},
  eprint = {1907.06430},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chiappa et al_2019_a causal bayesian networks viewpoint on fairness.pdf;/home/trung/Zotero/storage/CN3FSK2R/1907.html},
  journal = {arXiv:1907.06430 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{chien19_SelfAttentionVariational,
  title = {Self {{Attention}} in {{Variational Sequential Learning}} for {{Summarization}}},
  booktitle = {Interspeech 2019},
  author = {Chien, Jen-Tzung and Wang, Chun-Wei},
  year = {2019},
  month = sep,
  pages = {1318--1322},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1548},
  abstract = {Attention mechanism plays a crucial role in sequential learning for many speech and language applications. However, it is challenging to develop a stochastic attention in a sequenceto-sequence model which consists of two recurrent neural networks (RNNs) as the encoder and decoder. The problem of posterior collapse happens in variational inference and results in the estimated latent variables close to a standard Gaussian prior so that the information from input sequence is disregarded in learning process. This paper presents a new recurrent autoencoder for sentence representation where a self attention scheme is incorporated to activate the interaction between inference and generation in training procedure. In particular, a stochastic RNN decoder is implemented to provide additional latent variable to fulfill self attention for sentence reconstruction. The posterior collapse is alleviated. The latent information is sufficiently attended in variational sequential learning. During test phase, the estimated prior distribution of decoder is sampled for stochastic attention and generation. Experiments on Penn Treebank and Yelp 2013 show the desirable generation performance in terms of perplexity. The visualization of attention weights also illustrates the usefulness of self attention. The evaluation on DUC 2007 demonstrates the merit of variational recurrent autoencoder for document summarization.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/chien et al_2019_self attention in variational sequential learning for summarization.pdf},
  keywords = {attention,sequential,variational},
  language = {en}
}

@inproceedings{chien19_VariationalBayesianGAN,
  title = {Variational {{Bayesian GAN}}},
  booktitle = {2019 27th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Chien, Jen-Tzung and Kuo, Chun-Lin},
  year = {2019},
  month = sep,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{A Coruna, Spain}},
  doi = {10.23919/EUSIPCO.2019.8903084},
  file = {/home/trung/GoogleDrive/Zotero/chien et al_2019_variational bayesian gan.pdf},
  isbn = {978-90-827970-3-9},
  language = {en}
}

@inproceedings{chien19_VariationalHierarchicalRecurrent,
  title = {Variational and {{Hierarchical Recurrent Autoencoder}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Chien, Jen-Tzung and Wang, Chun-Wei},
  year = {2019},
  month = may,
  pages = {3202--3206},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683771},
  abstract = {Despite a great success in learning representation for image data, it is challenging to learn the stochastic latent features from natural language based on variational inference. The difficulty in stochastic sequential learning is due to the posterior collapse caused by an autoregressive decoder which is prone to be too strong to learn sufficient latent information during optimization. To compensate this weakness in learning procedure, a sophisticated latent structure is required to assure good convergence so that random features are sufficiently captured for sequential decoding. This study presents a new variational recurrent autoencoder (VRAE) for sequence reconstruction. There are two complementary encoders consisting of a long short-term memory (LSTM) and a pyramid bidirectional LSTM which are merged to discover the global and local dependencies in a hierarchical latent variable model, respectively. Experiments on Penn Treebank and Yelp 2013 demonstrate that the proposed hierarchical VRAE is able to learn the complementary representation as well as tackle the posterior collapse in stochastic sequential learning. The performance of recurrent autoencoder is substantially improved in terms of perplexity.},
  file = {/home/trung/GoogleDrive/Zotero/chien et al_2019_variational and hierarchical recurrent autoencoder.pdf},
  isbn = {978-1-4799-8131-1},
  keywords = {hierarchical,recurrent,variational},
  language = {en}
}

@article{child19_GeneratingLongSequences,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  month = apr,
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  annotation = {ZSCC: 0000028},
  archivePrefix = {arXiv},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/child et al_2019_generating long sequences with sparse transformers.pdf;/home/trung/Zotero/storage/JFLQ2VVV/1904.html},
  journal = {arXiv:1904.10509 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,sequence,sparse,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{child19_GeneratingLongSequencesa,
  title = {Generating {{Long Sequences}} with {{Sparse Transformers}}},
  author = {Child, Rewon and Gray, Scott and Radford, Alec and Sutskever, Ilya},
  year = {2019},
  month = apr,
  abstract = {Transformers are powerful sequence models, but require time and memory that grows quadratically with the sequence length. In this paper we introduce sparse factorizations of the attention matrix which reduce this to \$O(n \textbackslash sqrt\{n\})\$. We also introduce a) a variation on architecture and initialization to train deeper networks, b) the recomputation of attention matrices to save memory, and c) fast attention kernels for training. We call networks with these changes Sparse Transformers, and show they can model sequences tens of thousands of timesteps long using hundreds of layers. We use the same architecture to model images, audio, and text from raw bytes, setting a new state of the art for density modeling of Enwik8, CIFAR-10, and ImageNet-64. We generate unconditional samples that demonstrate global coherence and great diversity, and show it is possible in principle to use self-attention to model sequences of length one million or more.},
  archivePrefix = {arXiv},
  eprint = {1904.10509},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/child et al_2019_generating long sequences with sparse transformers2.pdf},
  journal = {arXiv:1904.10509 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chiu19_comparisonendtoendmodels,
  title = {A Comparison of End-to-End Models for Long-Form Speech Recognition},
  author = {Chiu, Chung-Cheng and Han, Wei and Zhang, Yu and Pang, Ruoming and Kishchenko, Sergey and Nguyen, Patrick and Narayanan, Arun and Liao, Hank and Zhang, Shuyuan and Kannan, Anjuli and Prabhavalkar, Rohit and Chen, Zhifeng and Sainath, Tara and Wu, Yonghui},
  year = {2019},
  month = nov,
  abstract = {End-to-end automatic speech recognition (ASR) models, including both attention-based models and the recurrent neural network transducer (RNN-T), have shown superior performance compared to conventional systems. However, previous studies have focused primarily on short utterances that typically last for just a few seconds or, at most, a few tens of seconds. Whether such architectures are practical on long utterances that last from minutes to hours remains an open question. In this paper, we both investigate and improve the performance of end-to-end models on long-form transcription. We first present an empirical comparison of different end-to-end models on a real world long-form task and demonstrate that the RNN-T model is much more robust than attention-based systems in this regime. We next explore two improvements to attention-based systems that significantly improve its performance: restricting the attention to be monotonic, and applying a novel decoding algorithm that breaks long utterances into shorter overlapping segments. Combining these two improvements, we show that attention-based end-to-end models can be very competitive to RNN-T on long-form speech recognition.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.02242},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chiu et al_2019_a comparison of end-to-end models for long-form speech recognition.pdf;/home/trung/Zotero/storage/TAV4PUZW/1911.html},
  journal = {arXiv:1911.02242 [cs, eess]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,end to end,speech recognition},
  primaryClass = {cs, eess}
}

@article{cho20_LearningSpeakerEmbedding,
  title = {Learning {{Speaker Embedding}} from {{Text}}-to-{{Speech}}},
  author = {Cho, Jaejin and Zelasko, Piotr and Villalba, Jesus and Watanabe, Shinji and Dehak, Najim},
  year = {2020},
  month = oct,
  abstract = {Zero-shot multi-speaker Text-to-Speech (TTS) generates target speaker voices given an input text and the corresponding speaker embedding. In this work, we investigate the effectiveness of the TTS reconstruction objective to improve representation learning for speaker verification. We jointly trained end-to-end Tacotron 2 TTS and speaker embedding networks in a self-supervised fashion. We hypothesize that the embeddings will contain minimal phonetic information since the TTS decoder will obtain that information from the textual input. TTS reconstruction can also be combined with speaker classification to enhance these embeddings further. Once trained, the speaker encoder computes representations for the speaker verification task, while the rest of the TTS blocks are discarded. We investigated training TTS from either manual or ASR-generated transcripts. The latter allows us to train embeddings on datasets without manual transcripts. We compared ASR transcripts and Kaldi phone alignments as TTS inputs, showing that the latter performed better due to their finer resolution. Unsupervised TTS embeddings improved EER by 2.06\textbackslash\% absolute with regard to i-vectors for the LibriTTS dataset. TTS with speaker classification loss improved EER by 0.28\textbackslash\% and 0.73\textbackslash\% absolutely from a model using only speaker classification loss in LibriTTS and Voxceleb1 respectively.},
  archivePrefix = {arXiv},
  eprint = {2010.11221},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cho et al_2020_learning speaker embedding from text-to-speech.pdf},
  journal = {arXiv:2010.11221 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{choi00_BayesianCausalStructural,
  title = {Bayesian {{Causal Structural Learning}} with {{Zero}}-{{Inflated Poisson Bayesian Networks}}},
  author = {Choi, Junsouk and Chapkin, Robert and Ni, Yang},
  pages = {11},
  abstract = {Multivariate zero-inflated count data arise in a wide range of areas such as economics, social sciences, and biology. To infer causal relationships in zero-inflated count data, we propose a new zero-inflated Poisson Bayesian network (ZIPBN) model. We show that the proposed ZIPBN is identifiable with cross-sectional data. The proof is based on the well-known characterization of Markov equivalence class which is applicable to other distribution families. For causal structural learning, we introduce a fully Bayesian inference approach which exploits the parallel tempering Markov chain Monte Carlo algorithm to efficiently explore the multi-modal network space. We demonstrate the utility of the proposed ZIPBN in causal discoveries for zero-inflated count data by simulation studies with comparison to alternative Bayesian network methods. Additionally, real single-cell RNA-sequencing data with known causal relationships will be used to assess the capability of ZIPBN for discovering causal relationships in real-world problems.},
  file = {/home/trung/GoogleDrive/Zotero/choi et al_bayesian causal structural learning with zero-inﬂated poisson bayesian networks.pdf},
  keywords = {_tablet,causal,favorite},
  language = {en}
}

@article{choi17_StarGANUnifiedGenerative,
  title = {{{StarGAN}}: {{Unified Generative Adversarial Networks}} for {{Multi}}-{{Domain Image}}-to-{{Image Translation}}},
  shorttitle = {{{StarGAN}}},
  author = {Choi, Yunjey and Choi, Minje and Kim, Munyoung and Ha, Jung-Woo and Kim, Sunghun and Choo, Jaegul},
  year = {2017},
  month = nov,
  abstract = {Recent studies have shown remarkable success in image-to-image translation for two domains. However, existing approaches have limited scalability and robustness in handling more than two domains, since different models should be built independently for every pair of image domains. To address this limitation, we propose StarGAN, a novel and scalable approach that can perform image-to-image translations for multiple domains using only a single model. Such a unified model architecture of StarGAN allows simultaneous training of multiple datasets with different domains within a single network. This leads to StarGAN's superior quality of translated images compared to existing models as well as the novel capability of flexibly translating an input image to any desired target domain. We empirically demonstrate the effectiveness of our approach on a facial attribute transfer and a facial expression synthesis tasks.},
  archivePrefix = {arXiv},
  eprint = {1711.09020},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/choi et al_2017_stargan.pdf},
  journal = {arXiv:1711.09020 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{choi18_WAICWhyGenerative,
  title = {{{WAIC}}, but {{Why}}? {{Generative Ensembles}} for {{Robust Anomaly Detection}}},
  shorttitle = {{{WAIC}}, but {{Why}}?},
  author = {Choi, Hyunsun and Jang, Eric and Alemi, Alexander A.},
  year = {2018},
  month = oct,
  abstract = {Machine learning models encounter Out-of-Distribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation -- although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.},
  annotation = {ZSCC: 0000022},
  archivePrefix = {arXiv},
  eprint = {1810.01392},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/choi et al_2018_waic, but why.pdf;/home/trung/Zotero/storage/BATKLSPX/1810.html},
  journal = {arXiv:1810.01392 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{choi19_StarGANv2Diverse,
  title = {{{StarGAN}} v2: {{Diverse Image Synthesis}} for {{Multiple Domains}}},
  shorttitle = {{{StarGAN}} V2},
  author = {Choi, Yunjey and Uh, Youngjung and Yoo, Jaejun and Ha, Jung-Woo},
  year = {2019},
  month = dec,
  abstract = {A good image-to-image translation model should learn a mapping between different visual domains while satisfying the following properties: 1) diversity of generated images and 2) scalability over multiple domains. Existing methods address either of the issues, having limited diversity or multiple models for all domains. We propose StarGAN v2, a single framework that tackles both and shows significantly improved results over the baselines. Experiments on CelebA-HQ and a new animal faces dataset (AFHQ) validate our superiority in terms of visual quality, diversity, and scalability. To better assess image-to-image translation models, we release AFHQ, high-quality animal faces with large inter- and intra-domain differences. The code, pretrained models, and dataset can be found at https://github.com/clovaai/stargan-v2.},
  archivePrefix = {arXiv},
  eprint = {1912.01865},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/choi et al_2019_stargan v2.pdf;/home/trung/Zotero/storage/T88TNJ3L/1912.html},
  journal = {arXiv:1912.01865 [cs]},
  primaryClass = {cs}
}

@article{choi20_scTypercomprehensivepipeline,
  title = {{{scTyper}}: A Comprehensive Pipeline for the Cell Typing Analysis of Single-Cell {{RNA}}-Seq Data},
  shorttitle = {{{scTyper}}},
  author = {Choi, Ji-Hye and In Kim, Hye and Woo, Hyun Goo},
  year = {2020},
  month = dec,
  volume = {21},
  pages = {342},
  issn = {1471-2105},
  doi = {10.1186/s12859-020-03700-5},
  abstract = {Background: Recent advances in single-cell RNA sequencing (scRNA-seq) technology have enabled the identification of individual cell types, such as epithelial cells, immune cells, and fibroblasts, in tissue samples containing complex cell populations. Cell typing is one of the key challenges in scRNA-seq data analysis that is usually achieved by estimating the expression of cell marker genes. However, there is no standard practice for cell typing, often resulting in variable and inaccurate outcomes. Results: We have developed a comprehensive and user-friendly R-based scRNA-seq analysis and cell typing package, scTyper. scTyper also provides a database of cell type markers, scTyper.db, which contains 213 cell marker sets collected from literature. These marker sets include but are not limited to markers for malignant cells, cancer-associated fibroblasts, and tumor-infiltrating T cells. Additionally, scTyper provides three customized methods for estimating cell-type marker expression, including nearest template prediction (NTP), gene set enrichment analysis (GSEA), and average expression values. DNA copy number inference method (inferCNV) has been implemented with an improved modification that can be used for malignant cell typing. The package also supports the data preprocessing pipelines by Cell Ranger from 10X Genomics and the Seurat package. A summary reporting system is also implemented, which may facilitate users to perform reproducible analyses. Conclusions: scTyper provides a comprehensive and user-friendly analysis pipeline for cell typing of scRNA-seq data with a curated cell marker database, scTyper.db.},
  file = {/home/trung/GoogleDrive/Zotero/choi et al_2020_sctyper.pdf},
  journal = {BMC Bioinformatics},
  language = {en},
  number = {1}
}

@article{choi20_SingleCellRNASequencing,
  title = {Single-{{Cell RNA Sequencing}} and {{Its Combination}} with {{Protein}} and {{DNA Analyses}}},
  author = {Choi, Jane Ru and Yong, Kar Wey and Choi, Jean Yu and Cowie, Alistair C.},
  year = {2020},
  month = may,
  volume = {9},
  pages = {1130},
  issn = {2073-4409},
  doi = {10.3390/cells9051130},
  abstract = {Heterogeneity in cell populations poses a significant challenge for understanding complex cell biological processes. The analysis of cells at the single-cell level, especially single-cell RNA sequencing (scRNA-seq), has made it possible to comprehensively dissect cellular heterogeneity and access unobtainable biological information from bulk analysis. Recent efforts have combined scRNA-seq profiles with genomic or proteomic data, and show added value in describing complex cellular heterogeneity than transcriptome measurements alone. With the rising demand for scRNAseq for biomedical and clinical applications, there is a strong need for a timely and comprehensive review on the scRNA-seq technologies and their potential biomedical applications. In this review, we first discuss the latest state of development by detailing each scRNA-seq technology, including both conventional and microfluidic technologies. We then summarize their advantages and limitations along with their biomedical applications. The efforts of integrating the transcriptome profile with highly multiplexed proteomic and genomic data are thoroughly reviewed with results showing the integrated data being more informative than transcriptome data alone. Lastly, the latest progress toward commercialization, the remaining challenges, and future perspectives on the development of scRNA-seq technologies are briefly discussed.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/choi et al_2020_single-cell rna sequencing and its combination with protein and dna analyses.pdf},
  journal = {Cells},
  language = {en},
  number = {5}
}

@article{choi20_SingleCellRNASequencinga,
  title = {Single-{{Cell RNA Sequencing}} and {{Its Combination}} with {{Protein}} and {{DNA Analyses}}},
  author = {Choi, Jane Ru and Yong, Kar Wey and Choi, Jean Yu and Cowie, Alistair C.},
  year = {2020},
  month = may,
  volume = {9},
  pages = {1130},
  issn = {2073-4409},
  doi = {10.3390/cells9051130},
  abstract = {Heterogeneity in cell populations poses a significant challenge for understanding complex cell biological processes. The analysis of cells at the single-cell level, especially single-cell RNA sequencing (scRNA-seq), has made it possible to comprehensively dissect cellular heterogeneity and access unobtainable biological information from bulk analysis. Recent efforts have combined scRNA-seq profiles with genomic or proteomic data, and show added value in describing complex cellular heterogeneity than transcriptome measurements alone. With the rising demand for scRNA-seq for biomedical and clinical applications, there is a strong need for a timely and comprehensive review on the scRNA-seq technologies and their potential biomedical applications. In this review, we first discuss the latest state of development by detailing each scRNA-seq technology, including both conventional and microfluidic technologies. We then summarize their advantages and limitations along with their biomedical applications. The efforts of integrating the transcriptome profile with highly multiplexed proteomic and genomic data are thoroughly reviewed with results showing the integrated data being more informative than transcriptome data alone. Lastly, the latest progress toward commercialization, the remaining challenges, and future perspectives on the development of scRNA-seq technologies are briefly discussed.},
  file = {/home/trung/GoogleDrive/Zotero/choi et al_2020_single-cell rna sequencing and its combination with protein and dna analyses2.pdf},
  journal = {Cells},
  language = {en},
  number = {5}
}

@article{chollet19_MeasureIntelligence,
  title = {The {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.01547},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chollet_2019_the measure of intelligence.pdf;/home/trung/Zotero/storage/GUC3V27C/1911.html},
  journal = {arXiv:1911.01547 [cs]},
  keywords = {Computer Science - Artificial Intelligence,explain,favorite,general intelligence},
  primaryClass = {cs}
}

@article{choromanski20_MaskedLanguageModeling,
  title = {Masked {{Language Modeling}} for {{Proteins}} via {{Linearly Scalable Long}}-{{Context Transformers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2020},
  month = aug,
  abstract = {Transformer models have achieved state-of-the-art results across a diverse range of domains. However, concern over the cost of training the attention mechanism to learn complex dependencies between distant inputs continues to grow. In response, solutions that exploit the structure and sparsity of the learned attention matrix have blossomed. However, real-world applications that involve long sequences, such as biological sequence analysis, may fall short of meeting these assumptions, precluding exploration of these models. To address this challenge, we present a new Transformer architecture, Performer, based on Fast Attention Via Orthogonal Random features (FAVOR). Our mechanism scales linearly rather than quadratically in the number of tokens in the sequence, is characterized by sub-quadratic space complexity and does not incorporate any sparsity pattern priors. Furthermore, it provides strong theoretical guarantees: unbiased estimation of the attention matrix and uniform convergence. It is also backwards-compatible with pre-trained regular Transformers. We demonstrate its effectiveness on the challenging task of protein sequence modeling and provide detailed theoretical analysis.},
  archivePrefix = {arXiv},
  eprint = {2006.03555},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/choromanski et al_2020_masked language modeling for proteins via linearly scalable long-context transformers.pdf},
  journal = {arXiv:2006.03555 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{choromanski20_RethinkingAttentionPerformers,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2020},
  month = sep,
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archivePrefix = {arXiv},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/choromanski et al_2020_rethinking attention with performers.pdf},
  journal = {arXiv:2009.14794 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{choromanski20_RethinkingAttentionPerformersa,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2020},
  month = sep,
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archivePrefix = {arXiv},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/choromanski et al_2020_rethinking attention with performers2.pdf},
  journal = {arXiv:2009.14794 [cs, stat]},
  keywords = {favorite},
  primaryClass = {cs, stat}
}

@article{choudrey00_VariationalMethodsBayesian,
  title = {Variational {{Methods}} for {{Bayesian Independent Component Analysis}}},
  author = {Choudrey, Rizwan A},
  pages = {261},
  abstract = {The fundamental area of research in this thesis is Independent Component Analysis (ICA). ICA is a tool for discovering structure and patterns in data by factoring a multidimensional data distribution into a product of one-dimensional, statistically independent component distributions. Statistical independence is equivalent to information-theoretic independence. Therefore, if the original M dimensional data distribution is factored into L {$\leq$} M independent components, then the M streams of observed numbers, which may have structure and information encoded across them, are transformed into L independent streams of numbers which will have structure and information encoded only within each stream. This has the effect of making patterns within the original data more cogent.},
  annotation = {ZSCC: 0000087},
  file = {/home/trung/GoogleDrive/Zotero/choudrey_variational methods for bayesian independent component analysis.pdf},
  language = {en}
}

@article{chowdhury20_WhatdoesEndtoEnd,
  title = {What Does an {{End}}-to-{{End Dialect Identification Model Learn}} about {{Non}}-Dialectal {{Information}}?},
  author = {Chowdhury, Shammur A and Ali, Ahmed and Shon, Suwon and Glass, James},
  year = {2020},
  pages = {6},
  abstract = {An end-to-end dialect identification system generates the likelihood of each dialect, given a speech utterance. The performance relies on its capabilities to discriminate the acoustic properties between the different dialects, even though the input signal contains non-dialectal information such as speaker and channel. In this work, we study how non-dialectal information are encoded inside the end-to-end dialect identification model. We design several proxy tasks to understand the model's ability to represent speech input for differentiating non-dialectal information \textendash{} such as (a) gender and voice identity of speakers, (b) languages, (c) channel (recording and transmission) quality \textendash{} and compare with dialectal information (i.e., predicting geographic region of the dialects). By analyzing non-dialectal representations from layers of an end-to-end Arabic dialect identification (ADI) model, we observe that the model retains gender and channel information throughout the network while learning a speaker-invariant representation. Our findings also suggest that the CNN layers of the end-to-end model mirror feature extractors capturing voice-specific information, while the fullyconnected layers encode more dialectal information.},
  file = {/home/trung/GoogleDrive/Zotero/chowdhury et al_2020_what does an end-to-end dialect identiﬁcation model learn about non-dialectal information.pdf},
  language = {en}
}

@article{chrysos20_UnsupervisedControllableGeneration,
  title = {Unsupervised {{Controllable Generation}} with {{Self}}-{{Training}}},
  author = {Chrysos, Grigorios G. and Kossaifi, Jean and Yu, Zhiding and Anandkumar, Anima},
  year = {2020},
  month = jul,
  abstract = {Recent generative adversarial networks (GANs) are able to generate impressive photo-realistic images. However, controllable generation with GANs remains a challenging research problem. Achieving controllable generation requires semantically interpretable and disentangled factors of variation. It is challenging to achieve this goal using simple fixed distributions such as Gaussian distribution. Instead, we propose an unsupervised framework to learn a distribution of latent codes that control the generator through self-training. Self-training provides an iterative feedback in the GAN training, from the discriminator to the generator, and progressively improves the proposal of the latent codes as training proceeds. The latent codes are sampled from a latent variable model that is learned in the feature space of the discriminator. We consider a normalized independent component analysis model and learn its parameters through tensor factorization of the higher-order moments. Our framework exhibits better disentanglement compared to other variants such as the variational autoencoder, and is able to discover semantically meaningful latent codes without any supervision. We demonstrate empirically on both cars and faces datasets that each group of elements in the learned code controls a mode of variation with a semantic meaning, e.g. pose or background change. We also demonstrate with quantitative metrics that our method generates better results compared to other approaches.},
  archivePrefix = {arXiv},
  eprint = {2007.09250},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chrysos et al_2020_unsupervised controllable generation with self-training.pdf},
  journal = {arXiv:2007.09250 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{chuang20_EstimatingGeneralizationDistribution,
  title = {Estimating {{Generalization}} under {{Distribution Shifts}} via {{Domain}}-{{Invariant Representations}}},
  author = {Chuang, Ching-Yao and Torralba, Antonio and Jegelka, Stefanie},
  year = {2020},
  month = jul,
  abstract = {When machine learning models are deployed on a test distribution different from the training distribution, they can perform poorly, but overestimate their performance. In this work, we aim to better estimate a model's performance under distribution shift, without supervision. To do so, we use a set of domain-invariant predictors as a proxy for the unknown, true target labels. Since the error of the resulting risk estimate depends on the target risk of the proxy model, we study generalization of domain-invariant representations and show that the complexity of the latent representation has a significant influence on the target risk. Empirically, our approach (1) enables self-tuning of domain adaptation models, and (2) accurately estimates the target error of given models under distribution shift. Other applications include model selection, deciding early stopping and error detection.},
  archivePrefix = {arXiv},
  eprint = {2007.03511},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chuang et al_2020_estimating generalization under distribution shifts via domain-invariant representations.pdf},
  journal = {arXiv:2007.03511 [cs, stat]},
  keywords = {early stopping},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{chung15_RecurrentLatentVariable,
  title = {A {{Recurrent Latent Variable Model}} for {{Sequential Data}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Chung, Junyoung and Kastner, Kyle and Dinh, Laurent and Goel, Kratarth and Courville, Aaron C and Bengio, Yoshua},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2980--2988},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/chung et al_2015_a recurrent latent variable model for sequential data.pdf;/home/trung/Zotero/storage/FEARKNP5/5653-a-recurrent-latent-variable-model-for-sequential-data.html},
  keywords = {recurrent,variational}
}

@article{chung16_HierarchicalMultiscaleRecurrent,
  title = {Hierarchical {{Multiscale Recurrent Neural Networks}}},
  author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
  year = {2016},
  month = sep,
  abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
  archivePrefix = {arXiv},
  eprint = {1609.01704},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chung et al_2016_hierarchical multiscale recurrent neural networks.pdf;/home/trung/Zotero/storage/YQPAVLUA/1609.html},
  journal = {arXiv:1609.01704 [cs]},
  keywords = {Computer Science - Machine Learning,hierarchical,recurrent},
  primaryClass = {cs}
}

@article{chung19_UnknownExamplesMachine,
  title = {Unknown {{Examples}} \& {{Machine Learning Model Generalization}}},
  author = {Chung, Yeounoh and Haas, Peter J. and Upfal, Eli and Kraska, Tim},
  year = {2019},
  month = oct,
  abstract = {Most machine learning (ML) technology assumes that the data for training an ML model has the same distribution as the test data to which the model will be applied. However, due to sample selection bias or, more generally, covariate shift, there exist potential training examples that are unknown to the modeler\textemdash ``unknown unknowns''. The resulting discrepancy between training and testing distributions leads to poor generalization performance of the ML model and hence biased predictions. Existing techniques use test data to detect and ameliorate such discrepancies, but in many real-world situations such test data is unavailable at training time. We exploit the fact that training data often comes from multiple overlapping sources, and combine species-estimation techniques with datadriven methods for estimating the feature values for the unknown unknowns. This information can then be used to correct the training set, prior to seeing any test data. Experiments on a variety of ML models and datasets indicate that our novel techniques can improve generalization performance and increase ML model robustness.},
  archivePrefix = {arXiv},
  eprint = {1808.08294},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/chung et al_2019_unknown examples & machine learning model generalization.pdf},
  journal = {arXiv:1808.08294 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{cicek18_SaaSSpeedSupervisor,
  title = {{{SaaS}}: {{Speed}} as a {{Supervisor}} for {{Semi}}-Supervised {{Learning}}},
  shorttitle = {{{SaaS}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Cicek, Safa and Fawzi, Alhussein and Soatto, Stefano},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11206},
  pages = {152--166},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01216-8_10},
  abstract = {We introduce the SaaS Algorithm for semi-supervised learning, which uses learning speed during stochastic gradient descent in a deep neural network to measure the quality of an iterative estimate of the posterior probability of unknown labels. Training speed in supervised learning correlates strongly with the percentage of correct labels, so we use it as an inference criterion for the unknown labels, without attempting to infer the model parameters at first. Despite its simplicity, SaaS achieves competitive results in semi-supervised learning benchmarks.},
  file = {/home/trung/Zotero/storage/HWQ85LFE/Cicek et al. - 2018 - SaaS Speed as a Supervisor for Semi-supervised Le.pdf},
  isbn = {978-3-030-01215-1 978-3-030-01216-8},
  language = {en}
}

@article{ciresan12_MulticolumnDeepNeural,
  title = {Multi-Column {{Deep Neural Networks}} for {{Image Classification}}},
  author = {Cire{\c s}an, Dan and Meier, Ueli and Schmidhuber, Juergen},
  year = {2012},
  month = feb,
  abstract = {Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winner-take-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1202.2745},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cireşan et al_2012_multi-column deep neural networks for image classification.pdf;/home/trung/Zotero/storage/DE3GS5EV/1202.html},
  journal = {arXiv:1202.2745 [cs]},
  primaryClass = {cs}
}

@article{clark19_Multiplehealthenvironmental,
  title = {Multiple Health and Environmental Impacts of Foods},
  author = {Clark, Michael A and Springmann, Marco and Hill, Jason and Tilman, David},
  year = {2019},
  month = nov,
  volume = {116},
  pages = {23357--23362},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1906908116},
  abstract = {Food choices are shifting globally in ways that are negatively affecting both human health and the environment. Here we consider how consuming an additional serving per day of each of 15 foods is associated with 5 health outcomes in adults and 5 aspects of agriculturally driven environmental degradation. We find that while there is substantial variation in the health outcomes of different foods, foods associated with a larger reduction in disease risk for one health outcome are often associated with larger reductions in disease risk for other health outcomes. Likewise, foods with lower impacts on one metric of environmental harm tend to have lower impacts on others. Additionally, of the foods associated with improved health (whole grain cereals, fruits, vegetables, legumes, nuts, olive oil, and fish), all except fish have among the lowest environmental impacts, and fish has markedly lower impacts than red meats and processed meats. Foods associated with the largest negative environmental impacts\textemdash unprocessed and processed red meat\textemdash are consistently associated with the largest increases in disease risk. Thus, dietary transitions toward greater consumption of healthier foods would generally improve environmental sustainability, although processed foods high in sugars harm health but can have relatively low environmental impacts. These findings could help consumers, policy makers, and food companies to better understand the multiple health and environmental implications of food choices.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/clark et al_2019_multiple health and environmental impacts of foods.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {food,vegan},
  language = {en},
  number = {46}
}

@misc{clegg09_GlucosamineChondroitinSulfate,
  title = {Glucosamine, {{Chondroitin Sulfate}}, and the {{Two}} in {{Combination}} for {{Painful Knee Osteoarthritis}}},
  author = {Clegg, Daniel O. and Reda, Domenic J. and Harris, Crystal L. and Klein, Marguerite A. and O'Dell, James R. and Hooper, Michele M. and Bradley, John D. and Bingham, Clifton O. III and Weisman, Michael H. and Jackson, Christopher G. and Lane, Nancy E. and Cush, John J. and Moreland, Larry W. and Schumacher, H. Ralph Jr and Oddis, Chester V. and Wolfe, Frederick and Molitor, Jerry A. and Yocum, David E. and Schnitzer, Thomas J. and Furst, Daniel E. and Sawitzke, Allen D. and Shi, Helen and Brandt, Kenneth D. and Moskowitz, Roland W. and Williams, H. James},
  year = {2009},
  month = oct,
  doi = {10.1056/NEJMoa052771},
  abstract = {Original Article from The New England Journal of Medicine \textemdash{} Glucosamine, Chondroitin Sulfate, and the Two in Combination for Painful Knee Osteoarthritis},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {Copyright \textcopyright{} 2006 Massachusetts Medical Society. All rights reserved.},
  file = {/home/trung/GoogleDrive/Zotero/clegg et al_2009_glucosamine, chondroitin sulfate, and the two in combination for painful knee osteoarthritis.pdf;/home/trung/Zotero/storage/L87YYYEL/NEJMoa052771.html},
  howpublished = {https://www.nejm.org/doi/10.1056/NEJMoa052771?url\_ver=Z39.88-2003\&rfr\_id=ori\%3Arid\%3Acrossref.org\&rfr\_dat=cr\_pub\%3Dwww.ncbi.nlm.nih.gov},
  journal = {http://dx.doi.org/10.1056/NEJMoa052771},
  language = {EN},
  type = {Research-Article}
}

@article{clifford19_effectscollagenpeptides,
  title = {The Effects of Collagen Peptides on Muscle Damage, Inflammation and Bone Turnover Following Exercise: A Randomized, Controlled Trial},
  shorttitle = {The Effects of Collagen Peptides on Muscle Damage, Inflammation and Bone Turnover Following Exercise},
  author = {Clifford, Tom and Ventress, Matthew and Allerton, Dean M. and Stansfield, Sarah and Tang, Jonathan C. Y. and Fraser, William D. and Vanhoecke, Barbara and Prawitt, Janne and Stevenson, Emma},
  year = {2019},
  month = apr,
  volume = {51},
  pages = {691--704},
  issn = {1438-2199},
  doi = {10.1007/s00726-019-02706-5},
  abstract = {This study examined whether consuming collagen peptides (CP) before and after strenuous exercise alters markers of muscle damage, inflammation and bone turnover. Using a double-blind, independent group's design, 24 recreationally active males consumed either 20~g~day-1 of CP or a placebo control (CON) for 7~days before and 2~days after performing 150 drop jumps. Maximal isometric voluntary contractions, countermovement jumps (CMJ), muscle soreness (200~mm visual analogue scale), pressure pain threshold, Brief Assessment of Mood Adapted (BAM\,+) and a range of blood markers associated with muscle damage, inflammation and bone turnover C-terminal telopeptide of type 1 collagen ({$\beta$}-CTX) and N-terminal propeptides of type 1 pro-collagen (P1NP) were measured before supplementation (baseline; BL), pre, post, 1.5, 24 and 48~h post-exercise. Muscle soreness was not significantly different in CP and CON (P\,=\,0.071) but a large effect size was evident at 48~h post-exercise, indicative of lower soreness in the CP group (90.42\,{$\pm$}\,45.33~mm vs. CON 125.67\,{$\pm$}\,36.50~mm; ES\,=\,2.64). CMJ height recovered quicker with CP than CON at 48~h (P\,=\,0.050; CP 89.96\,{$\pm$}\,12.85 vs. CON 78.67\,{$\pm$}\,14.41\% of baseline values; ES\,=\,0.55). There were no statistically significant effects for the other dependent variables (P\,{$>$}\,0.05). {$\beta$}-CTX and P1NP were unaffected by CP supplementation (P\,{$>$}\,0.05). In conclusion, CP had moderate benefits for the recovery of CMJ and muscle soreness but had no influence on inflammation and bone collagen synthesis.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/clifford et al_2019_the effects of collagen peptides on muscle damage, inflammation and bone turnover following exercise.pdf},
  journal = {Amino Acids},
  language = {eng},
  number = {4},
  pmid = {30783776}
}

@article{clivio19_AutoZIDetectingZeroInflated,
  title = {{{AutoZI}}: {{Detecting Zero}}-{{Inflated Genes}} in {{Single}}-{{Cell Transcriptomics Data}}},
  author = {Clivio, Oscar and Lopez, Romain and Regier, Jeffrey and Gayoso, Adam and Jordan, Michael I. and Yosef, Nir},
  year = {2019},
  month = oct,
  doi = {10.1101/794875},
  abstract = {In single-cell RNA sequencing data, biological processes or technical factors may induce an overabundance of zero measurements. Existing probabilistic approaches to interpreting these data either model all genes as zero-inflated, or none. But the overabundance of zeros might be gene-specific. Hence, we propose the AutoZI model, which, for each gene, places a spike-and-slab prior on a mixture assignment between a negative binomial (NB) component and a zero-inflated negative binomial (ZINB) component. We approximate the posterior distribution under this model using variational inference, and employ Bayesian decision theory to decide whether each gene is zero-inflated. On simulated data, AutoZI outperforms the alternatives. On negative control data, AutoZI retrieves predictions consistent to a previous study on ERCC spike-ins and recovers similar results on control RNAs. Applied to several datasets and instances of the 10x Chromium protocol, AutoZI allows both biological and technical interpretations of zero-inflation. Finally, AutoZI's decisions on mouse embyronic stem-cells suggest that zero-inflation might be due to transcriptional bursting.},
  file = {/home/trung/GoogleDrive/Zotero/clivio et al_2019_autozi.pdf},
  journal = {bioRxiv},
  keywords = {variational},
  language = {en}
}

@techreport{clivio19_DetectingZeroInflatedGenes,
  title = {Detecting {{Zero}}-{{Inflated Genes}} in {{Single}}-{{Cell Transcriptomics Data}}},
  author = {Clivio, Oscar and Lopez, Romain and Regier, Jeffrey and Gayoso, Adam and Jordan, Michael I. and Yosef, Nir},
  year = {2019},
  month = oct,
  institution = {{Bioinformatics}},
  doi = {10.1101/794875},
  abstract = {In single-cell RNA sequencing data, biological processes or technical factors may induce an overabundance of zero measurements. Existing probabilistic approaches to interpreting these data either model all genes as zero-inflated, or none. But the overabundance of zeros might be gene-specific. Hence, we propose the AutoZI model, which, for each gene, places a spike-and-slab prior on a mixture assignment between a negative binomial (NB) component and a zero-inflated negative binomial (ZINB) component. We approximate the posterior distribution under this model using variational inference, and employ Bayesian decision theory to decide whether each gene is zero-inflated. On simulated data, AutoZI outperforms the alternatives. On negative control data, AutoZI retrieves predictions consistent to a previous study on ERCC spike-ins and recovers similar results on control RNAs. Applied to several datasets and instances of the 10x Chromium protocol, AutoZI allows both biological and technical interpretations of zero-inflation. Finally, AutoZI's decisions on mouse embyronic stem-cells suggest that zero-inflation might be due to transcriptional bursting.},
  file = {/home/trung/GoogleDrive/Zotero/clivio et al_2019_detecting zero-inflated genes in single-cell transcriptomics data.pdf},
  language = {en},
  type = {Preprint}
}

@book{coelho15_Buildingmachinelearning,
  title = {Building Machine Learning Systems with {{Python}}: Get More from Your Data through Creating Practical Machine Learning Systemw with {{Phython}}},
  shorttitle = {Building Machine Learning Systems with {{Python}}},
  author = {Coelho, Luis Pedro and Richert, Willi},
  year = {2015},
  edition = {Second edition},
  publisher = {{Packt Publishing}},
  address = {{Birmingham Mumbai}},
  file = {/home/trung/GoogleDrive/Zotero/coelho et al_2015_building machine learning systems with python.pdf},
  isbn = {978-1-78439-277-2},
  language = {en},
  series = {Community Experience Distilled}
}

@article{cohen16_GroupEquivariantConvolutional,
  title = {Group {{Equivariant Convolutional Networks}}},
  author = {Cohen, Taco S. and Welling, Max},
  year = {2016},
  month = jun,
  abstract = {We introduce Group equivariant Convolutional Neural Networks (G-CNNs), a natural generalization of convolutional neural networks that reduces sample complexity by exploiting symmetries. G-CNNs use G-convolutions, a new type of layer that enjoys a substantially higher degree of weight sharing than regular convolution layers. G-convolutions increase the expressive capacity of the network without increasing the number of parameters. Group convolution layers are easy to use and can be implemented with negligible computational overhead for discrete groups generated by translations, reflections and rotations. G-CNNs achieve state of the art results on CIFAR10 and rotated MNIST.},
  annotation = {ZSCC: 0000312},
  archivePrefix = {arXiv},
  eprint = {1602.07576},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cohen et al_2016_group equivariant convolutional networks.pdf;/home/trung/Zotero/storage/U3HH6VL8/1602.html},
  journal = {arXiv:1602.07576 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{cohen19_BidirectionalOneShotUnsupervised,
  title = {Bidirectional {{One}}-{{Shot Unsupervised Domain Mapping}}},
  author = {Cohen, Tomer and Wolf, Lior},
  year = {2019},
  month = sep,
  abstract = {We study the problem of mapping between a domain \$A\$, in which there is a single training sample and a domain \$B\$, for which we have a richer training set. The method we present is able to perform this mapping in both directions. For example, we can transfer all MNIST images to the visual domain captured by a single SVHN image and transform the SVHN image to the domain of the MNIST images. Our method is based on employing one encoder and one decoder for each domain, without utilizing weight sharing. The autoencoder of the single sample domain is trained to match both this sample and the latent space of domain \$B\$. Our results demonstrate convincing mapping between domains, where either the source or the target domain are defined by a single sample, far surpassing existing solutions. Our code is made publicly available at https://github.com/tomercohen11/BiOST},
  archivePrefix = {arXiv},
  eprint = {1909.01595},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cohen et al_2019_bidirectional one-shot unsupervised domain mapping.pdf;/home/trung/Zotero/storage/3MJT3LM9/1909.html},
  journal = {arXiv:1909.01595 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{cohen20_COVID19ImageData,
  title = {{{COVID}}-19 {{Image Data Collection}}: {{Prospective Predictions Are}} the {{Future}}},
  shorttitle = {{{COVID}}-19 {{Image Data Collection}}},
  author = {Cohen, Joseph Paul and Morrison, Paul and Dao, Lan and Roth, Karsten and Duong, Tim Q. and Ghassemi, Marzyeh},
  year = {2020},
  month = jun,
  abstract = {Across the world's coronavirus disease 2019 (COVID-19) hot spots, the need to streamline patient diagnosis and management has become more pressing than ever. As one of the main imaging tools, chest X-rays (CXRs) are common, fast, non-invasive, relatively cheap, and potentially bedside to monitor the progression of the disease. This paper describes the first public COVID-19 image data collection as well as a preliminary exploration of possible use cases for the data. This dataset currently contains hundreds of frontal view X-rays and is the largest public resource for COVID-19 image and prognostic data, making it a necessary resource to develop and evaluate tools to aid in the treatment of COVID-19. It was manually aggregated from publication figures as well as various web based repositories into a machine learning (ML) friendly format with accompanying dataloader code. We collected frontal and lateral view imagery and metadata such as the time since first symptoms, intensive care unit (ICU) status, survival status, intubation status, or hospital location. We present multiple possible use cases for the data such as predicting the need for the ICU, predicting patient survival, and understanding a patient's trajectory during treatment. Data can be accessed here: https://github.com/ieee8023/covid-chestxray-dataset},
  archivePrefix = {arXiv},
  eprint = {2006.11988},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cohen et al_2020_covid-19 image data collection.pdf},
  journal = {arXiv:2006.11988 [cs, eess, q-bio]},
  language = {en},
  primaryClass = {cs, eess, q-bio}
}

@article{cohen20_PredictingCOVID19Pneumonia,
  title = {Predicting {{COVID}}-19 {{Pneumonia Severity}} on {{Chest X}}-Ray with {{Deep Learning}}},
  author = {Cohen, Joseph Paul and Dao, Lan and Morrison, Paul and Roth, Karsten and Bengio, Yoshua and Shen, Beiyi and Abbasi, Almas and {Hoshmand-Kochi}, Mahsa and Ghassemi, Marzyeh and Li, Haifang and Duong, Tim Q.},
  year = {2020},
  month = jun,
  abstract = {Purpose: The need to streamline patient management for COVID-19 has become more pressing than ever. Chest X-rays provide a non-invasive (potentially bedside) tool to monitor the progression of the disease. In this study, we present a severity score prediction model for COVID-19 pneumonia for frontal chest X-ray images. Such a tool can gauge severity of COVID-19 lung infections (and pneumonia in general) that can be used for escalation or de-escalation of care as well as monitoring treatment efficacy, especially in the ICU. Methods: Images from a public COVID-19 database were scored retrospectively by three blinded experts in terms of the extent of lung involvement as well as the degree of opacity. A neural network model that was pre-trained on large (non-COVID-19) chest X-ray datasets is used to construct features for COVID-19 images which are predictive for our task. Results: This study finds that training a regression model on a subset of the outputs from an this pre-trained chest X-ray model predicts our geographic extent score (range 0-8) with 1.14 mean absolute error (MAE) and our lung opacity score (range 0-6) with 0.78 MAE. Conclusions: These results indicate that our model's ability to gauge severity of COVID-19 lung infections could be used for escalation or de-escalation of care as well as monitoring treatment efficacy, especially in the intensive care unit (ICU). A proper clinical trial is needed to evaluate efficacy. To enable this we make our code, labels, and data available online at https://github.com/mlmed/torchxrayvision/tree/master/scripts/covid-severity and https://github.com/ieee8023/covid-chestxray-dataset},
  archivePrefix = {arXiv},
  eprint = {2005.11856},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cohen et al_2020_predicting covid-19 pneumonia severity on chest x-ray with deep learning.pdf},
  journal = {arXiv:2005.11856 [cs, eess, q-bio, stat]},
  primaryClass = {cs, eess, q-bio, stat}
}

@article{collier18_ImplementingNeuralTuring,
  title = {Implementing {{Neural Turing Machines}}},
  author = {Collier, Mark and Beel, Joeran},
  year = {2018},
  month = jul,
  abstract = {Neural Turing Machines (NTMs) are an instance of Memory Augmented Neural Networks, a new class of recurrent neural networks which decouple computation from memory by introducing an external memory unit. NTMs have demonstrated superior performance over Long Short-Term Memory Cells in several sequence learning tasks. A number of open source implementations of NTMs exist but are unstable during training and/or fail to replicate the reported performance of NTMs. This paper presents the details of our successful implementation of a NTM. Our implementation learns to solve three sequential learning tasks from the original NTM paper. We find that the choice of memory contents initialization scheme is crucial in successfully implementing a NTM. Networks with memory contents initialized to small constant values converge on average 2 times faster than the next best memory contents initialization scheme.},
  archivePrefix = {arXiv},
  eprint = {1807.08518},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/collier et al_2018_implementing neural turing machines.pdf;/home/trung/Zotero/storage/F88QLQJR/1807.html},
  journal = {arXiv:1807.08518 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,memory,Statistics - Machine Learning,turing machine},
  primaryClass = {cs, stat}
}

@article{conesa19_Makingmultiomicsdata,
  title = {Making Multi-Omics Data Accessible to Researchers},
  author = {Conesa, Ana and Beck, Stephan},
  year = {2019},
  month = dec,
  volume = {6},
  pages = {251},
  issn = {2052-4463},
  doi = {10.1038/s41597-019-0258-4},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/Zotero/storage/IBZDYA2J/Conesa and Beck - 2019 - Making multi-omics data accessible to researchers.pdf},
  journal = {Scientific Data},
  language = {en},
  number = {1}
}

@article{conneau20_UnsupervisedCrosslingualRepresentation,
  title = {Unsupervised {{Cross}}-Lingual {{Representation Learning}} for {{Speech Recognition}}},
  author = {Conneau, Alexis and Baevski, Alexei and Collobert, Ronan and Mohamed, Abdelrahman and Auli, Michael},
  year = {2020},
  month = jun,
  abstract = {This paper presents XLSR which learns cross-lingual speech representations by pretraining a single model from the raw waveform of speech in multiple languages. We build on a concurrently introduced self-supervised model which is trained by solving a contrastive task over masked latent speech representations and jointly learns a quantization of the latents shared across languages. The resulting model is fine-tuned on labeled data and experiments show that cross-lingual pretraining significantly outperforms monolingual pretraining. On the CommonVoice benchmark, XLSR shows a relative phoneme error rate reduction of 72\% compared to the best known results. On BABEL, our approach improves word error rate by 16\% relative compared to the strongest comparable system. Our approach enables a single multilingual speech recognition model which is competitive to strong individual models. Analysis shows that the latent discrete speech representations are shared across languages with increased sharing for related languages.},
  archivePrefix = {arXiv},
  eprint = {2006.13979},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/conneau et al_2020_unsupervised cross-lingual representation learning for speech recognition.pdf},
  journal = {arXiv:2006.13979 [cs, eess]},
  primaryClass = {cs, eess}
}

@book{conway12_Machinelearninghackers,
  title = {Machine Learning for Hackers},
  author = {Conway, Drew and White, John Myles},
  year = {2012},
  edition = {1st ed},
  publisher = {{O'Reilly Media}},
  address = {{Sebastopol, CA}},
  annotation = {OCLC: ocn783384312},
  file = {/home/trung/GoogleDrive/Zotero/conway et al_2012_machine learning for hackers.pdf},
  isbn = {978-1-4493-0371-6},
  language = {en},
  lccn = {QA76.9.A43 C674 2012}
}

@book{coolen00_Beginnerguidemathematic,
  title = {A {{Beginner}} Guide to Mathematic of {{Neural Network}}},
  author = {Coolen, A. C. C.},
  abstract = {In this paper I try to describe both the role of mathematics in shaping our understanding of how neural networks operate, and the curious new mathematical concepts generated by our attempts to capture neural networks in equations. My target reader being the non-expert, I will present a biased selection of relatively simple examples of neural network tasks, models and calculations, rather than try to give a full encyclopedic},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/coolen_a beginner guide to mathematic of neural network.pdf;/home/trung/Zotero/storage/6H6J4ZIQ/summary.html}
}

@article{corces20_Singlecellepigenomicanalyses,
  title = {Single-Cell Epigenomic Analyses Implicate Candidate Causal Variants at Inherited Risk Loci for {{Alzheimer}}'s and {{Parkinson}}'s Diseases},
  author = {Corces, M. Ryan and Shcherbina, Anna and Kundu, Soumya and Gloudemans, Michael J. and Fr{\'e}sard, Laure and Granja, Jeffrey M. and Louie, Bryan H. and Eulalio, Tiffany and Shams, Shadi and Bagdatli, S. Tansu and Mumbach, Maxwell R. and Liu, Boxiang and Montine, Kathleen S. and Greenleaf, William J. and Kundaje, Anshul and Montgomery, Stephen B. and Chang, Howard Y. and Montine, Thomas J.},
  year = {2020},
  month = nov,
  volume = {52},
  pages = {1158--1168},
  issn = {1546-1718},
  doi = {10.1038/s41588-020-00721-x},
  abstract = {Genome-wide association studies of neurological diseases have identified thousands of variants associated with disease phenotypes. However, most of these variants do not alter coding sequences, making it difficult to assign their function. Here, we present a multi-omic epigenetic atlas of the adult human brain through profiling of single-cell chromatin accessibility landscapes and three-dimensional chromatin interactions of diverse adult brain regions across a cohort of cognitively healthy individuals. We developed a machine-learning classifier to integrate this multi-omic framework and predict dozens of functional SNPs for Alzheimer's and Parkinson's diseases, nominating target genes and cell types for previously orphaned loci from genome-wide association studies. Moreover, we dissected the complex inverted haplotype of the MAPT (encoding tau) Parkinson's disease risk locus, identifying putative ectopic regulatory interactions in neurons that may mediate this disease association. This work expands understanding of inherited variation and provides a roadmap for the epigenomic dissection of causal regulatory variation in disease.},
  file = {/home/trung/GoogleDrive/Zotero/corces et al_2020_single-cell epigenomic analyses implicate candidate causal variants at inherited risk loci for alzheimer’s and parkinson’s diseases.pdf},
  journal = {Nature Genetics},
  keywords = {_tablet},
  number = {11}
}

@article{cordonnier20_RelationshipSelfAttentionConvolutional,
  title = {On the {{Relationship}} between {{Self}}-{{Attention}} and {{Convolutional Layers}}},
  author = {Cordonnier, Jean-Baptiste and Loukas, Andreas and Jaggi, Martin},
  year = {2020},
  month = jan,
  abstract = {Recent trends of incorporating attention mechanisms in vision have led researchers to reconsider the supremacy of convolutional layers as a primary building block. Beyond helping CNNs to handle long-range dependencies, Ramachandran et al. (2019) showed that attention can completely replace convolution and achieve state-of-the-art performance on vision tasks. This raises the question: do learned attention layers operate similarly to convolutional layers? This work provides evidence that attention layers can perform convolution and, indeed, they often learn to do so in practice. Specifically, we prove that a multi-head self-attention layer with sufficient number of heads is at least as expressive as any convolutional layer. Our numerical experiments then show that the phenomenon also occurs in practice, corroborating our analysis. Our code is publicly available1.},
  archivePrefix = {arXiv},
  eprint = {1911.03584},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cordonnier et al_2020_on the relationship between self-attention and convolutional layers.pdf},
  journal = {arXiv:1911.03584 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{correia19_AdaptivelySparseTransformers,
  title = {Adaptively {{Sparse Transformers}}},
  author = {Correia, Gon{\c c}alo M. and Niculae, Vlad and Martins, Andr{\'e} F. T.},
  year = {2019},
  month = aug,
  abstract = {Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with \$\textbackslash alpha\$-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the \$\textbackslash alpha\$ parameter -- which controls the shape and sparsity of \$\textbackslash alpha\$-entmax -- allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1909.00015},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/correia et al_2019_adaptively sparse transformers.pdf;/home/trung/Zotero/storage/GHLQWI99/1909.html},
  journal = {arXiv:1909.00015 [cs, stat]},
  keywords = {attention,Computer Science - Computation and Language,sparse,Statistics - Machine Learning,transformer},
  primaryClass = {cs, stat}
}

@article{correia19_AdaptivelySparseTransformersa,
  title = {Adaptively {{Sparse Transformers}}},
  author = {Correia, Gon{\c c}alo M. and Niculae, Vlad and Martins, Andr{\'e} F. T.},
  year = {2019},
  month = sep,
  abstract = {Attention mechanisms have become ubiquitous in NLP. Recent architectures, notably the Transformer, learn powerful context-aware word representations through layered, multi-headed attention. The multiple heads learn diverse types of word relationships. However, with standard softmax attention, all attention heads are dense, assigning a non-zero weight to all context words. In this work, we introduce the adaptively sparse Transformer, wherein attention heads have flexible, context-dependent sparsity patterns. This sparsity is accomplished by replacing softmax with \$\textbackslash alpha\$-entmax: a differentiable generalization of softmax that allows low-scoring words to receive precisely zero weight. Moreover, we derive a method to automatically learn the \$\textbackslash alpha\$ parameter -- which controls the shape and sparsity of \$\textbackslash alpha\$-entmax -- allowing attention heads to choose between focused or spread-out behavior. Our adaptively sparse Transformer improves interpretability and head diversity when compared to softmax Transformers on machine translation datasets. Findings of the quantitative and qualitative analysis of our approach include that heads in different layers learn different sparsity preferences and tend to be more diverse in their attention distributions than softmax Transformers. Furthermore, at no cost in accuracy, sparsity in attention heads helps to uncover different head specializations.},
  archivePrefix = {arXiv},
  eprint = {1909.00015},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/correia et al_2019_adaptively sparse transformers2.pdf},
  journal = {arXiv:1909.00015 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{corro19_DifferentiableperturbandparseSemisupervised,
  title = {Differentiable Perturb-and-Parse: {{Semi}}-Supervised Parsing with a Structured Variational Autoencoder},
  booktitle = {International Conference on Learning Representations},
  author = {Corro, Caio and Titov, Ivan},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/corro et al_2019_differentiable perturb-and-parse.pdf}
}

@article{cotta20_UnsupervisedJointnode,
  title = {Unsupervised {{Joint}} \$k\$-Node {{Graph Representations}} with {{Compositional Energy}}-{{Based Models}}},
  author = {Cotta, Leonardo and Teixeira, Carlos H. C. and Swami, Ananthram and Ribeiro, Bruno},
  year = {2020},
  month = oct,
  abstract = {Existing Graph Neural Network (GNN) methods that learn inductive unsupervised graph representations focus on learning node and edge representations by predicting observed edges in the graph. Although such approaches have shown advances in downstream node classification tasks, they are ineffective in jointly representing larger k-node sets, k{$>$}2. We propose MHM-GNN, an inductive unsupervised graph representation approach that combines joint k-node representations with energy-based models (hypergraph Markov networks) and GNNs. To address the intractability of the loss that arises from this combination, we endow our optimization with a loss upper bound using a finite-sample unbiased Markov Chain Monte Carlo estimator. Our experiments show that the unsupervised joint k-node representations of MHM-GNN produce better unsupervised representations than existing approaches from the literature.},
  archivePrefix = {arXiv},
  eprint = {2010.04259},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cotta et al_2020_unsupervised joint $k$-node graph representations with compositional energy-based models.pdf},
  journal = {arXiv:2010.04259 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{courbariaux16_BinarizedNeuralNetworks,
  title = {Binarized {{Neural Networks}}: {{Training Deep Neural Networks}} with {{Weights}} and {{Activations Constrained}} to +1 or -1},
  shorttitle = {Binarized {{Neural Networks}}},
  author = {Courbariaux, Matthieu and Hubara, Itay and Soudry, Daniel and {El-Yaniv}, Ran and Bengio, Yoshua},
  year = {2016},
  month = mar,
  abstract = {We introduce a method to train Binarized Neural Networks (BNNs) - neural networks with binary weights and activations at run-time. At training-time the binary weights and activations are used for computing the parameters gradients. During the forward pass, BNNs drastically reduce memory size and accesses, and replace most arithmetic operations with bit-wise operations, which is expected to substantially improve power-efficiency. To validate the effectiveness of BNNs we conduct two sets of experiments on the Torch7 and Theano frameworks. On both, BNNs achieved nearly state-of-the-art results over the MNIST, CIFAR-10 and SVHN datasets. Last but not least, we wrote a binary matrix multiplication GPU kernel with which it is possible to run our MNIST BNN 7 times faster than with an unoptimized GPU kernel, without suffering any loss in classification accuracy. The code for training and running our BNNs is available on-line.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1602.02830},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/courbariaux et al_2016_binarized neural networks.pdf;/home/trung/Zotero/storage/F4P8F7A9/1602.html},
  journal = {arXiv:1602.02830 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{coussens01_InflammatoryCellsCancer,
  title = {Inflammatory {{Cells}} and {{Cancer}}: {{Think Different}}!},
  shorttitle = {Inflammatory {{Cells}} and {{Cancer}}},
  author = {Coussens, Lisa M. and Werb, Zena},
  year = {2001},
  month = mar,
  volume = {193},
  pages = {F23-F26},
  issn = {0022-1007},
  doi = {10.1084/jem.193.6.F23},
  abstract = {It is well established that cancer is a progressive disease, occurring in a series of well-defined steps, typically arising as a consequence of activating mutations (oncogenes) or deactivating mutations (tumor suppressor genes) in proliferating cells. From studies exploiting cultured tumor cells, two-stage carcinogenesis protocols in mice, and transgenic models of tumorigenesis, it is now evident that a single mutagenic event does not result in formation of a malignant tumor 1. Additional genetic and epigenetic events are necessary for progression to the tumor state. Initiated cells therefore require alterations rendering them self-sufficient for growth, insensitive to growth-inhibitory signals, resistant to programs of terminal differentiation, senescence, or apoptosis, as well as endowing them with unlimited self-renewal capacity, the ability to orchestrate and direct sustained angiogenesis, and the ability to invade and thrive in ectopic tissue environments 1. In this issue, Lin et al. 2 report that CSF-1 expression is a critical factor in a transgenic mouse model of mammary cancer development. This study provides compelling data that one subset of inflammatory cells, macrophages, and the dynamic microenvironment in which they live facilitate malignant outgrowth and eventual metastatic spread of evolving neoplastic cells.},
  file = {/home/trung/GoogleDrive/Zotero/coussens et al_2001_inflammatory cells and cancer.pdf},
  journal = {Journal of Experimental Medicine},
  number = {6}
}

@article{cover00_ElementsInformationTheorya,
  title = {Elements of {{Information Theory Second Edition}} ({{Solutions}})},
  author = {Cover, Thomas M and Thomas, Joy A},
  pages = {413},
  file = {/home/trung/GoogleDrive/Zotero/cover et al_elements of information theory second edition (solutions).pdf},
  language = {en}
}

@book{cover06_Elementsinformationtheory,
  title = {Elements of Information Theory (Wiley Series in Telecommunications and Signal Processing)},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2006},
  publisher = {{Wiley-Interscience}},
  address = {{USA}},
  file = {/home/trung/GoogleDrive/Zotero/cover et al_2006_elements of information theory (wiley series in telecommunications and signal processing).pdf},
  isbn = {0-471-24195-4},
  keywords = {_tablet}
}

@article{crabbe00_LearningoutsideBlackBox,
  title = {Learning Outside the {{Black}}-{{Box}}: {{The}} Pursuit of Interpretable Models},
  author = {Crabbe, Jonathan and Zame, William R and Zhang, Yao},
  pages = {12},
  abstract = {Machine Learning has proved its ability to produce accurate models \textendash{} but the deployment of these models outside the machine learning community has been hindered by the difficulties of interpreting these models. This paper proposes an algorithm that produces a continuous global interpretation of any given continuous black-box function. Our algorithm employs a variation of projection pursuit in which the ridge functions are chosen to be Meijer G-functions, rather than the usual polynomial splines. Because Meijer G-functions are differentiable in their parameters, we can ``tune'' the parameters of the representation by gradient descent; as a consequence, our algorithm is efficient. Using five familiar data sets from the UCI repository and two familiar machine learning algorithms, we demonstrate that our algorithm produces global interpretations that are both highly accurate and parsimonious (involve a small number of terms). Our interpretations permit easy understanding of the relative importance of features and feature interactions. Our interpretation algorithm represents a leap forward from the previous state of the art.},
  file = {/home/trung/GoogleDrive/Zotero/crabbe et al_learning outside the black-box.pdf},
  keywords = {_tablet,favorite,representation},
  language = {en}
}

@article{creager19_FlexiblyFairRepresentation,
  title = {Flexibly {{Fair Representation Learning}} by {{Disentanglement}}},
  author = {Creager, Elliot and Madras, David and Jacobsen, J{\"o}rn-Henrik and Weis, Marissa A. and Swersky, Kevin and Pitassi, Toniann and Zemel, Richard},
  year = {2019},
  month = jun,
  abstract = {We consider the problem of learning representations that achieve group and subgroup fairness with respect to multiple sensitive attributes. Taking inspiration from the disentangled representation learning literature, we propose an algorithm for learning compact representations of datasets that are useful for reconstruction and prediction, but are also \textbackslash emph\{flexibly fair\}, meaning they can be easily modified at test time to achieve subgroup demographic parity with respect to multiple sensitive attributes and their conjunctions. We show empirically that the resulting encoder---which does not require the sensitive attributes for inference---enables the adaptation of a single representation to a variety of fair classification tasks with new target labels and subgroup definitions.},
  annotation = {ZSCC: 0000005},
  archivePrefix = {arXiv},
  eprint = {1906.02589},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/creager et al_2019_flexibly fair representation learning by disentanglement.pdf;/home/trung/Zotero/storage/HSHT3HCP/1906.html},
  journal = {arXiv:1906.02589 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{crescimanna19_variationalinfomaxautoencoder,
  title = {The Variational Infomax Autoencoder},
  author = {Crescimanna, Vincenzo and Graham, Bruce},
  year = {2019},
  month = may,
  abstract = {We propose the Variational InfoMax AutoEncoder (VIMAE), a method to train a generative model, maximizing the variational lower bound of the mutual information between the visible data and the hidden representation, maintaining bounded the capacity of the network. In the paper we investigate the capacity role in a neural network and deduce that a small capacity network tends to learn a more robust and disentangled representation than an high capacity one. Such observations are confirmed by the computational experiments.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1905.10549},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/crescimanna et al_2019_the variational infomax autoencoder.pdf;/home/trung/GoogleDrive/Zotero/crescimanna et al_2019_the variational infomax autoencoder2.pdf},
  journal = {arXiv:1905.10549 [cs, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, stat}
}

@article{creswell16_InvertingGeneratorGenerative,
  title = {Inverting {{The Generator Of A Generative Adversarial Network}}},
  author = {Creswell, Antonia and Bharath, Anil Anthony},
  year = {2016},
  month = nov,
  abstract = {Generative adversarial networks (GANs) learn to synthesise new samples from a high-dimensional distribution by passing samples drawn from a latent space through a generative network. When the high-dimensional distribution describes images of a particular data set, the network should learn to generate visually similar image samples for latent variables that are close to each other in the latent space. For tasks such as image retrieval and image classification, it may be useful to exploit the arrangement of the latent space by projecting images into it, and using this as a representation for discriminative tasks. GANs often consist of multiple layers of non-linear computations, making them very difficult to invert. This paper introduces techniques for projecting image samples into the latent space using any pre-trained GAN, provided that the computational graph is available. We evaluate these techniques on both MNIST digits and Omniglot handwritten characters. In the case of MNIST digits, we show that projections into the latent space maintain information about the style and the identity of the digit. In the case of Omniglot characters, we show that even characters from alphabets that have not been seen during training may be projected well into the latent space; this suggests that this approach may have applications in one-shot learning.},
  archivePrefix = {arXiv},
  eprint = {1611.05644},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/A3R8KX6C/Creswell and Bharath - 2016 - Inverting The Generator Of A Generative Adversaria.pdf},
  journal = {arXiv:1611.05644 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{creswell18_AdversarialInformationFactorization,
  title = {Adversarial {{Information Factorization}}},
  author = {Creswell, Antonia and Mohamied, Yumnah and Sengupta, Biswa and Bharath, Anil A.},
  year = {2018},
  month = sep,
  abstract = {We propose a novel generative model architecture designed to learn representations for images that factor out a single attribute from the rest of the representation. A single object may have many attributes which when altered do not change the identity of the object itself. Consider the human face; the identity of a particular person is independent of whether or not they happen to be wearing glasses. The attribute of wearing glasses can be changed without changing the identity of the person. However, the ability to manipulate and alter image attributes without altering the object identity is not a trivial task. Here, we are interested in learning a representation of the image that separates the identity of an object (such as a human face) from an attribute (such as `wearing glasses'). We demonstrate the success of our factorization approach by using the learned representation to synthesize the same face with and without a chosen attribute. We refer to this specific synthesis process as image attribute manipulation. We further demonstrate that our model achieves competitive scores, with state of the art, on a facial attribute classification task.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1711.05175},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/creswell et al_2018_adversarial information factorization.pdf},
  journal = {arXiv:1711.05175 [cs]},
  keywords = {information},
  language = {en},
  primaryClass = {cs}
}

@article{creswell18_GenerativeAdversarialNetworks,
  title = {Generative {{Adversarial Networks}}: {{An Overview}}},
  shorttitle = {Generative {{Adversarial Networks}}},
  author = {Creswell, Antonia and White, Tom and Dumoulin, Vincent and Arulkumaran, Kai and Sengupta, Biswa and Bharath, Anil A.},
  year = {2018},
  month = jan,
  volume = {35},
  pages = {53--65},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2765202},
  abstract = {Generative adversarial networks (GANs) provide a way to learn deep representations without extensively annotated training data. They achieve this through deriving backpropagation signals through a competitive process involving a pair of networks. The representations that can be learned by GANs may be used in a variety of applications, including image synthesis, semantic image editing, style transfer, image super-resolution and classification. The aim of this review paper is to provide an overview of GANs for the signal processing community, drawing on familiar analogies and concepts where possible. In addition to identifying different methods for training and constructing GANs, we also point to remaining challenges in their theory and application.},
  annotation = {ZSCC: 0000194},
  archivePrefix = {arXiv},
  eprint = {1710.07035},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/creswell et al_2018_generative adversarial networks.pdf;/home/trung/Zotero/storage/4V2BWMQ3/1710.html},
  journal = {IEEE Signal Processing Magazine},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,gan,overview},
  number = {1}
}

@article{csordas00_AreNeuralNets,
  title = {Are {{Neural Nets Modular}}? {{Inspecting Their Functionality Through Differentiable Weight Masks}}},
  author = {Csord{\'a}s, R{\'o}bert},
  pages = {15},
  abstract = {Neural networks (NNs) whose subnetworks implement reusable functions are expected to offer numerous advantages, e.g., compositionality through efficient recombination of functional building blocks, interpretability, preventing catastrophic interference by separation, etc. Understanding if and how NNs are modular could provide insights into how to improve them. Current inspection methods, however, fail to link modules to their function. We present a novel method based on learning binary weight masks to identify individual weights and subnets responsible for specific functions. This powerful tool shows that typical NNs fail to reuse submodules, becoming redundant instead. It also yields new insights into known generalization issues with the SCAN dataset. Our method also unveils class-specific weights of CNN classifiers, and shows to which extent classifications depend on them. Our findings open many new important directions for future research.},
  file = {/home/trung/GoogleDrive/Zotero/csordás_are neural nets modular.pdf},
  language = {en}
}

@article{cubuk00_AutoAugmentLearningAugmentation,
  title = {{{AutoAugment}}: {{Learning Augmentation Strategies From Data}}},
  author = {Cubuk, Ekin D and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V},
  pages = {11},
  abstract = {Data augmentation is an effective technique for improving the accuracy of modern image classifiers. However, current data augmentation implementations are manually designed. In this paper, we describe a simple procedure called AutoAugment to automatically search for improved data augmentation policies. In our implementation, we have designed a search space where a policy consists of many subpolicies, one of which is randomly chosen for each image in each mini-batch. A sub-policy consists of two operations, each operation being an image processing function such as translation, rotation, or shearing, and the probabilities and magnitudes with which the functions are applied. We use a search algorithm to find the best policy such that the neural network yields the highest validation accuracy on a target dataset. Our method achieves state-of-the-art accuracy on CIFAR-10, CIFAR-100, SVHN, and ImageNet (without additional data). On ImageNet, we attain a Top-1 accuracy of 83.5\% which is 0.4\% better than the previous record of 83.1\%. On CIFAR-10, we achieve an error rate of 1.5\%, which is 0.6\% better than the previous state-of-theart. Augmentation policies we find are transferable between datasets. The policy learned on ImageNet transfers well to achieve significant improvements on other datasets, such as Oxford Flowers, Caltech-101, Oxford-IIT Pets, FGVC Aircraft, and Stanford Cars.},
  file = {/home/trung/GoogleDrive/Zotero/cubuk et al_autoaugment.pdf},
  keywords = {data augmentation},
  language = {en}
}

@article{cubuk19_RandAugmentPracticalautomated,
  title = {{{RandAugment}}: {{Practical}} Automated Data Augmentation with a Reduced Search Space},
  shorttitle = {{{RandAugment}}},
  author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0\% accuracy, a 0.6\% increase over the previous state-of-the-art and 1.0\% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3\% improvement over baseline augmentation, and is within 0.3\% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1909.13719},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cubuk et al_2019_randaugment.pdf;/home/trung/Zotero/storage/PSKRRIZ9/1909.html},
  journal = {arXiv:1909.13719 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,sem supervised},
  primaryClass = {cs}
}

@article{cui19_AcceleratingMonteCarlo,
  title = {Accelerating {{Monte Carlo Bayesian Inference}} via {{Approximating Predictive Uncertainty}} over {{Simplex}}},
  author = {Cui, Yufei and Yao, Wuguannan and Li, Qiao and Chan, Antoni B. and Xue, Chun Jason},
  year = {2019},
  month = sep,
  abstract = {Estimating the predictive uncertainty of a Bayesian learning model is critical in various decision-making problems, e.g., reinforcement learning, detecting adversarial attack, self-driving car. As the model posterior is almost always intractable, most efforts were made on finding an accurate approximation the true posterior. Even though a decent estimation of the model posterior is obtained, another approximation is required to compute the predictive distribution over the desired output. A common accurate solution is to use Monte Carlo (MC) integration. However, it needs to maintain a large number of samples, evaluate the model repeatedly and average multiple model outputs. In many real-world cases, this is computationally prohibitive. In this work, assuming that the exact posterior or a decent approximation is obtained, we propose a generic framework to approximate the output probability distribution induced by model posterior with a parameterized model and in an amortized fashion. The aim is to approximate the true uncertainty of a specific Bayesian model, meanwhile alleviating the heavy workload of MC integration at testing time. The proposed method is universally applicable to Bayesian classification models that allow for posterior sampling. Theoretically, we show that the idea of amortization incurs no additional costs on approximation performance. Empirical results validate the strong practical performance of our approach.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1905.12194},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/cui et al_2019_accelerating monte carlo bayesian inference via approximating predictive uncertainty over simplex.pdf;/home/trung/Zotero/storage/V3H5Y8JW/1905.html},
  journal = {arXiv:1905.12194 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{cusanovich18_SingleCellAtlasVivo,
  title = {A {{Single}}-{{Cell Atlas}} of {{In~Vivo Mammalian Chromatin Accessibility}}},
  author = {Cusanovich, Darren A. and Hill, Andrew J. and Aghamirzaie, Delasa and Daza, Riza M. and Pliner, Hannah A. and Berletch, Joel B. and Filippova, Galina N. and Huang, Xingfan and Christiansen, Lena and DeWitt, William S. and Lee, Choli and Regalado, Samuel G. and Read, David F. and Steemers, Frank J. and Disteche, Christine M. and Trapnell, Cole and Shendure, Jay},
  year = {2018},
  month = aug,
  volume = {174},
  pages = {1309-1324.e18},
  publisher = {{Elsevier}},
  issn = {0092-8674},
  doi = {10.1016/j.cell.2018.06.052},
  file = {/home/trung/GoogleDrive/Zotero/cusanovich et al_2018_a single-cell atlas of in vivo mammalian chromatin accessibility.pdf},
  journal = {Cell},
  number = {5}
}

@article{dai00_WhenVariationalAutoencoders,
  title = {When {{Do Variational Autoencoders Know What They Don}}'t {{Know}}?},
  author = {Dai, Bin and Wipf, David},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_when do variational autoencoders know what they don't know.pdf}
}

@article{dai13_MultivariateBernoullidistribution,
  title = {Multivariate {{Bernoulli}} Distribution},
  author = {Dai, Bin and Ding, Shilin and Wahba, Grace},
  year = {2013},
  month = sep,
  volume = {19},
  pages = {1465--1483},
  issn = {1350-7265},
  doi = {10.3150/12-BEJSP10},
  abstract = {In this paper, we consider the multivariate Bernoulli distribution as a model to estimate the structure of graphs with binary nodes. This distribution is discussed in the framework of the exponential family, and its statistical properties regarding independence of the nodes are demonstrated. Importantly the model can estimate not only the main effects and pairwise interactions among the nodes but also is capable of modeling higher order interactions, allowing for the existence of complex clique effects. We compare the multivariate Bernoulli model with existing graphical inference models - the Ising model and the multivariate Gaussian model, where only the pairwise interactions are considered. On the other hand, the multivariate Bernoulli distribution has an interesting property in that independence and uncorrelatedness of the component random variables are equivalent. Both the marginal and conditional distributions of a subset of variables in the multivariate Bernoulli distribution still follow the multivariate Bernoulli distribution. Furthermore, the multivariate Bernoulli logistic model is developed under generalized linear model theory by utilizing the canonical link function in order to include covariate information on the nodes, edges and cliques. We also consider variable selection techniques such as LASSO in the logistic model to impose sparsity structure on the graph. Finally, we discuss extending the smoothing spline ANOVA approach to the multivariate Bernoulli logistic model to enable estimation of non-linear effects of the predictor variables.},
  archivePrefix = {arXiv},
  eprint = {1206.1874},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2013_multivariate bernoulli distribution.pdf},
  journal = {Bernoulli},
  number = {4}
}

@article{dai15_SemisupervisedSequenceLearning,
  title = {Semi-Supervised {{Sequence Learning}}},
  author = {Dai, Andrew M. and Le, Quoc V.},
  year = {2015},
  month = nov,
  abstract = {We present two approaches that use unlabeled data to improve sequence learning with recurrent networks. The first approach is to predict what comes next in a sequence, which is a conventional language model in natural language processing. The second approach is to use a sequence autoencoder, which reads the input sequence into a vector and predicts the input sequence again. These two algorithms can be used as a "pretraining" step for a later supervised sequence learning algorithm. In other words, the parameters obtained from the unsupervised step can be used as a starting point for other supervised training models. In our experiments, we find that long short term memory recurrent networks after being pretrained with the two approaches are more stable and generalize better. With pretraining, we are able to train long short term memory recurrent networks up to a few hundred timesteps, thereby achieving strong performance in many text classification tasks, such as IMDB, DBpedia and 20 Newsgroups.},
  annotation = {ZSCC: 0000480},
  archivePrefix = {arXiv},
  eprint = {1511.01432},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2015_semi-supervised sequence learning.pdf;/home/trung/Zotero/storage/CP3BD2RN/1511.html},
  journal = {arXiv:1511.01432 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{dai18_CompressingNeuralNetworks,
  title = {Compressing {{Neural Networks}} Using the {{Variational Information Bottleneck}}},
  author = {Dai, Bin and Zhu, Chen and Wipf, David},
  year = {2018},
  month = apr,
  abstract = {Neural networks can be compressed to reduce memory and computational requirements, or to increase accuracy by facilitating the use of a larger base architecture. In this paper we focus on pruning individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory. To improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound. Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved. In contrast, the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework, providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape. We demonstrate state-of-the-art compression rates across an array of datasets and network architectures.},
  annotation = {ZSCC: 0000031},
  archivePrefix = {arXiv},
  eprint = {1802.10399},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2018_compressing neural networks using the variational information bottleneck2.pdf;/home/trung/Zotero/storage/GM7F4NAR/1802.html},
  journal = {arXiv:1802.10399 [cs]},
  keywords = {bottleneck,compression,Computer Science - Computer Vision and Pattern Recognition,information,variational},
  primaryClass = {cs}
}

@article{dai18_CompressingNeuralNetworksa,
  title = {Compressing {{Neural Networks}} Using the {{Variational Information Bottleneck}}},
  author = {Dai, Bin and Zhu, Chen and Guo, Baining and Wipf, David},
  year = {2018},
  pages = {10},
  abstract = {Neural networks can be compressed to reduce memory and computational requirements, or to increase accuracy by facilitating the use of a larger base architecture. In this paper we focus on pruning individual neurons, which can simultaneously trim model size, FLOPs, and run-time memory. To improve upon the performance of existing compression algorithms we utilize the information bottleneck principle instantiated via a tractable variational bound. Minimization of this information theoretic bound reduces the redundancy between adjacent layers by aggregating useful information into a subset of neurons that can be preserved. In contrast, the activations of disposable neurons are shut off via an attractive form of sparse regularization that emerges naturally from this framework, providing tangible advantages over traditional sparsity penalties without contributing additional tuning parameters to the energy landscape. We demonstrate state-of-theart compression rates across an array of datasets and network architectures.},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2018_compressing neural networks using the variational information bottleneck.pdf},
  keywords = {information},
  language = {en}
}

@article{dai18_ConnectionsrobustPCA,
  title = {Connections with Robust {{PCA}} and the Role of Emergent Sparsity in Variational Autoencoder Models},
  author = {Dai, Bin and Wang, Yu and Aston, John and Hua, Gang and Wipf, David},
  year = {2018},
  volume = {19},
  pages = {1--42},
  file = {/home/trung/GoogleDrive/Zotero/false},
  journal = {Journal of Machine Learning Research},
  keywords = {disentanglement},
  number = {41}
}

@article{dai19_DiagnosingEnhancingVAE,
  title = {Diagnosing and {{Enhancing VAE Models}}},
  author = {Dai, Bin and Wipf, David},
  year = {2019},
  month = mar,
  abstract = {Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at https://github.com/daib13/ TwoStageVAE.},
  annotation = {ZSCC: 0000014},
  archivePrefix = {arXiv},
  eprint = {1903.05789},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2019_diagnosing and enhancing vae models.pdf},
  journal = {arXiv:1903.05789 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,disentanglement,favorite,Statistics - Machine Learning,vae_issues,variational},
  primaryClass = {cs, stat}
}

@article{dai19_DiagnosingEnhancingVAEa,
  title = {Diagnosing and {{Enhancing VAE Models}}},
  author = {Dai, Bin and Wipf, David},
  year = {2019},
  month = oct,
  abstract = {Although variational autoencoders (VAEs) represent a widely influential deep generative model, many aspects of the underlying energy function remain poorly understood. In particular, it is commonly believed that Gaussian encoder/decoder assumptions reduce the effectiveness of VAEs in generating realistic samples. In this regard, we rigorously analyze the VAE objective, differentiating situations where this belief is and is not actually true. We then leverage the corresponding insights to develop a simple VAE enhancement that requires no additional hyperparameters or sensitive tuning. Quantitatively, this proposal produces crisp samples and stable FID scores that are actually competitive with a variety of GAN models, all while retaining desirable attributes of the original VAE architecture. A shorter version of this work will appear in the ICLR 2019 conference proceedings (Dai and Wipf, 2019). The code for our model is available at https://github.com/daib13/ TwoStageVAE.},
  archivePrefix = {arXiv},
  eprint = {1903.05789},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2019_diagnosing and enhancing vae models2.pdf},
  journal = {arXiv:1903.05789 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dai19_HiddenTalentsVariational,
  title = {Hidden {{Talents}} of the {{Variational Autoencoder}}},
  author = {Dai, Bin and Wang, Yu and Aston, John and Hua, Gang and Wipf, David},
  year = {2019},
  month = oct,
  abstract = {Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an information-theoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1706.05148},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2019_hidden talents of the variational autoencoder.pdf;/home/trung/Zotero/storage/LGHK7CST/1706.html},
  journal = {arXiv:1706.05148 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{dai19_HiddenTalentsVariationala,
  title = {Hidden {{Talents}} of the {{Variational Autoencoder}}},
  author = {Dai, Bin and Wang, Yu and Aston, John and Hua, Gang and Wipf, David},
  year = {2019},
  month = oct,
  abstract = {Variational autoencoders (VAE) represent a popular, flexible form of deep generative model that can be stochastically fit to samples from a given random process using an informationtheoretic variational bound on the true underlying distribution. Once so-obtained, the model can be putatively used to generate new samples from this distribution, or to provide a low-dimensional latent representation of existing samples. While quite effective in numerous application domains, certain important mechanisms which govern the behavior of the VAE are obfuscated by the intractable integrals and resulting stochastic approximations involved. Moreover, as a highly non-convex model, it remains unclear exactly how minima of the underlying energy relate to original design purposes. We attempt to better quantify these issues by analyzing a series of tractable special cases of increasing complexity. In doing so, we unveil interesting connections with more traditional dimensionality reduction models, as well as an intrinsic yet underappreciated propensity for robustly dismissing sparse outliers when estimating latent manifolds. With respect to the latter, we demonstrate that the VAE can be viewed as the natural evolution of recent robust PCA models, capable of learning nonlinear manifolds of unknown dimension obscured by gross corruptions. A version of this work has appeared in the Journal of Machine Learning Research (JMLR) [13]; however, we include several small updates here.},
  archivePrefix = {arXiv},
  eprint = {1706.05148},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2019_hidden talents of the variational autoencoder2.pdf},
  journal = {arXiv:1706.05148 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{dai19_UsualSuspectsReassessing,
  title = {The {{Usual Suspects}}? {{Reassessing Blame}} for {{VAE Posterior Collapse}}},
  shorttitle = {The {{Usual Suspects}}?},
  author = {Dai, Bin and Wang, Ziyu and Wipf, David},
  year = {2019},
  month = dec,
  abstract = {In narrow asymptotic settings Gaussian VAE models of continuous data have been shown to possess global optima aligned with ground-truth distributions. Even so, it is well known that poor solutions whereby the latent posterior collapses to an uninformative prior are sometimes obtained in practice. However, contrary to conventional wisdom that largely assigns blame for this phenomena on the undue influence of KL-divergence regularization, we will argue that posterior collapse is, at least in part, a direct consequence of bad local minima inherent to the loss surface of deep autoencoder networks. In particular, we prove that even small nonlinear perturbations of affine VAE decoder models can produce such minima, and in deeper models, analogous minima can force the VAE to behave like an aggressive truncation operator, provably discarding information along all latent dimensions in certain circumstances. Regardless, the underlying message here is not meant to undercut valuable existing explanations of posterior collapse, but rather, to refine the discussion and elucidate alternative risk factors that may have been previously underappreciated.},
  archivePrefix = {arXiv},
  eprint = {1912.10702},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2019_the usual suspects.pdf},
  journal = {arXiv:1912.10702 [cs, stat]},
  keywords = {vae_issues},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dai20_LearningTaskorientedDisentangled,
  title = {Learning {{Task}}-Oriented {{Disentangled Representations}} for {{Unsupervised Domain Adaptation}}},
  author = {Dai, Pingyang and Chen, Peixian and Wu, Qiong and Hong, Xiaopeng and Ye, Qixiang and Tian, Qi and Ji, Rongrong},
  year = {2020},
  month = jul,
  abstract = {Unsupervised domain adaptation (UDA) aims to address the domain-shift problem between a labeled source domain and an unlabeled target domain. Many efforts have been made to address the mismatch between the distributions of training and testing data, but unfortunately, they ignore the task-oriented information across domains and are inflexible to perform well in complicated open-set scenarios. Many efforts have been made to eliminate the mismatch between the distributions of training and testing data by learning domain-invariant representations. However, the learned representations are usually not task-oriented, i.e., being class-discriminative and domain-transferable simultaneously. This drawback limits the flexibility of UDA in complicated open-set tasks where no labels are shared between domains. In this paper, we break the concept of task-orientation into task-relevance and task-irrelevance, and propose a dynamic task-oriented disentangling network (DTDN) to learn disentangled representations in an end-to-end fashion for UDA. The dynamic disentangling network effectively disentangles data representations into two components: the task-relevant ones embedding critical information associated with the task across domains, and the task-irrelevant ones with the remaining non-transferable or disturbing information. These two components are regularized by a group of task-specific objective functions across domains. Such regularization explicitly encourages disentangling and avoids the use of generative models or decoders. Experiments in complicated, open-set scenarios (retrieval tasks) and empirical benchmarks (classification tasks) demonstrate that the proposed method captures rich disentangled information and achieves superior performance.},
  archivePrefix = {arXiv},
  eprint = {2007.13264},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dai et al_2020_learning task-oriented disentangled representations for unsupervised domain adaptation.pdf},
  journal = {arXiv:2007.13264 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{damevski18_PredictingFutureDeveloper,
  title = {Predicting {{Future Developer Behavior}} in the {{IDE Using Topic Models}}},
  author = {Damevski, Kostadin and Chen, Hui and Shepherd, David C. and Kraft, Nicholas A. and Pollock, Lori},
  year = {2018},
  month = nov,
  volume = {44},
  pages = {1100--1111},
  issn = {0098-5589, 1939-3520, 2326-3881},
  doi = {10.1109/TSE.2017.2748134},
  abstract = {While early software command recommender systems drew negative user reaction, recent studies show that users of unusually complex applications will accept and utilize command recommendations. Given this new interest, more than a decade after first attempts, both the recommendation generation (backend) and the user experience (frontend) should be revisited. In this work, we focus on recommendation generation. One shortcoming of existing command recommenders is that algorithms focus primarily on mirroring the short-term past \textemdash{} i.e., assuming that a developer who is currently debugging will continue to debug endlessly. We propose an approach to improve on the state of the art by modeling future task context to make better recommendations to developers. That is, the approach can predict that a developer who is currently debugging may continue to debug OR may edit their program. To predict future development commands, we applied Temporal Latent Dirichlet Allocation, a topic model used primarily for natural language, to software development interaction data (i.e., command streams). We evaluated this approach on two large interaction datasets for two different IDEs, Microsoft Visual Studio and ABB Robot Studio. Our evaluation shows that this is a promising approach for both predicting future IDE commands and producing empirically-interpretable observations.},
  file = {/home/trung/GoogleDrive/Zotero/damevski et al_2018_predicting future developer behavior in the ide using topic models.pdf},
  journal = {IEEE Transactions on Software Engineering},
  language = {en},
  number = {11}
}

@article{damianou12_DeepGaussianProcesses,
  title = {Deep {{Gaussian Processes}}},
  author = {Damianou, Andreas C. and Lawrence, Neil D.},
  year = {2012},
  month = nov,
  abstract = {In this paper we introduce deep Gaussian process (GP) models. Deep GPs are a deep belief network based on Gaussian process mappings. The data is modeled as the output of a multivariate GP. The inputs to that Gaussian process are then governed by another GP. A single layer model is equivalent to a standard GP or the GP latent variable model (GP-LVM). We perform inference in the model by approximate variational marginalization. This results in a strict lower bound on the marginal likelihood of the model which we use for model selection (number of layers and nodes per layer). Deep belief networks are typically applied to relatively large data sets using stochastic gradient descent for optimization. Our fully Bayesian treatment allows for the application of deep models even when data is scarce. Model selection by our variational bound shows that a five layer hierarchy is justified even when modelling a digit data set containing only 150 examples.},
  archivePrefix = {arXiv},
  eprint = {1211.0358},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/damianou et al_2012_deep gaussian processes.pdf;/home/trung/Zotero/storage/I643IPWM/1211.html},
  journal = {arXiv:1211.0358 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Probability,Statistics - Machine Learning,variational},
  primaryClass = {cs, math, stat}
}

@article{damour20_UnderspecificationPresentsChallenges,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  year = {2020},
  month = nov,
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archivePrefix = {arXiv},
  eprint = {2011.03395},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/d'amour et al_2020_underspecification presents challenges for credibility in modern machine learning.pdf},
  journal = {arXiv:2011.03395 [cs, stat]},
  primaryClass = {cs, stat}
}

@book{dangeti17_StatisticsMachineLearning,
  title = {Statistics for {{Machine Learning}}},
  author = {Dangeti, Pratap},
  year = {2017},
  annotation = {ZSCC: 0000062  OCLC: 1015996007},
  file = {/home/trung/GoogleDrive/Zotero/dangeti_2017_statistics for machine learning.pdf},
  isbn = {978-1-78829-575-8},
  language = {en}
}

@article{darbinyan00_Rhodiolaroseastress,
  title = {Rhodiola Rosea in Stress Induced Fatigue \textemdash{} {{A}} Double Blind Cross-over Study of a Standardized Extract {{SHR}}-5 with a Repeated Low-Dose Regimen on the Mental Performance of Healthy Physicians during Night Duty},
  author = {Darbinyan, V. and Kteyan, A. and Panossian, A. and Gabrielian, E. and Wikman, G. and Wagner, H.},
  year = {2000},
  month = oct,
  volume = {7},
  pages = {365--371},
  issn = {09447113},
  doi = {10.1016/S0944-7113(00)80055-0},
  abstract = {The aim of this study was to investigate the effect of repeated low-dose treatment with a standardized extract SHR/5 of rhizome Rhodiola rosea L, (RRE) on fatigue during night duty among a group of 56 young, healthy physicians. The effect was measured as total mental performance calculated as Fatigue Index. The tests chosen reflect an overall level of mental fatigue, involving complex perceptive and cognitive cerebral functions, such as associative thinking, short-term memory, calculation and ability of concentration, and speed of audio-visual perception. These parameters were tested before and after night duty during three periods of two weeks each: a) a test period of one RRE/placebo tablet daily, b) a washout period and c) a third period of one placebo/RRE tablet daily, in a double-blind cross-over trial. The perceptive and cognitive cerebral functions mentioned above were investigated using 5 different tests. A statistically significant improvement in these tests was observed in the treatment group (RRE) during the first two weeks period. No side-effects were reported for either treatment noted. These results suggest that RRE can reduce general fatigue under certain stressful conditions.},
  file = {/home/trung/GoogleDrive/Zotero/darbinyan et al_2000_rhodiola rosea in stress induced fatigue — a double blind cross-over study of a standardized extract shr-5 with a repeated low-dose regimen on the mental performance of healthy physicians during.pdf},
  journal = {Phytomedicine},
  language = {en},
  number = {5}
}

@inproceedings{das15_GaussianLDATopic,
  title = {Gaussian {{LDA}} for {{Topic Models}} with {{Word Embeddings}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Das, Rajarshi and Zaheer, Manzil and Dyer, Chris},
  year = {2015},
  pages = {795--804},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.3115/v1/P15-1077},
  abstract = {Continuous space word embeddings learned from large, unstructured corpora have been shown to be effective at capturing semantic regularities in language. In this paper we replace LDA's parameterization of ``topics'' as categorical distributions over opaque word types with multivariate Gaussian distributions on the embedding space. This encourages the model to group words that are a priori known to be semantically related into topics. To perform inference, we introduce a fast collapsed Gibbs sampling algorithm based on Cholesky decompositions of covariance matrices of the posterior predictive distributions. We further derive a scalable algorithm that draws samples from stale posterior predictive distributions and corrects them with a Metropolis\textendash Hastings step. Using vectors learned from a domain-general corpus (English Wikipedia), we report results on two document collections (20-newsgroups and NIPS). Qualitatively, Gaussian LDA infers different (but still very sensible) topics relative to standard LDA. Quantitatively, our technique outperforms existing models at dealing with OOV words in held-out documents.},
  file = {/home/trung/GoogleDrive/Zotero/das et al_2015_gaussian lda for topic models with word embeddings.pdf},
  language = {en}
}

@article{das19_LikelihoodContributionbased,
  title = {Likelihood {{Contribution}} Based {{Multi}}-Scale {{Architecture}} for {{Generative Flows}}},
  author = {Das, Hari Prasanna and Abbeel, Pieter and Spanos, Costas J.},
  year = {2019},
  month = sep,
  abstract = {Deep generative modeling using flows has gained popularity owing to the tractable exact log-likelihood estimation with efficient training and synthesis process. However, flow models suffer from the challenge of having high dimensional latent space, same in dimension as the input space. An effective solution to the above challenge as proposed by Dinh et al. (2016) is a multi-scale architecture, which is based on iterative early factorization of a part of the total dimensions at regular intervals. Prior works on generative flows involving a multi-scale architecture perform the dimension factorization based on a static masking. We propose a novel multi-scale architecture that performs data dependent factorization to decide which dimensions should pass through more flow layers. To facilitate the same, we introduce a heuristic based on the contribution of each dimension to the total log-likelihood which encodes the importance of the dimensions. Our proposed heuristic is readily obtained as part of the flow training process, enabling versatile implementation of our likelihood contribution based multi-scale architecture for generic flow models. We present such an implementation for the original flow introduced in Dinh et al. (2016), and demonstrate improvements in log-likelihood score and sampling quality on standard image benchmarks. We also conduct ablation studies to compare proposed method with other options for dimension factorization.},
  archivePrefix = {arXiv},
  eprint = {1908.01686},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/das et al_2019_likelihood contribution based multi-scale architecture for generative flows.pdf},
  journal = {arXiv:1908.01686 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{daskalakis20_ComplexityConstrainedMinMax,
  title = {The {{Complexity}} of {{Constrained Min}}-{{Max Optimization}}},
  author = {Daskalakis, Constantinos and Skoulakis, Stratis and Zampetakis, Manolis},
  year = {2020},
  month = sep,
  abstract = {Despite its important applications in Machine Learning, min-max optimization of nonconvex-nonconcave objectives remains elusive. Not only are there no known first-order methods converging even to approximate local min-max points, but the computational complexity of identifying them is also poorly understood. In this paper, we provide a characterization of the computational complexity of the problem, as well as of the limitations of first-order methods in constrained min-max optimization problems with nonconvex-nonconcave objectives and linear constraints. As a warm-up, we show that, even when the objective is a Lipschitz and smooth differentiable function, deciding whether a min-max point exists, in fact even deciding whether an approximate min-max point exists, is NP-hard. More importantly, we show that an approximate local min-max point of large enough approximation is guaranteed to exist, but finding one such point is PPAD-complete. The same is true of computing an approximate fixed point of Gradient Descent/Ascent. An important byproduct of our proof is to establish an unconditional hardness result in the Nemirovsky-Yudin model. We show that, given oracle access to some function \$f : P \textbackslash to [-1, 1]\$ and its gradient \$\textbackslash nabla f\$, where \$P \textbackslash subseteq [0, 1]\^d\$ is a known convex polytope, every algorithm that finds a \$\textbackslash varepsilon\$-approximate local min-max point needs to make a number of queries that is exponential in at least one of \$1/\textbackslash varepsilon\$, \$L\$, \$G\$, or \$d\$, where \$L\$ and \$G\$ are respectively the smoothness and Lipschitzness of \$f\$ and \$d\$ is the dimension. This comes in sharp contrast to minimization problems, where finding approximate local minima in the same setting can be done with Projected Gradient Descent using \$O(L/\textbackslash varepsilon)\$ many queries. Our result is the first to show an exponential separation between these two fundamental optimization problems.},
  archivePrefix = {arXiv},
  eprint = {2009.09623},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/daskalakis et al_2020_the complexity of constrained min-max optimization.pdf},
  journal = {arXiv:2009.09623 [cs, math]},
  primaryClass = {cs, math}
}

@misc{daum19_WritingBookTrump,
  title = {Writing a {{Book About}} the {{Trump Era Devoured My Life}}},
  author = {Daum, Meghan},
  year = {2019},
  month = oct,
  abstract = {You try writing a current affairs book in our bonkers news environment without tossing out hundreds of pages},
  file = {/home/trung/Zotero/storage/NG35CTU9/my-three-years-in-book-writing-hell-724a74f113c1.html},
  howpublished = {https://gen.medium.com/my-three-years-in-book-writing-hell-724a74f113c1},
  journal = {Medium},
  language = {en}
}

@article{daunhawer20_ImprovingMultimodalGenerative,
  title = {Improving {{Multimodal Generative Models}} with {{Disentangled Latent Partitions}}},
  author = {Daunhawer, Imant and Sutter, Thomas and Vogt, Julia E},
  year = {2020},
  pages = {10},
  abstract = {Multimodal generative models learn a joint distribution of data from different modalities\textemdash a task which arguably benefits from the disentanglement of modalityspecific and modality-invariant information. We propose a factorized latent variable model that learns named disentanglement on multimodal data without additional supervision. We demonstrate the disentanglement capabilities on simulated data, and show that disentangled representations can improve the conditional generation of missing modalities without sacrificing unconditional generation.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/daunhawer et al_2020_improving multimodal generative models with disentangled latent partitions.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{dautume19_ScratchGANTraininglanguage,
  title = {{{ScratchGAN}}: {{Training}} Language {{GANs}} from {{Scratch}}},
  author = {{d'Autume}, Cyprien de Masson and Rosca, Mihaela and Rae, Jack and Mohamed, Shakir},
  year = {2019},
  month = may,
  abstract = {Generative Adversarial Networks (GANs) enjoy great success at image generation, but have proven difficult to train in the domain of natural language. Challenges with gradient estimation, optimization instability, and mode collapse have lead practitioners to resort to maximum likelihood pre-training, followed by small amounts of adversarial fine-tuning. The benefits of GAN fine-tuning for language generation are unclear, as the resulting models produce comparable or worse samples than traditional language models. We show it is in fact possible to train a language GAN from scratch -- without maximum likelihood pre-training. We combine existing techniques such as large batch sizes, dense rewards and discriminator regularization to stabilize and improve language GANs. The resulting model, ScratchGAN, performs comparably to maximum likelihood training on EMNLP2017 News and WikiText-103 corpora according to quality and diversity metrics.},
  archivePrefix = {arXiv},
  eprint = {1905.09922},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/d'autume et al_2019_scratchgan.pdf;/home/trung/Zotero/storage/YF9B6CXK/1905.html},
  journal = {arXiv:1905.09922 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{davidson00_IncreasingExpressivityHyperspherical,
  title = {Increasing {{Expressivity}} of a {{Hyperspherical VAE}}},
  author = {Davidson, Tim R and Tomczak, Jakub M and Gavves, Efstratios},
  pages = {8},
  abstract = {Learning suitable latent representations for observed, high-dimensional data is an important research topic underlying many recent advances in machine learning. While traditionally the Gaussian normal distribution has been the go-to latent parameterization, recently a variety of works have successfully proposed the use of manifold-valued latents. In one such work [4], the authors empirically show the potential benefits of using a hyperspherical von Mises-Fisher (vMF) distribution in low dimensionality. However, due to the unique distributional form of the vMF, expressivity in higher dimensional space is limited as a result of its scalar concentration parameter leading to a `hyperspherical bottleneck'. In this work we propose to extend the usability of hyperspherical parameterizations to higher dimensions using a product-space instead, showing improved results on a selection of image datasets.},
  file = {/home/trung/GoogleDrive/Zotero/davidson et al_increasing expressivity of a hyperspherical vae.pdf},
  keywords = {_tablet},
  language = {en}
}

@article{davidson18_HypersphericalVariationalAutoEncoders,
  title = {Hyperspherical {{Variational Auto}}-{{Encoders}}},
  author = {Davidson, Tim R. and Falorsi, Luca and De Cao, Nicola and Kipf, Thomas and Tomczak, Jakub M.},
  year = {2018},
  month = sep,
  abstract = {The Variational Auto-Encoder (VAE) is one of the most used unsupervised machine learning models. But although the default choice of a Gaussian distribution for both the prior and posterior represents a mathematically convenient distribution often leading to competitive results, we show that this parameterization fails to model data with a latent hyperspherical structure. To address this issue we propose using a von Mises-Fisher (vMF) distribution instead, leading to a hyperspherical latent space. Through a series of experiments we show how such a hyperspherical VAE, or S-VAE, is more suitable for capturing data with a hyperspherical latent structure, while outperforming a normal, N -VAE, in low dimensions on other data types.},
  archivePrefix = {arXiv},
  eprint = {1804.00891},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/davidson et al_2018_hyperspherical variational auto-encoders.pdf},
  journal = {arXiv:1804.00891 [cs, stat]},
  keywords = {_tablet},
  language = {en},
  primaryClass = {cs, stat}
}

@article{davidson19_IncreasingExpressivityHyperspherical,
  title = {Increasing {{Expressivity}} of a {{Hyperspherical VAE}}},
  author = {Davidson, Tim R. and Tomczak, Jakub M. and Gavves, Efstratios},
  year = {2019},
  month = oct,
  abstract = {Learning suitable latent representations for observed, high-dimensional data is an important research topic underlying many recent advances in machine learning. While traditionally the Gaussian normal distribution has been the go-to latent parameterization, recently a variety of works have successfully proposed the use of manifold-valued latents. In one such work [4], the authors empirically show the potential benefits of using a hyperspherical von Mises-Fisher (vMF) distribution in low dimensionality. However, due to the unique distributional form of the vMF, expressivity in higher dimensional space is limited as a result of its scalar concentration parameter leading to a `hyperspherical bottleneck'. In this work we propose to extend the usability of hyperspherical parameterizations to higher dimensions using a product-space instead, showing improved results on a selection of image datasets.},
  archivePrefix = {arXiv},
  eprint = {1910.02912},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/davidson et al_2019_increasing expressivity of a hyperspherical vae.pdf},
  journal = {arXiv:1910.02912 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{davis15_ScopeLimitsSimulation,
  title = {The {{Scope}} and {{Limits}} of {{Simulation}} in {{Cognitive Models}}},
  author = {Davis, Ernest and Marcus, Gary},
  year = {2015},
  month = jun,
  abstract = {It has been proposed that human physical reasoning consists largely of running "physics engines in the head" in which the future trajectory of the physical system under consideration is computed precisely using accurate scientific theories. In such models, uncertainty and incomplete knowledge is dealt with by sampling probabilistically over the space of possible trajectories ("Monte Carlo simulation"). We argue that such simulation-based models are too weak, in that there are many important aspects of human physical reasoning that cannot be carried out this way, or can only be carried out very inefficiently; and too strong, in that humans make large systematic errors that the models cannot account for. We conclude that simulation-based reasoning makes up at most a small part of a larger system that encompasses a wide range of additional cognitive processes.},
  archivePrefix = {arXiv},
  eprint = {1506.04956},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/davis et al_2015_the scope and limits of simulation in cognitive models.pdf},
  journal = {arXiv:1506.04956 [cs]},
  primaryClass = {cs}
}

@misc{davis20_seandaviawesomesinglecell,
  title = {Seandavi/Awesome-Single-Cell},
  author = {Davis, Sean},
  year = {2020},
  month = feb,
  abstract = {Community-curated list of software packages and data resources for single-cell, including RNA-seq, ATAC-seq, etc.},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {MIT}
}

@article{davison00_CrosspopulationVariationalAutoencoders,
  title = {Cross-Population {{Variational Autoencoders}}},
  author = {Davison, Joe and Severson, Kristen A and Ghosh, Soumya},
  pages = {9},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/davison et al_cross-population variational autoencoders.pdf},
  language = {en}
}

@article{davison20_CrosspopulationVariationalAutoencoders,
  title = {Cross-Population {{Variational Autoencoders}}},
  author = {Davison, Joe and Severson, Kristen A and Ghosh, Soumya},
  year = {2020},
  pages = {9},
  file = {/home/trung/GoogleDrive/Zotero/davison et al_2020_cross-population variational autoencoders.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{dawid00_CausalInferenceCounterfactual,
  title = {Causal {{Inference}} without {{Counterfactual}}},
  author = {Dawid, A. Philip},
  year = {2000},
  file = {/home/trung/GoogleDrive/Zotero/dawid_2000_causal inference without counterfactual.pdf},
  keywords = {causal,favorite}
}

@inproceedings{dawid10_BewareDAG,
  title = {Beware of the {{DAG}}!},
  booktitle = {Proceedings of Workshop on Causality: {{Objectives}} and Assessment at {{NIPS}} 2008},
  author = {Dawid, A. Philip},
  editor = {Guyon, Isabelle and Janzing, Dominik and Sch{\"o}lkopf, Bernhard},
  year = {2010},
  month = dec,
  volume = {6},
  pages = {59--86},
  publisher = {{PMLR}},
  address = {{Whistler, Canada}},
  abstract = {Directed acyclic graph (DAG) models are popular tools for describing causal relationships and for guiding attempts to learn them from data. They appear to supply a means of extracting causal conclusions from probabilistic conditional independence properties inferred from purely observational data. I take a critical look at this enterprise, and suggest that it is in need of more, and more explicit, methodological and philosophical justification than it typically receives. In particular, I argue for the value of a clean separation between formal causal language and intuitive causal assumptions.},
  file = {/home/trung/GoogleDrive/Zotero/dawid_2010_beware of the dag.pdf},
  keywords = {_tablet,causal,favorite},
  pdf = {http://proceedings.mlr.press/v6/dawid10a/dawid10a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{dawid79_ConditionalIndependenceStatistical,
  title = {Conditional {{Independence}} in {{Statistical Theory}}},
  author = {Dawid, A. Philip},
  year = {1979},
  file = {/home/trung/GoogleDrive/Zotero/dawid_1979_conditional independence in statistical theory.pdf},
  keywords = {favorite}
}

@article{deasy20_ConstrainingVariationalInference,
  title = {Constraining {{Variational Inference}} with {{Geometric Jensen}}-{{Shannon Divergence}}},
  author = {Deasy, Jacob and Simidjievski, Nikola and Li{\`o}, Pietro},
  year = {2020},
  month = jun,
  abstract = {We examine the problem of controlling divergences for latent space regularisation in variational autoencoders. Specifically, when aiming to reconstruct example \$x\textbackslash in\textbackslash mathbb\{R\}\^\{m\}\$ via latent space \$z\textbackslash in\textbackslash mathbb\{R\}\^\{n\}\$ (\$n\textbackslash leq m\$), while balancing this against the need for generalisable latent representations. We present a regularisation mechanism based on the skew geometric-Jensen-Shannon divergence \$\textbackslash left(\textbackslash textrm\{JS\}\^\{\textbackslash textrm\{G\}\_\{\textbackslash alpha\}\}\textbackslash right)\$. We find a variation in \$\textbackslash textrm\{JS\}\^\{\textbackslash textrm\{G\}\_\{\textbackslash alpha\}\}\$, motivated by limiting cases, which leads to an intuitive interpolation between forward and reverse KL in the space of both distributions and divergences. We motivate its potential benefits for VAEs through low-dimensional examples, before presenting quantitative and qualitative results. Our experiments demonstrate that skewing our variant of \$\textbackslash textrm\{JS\}\^\{\textbackslash textrm\{G\}\_\{\textbackslash alpha\}\}\$, in the context of \$\textbackslash textrm\{JS\}\^\{\textbackslash textrm\{G\}\_\{\textbackslash alpha\}\}\$-VAEs, leads to better reconstruction and generation when compared to several baseline VAEs. Our approach is entirely unsupervised and utilises only one hyperparameter which can be easily interpreted in latent space.},
  archivePrefix = {arXiv},
  eprint = {2006.10599},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/deasy et al_2020_constraining variational inference with geometric jensen-shannon divergence.pdf},
  journal = {arXiv:2006.10599 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{debotton00_CamusCoronavirus,
  title = {Camus on {{Coronavirus}}},
  author = {{de Botton}, Alain},
  pages = {3},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/de botton_camus on coronavirus.pdf},
  language = {en}
}

@article{decao18_MolGANimplicitgenerative,
  title = {{{MolGAN}}: {{An}} Implicit Generative Model for Small Molecular Graphs},
  shorttitle = {{{MolGAN}}},
  author = {De Cao, Nicola and Kipf, Thomas},
  year = {2018},
  month = may,
  abstract = {Deep generative models for graph-structured data offer a new angle on the problem of chemical synthesis: by optimizing differentiable models that directly generate molecular graphs, it is possible to side-step expensive search procedures in the discrete and vast space of chemical structures. We introduce MolGAN, an implicit, likelihood-free generative model for small molecular graphs that circumvents the need for expensive graph matching procedures or node ordering heuristics of previous likelihood-based methods. Our method adapts generative adversarial networks (GANs) to operate directly on graph-structured data. We combine our approach with a reinforcement learning objective to encourage the generation of molecules with specific desired chemical properties. In experiments on the QM9 chemical database, we demonstrate that our model is capable of generating close to 100\% valid compounds. MolGAN compares favorably both to recent proposals that use string-based (SMILES) representations of molecules and to a likelihood-based method that directly generates graphs, albeit being susceptible to mode collapse.},
  archivePrefix = {arXiv},
  eprint = {1805.11973},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/de cao et al_2018_molgan.pdf},
  journal = {arXiv:1805.11973 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{decao20_PowerSphericaldistribution,
  title = {The {{Power Spherical}} Distribution},
  author = {De Cao, Nicola and Aziz, Wilker},
  year = {2020},
  month = jun,
  abstract = {There is a growing interest in probabilistic models defined in hyper-spherical spaces, be it to accommodate observed data or latent structure. The von Mises-Fisher (vMF) distribution, often regarded as the Normal distribution on the hyper-sphere, is a standard modeling choice: it is an exponential family and thus enjoys important statistical results, for example, known Kullback-Leibler (KL) divergence from other vMF distributions. Sampling from a vMF distribution, however, requires a rejection sampling procedure which besides being slow poses difficulties in the context of stochastic backpropagation via the reparameterization trick. Moreover, this procedure is numerically unstable for certain vMFs, e.g., those with high concentration and/or in high dimensions. We propose a novel distribution, the Power Spherical distribution, which retains some of the important aspects of the vMF (e.g., support on the hyper-sphere, symmetry about its mean direction parameter, known KL from other vMF distributions) while addressing its main drawbacks (i.e., scalability and numerical stability). We demonstrate the stability of Power Spherical distributions with a numerical experiment and further apply it to a variational auto-encoder trained on MNIST. Code at: https://github.com/nicola-decao/power\_spherical},
  archivePrefix = {arXiv},
  eprint = {2006.04437},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/de cao et al_2020_the power spherical distribution.pdf},
  journal = {arXiv:2006.04437 [cs, stat]},
  primaryClass = {cs, stat}
}

@incollection{defferrard16_ConvolutionalNeuralNetworks,
  title = {Convolutional {{Neural Networks}} on {{Graphs}} with {{Fast Localized Spectral Filtering}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Defferrard, Micha{\"e}l and Bresson, Xavier and Vandergheynst, Pierre},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3844--3852},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/defferrard et al_2016_convolutional neural networks on graphs with fast localized spectral filtering.pdf;/home/trung/Zotero/storage/KYBEKEBP/6081-convolutional-neural-networks-on-graphs-with-fast-localized-spectral-filtering.html},
  keywords = {graph}
}

@article{dehghani19_UniversalTransformers,
  title = {Universal {{Transformers}}},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, {\L}ukasz},
  year = {2019},
  month = mar,
  abstract = {Recurrent neural networks (RNNs) sequentially process data by updating their state with each new data point, and have long been the de facto choice for sequence modeling tasks. However, their inherently sequential computation makes them slow to train. Feed-forward and convolutional architectures have recently been shown to achieve superior results on some sequence modeling tasks such as machine translation, with the added advantage that they concurrently process all inputs in the sequence, leading to easy parallelization and faster training times. Despite these successes, however, popular feed-forward sequence models like the Transformer fail to generalize in many simple tasks that recurrent models handle with ease, e.g. copying strings or even simple logical inference when the string or formula lengths exceed those observed at training time. We propose the Universal Transformer (UT), a parallel-in-time self-attentive recurrent sequence model which can be cast as a generalization of the Transformer model and which addresses these issues. UTs combine the parallelizability and global receptive field of feed-forward sequence models like the Transformer with the recurrent inductive bias of RNNs. We also add a dynamic per-position halting mechanism and find that it improves accuracy on several tasks. In contrast to the standard Transformer, under certain assumptions UTs can be shown to be Turing-complete. Our experiments show that UTs outperform standard Transformers on a wide range of algorithmic and language understanding tasks, including the challenging LAMBADA language modeling task where UTs achieve a new state of the art, and machine translation where UTs achieve a 0.9 BLEU improvement over Transformers on the WMT14 En-De dataset.},
  archivePrefix = {arXiv},
  eprint = {1807.03819},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dehghani et al_2019_universal transformers.pdf},
  journal = {arXiv:1807.03819 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@book{deisenroth00_MathematicsMachineLearning,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  file = {/home/trung/GoogleDrive/Zotero/deisenroth et al_mathematics for machine learning.pdf},
  keywords = {_tablet,favorite},
  language = {en}
}

@article{delmerico19_currentstatefuture,
  title = {The Current State and Future Outlook of Rescue Robotics},
  author = {Delmerico, Jeffrey and Mintchev, Stefano and Giusti, Alessandro and Gromov, Boris and Melo, Kamilo and Horvat, Tomislav and Cadena, Cesar and Hutter, Marco and Ijspeert, Auke and Floreano, Dario and Gambardella, Luca M. and Siegwart, Roland and Scaramuzza, Davide},
  year = {2019},
  month = oct,
  volume = {36},
  pages = {1171--1191},
  issn = {1556-4959, 1556-4967},
  doi = {10.1002/rob.21887},
  file = {/home/trung/GoogleDrive/Zotero/delmerico et al_2019_the current state and future outlook of rescue robotics.pdf},
  journal = {Journal of Field Robotics},
  language = {en},
  number = {7}
}

@inproceedings{deng17_FactorizedVariationalAutoencoders,
  title = {Factorized {{Variational Autoencoders}} for {{Modeling Audience Reactions}} to {{Movies}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Deng, Zhiwei and Navarathna, Rajitha and Carr, Peter and Mandt, Stephan and Yue, Yisong and Matthews, Iain and Mori, Greg},
  year = {2017},
  month = jul,
  pages = {6014--6023},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.637},
  annotation = {ZSCC: 0000024},
  file = {/home/trung/Zotero/storage/FWDH3IEZ/Deng et al. - 2017 - Factorized Variational Autoencoders for Modeling A.pdf},
  isbn = {978-1-5386-0457-1},
  language = {en}
}

@article{deng18_LatentAlignmentVariational,
  title = {Latent {{Alignment}} and {{Variational Attention}}},
  author = {Deng, Yuntian and Kim, Yoon and Chiu, Justin and Guo, Demi and Rush, Alexander M.},
  year = {2018},
  month = jul,
  abstract = {Neural attention has become central to many state-of-the-art models in natural language processing and related domains. Attention networks are an easy-to-train and effective method for softly simulating alignment; however, the approach does not marginalize over latent alignments in a probabilistic sense. This property makes it difficult to compare attention to other alignment approaches, to compose it with probabilistic models, and to perform posterior inference conditioned on observed data. A related latent approach, hard attention, fixes these issues, but is generally harder to train and less accurate. This work considers variational attention networks, alternatives to soft and hard attention for learning latent variable alignment models, with tighter approximation bounds based on amortized variational inference. We further propose methods for reducing the variance of gradients to make these approaches computationally feasible. Experiments show that for machine translation and visual question answering, inefficient exact latent variable models outperform standard neural attention, but these gains go away when using hard attention based training. On the other hand, variational attention retains most of the performance gain but with training speed comparable to neural attention.},
  archivePrefix = {arXiv},
  eprint = {1807.03756},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/deng et al_2018_latent alignment and variational attention.pdf},
  journal = {arXiv:1807.03756 [cs, stat]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{deng19_GenerativeHierarchicalModels,
  title = {Generative {{Hierarchical Models}} for {{Parts}}, {{Objects}}, and {{Scenes}}},
  author = {Deng, Fei and Zhi, Zhuo and Ahn, Sungjin},
  year = {2019},
  month = oct,
  abstract = {Compositional structures between parts and objects are inherent in natural scenes. Modeling such compositional hierarchies via unsupervised learning can bring various benefits such as interpretability and transferability, which are important in many downstream tasks. In this paper, we propose the first deep latent variable model, called RICH, for learning Representation of Interpretable Compositional Hierarchies. At the core of RICH is a latent scene graph representation that organizes the entities of a scene into a tree structure according to their compositional relationships. During inference, taking top-down approach, RICH is able to use higher-level representation to guide lower-level decomposition. This avoids the difficult problem of routing between parts and objects that is faced by bottom-up approaches. In experiments on images containing multiple objects with different part compositions, we demonstrate that RICH is able to learn the latent compositional hierarchy and generate imaginary scenes.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.09119},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/deng et al_2019_generative hierarchical models for parts, objects, and scenes.pdf;/home/trung/Zotero/storage/D6HS7QG7/1910.html},
  journal = {arXiv:1910.09119 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,generative,hierarchical,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{deng19_scScopeScalableanalysis,
  title = {{{scScope}}: {{Scalable}} Analysis of Cell-Type Composition from Single-Cell Transcriptomics Using Deep Recurrent Learning},
  author = {Deng, Yue and Bao, Feng and Dai, Qionghai and Wu, Lani F. and Altschuler, Steven J.},
  year = {2019},
  month = apr,
  volume = {16},
  pages = {311--314},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0353-7},
  file = {/home/trung/GoogleDrive/Zotero/deng et al_2019_scscope.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {4}
}

@book{deng20_RoleGradientNoise,
  title = {The {{Role}} of {{Gradient Noise}} in the {{Optimization}} of {{Neural Networks}}},
  author = {Deng, Zhun and Huang, Jiaoyang and Kawaguchi, Kenji},
  year = {2020},
  month = feb,
  abstract = {Adding gradient noise has been widely used in training models with gradient descent, for example, in the non-convex optimization, robust training and differentially private learning. In this paper, we study the role of gradient noise in the optimization of neural networks: in what cases does adding gradient noise help optimization? We experimentally observe that there is an inherent tension between training accuracy and generalization ability due to the size of noise added in multiple scenarios. Motivated by that, we study the training dynamics and generalization gaps of noisy gradient descent of shallow and wide neural networks. The analysis can shed light upon why compared to training without noise, gradient Langevin dynamics cannot reach good test accuracy while suitably shrinking gradient noise can still yield good and even better test accuracy on neural networks. Our results also partially explain the cost of ensuring differential privacy in learning. Lastly, we identify an interesting phenomenon in the implementation that even we add much larger noise than gradient, noisy gradient descent can still result in loss decay at every single step. The insights gained above provide useful guidelines to calibrating noise in training neural networks for the future studies.}
}

@article{denisov19_EndtoEndMultiSpeakerSpeech,
  title = {End-to-{{End Multi}}-{{Speaker Speech Recognition}} Using {{Speaker Embeddings}} and {{Transfer Learning}}},
  author = {Denisov, Pavel and Vu, Ngoc Thang},
  year = {2019},
  month = aug,
  abstract = {This paper presents our latest investigation on end-to-end automatic speech recognition (ASR) for overlapped speech. We propose to train an end-to-end system conditioned on speaker embeddings and further improved by transfer learning from clean speech. This proposed framework does not require any parallel non-overlapped speech materials and is independent of the number of speakers. Our experimental results on overlapped speech datasets show that joint conditioning on speaker embeddings and transfer learning significantly improves the ASR performance.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.04737},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/denisov et al_2019_end-to-end multi-speaker speech recognition using speaker embeddings and transfer learning.pdf;/home/trung/GoogleDrive/Zotero/denisov et al_2019_end-to-end multi-speaker speech recognition using speaker embeddings and transfer learning2.pdf;/home/trung/Zotero/storage/3ABDTNTB/1908.html;/home/trung/Zotero/storage/LQ49PZ6T/1908.html},
  journal = {arXiv:1908.04737 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{denton17_UnsupervisedLearningDisentangled,
  title = {Unsupervised {{Learning}} of {{Disentangled Representations}} from {{Video}}},
  author = {Denton, Emily and Birodkar, Vighnesh},
  year = {2017},
  month = may,
  abstract = {We present a new model DrNET that learns disentangled image representations from video. Our approach leverages the temporal coherence of video and a novel adversarial loss to learn a representation that factorizes each frame into a stationary part and a temporally varying component. The disentangled representation can be used for a range of tasks. For example, applying a standard LSTM to the time-vary components enables prediction of future frames. We evaluate our approach on a range of synthetic and real videos, demonstrating the ability to coherently generate hundreds of steps into the future.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1705.10915},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/denton et al_2017_unsupervised learning of disentangled representations from video.pdf;/home/trung/Zotero/storage/AHE4ZLM4/1705.html},
  journal = {arXiv:1705.10915 [cs, stat]},
  primaryClass = {cs, stat}
}

@book{deo74_Graphtheoryapplications,
  title = {Graph Theory with Applications to Engineering and Computer Science (Prentice Hall Series in Automatic Computation)},
  author = {Deo, Narsingh},
  year = {1974},
  publisher = {{Prentice-Hall, Inc.}},
  address = {{USA}},
  file = {/home/trung/GoogleDrive/Zotero/deo_1974_graph theory with applications to engineering and computer science (prentice hall series in automatic computation).pdf},
  isbn = {0-13-363473-6}
}

@article{dephillipo18_EfficacyVitaminSupplementation,
  title = {Efficacy of {{Vitamin C Supplementation}} on {{Collagen Synthesis}} and {{Oxidative Stress After Musculoskeletal Injuries}}: {{A Systematic Review}}},
  shorttitle = {Efficacy of {{Vitamin C Supplementation}} on {{Collagen Synthesis}} and {{Oxidative Stress After Musculoskeletal Injuries}}},
  author = {DePhillipo, Nicholas N. and Aman, Zachary S. and Kennedy, Mitchell I. and Begley, J.P. and Moatshe, Gilbert and LaPrade, Robert F.},
  year = {2018},
  month = oct,
  volume = {6},
  issn = {2325-9671},
  doi = {10.1177/2325967118804544},
  abstract = {Background: Recent investigations on the biochemical pathways after a musculoskeletal injury have suggested that vitamin C (ascorbic acid) may be a viable supplement to enhance collagen synthesis and soft tissue healing. Purpose: To (1) summarize vitamin C treatment protocols; (2) report on the efficacy of vitamin C in accelerating healing after bone, tendon, and ligament injuries in vivo and in vitro; and (3) report on the efficacy of vitamin C as an antioxidant protecting against fibrosis and promoting collagen synthesis. Study Design: Systematic review; Level of evidence, 2. Methods: A systematic review was performed, with the inclusion criteria of animal and human studies on vitamin C supplementation after a musculoskeletal injury specific to collagen cross-linking, collagen synthesis, and biologic healing of the bone, ligament, and tendon. Results: The initial search yielded 286 articles. After applying the inclusion and exclusion criteria, 10 articles were included in the final analysis. Of the preclinical studies evaluating fracture healing, 2 studies reported significantly accelerated bone healing in the vitamin C supplementation group compared with control groups. The 2 preclinical studies evaluating tendon healing reported significant increases in type I collagen fibers and scar tissue formation with vitamin C compared with control groups. The 1 preclinical study after anterior cruciate ligament (ACL) reconstruction reported significant short-term (1-6 weeks) improvements in ACL graft incorporation in the vitamin C group compared with control groups; however, there was no long-term (42 weeks) difference. Of the clinical studies evaluating fracture healing, 1 study reported no significant differences in the rate of fracture healing at 50 days or functional outcomes at 1 year. Vitamin C supplementation was shown to decrease oxidative stress parameters by neutralizing reactive oxygen species through redox modulation in animal models. No animal or human studies reported any adverse effects of vitamin C supplementation. Conclusion: Preclinical studies demonstrated that vitamin C has the potential to accelerate bone healing after a fracture, increase type I collagen synthesis, and reduce oxidative stress parameters. No adverse effects were reported with vitamin C supplementation in either animal models or human participants; thus, oral vitamin C appears to be a safe supplement but lacks clinical evidence compared with controls. Because of the limited number of human studies, further clinical investigations are needed before the implementation of vitamin C as a postinjury supplement.},
  annotation = {ZSCC: 0000015},
  file = {/home/trung/GoogleDrive/Zotero/dephillipo et al_2018_efficacy of vitamin c supplementation on collagen synthesis and oxidative stress after musculoskeletal injuries.pdf},
  journal = {Orthopaedic Journal of Sports Medicine},
  number = {10},
  pmcid = {PMC6204628},
  pmid = {30386805}
}

@article{derbyshire00_Couldwebe,
  title = {Could We Be Overlooking a Potential Choline Crisis in the {{United Kingdom}}?},
  author = {Derbyshire, Emma},
  pages = {4},
  file = {/home/trung/GoogleDrive/Zotero/derbyshire_could we be overlooking a potential choline crisis in the united kingdom.pdf},
  language = {en}
}

@article{derrington00_ArtificialIntelligenceHealth,
  title = {Artificial {{Intelligence}} for {{Health}} and {{Health Care}}},
  author = {Derrington, Dolores},
  pages = {69},
  annotation = {ZSCC: 0000005},
  file = {/home/trung/GoogleDrive/Zotero/derrington_artificial intelligence for health and health care.pdf},
  language = {en}
}

@article{desai20_VIGNVariationalIntegrator,
  title = {{{VIGN}}: {{Variational Integrator Graph Networks}}},
  shorttitle = {{{VIGN}}},
  author = {Desai, Shaan and Roberts, Stephen},
  year = {2020},
  month = apr,
  abstract = {Rich, physically-informed inductive biases play an imperative role in accurately modelling the time dynamics of physical systems. In this paper, we introduce Variational Integrator Graph Networks (VIGNs), the first approach to combine a Variational Integrator (VI) inductive bias with a Graph Network (GN) and demonstrate an order of magnitude improvement in performance, both in terms of data-efficient learning and predictive accuracy, over existing methods. We show that this improvement arises because VIs induce coupled learning of generalized position and momentum updates which can be formulated as a Partitioned Runge-Kutta (PRK) method. We empirically establish that VIGN outperforms numerous methods in learning from existing datasets with noise.},
  archivePrefix = {arXiv},
  eprint = {2004.13688},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/desai et al_2020_vign.pdf},
  journal = {arXiv:2004.13688 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{detomaso19_Functionalinterpretationsingle,
  title = {Functional Interpretation of Single Cell Similarity Maps},
  author = {DeTomaso, David and Jones, Matthew G. and Subramaniam, Meena and Ashuach, Tal and Ye, Chun J. and Yosef, Nir},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {4376},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12235-0},
  file = {/home/trung/GoogleDrive/Zotero/detomaso et al_2019_functional interpretation of single cell similarity maps.pdf},
  journal = {Nature Communications},
  keywords = {scvi},
  language = {en},
  number = {1}
}

@article{detorakis19_InherentWeightNormalization,
  title = {Inherent {{Weight Normalization}} in {{Stochastic Neural Networks}}},
  author = {Detorakis, Georgios and Dutta, Sourav and Khanna, Abhishek and Jerry, Matthew and Datta, Suman and Neftci, Emre},
  year = {2019},
  month = oct,
  abstract = {Multiplicative stochasticity such as Dropout improves the robustness and generalizability of deep neural networks. Here, we further demonstrate that always-on multiplicative stochasticity combined with simple threshold neurons are sufficient operations for deep neural networks. We call such models Neural Sampling Machines (NSM). We find that the probability of activation of the NSM exhibits a self-normalizing property that mirrors Weight Normalization, a previously studied mechanism that fulfills many of the features of Batch Normalization in an online fashion. The normalization of activities during training speeds up convergence by preventing internal covariate shift caused by changes in the input distribution. The always-on stochasticity of the NSM confers the following advantages: the network is identical in the inference and learning phases, making the NSM suitable for online learning, it can exploit stochasticity inherent to a physical substrate such as analog non-volatile memories for in-memory computing, and it is suitable for Monte Carlo sampling, while requiring almost exclusively addition and comparison operations. We demonstrate NSMs on standard classification benchmarks (MNIST and CIFAR) and event-based classification benchmarks (N-MNIST and DVS Gestures). Our results show that NSMs perform comparably or better than conventional artificial neural networks with the same architecture.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.12316},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/detorakis et al_2019_inherent weight normalization in stochastic neural networks.pdf;/home/trung/Zotero/storage/SNW7SYQ2/1910.html},
  journal = {arXiv:1910.12316 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{devlin18_BERTPretrainingDeep,
  title = {{{BERT}}: {{Pre}}-Training of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  shorttitle = {{{BERT}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2018},
  month = oct,
  abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5\% (7.7\% point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
  archivePrefix = {arXiv},
  eprint = {1810.04805},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/devlin et al_2018_bert.pdf;/home/trung/Zotero/storage/QKB5GD5F/1810.html},
  journal = {arXiv:1810.04805 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{dhariwal20_JukeboxGenerativeModel,
  title = {Jukebox: {{A Generative Model}} for {{Music}}},
  shorttitle = {Jukebox},
  author = {Dhariwal, Prafulla and Jun, Heewoo and Payne, Christine and Kim, Jong Wook and Radford, Alec and Sutskever, Ilya},
  year = {2020},
  month = apr,
  abstract = {We introduce Jukebox, a model that generates music with singing in the raw audio domain. We tackle the long context of raw audio using a multi-scale VQ-VAE to compress it to discrete codes, and modeling those using autoregressive Transformers. We show that the combined model at scale can generate high-fidelity and diverse songs with coherence up to multiple minutes. We can condition on artist and genre to steer the musical and vocal style, and on unaligned lyrics to make the singing more controllable. We are releasing thousands of non cherry-picked samples at https://jukebox.openai.com, along with model weights and code at https://github.com/openai/jukebox},
  archivePrefix = {arXiv},
  eprint = {2005.00341},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dhariwal et al_2020_jukebox.pdf},
  journal = {arXiv:2005.00341 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{dieleman18_challengerealisticmusic,
  title = {The Challenge of Realistic Music Generation: Modelling Raw Audio at Scale},
  shorttitle = {The Challenge of Realistic Music Generation},
  author = {Dieleman, Sander and van den Oord, A{\"a}ron and Simonyan, Karen},
  year = {2018},
  month = jun,
  abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
  archivePrefix = {arXiv},
  eprint = {1806.10474},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dieleman et al_2018_the challenge of realistic music generation.pdf;/home/trung/Zotero/storage/W5T2STFX/1806.html},
  journal = {arXiv:1806.10474 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{dieleman18_challengerealisticmusica,
  title = {The Challenge of Realistic Music Generation: Modelling Raw Audio at Scale},
  shorttitle = {The Challenge of Realistic Music Generation},
  author = {Dieleman, Sander and van den Oord, A{\"a}ron and Simonyan, Karen},
  year = {2018},
  month = jun,
  abstract = {Realistic music generation is a challenging task. When building generative models of music that are learnt from data, typically high-level representations such as scores or MIDI are used that abstract away the idiosyncrasies of a particular performance. But these nuances are very important for our perception of musicality and realism, so in this work we embark on modelling music in the raw audio domain. It has been shown that autoregressive models excel at generating raw audio waveforms of speech, but when applied to music, we find them biased towards capturing local signal structure at the expense of modelling long-range correlations. This is problematic because music exhibits structure at many different timescales. In this work, we explore autoregressive discrete autoencoders (ADAs) as a means to enable autoregressive models to capture long-range correlations in waveforms. We find that they allow us to unconditionally generate piano music directly in the raw audio domain, which shows stylistic consistency across tens of seconds.},
  archivePrefix = {arXiv},
  eprint = {1806.10474},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dieleman et al_2018_the challenge of realistic music generation2.pdf},
  journal = {arXiv:1806.10474 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{dieng17_VariationalInferencechi,
  title = {Variational {{Inference}} via \$\textbackslash chi\$-{{Upper Bound Minimization}}},
  author = {Dieng, Adji B. and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David M.},
  year = {2017},
  month = nov,
  abstract = {Variational inference (VI) is widely used as an efficient alternative to Markov chain Monte Carlo. It posits a family of approximating distributions q and finds the closest member to the exact posterior p. Closeness is usually measured via a divergence D(q||p) from q to p. While successful, this approach also has problems. Notably, it typically leads to underestimation of the posterior variance. In this paper we propose CHIVI, a black-box variational inference algorithm that minimizes D{$\chi$}(p||q), the {$\chi$}-divergence from p to q. CHIVI minimizes an upper bound of the model evidence, which we term the {$\chi$} upper bound (CUBO). Minimizing the CUBO leads to improved posterior uncertainty, and it can also be used with the classical VI lower bound (ELBO) to provide a sandwich estimate of the model evidence. We study CHIVI on three models: probit regression, Gaussian process classification, and a Cox process model of basketball plays. When compared to expectation propagation and classical VI, CHIVI produces better error rates and more accurate estimates of posterior variance.},
  annotation = {ZSCC: 0000047},
  archivePrefix = {arXiv},
  eprint = {1611.00328},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/U9Y7X4IL/Dieng et al. - 2017 - Variational Inference via $chi$-Upper Bound Minim.pdf},
  journal = {arXiv:1611.00328 [cs, stat]},
  keywords = {favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dieng18_AvoidingLatentVariable,
  title = {Avoiding {{Latent Variable Collapse With Generative Skip Models}}},
  author = {Dieng, Adji B. and Kim, Yoon and Rush, Alexander M. and Blei, David M.},
  year = {2018},
  month = jul,
  abstract = {Variational autoencoders learn distributions of high-dimensional data. They model data with a deep latent-variable model and then fit the model by maximizing a lower bound of the log marginal likelihood. VAEs can capture complex distributions, but they can also suffer from an issue known as "latent variable collapse," especially if the likelihood model is powerful. Specifically, the lower bound involves an approximate posterior of the latent variables; this posterior "collapses" when it is set equal to the prior, i.e., when the approximate posterior is independent of the data. While VAEs learn good generative models, latent variable collapse prevents them from learning useful representations. In this paper, we propose a simple new way to avoid latent variable collapse by including skip connections in our generative model; these connections enforce strong links between the latent variables and the likelihood function. We study generative skip models both theoretically and empirically. Theoretically, we prove that skip models increase the mutual information between the observations and the inferred latent variables. Empirically, we study images (MNIST and Omniglot) and text (Yahoo). Compared to existing VAE architectures, we show that generative skip models maintain similar predictive performance but lead to less collapse and provide more meaningful representations of the data.},
  annotation = {ZSCC: 0000037},
  archivePrefix = {arXiv},
  eprint = {1807.04863},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dieng et al_2018_avoiding latent variable collapse with generative skip models.pdf;/home/trung/Zotero/storage/TVBX85SW/1807.html},
  journal = {arXiv:1807.04863 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,elbo broken,Statistics - Machine Learning,vae_issues,variational},
  primaryClass = {cs, stat}
}

@article{dieng19_DynamicEmbeddedTopic,
  title = {The {{Dynamic Embedded Topic Model}}},
  author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
  year = {2019},
  month = jul,
  abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. Dynamic topic models capture how these patterns vary over time for a set of documents that were collected over a large time span. We develop the dynamic embedded topic model (D-ETM), a generative model of documents that combines dynamic latent Dirichlet allocation (D-LDA) and word embeddings. The D-ETM models each word with a categorical distribution whose parameter is given by the inner product between the word embedding and an embedding representation of its assigned topic at a particular time step. The word embeddings allow the D-ETM to generalize to rare words. The D-ETM learns smooth topic trajectories by defining a random walk prior over the embeddings of the topics. We fit the D-ETM using structured amortized variational inference. On a collection of United Nations debates, we find that the D-ETM learns interpretable topics and outperforms D-LDA in terms of both topic quality and predictive performance.},
  archivePrefix = {arXiv},
  eprint = {1907.05545},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dieng et al_2019_the dynamic embedded topic model.pdf;/home/trung/Zotero/storage/J4G4M7G2/1907.html},
  journal = {arXiv:1907.05545 [cs, stat]},
  keywords = {Computer Science - Computation and Language,lda,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{dieng19_PrescribedGenerativeAdversarial,
  title = {Prescribed {{Generative Adversarial Networks}}},
  author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M. and Titsias, Michalis K.},
  year = {2019},
  month = oct,
  abstract = {Generative adversarial networks (GANs) are a powerful approach to unsupervised learning. They have achieved state-of-the-art performance in the image domain. However, GANs are limited in two ways. They often learn distributions with low support---a phenomenon known as mode collapse---and they do not guarantee the existence of a probability density, which makes evaluating generalization using predictive log-likelihood impossible. In this paper, we develop the prescribed GAN (PresGAN) to address these shortcomings. PresGANs add noise to the output of a density network and optimize an entropy-regularized adversarial loss. The added noise renders tractable approximations of the predictive log-likelihood and stabilizes the training procedure. The entropy regularizer encourages PresGANs to capture all the modes of the data distribution. Fitting PresGANs involves computing the intractable gradients of the entropy regularization term; PresGANs sidestep this intractability using unbiased stochastic estimates. We evaluate PresGANs on several datasets and found they mitigate mode collapse and generate samples with high perceptual quality. We further found that PresGANs reduce the gap in performance in terms of predictive log-likelihood between traditional GANs and variational autoencoders (VAEs).},
  archivePrefix = {arXiv},
  eprint = {1910.04302},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dieng et al_2019_prescribed generative adversarial networks.pdf;/home/trung/Zotero/storage/HFAQURLU/1910.html},
  journal = {arXiv:1910.04302 [cs, stat]},
  keywords = {adversarial,Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{dieng19_TopicModelingEmbedding,
  title = {Topic {{Modeling}} in {{Embedding Spaces}}},
  author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
  year = {2019},
  month = jul,
  abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavy-tailed vocabularies. To this end, we develop the Embedded Topic Model (ETM), a generative model of documents that marries traditional topic models with word embeddings. In particular, it models each word with a categorical distribution whose natural parameter is the inner product between a word embedding and an embedding of its assigned topic. To fit the ETM, we develop an efficient amortized variational inference algorithm. The ETM discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation (LDA), in terms of both topic quality and predictive performance.},
  archivePrefix = {arXiv},
  eprint = {1907.04907},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dieng et al_2019_topic modeling in embedding spaces.pdf;/home/trung/Zotero/storage/7LCY7DVT/1907.html},
  journal = {arXiv:1907.04907 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,embedding,information,lda,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{dieng20_DeepProbabilisticGraphical,
  title = {Deep {{Probabilistic Graphical Modeling}}},
  author = {Dieng, Adji Bousso},
  year = {2020},
  pages = {142},
  file = {/home/trung/GoogleDrive/Zotero/dieng_2020_deep probabilistic graphical modeling.pdf},
  keywords = {_tablet,favorite},
  language = {en}
}

@article{dieng20_TopicModelingEmbedding,
  title = {Topic {{Modeling}} in {{Embedding Spaces}}},
  author = {Dieng, Adji B. and Ruiz, Francisco J. R. and Blei, David M.},
  year = {2020},
  month = jul,
  volume = {8},
  pages = {439--453},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00325},
  abstract = {Topic modeling analyzes documents to learn meaningful patterns of words. However, existing topic models fail to learn interpretable topics when working with large and heavytailed vocabularies. To this end, we develop the embedded topic model (ETM), a generative model of documents that marries traditional topic models with word embeddings. More specifically, the ETM models each word with a categorical distribution whose natural parameter is the inner product between the word's embedding and an embedding of its assigned topic. To fit the ETM, we develop an efficient amortized variational inference algorithm. The ETM discovers interpretable topics even with large vocabularies that include rare words and stop words. It outperforms existing document models, such as latent Dirichlet allocation, in terms of both topic quality and predictive performance.},
  file = {/home/trung/GoogleDrive/Zotero/dieng et al_2020_topic modeling in embedding spaces.pdf},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en}
}

@article{diethe19_ContinualLearningPractice,
  title = {Continual {{Learning}} in {{Practice}}},
  author = {Diethe, Tom and Borchert, Tom and Thereska, Eno and Balle, Borja and Lawrence, Neil},
  year = {2019},
  month = mar,
  abstract = {This paper describes a reference architecture for self-maintaining systems that can learn continually, as data arrives. In environments where data evolves, we need architectures that manage Machine Learning (ML) models in production, adapt to shifting data distributions, cope with outliers, retrain when necessary, and adapt to new tasks. This represents continual AutoML or Automatically Adaptive Machine Learning. We describe the challenges and proposes a reference architecture.},
  archivePrefix = {arXiv},
  eprint = {1903.05202},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/diethe et al_2019_continual learning in practice.pdf;/home/trung/Zotero/storage/9VTAJVEI/1903.html},
  journal = {arXiv:1903.05202 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{dillon17_TensorFlowDistributions,
  title = {{{TensorFlow Distributions}}},
  author = {Dillon, Joshua V. and Langmore, Ian and Tran, Dustin and Brevdo, Eugene and Vasudevan, Srinivas and Moore, Dave and Patton, Brian and Alemi, Alex and Hoffman, Matt and Saurous, Rif A.},
  year = {2017},
  month = nov,
  abstract = {The TensorFlow Distributions library implements a vision of probability theory adapted to the modern deep-learning paradigm of end-to-end differentiable computation. Building on two basic abstractions, it offers flexible building blocks for probabilistic computation. Distributions provide fast, numerically stable methods for generating samples and computing statistics, e.g., log density. Bijectors provide composable volume-tracking transformations with automatic caching. Together these enable modular construction of high dimensional distributions and transformations not possible with previous libraries (e.g., pixelCNNs, autoregressive flows, and reversible residual networks). They are the workhorse behind deep probabilistic programming systems like Edward and empower fast black-box inference in probabilistic models built on deep-network components. TensorFlow Distributions has proven an important part of the TensorFlow toolkit within Google and in the broader deep learning community.},
  annotation = {ZSCC: 0000063},
  archivePrefix = {arXiv},
  eprint = {1711.10604},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dillon et al_2017_tensorflow distributions.pdf;/home/trung/Zotero/storage/RQTLCYGY/1711.html},
  journal = {arXiv:1711.10604 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{dilokthanakul17_DeepUnsupervisedClustering,
  title = {Deep {{Unsupervised Clustering}} with {{Gaussian Mixture Variational Autoencoders}}},
  author = {Dilokthanakul, Nat and Mediano, Pedro A. M. and Garnelo, Marta and Lee, Matthew C. H. and Salimbeni, Hugh and Arulkumaran, Kai and Shanahan, Murray},
  year = {2017},
  month = jan,
  abstract = {We study a variant of the variational autoencoder model (VAE) with a Gaussian mixture as a prior distribution, with the goal of performing unsupervised clustering through deep generative models. We observe that the known problem of over-regularisation that has been shown to arise in regular VAEs also manifests itself in our model and leads to cluster degeneracy. We show that a heuristic called minimum information constraint that has been shown to mitigate this effect in VAEs can also be applied to improve unsupervised clustering performance with our model. Furthermore we analyse the effect of this heuristic and provide an intuition of the various processes with the help of visualizations. Finally, we demonstrate the performance of our model on synthetic data, MNIST and SVHN, showing that the obtained clusters are distinct, interpretable and result in achieving competitive performance on unsupervised clustering to the state-of-the-art results.},
  archivePrefix = {arXiv},
  eprint = {1611.02648},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dilokthanakul et al_2017_deep unsupervised clustering with gaussian mixture variational autoencoders.pdf},
  journal = {arXiv:1611.02648 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{dinel20_ChronicSupplementationMix,
  title = {Chronic {{Supplementation}} with a {{Mix}} of {{Salvia}} Officinalis and {{Salvia}} Lavandulaefolia {{Improves Morris Water Maze Learning}} in {{Normal Adult C57Bl}}/{{6J Mice}}},
  author = {Dinel, Anne-Laure and Lucas, C{\'e}line and Guillemet, Damien and Lay{\'e}, Sophie and Pallet, V{\'e}ronique and Joffre, Corinne},
  year = {2020},
  month = jun,
  volume = {12},
  issn = {2072-6643},
  doi = {10.3390/nu12061777},
  abstract = {Background: Two different species of sage, Salvia officinalis and Salvia lavandulaefolia, have demonstrated activities in cognitive function during preclinical and clinical studies related to impaired health situations or single administration. Different memory processes have been described to be significantly and positively impacted. Objective: Our objective is to explore the potential of these Salvia, and their additional activities, in healthy situations, and during prolonged administration, on memory and subsequent mechanisms of action related to putative effects. Design: This mouse study has implicated four investigational arms dedicated to control, Salvia officinalis aqueous extract, Salvia lavandulaefolia-encapsulated essential oil and a mix thereof (Cognivia\texttrademark ) for 2 weeks of administration. Cognitive functions have been assessed throughout Y-maze and Morris water maze models. The impact of supplementation on lipid peroxidation, oxidative stress, neurogenesis, neuronal activity, neurotrophins, neurotrophin receptors, CaM kinase II and glucocorticoid receptors has been assessed via post-interventional tissue collection. Results: All Salvia groups had a significant effect on Y-maze markers on day 1 of administration. Only the mix of two Salvia species demonstrated significant improvements in Morris water maze markers at the end of administration. Considering all biological and histological markers, we did not observe any significant effect of S. officinalis, S. lavandulaefolia and a mix of Salvia supplementation on lipid peroxidation, oxidative stress and neuronal plasticity (neurogenesis, neuronal activity, neurotrophins). Interestingly, CaM kinase II protein expression is significantly increased in animals supplemented with Salvia. Conclusion: The activities of Salvia alone after one intake have been confirmed; however, a particular combination of different types of Salvia have been shown to improve memory and present specific synergistic effects after chronic administration in healthy mice.},
  file = {/home/trung/GoogleDrive/Zotero/dinel et al_2020_chronic supplementation with a mix of salvia officinalis and salvia lavandulaefolia improves morris water maze learning in normal adult c57bl-6j mice.pdf},
  journal = {Nutrients},
  number = {6},
  pmcid = {PMC7353372},
  pmid = {32549250}
}

@article{ding18_Interpretabledimensionalityreduction,
  title = {Interpretable Dimensionality Reduction of Single Cell Transcriptome Data with Deep Generative Models},
  author = {Ding, Jiarui and Condon, Anne and Shah, Sohrab P.},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {2002},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04368-5},
  annotation = {ZSCC: 0000068},
  file = {/home/trung/Zotero/storage/X3QKFQTN/Ding et al. - 2018 - Interpretable dimensionality reduction of single c.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@techreport{ding19_Deepgenerativemodel,
  title = {Deep Generative Model Embedding of Single-Cell {{RNA}}-{{Seq}} Profiles on Hyperspheres and Hyperbolic Spaces},
  author = {Ding, Jiarui and Regev, Aviv},
  year = {2019},
  month = nov,
  institution = {{Bioinformatics}},
  doi = {10.1101/853457},
  abstract = {Single-cell RNA-Seq (scRNA-seq) has become an invaluable tool for studying biological systems in health and diseases. While dimensionality reduction is a crucial step in interpreting the relation between cells based on scRNA-seq, current methods often are hampered by ``crowding'' of cells in the center of the latent space, biased by batch effects, or inadequately capture developmental relationships. Here, we introduced scPhere, a scalable deep generative model to embed cells into low-dimensional hyperspherical or hyperbolic spaces, as a more accurate representation of the data. ScPhere resolves cell crowding, corrects multiple, complex batch factors, facilitates interactive visualization of large datasets, and gracefully uncovers pseudotemporal trajectories. We demonstrate scPhere on six large datasets in complex tissue from human patients or animal development, demonstrating how it controls for both technical and biological factors and highlights complex cellular relations and biological insights.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/ding et al_2019_deep generative model embedding of single-cell rna-seq profiles on hyperspheres and hyperbolic spaces.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{ding19_Deepgenerativemodela,
  title = {Deep Generative Model Embedding of Single-Cell {{RNA}}-{{Seq}} Profiles on Hyperspheres and Hyperbolic Spaces},
  author = {Ding, Jiarui and Regev, Aviv},
  year = {2019},
  month = nov,
  institution = {{Bioinformatics}},
  doi = {10.1101/853457},
  abstract = {Single-cell RNA-Seq (scRNA-seq) has become an invaluable tool for studying biological systems in health and diseases. While dimensionality reduction is a crucial step in interpreting the relation between cells based on scRNA-seq, current methods often are hampered by ``crowding'' of cells in the center of the latent space, biased by batch effects, or inadequately capture developmental relationships. Here, we introduced scPhere, a scalable deep generative model to embed cells into low-dimensional hyperspherical or hyperbolic spaces, as a more accurate representation of the data. ScPhere resolves cell crowding, corrects multiple, complex batch factors, facilitates interactive visualization of large datasets, and gracefully uncovers pseudotemporal trajectories. We demonstrate scPhere on six large datasets in complex tissue from human patients or animal development, demonstrating how it controls for both technical and biological factors and highlights complex cellular relations and biological insights.},
  file = {/home/trung/GoogleDrive/Zotero/false},
  language = {en},
  type = {Preprint}
}

@article{dinh17_Densityestimationusing,
  title = {Density Estimation Using {{Real NVP}}},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Bengio, Samy},
  year = {2017},
  month = feb,
  abstract = {Unsupervised learning of probabilistic models is a central yet challenging problem in machine learning. Specifically, designing models with tractable learning, sampling, inference and evaluation is crucial in solving this task. We extend the space of such models using real-valued non-volume preserving (real NVP) transformations, a set of powerful invertible and learnable transformations, resulting in an unsupervised learning algorithm with exact log-likelihood computation, exact sampling, exact inference of latent variables, and an interpretable latent space. We demonstrate its ability to model natural images on four datasets through sampling, log-likelihood evaluation and latent variable manipulations.},
  archivePrefix = {arXiv},
  eprint = {1605.08803},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dinh et al_2017_density estimation using real nvp.pdf},
  journal = {arXiv:1605.08803 [cs, stat]},
  primaryClass = {cs, stat}
}

@phdthesis{dinh18_ReparametrizationDeepLearning,
  title = {Reparametrization in {{Deep Learning}}},
  author = {Dinh, Laurent},
  year = {2018},
  file = {/home/trung/GoogleDrive/Zotero/dinh_2018_reparametrization in deep learning.pdf}
}

@article{dinh19_RADapproachdeep,
  title = {A {{RAD}} Approach to Deep Mixture Models},
  author = {Dinh, Laurent and {Sohl-Dickstein}, Jascha and Pascanu, Razvan and Larochelle, Hugo},
  year = {2019},
  month = mar,
  abstract = {Flow based models such as Real NVP are an extremely powerful approach to density estimation. However, existing flow based models are restricted to transforming continuous densities over a continuous input space into similarly continuous distributions over continuous latent variables. This makes them poorly suited for modeling and representing discrete structures in data distributions, for example class membership or discrete symmetries. To address this difficulty, we present a normalizing flow architecture which relies on domain partitioning using locally invertible functions, and possesses both real and discrete valued latent variables. This Real and Discrete (RAD) approach retains the desirable normalizing flow properties of exact sampling, exact inference, and analytically computable probabilities, while at the same time allowing simultaneous modeling of both continuous and discrete structure in a data distribution.},
  archivePrefix = {arXiv},
  eprint = {1903.07714},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dinh et al_2019_a rad approach to deep mixture models.pdf;/home/trung/Zotero/storage/WRTGPICX/1903.html},
  journal = {arXiv:1903.07714 [cs, stat]},
  keywords = {Computer Science - Machine Learning,mixture,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{dittadi19_LAVAEDisentanglingLocation,
  title = {{{LAVAE}}: {{Disentangling Location}} and {{Appearance}}},
  shorttitle = {{{LAVAE}}},
  author = {Dittadi, Andrea and Winther, Ole},
  year = {2019},
  month = sep,
  abstract = {We propose a probabilistic generative model for unsupervised learning of structured, interpretable, object-based representations of visual scenes. We use amortized variational inference to train the generative model end-to-end. The learned representations of object location and appearance are fully disentangled, and objects are represented independently of each other in the latent space. Unlike previous approaches that disentangle location and appearance, ours generalizes seamlessly to scenes with many more objects than encountered in the training regime. We evaluate the proposed model on multi-MNIST and multi-dSprites data sets.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1909.11813},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Dittadi_Winther_2019_LAVAE2.pdf;/home/trung/Zotero/storage/ULFLD2XM/1909.html},
  journal = {arXiv:1909.11813 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{dittadi20_TransferDisentangledRepresentations,
  title = {On the {{Transfer}} of {{Disentangled Representations}} in {{Realistic Settings}}},
  author = {Dittadi, Andrea and Tr{\"a}uble, Frederik and Locatello, Francesco and W{\"u}thrich, Manuel and Agrawal, Vaibhav and Winther, Ole and Bauer, Stefan and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = oct,
  abstract = {Learning meaningful representations that disentangle the underlying structure of the data generating process is considered to be of key importance in machine learning. While disentangled representations were found to be useful for diverse tasks such as abstract reasoning and fair classification, their scalability and real-world impact remain questionable. We introduce a new high-resolution dataset with 1M simulated images and over 1,800 annotated real-world images of the same robotic setup. In contrast to previous work, this new dataset exhibits correlations, a complex underlying structure, and allows to evaluate transfer to unseen simulated and real-world settings where the encoder i) remains in distribution or ii) is out of distribution. We propose new architectures in order to scale disentangled representation learning to realistic high-resolution settings and conduct a large-scale empirical study of disentangled representations on this dataset. We observe that disentanglement is a good predictor for out-of-distribution (OOD) task performance.},
  archivePrefix = {arXiv},
  eprint = {2010.14407},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dittadi et al_2020_on the transfer of disentangled representations in realistic settings.pdf},
  journal = {arXiv:2010.14407 [cs, stat]},
  keywords = {_tablet,disentanglement},
  primaryClass = {cs, stat}
}

@article{djolonga19_EvaluatingGenerativeModels,
  title = {Evaluating {{Generative Models Using Divergence Frontiers}}},
  author = {Djolonga, Josip and Lucic, Mario and Cuturi, Marco and Bachem, Olivier and Bousquet, Olivier and Gelly, Sylvain},
  year = {2019},
  month = may,
  abstract = {Despite the tremendous progress in the estimation of generative models, the development of tools for diagnosing their failures and assessing their performance has advanced at a much slower pace. Recent developments have investigated metrics that quantify which parts of the true distribution are modeled well, and, on the contrary, what the model fails to capture, akin to precision and recall in information retrieval. In this paper, we present a general evaluation framework for generative models that measures the trade-off between precision and recall using R\textbackslash 'enyi divergences. Our framework provides a novel perspective on existing techniques and extends them to more general domains. As a key advantage, it allows for efficient algorithms that are directly applicable to continuous distributions directly without discretization. We further showcase the proposed techniques on a set of image synthesis models.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1905.10768},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/djolonga et al_2019_evaluating generative models using divergence frontiers.pdf;/home/trung/Zotero/storage/THEGHP73/1905.html},
  journal = {arXiv:1905.10768 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@inproceedings{Do2020Theory,
  title = {Theory and Evaluation Metrics for Learning Disentangled Representations},
  booktitle = {International Conference on Learning Representations},
  author = {Do, Kien and Tran, Truyen},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/do et al_2020_theory and evaluation metrics for learning disentangled representations.pdf},
  keywords = {disentanglement}
}

@article{doersch16_TutorialVariationalAutoencoders,
  title = {Tutorial on {{Variational Autoencoders}}},
  author = {Doersch, Carl},
  year = {2016},
  month = aug,
  abstract = {In just three years, Variational Autoencoders (VAEs) have emerged as one of the most popular approaches to unsupervised learning of complicated distributions. VAEs are appealing because they are built on top of standard function approximators (neural networks), and can be trained with stochastic gradient descent. VAEs have already shown promise in generating many kinds of complicated data, including handwritten digits, faces, house numbers, CIFAR images, physical models of scenes, segmentation, and predicting the future from static images. This tutorial introduces the intuitions behind VAEs, explains the mathematics behind them, and describes some empirical behavior. No prior knowledge of variational Bayesian methods is assumed.},
  annotation = {ZSCC: 0000571},
  archivePrefix = {arXiv},
  eprint = {1606.05908},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/doersch_2016_tutorial on variational autoencoders.pdf;/home/trung/Zotero/storage/BL8W8R5Q/1606.html},
  journal = {arXiv:1606.05908 [cs, stat]},
  keywords = {_tablet},
  primaryClass = {cs, stat}
}

@article{donahue19_LargeScaleAdversarial,
  title = {Large {{Scale Adversarial Representation Learning}}},
  author = {Donahue, Jeff and Simonyan, Karen},
  year = {2019},
  month = jul,
  abstract = {Adversarially trained generative models (GANs) have recently achieved compelling image synthesis results. But despite early successes in using GANs for unsupervised representation learning, they have since been superseded by approaches based on self-supervision. In this work we show that progress in image generation quality translates to substantially improved representation learning performance. Our approach, BigBiGAN, builds upon the state-of-the-art BigGAN model, extending it to representation learning by adding an encoder and modifying the discriminator. We extensively evaluate the representation learning and generation capabilities of these BigBiGAN models, demonstrating that these generation-based models achieve the state of the art in unsupervised representation learning on ImageNet, as well as in unconditional image generation.},
  annotation = {ZSCC: 0000004},
  archivePrefix = {arXiv},
  eprint = {1907.02544},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/donahue et al_2019_large scale adversarial representation learning.pdf},
  journal = {arXiv:1907.02544 [cs, stat]},
  keywords = {adversarial,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,representation,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dong18_Diseasepreventiondelayed,
  title = {Disease Prevention and Delayed Aging by Dietary Sulfur Amino Acid Restriction: Translational Implications},
  shorttitle = {Disease Prevention and Delayed Aging by Dietary Sulfur Amino Acid Restriction},
  author = {Dong, Zhen and Sinha, Raghu and Richie, John P.},
  year = {2018},
  volume = {1418},
  pages = {44--55},
  issn = {1749-6632},
  doi = {10.1111/nyas.13584},
  abstract = {Sulfur amino acids (SAAs) play numerous critical roles in metabolism and overall health maintenance. Preclinical studies have demonstrated that SAA-restricted diets have many beneficial effects, including extending life span and preventing the development of a variety of diseases. Dietary sulfur amino acid restriction (SAAR) is characterized by chronic restrictions of methionine and cysteine but not calories and is associated with reductions in body weight, adiposity and oxidative stress, and metabolic changes in adipose tissue and liver resulting in enhanced insulin sensitivity and energy expenditure. SAAR-induced changes in blood biomarkers include reductions in insulin, insulin-like growth factor-1, glucose, and leptin and increases in adiponectin and fibroblast growth factor 21. On the basis of these preclinical data, SAAR may also have similar benefits in humans. While little is known of the translational significance of SAAR, its potential feasibility in humans is supported by findings of its effectiveness in rodents, even when initiated in adult animals. To date, there have been no controlled feeding studies of SAAR in humans; however, there have been numerous relevant epidemiologic and disease-based clinical investigations reported. Here, we summarize observations from these clinical investigations to provide insight into the potential effectiveness of SAAR for humans.},
  annotation = {ZSCC: 0000014},
  copyright = {\textcopyright{} 2018 New York Academy of Sciences.},
  file = {/home/trung/Zotero/storage/PRZQBSAL/nyas.html},
  journal = {Annals of the New York Academy of Sciences},
  language = {en},
  number = {1}
}

@article{dong19_DistillationapproxEarly,
  title = {Distillation \$\textbackslash approx\$ {{Early Stopping}}? {{Harvesting Dark Knowledge Utilizing Anisotropic Information Retrieval For Overparameterized Neural Network}}},
  shorttitle = {Distillation \$\textbackslash approx\$ {{Early Stopping}}?},
  author = {Dong, Bin and Hou, Jikai and Lu, Yiping and Zhang, Zhihua},
  year = {2019},
  month = oct,
  abstract = {Distillation is a method to transfer knowledge from one model to another and often achieves higher accuracy with the same capacity. In this paper, we aim to provide a theoretical understanding on what mainly helps with the distillation. Our answer is "early stopping". Assuming that the teacher network is overparameterized, we argue that the teacher network is essentially harvesting dark knowledge from the data via early stopping. This can be justified by a new concept, \{Anisotropic Information Retrieval (AIR)\}, which means that the neural network tends to fit the informative information first and the non-informative information (including noise) later. Motivated by the recent development on theoretically analyzing overparameterized neural networks, we can characterize AIR by the eigenspace of the Neural Tangent Kernel(NTK). AIR facilities a new understanding of distillation. With that, we further utilize distillation to refine noisy labels. We propose a self-distillation algorithm to sequentially distill knowledge from the network in the previous training epoch to avoid memorizing the wrong labels. We also demonstrate, both theoretically and empirically, that self-distillation can benefit from more than just early stopping. Theoretically, we prove convergence of the proposed algorithm to the ground truth labels for randomly initialized overparameterized neural networks in terms of \$\textbackslash ell\_2\$ distance, while the previous result was on convergence in \$0\$-\$1\$ loss. The theoretical result ensures the learned neural network enjoy a margin on the training data which leads to better generalization. Empirically, we achieve better testing accuracy and entirely avoid early stopping which makes the algorithm more user-friendly.},
  archivePrefix = {arXiv},
  eprint = {1910.01255},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dong et al_2019_distillation $-approx$ early stopping.pdf},
  journal = {arXiv:1910.01255 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{dong19_PredictiveInformationSuboptimality,
  title = {On {{Predictive Information Sub}}-Optimality of {{RNNs}}},
  author = {Dong, Zhe and Oktay, Deniz and Poole, Ben and Alemi, Alexander A.},
  year = {2019},
  month = oct,
  abstract = {Certain biological neurons demonstrate a remarkable capability to optimally compress the history of sensory inputs while being maximally informative about the future. In this work, we investigate if the same can be said of artificial neurons in recurrent neural networks (RNNs) trained with maximum likelihood. In experiments on two datasets, restorative Brownian motion and a hand-drawn sketch dataset, we find that RNNs are sub-optimal in the information plane. Instead of optimally compressing past information, they extract additional information that is not relevant for predicting the future. Overcoming this limitation may require alternative training procedures and architectures, or objectives beyond maximum likelihood estimation.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.09578},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dong et al_2019_on predictive information sub-optimality of rnns.pdf;/home/trung/Zotero/storage/TXBQRCZK/1910.html},
  journal = {arXiv:1910.09578 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,information,recurrent,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{dong20_Associationsulfuramino,
  title = {Association of Sulfur Amino Acid Consumption with Cardiometabolic Risk Factors: {{Cross}}-Sectional Findings from {{NHANES III}}},
  shorttitle = {Association of Sulfur Amino Acid Consumption with Cardiometabolic Risk Factors},
  author = {Dong, Zhen and Gao, Xiang and Chinchilli, Vernon M. and Sinha, Raghu and Muscat, Joshua and Winkels, Renate M. and Richie, John P.},
  year = {2020},
  month = feb,
  pages = {100248},
  issn = {25895370},
  doi = {10.1016/j.eclinm.2019.100248},
  abstract = {Background: An average adult American consumes sulfur amino acids (SAA) at levels far above the Estimated Average Requirement (EAR) and recent preclinical data suggest that higher levels of SAA intake may be associated with a variety of aging-related chronic diseases. However, there are little data regarding the relationship between SAA intake and chronic disease risk in humans. The aim of this study was to examine the associations between consumption of SAA and risk factors for cardiometabolic diseases.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/EDANUDZW/Dong et al. - 2020 - Association of sulfur amino acid consumption with .pdf},
  journal = {EClinicalMedicine},
  language = {en}
}

@article{dong20_netAESemisuperviseddimensionality,
  title = {{{netAE}}: {{Semi}}-Supervised Dimensionality Reduction of Single-Cell {{RNA}} Sequencing to Facilitate Cell Labeling},
  shorttitle = {{{netAE}}},
  author = {Dong, Zhengyang and Alterovitz, Gil},
  editor = {Gorodkin, Jan},
  year = {2020},
  month = jul,
  pages = {btaa669},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btaa669},
  abstract = {Motivation: Single-cell RNA sequencing allows us to study cell heterogeneity at an unprecedented cell-level resolution and identify known and new cell populations. Current cell labeling pipeline uses unsupervised clustering and assigns labels to clusters by manual inspection. However, this pipeline does not utilize available gold-standard labels because there are usually too few of them to be useful to most computational methods. This paper aims to facilitate cell labeling with a semi-supervised method in an alternative pipeline, in which a few gold-standard labels are first identified and then extended to the rest of the cells computationally.},
  file = {/home/trung/GoogleDrive/Zotero/dong et al_2020_netae.pdf},
  journal = {Bioinformatics},
  language = {en}
}

@article{donnat19_ConstrainedBayesianICA,
  title = {Constrained {{Bayesian ICA}} for {{Brain Connectome Inference}}},
  author = {Donnat, Claire and Tozzi, Leonardo and Holmes, Susan},
  year = {2019},
  month = nov,
  abstract = {Brain connectomics is a developing field in neurosciences which strives to understand cognitive processes and psychiatric diseases through the analysis of interactions between brain regions. However, in the high-dimensional, low-sample, and noisy regimes that typically characterize fMRI data, the recovery of such interactions remains an ongoing challenge: how can we discover patterns of co-activity between brain regions that could then be associated to cognitive processes or psychiatric disorders? In this paper, we investigate a constrained Bayesian ICA approach which, in comparison to current methods, simultaneously allows (a) the flexible integration of multiple sources of information (fMRI, DWI, anatomical, etc.), (b) an automatic and parameter-free selection of the appropriate sparsity level and number of connected submodules and (c) the provision of estimates on the uncertainty of the recovered interactions. Our experiments, both on synthetic and real-life data, validate the flexibility of our method and highlight the benefits of integrating anatomical information for connectome inference.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.05770},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/donnat et al_2019_constrained bayesian ica for brain connectome inference.pdf},
  journal = {arXiv:1911.05770 [stat]},
  keywords = {Quantitative Biology - Neurons and Cognition,Statistics - Applications},
  language = {en},
  primaryClass = {stat}
}

@article{donnelly19_CounterfactualInferenceConsumer,
  title = {Counterfactual {{Inference}} for {{Consumer Choice Across Many Product Categories}}},
  author = {Donnelly, Rob and Ruiz, Francisco R. and Blei, David and Athey, Susan},
  year = {2019},
  month = jun,
  abstract = {This paper proposes a method for estimating consumer preferences among discrete choices, where the consumer chooses at most one product in a category, but selects from multiple categories in parallel. The consumer's utility is additive in the different categories. Her preferences about product attributes as well as her price sensitivity vary across products and are in general correlated across products. We build on techniques from the machine learning literature on probabilistic models of matrix factorization, extending the methods to account for time-varying product attributes and products going out of stock. We evaluate the performance of the model using held-out data from weeks with price changes or out of stock products. We show that our model improves over traditional modeling approaches that consider each category in isolation. One source of the improvement is the ability of the model to accurately estimate heterogeneity in preferences (by pooling information across categories); another source of improvement is its ability to estimate the preferences of consumers who have rarely or never made a purchase in a given category in the training data. Using held-out data, we show that our model can accurately distinguish which consumers are most price sensitive to a given product. We consider counterfactuals such as personally targeted price discounts, showing that using a richer model such as the one we propose substantially increases the benefits of personalization in discounts.},
  archivePrefix = {arXiv},
  eprint = {1906.02635},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/donnelly et al_2019_counterfactual inference for consumer choice across many product categories.pdf;/home/trung/Zotero/storage/NRAFWAW3/1906.html},
  journal = {arXiv:1906.02635 [cs, econ, stat]},
  keywords = {causal,Computer Science - Machine Learning,Economics - Econometrics,Statistics - Machine Learning},
  primaryClass = {cs, econ, stat}
}

@article{dony20_Variationalautoencodersflexible,
  title = {Variational Autoencoders with Flexible Priors Enable Robust Distribution Learning on Single-Cell {{RNA}} Sequencing Data},
  author = {Dony, Leander and K{\"o}nig, Martin and Fischer, David S and Theis, Fabian J},
  year = {2020},
  pages = {5},
  abstract = {Generative modeling in single cell transcriptomics allows the efficient construction of latent spaces for denoising, batch-effect removal and prediction of experimental perturbations. To obtain biologically informative latent representations, recently established methods, however, rely on adapting the variational autoencoder (VAE) loss through down-weighting the Kullback-Leibler divergence term. These adaptations can limit the model's ability to learn the underlying data distribution. Here, we adapt two enhanced VAE architectures to the scRNA-seq setting which do not require tuning the loss: (i) a VAE with inverse autoregressive flow (IAF) and (ii) a VAE with a Variational Mixture of Posteriors (VAMP) prior. We assess the models' ability to learn biologically informative embeddings using four metrics in a large-scale comparison on 16 public scRNA-seq datasets from 9 tissues with over 700,000 cells. We find that in particular the VAE with a VAMP prior is capable of learning biologically informative embeddings without compromising on generative properties. This suggests that the VAEVAMP is a useful starting point for improved generative modelling of scRNA-seq data.},
  file = {/home/trung/GoogleDrive/Zotero/dony et al_2020_variational autoencoders with flexible priors enable robust distribution learning on single-cell rna sequencing data.pdf},
  keywords = {_tablet},
  language = {en}
}

@article{doogan20_PublicPerceptionsAttitudes,
  title = {Public {{Perceptions}} and {{Attitudes Towards COVID}}-19 {{Non}}-{{Pharmaceutical Interventions Across Six Countries}}: {{A Topic Modeling Analysis}} of {{Twitter Data}} ({{Preprint}})},
  shorttitle = {Public {{Perceptions}} and {{Attitudes Towards COVID}}-19 {{Non}}-{{Pharmaceutical Interventions Across Six Countries}}},
  author = {Doogan, Caitlin and Buntine, Wray and Linger, Henry and Brunt, Samantha},
  year = {2020},
  month = jun,
  issn = {1438-8871},
  doi = {10.2196/21419},
  abstract = {Background: Non-pharmaceutical interventions (NPIs) have been implemented by governments around the world to slow the spread of COVID-19. To promote public adherence to these regimes, governments need to understand the public perceptions and attitudes towards NPI regimes and the factors that influence these. Twitter data offers a means to capture these insights. Objective: The objective of this study is to identify tweets about COVID-19 NPIs in six countries and compare the trends in public perceptions and attitudes towards NPIs across these countries. The aim is to identify factors that influenced the public perceptions and attitudes about NPI regimes during the early phases of the COVID-19 pandemic. Methods: We analyzed 777,869 English language tweets about COVID-19 NPIs in six countries (Australia, Canada, New Zealand, Ireland, the United Kingdom [UK], and the United States [US]). The relationship between tweet frequencies and case numbers was assessed using a Pearson correlation analysis. Topic modeling was used to isolate tweets about NPIs. A comparative analysis of NPIs between countries was conducted. Results: The proportion of NPI related topics, relative to all topics, varied between countries. The New Zealand dataset displayed the greatest attention to NPIs, and the US dataset showed the lowest. The relationship between tweet frequencies and case numbers was statistically significant only for Australia (r=0.837, P{$<$}.001) and New Zealand (r=0.747, P{$<$}.001). Topic modeling produced 131 topics relating to one of 22 NPIs, grouped into seven NPI categories: Personal Protection (n=15), Social Distancing (n=9), Testing and Tracing (n=10), Gathering Restrictions (n=18), Lockdown (n=42), Travel Restrictions (n=14), and Workplace Closures (n=23). While less restrictive NPIs gained widespread support, more restrictive NPIs were perceived differently between countries. Four characteristics of these regimes were seen to influence public adherence to NPIs: timeliness of implementation, NPI campaign strategies, inconsistent information, and enforcement strategies. Conclusions: Twitter offers a means to obtain timely feedback about the public response to COVID-19 NPI regimes. Insights gained from this analysis would support government decision-making, implementation, and communication strategies about NPI regimes, as well as encourage further discussion about the management of NPI programs for global health events, such as the COVID-19 pandemic.},
  file = {/home/trung/GoogleDrive/Zotero/doogan et al_2020_public perceptions and attitudes towards covid-19 non-pharmaceutical interventions across six countries.pdf},
  journal = {Journal of Medical Internet Research},
  language = {en}
}

@inproceedings{doquire12_ComparisonMultivariateMutual,
  title = {A {{Comparison}} of {{Multivariate Mutual Information Estimators}} for {{Feature Selection}}},
  shorttitle = {A {{Comparison}} of {{Multivariate Mutual Information Estimators}} for {{Feature Selection}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{Pattern Recognition Applications}} and {{Methods}}},
  author = {Doquire, Gauthier and Verleysen, Michel},
  year = {2012},
  pages = {176--185},
  publisher = {{SciTePress - Science and and Technology Publications}},
  address = {{Vilamoura, Algarve, Portugal}},
  doi = {10.5220/0003726101760185},
  abstract = {Mutual Information estimation is an important task for many data mining and machine learning applications.},
  file = {/home/trung/GoogleDrive/Zotero/2012_a comparison of multivariate mutual information estimators for feature selection.pdf},
  isbn = {978-989-8425-98-0 978-989-8425-99-7},
  keywords = {information},
  language = {en}
}

@article{dorrity20_DimensionalityreductionUMAP,
  title = {Dimensionality Reduction by {{UMAP}} to Visualize Physical and Genetic Interactions},
  author = {Dorrity, Michael W. and Saunders, Lauren M. and Queitsch, Christine and Fields, Stanley and Trapnell, Cole},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {1537},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-15351-4},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/Zotero/storage/CDLVTMSE/Dorrity et al. - 2020 - Dimensionality reduction by UMAP to visualize phys.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{du19_GoodRepresentationSufficient,
  title = {Is a {{Good Representation Sufficient}} for {{Sample Efficient Reinforcement Learning}}?},
  author = {Du, Simon S. and Kakade, Sham M. and Wang, Ruosong and Yang, Lin F.},
  year = {2019},
  month = nov,
  abstract = {Modern deep learning methods provide an effective means to learn good representations. However, is a good representation itself sufficient for efficient reinforcement learning? This question is largely unexplored, and the extant body of literature mainly focuses on conditions which permit efficient reinforcement learning with little understanding of what are necessary conditions for efficient reinforcement learning. This work provides strong negative results for reinforcement learning methods with function approximation for which a good representation (feature extractor) is known to the agent, focusing on natural representational conditions relevant to value-based learning and policy-based learning. For value-based learning, we show that even if the agent has a highly accurate linear representation, the agent still needs to sample exponentially many trajectories in order to find a near-optimal policy. For policy-based learning, we show even if the agent's linear representation is capable of perfectly representing the optimal policy, the agent still needs to sample exponentially many trajectories in order to find a near-optimal policy. These lower bounds highlight the fact that having a good (value-based or policy-based) representation in and of itself is insufficient for efficient reinforcement learning. In particular, these results provide new insights into why the existing provably efficient reinforcement learning methods rely on further assumptions, which are often model-based in nature. Additionally, our lower bounds imply exponential separations in the sample complexity between 1) valuebased learning with perfect representation and value-based learning with a good-but-not-perfect representation, 2) value-based learning and policy-based learning, 3) policy-based learning and supervised learning and 4) reinforcement learning and imitation learning.},
  archivePrefix = {arXiv},
  eprint = {1910.03016},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2019_is a good representation sufficient for sample efficient reinforcement learning.pdf},
  journal = {arXiv:1910.03016 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{du19_GraphNeuralTangent,
  title = {Graph {{Neural Tangent Kernel}}: {{Fusing Graph Neural Networks}} with {{Graph Kernels}}},
  shorttitle = {Graph {{Neural Tangent Kernel}}},
  author = {Du, Simon S. and Hou, Kangcheng and P{\'o}czos, Barnab{\'a}s and Salakhutdinov, Ruslan and Wang, Ruosong and Xu, Keyulu},
  year = {2019},
  month = nov,
  abstract = {While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to infinitely wide multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1905.13192},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2019_graph neural tangent kernel.pdf;/home/trung/Zotero/storage/VET6IFWP/1905.html},
  journal = {arXiv:1905.13192 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,graph,graph kernel,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{du19_ImplicitGenerationGeneralization,
  title = {Implicit {{Generation}} and {{Generalization}} in {{Energy}}-{{Based Models}}},
  author = {Du, Yilun and Mordatch, Igor},
  year = {2019},
  month = mar,
  abstract = {Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
  archivePrefix = {arXiv},
  eprint = {1903.08689},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2019_implicit generation and generalization in energy-based models.pdf;/home/trung/Zotero/storage/HJ9NTR6M/1903.html},
  journal = {arXiv:1903.08689 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,energy model,generalization,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{du19_ImplicitGenerationGeneralizationa,
  title = {Implicit {{Generation}} and {{Generalization}} in {{Energy}}-{{Based Models}}},
  author = {Du, Yilun and Mordatch, Igor},
  year = {2019},
  month = nov,
  abstract = {Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.},
  annotation = {ZSCC: 0000028},
  archivePrefix = {arXiv},
  eprint = {1903.08689},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2019_implicit generation and generalization in energy-based models2.pdf},
  journal = {arXiv:1903.08689 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{du20_CompositionalVisualGeneration,
  title = {Compositional {{Visual Generation}} and {{Inference}} with {{Energy Based Models}}},
  author = {Du, Yilun and Li, Shuang and Mordatch, Igor},
  year = {2020},
  month = apr,
  abstract = {A vital aspect of human intelligence is the ability to compose increasingly complex concepts out of simpler ideas, enabling both rapid learning and adaptation of knowledge. In this paper we show that energy-based models can exhibit this ability by directly combining probability distributions. Samples from the combined distribution correspond to compositions of concepts. For example, given a distribution for smiling faces, and another for male faces, we can combine them to generate smiling male faces. This allows us to generate natural images that simultaneously satisfy conjunctions, disjunctions, and negations of concepts. We evaluate compositional generation abilities of our model on the CelebA dataset of natural faces and synthetic 3D scene images. We also demonstrate other unique advantages of our model, such as the ability to continually learn and incorporate new concepts, or infer compositions of concept properties underlying an image.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2004.06030},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2020_compositional visual generation and inference with energy based models.pdf},
  journal = {arXiv:2004.06030 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{du20_DualAdversarialDomain,
  title = {Dual {{Adversarial Domain Adaptation}}},
  author = {Du, Yuntao and Tan, Zhiwen and Chen, Qian and Zhang, Xiaowen and Yao, Yirong and Wang, Chongjun},
  year = {2020},
  month = jan,
  abstract = {Unsupervised domain adaptation aims at transferring knowledge from the labeled source domain to the unlabeled target domain. Previous adversarial domain adaptation methods mostly adopt the discriminator with binary or \$K\$-dimensional output to perform marginal or conditional alignment independently. Recent experiments have shown that when the discriminator is provided with domain information in both domains and label information in the source domain, it is able to preserve the complex multimodal information and high semantic information in both domains. Following this idea, we adopt a discriminator with \$2K\$-dimensional output to perform both domain-level and class-level alignments simultaneously in a single discriminator. However, a single discriminator can not capture all the useful information across domains and the relationships between the examples and the decision boundary are rarely explored before. Inspired by multi-view learning and latest advances in domain adaptation, besides the adversarial process between the discriminator and the feature extractor, we also design a novel mechanism to make two discriminators pit against each other, so that they can provide diverse information for each other and avoid generating target features outside the support of the source domain. To the best of our knowledge, it is the first time to explore a dual adversarial strategy in domain adaptation. Moreover, we also use the semi-supervised learning regularization to make the representations more discriminative. Comprehensive experiments on two real-world datasets verify that our method outperforms several state-of-the-art domain adaptation methods.},
  archivePrefix = {arXiv},
  eprint = {2001.00153},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2020_dual adversarial domain adaptation.pdf;/home/trung/Zotero/storage/X895VJ2L/2001.html},
  journal = {arXiv:2001.00153 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{du20_LearningLearnVariational,
  title = {Learning to {{Learn}} with {{Variational Information Bottleneck}} for {{Domain Generalization}}},
  author = {Du, Yingjun and Xu, Jun and Xiong, Huan and Qiu, Qiang and Zhen, Xiantong and Snoek, Cees G. M. and Shao, Ling},
  year = {2020},
  month = jul,
  abstract = {Domain generalization models learn to generalize to previously unseen domains, but suffer from prediction uncertainty and domain shift. In this paper, we address both problems. We introduce a probabilistic meta-learning model for domain generalization, in which classifier parameters shared across domains are modeled as distributions. This enables better handling of prediction uncertainty on unseen domains. To deal with domain shift, we learn domain-invariant representations by the proposed principle of meta variational information bottleneck, we call MetaVIB. MetaVIB is derived from novel variational bounds of mutual information, by leveraging the meta-learning setting of domain generalization. Through episodic training, MetaVIB learns to gradually narrow domain gaps to establish domain-invariant representations, while simultaneously maximizing prediction accuracy. We conduct experiments on three benchmarks for cross-domain visual recognition. Comprehensive ablation studies validate the benefits of MetaVIB for domain generalization. The comparison results demonstrate our method outperforms previous approaches consistently.},
  archivePrefix = {arXiv},
  eprint = {2007.07645},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2020_learning to learn with variational information bottleneck for domain generalization.pdf},
  journal = {arXiv:2007.07645 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{du20_SelftrainingImprovesPretraining,
  title = {Self-Training {{Improves Pre}}-Training for {{Natural Language Understanding}}},
  author = {Du, Jingfei and Grave, Edouard and Gunel, Beliz and Chaudhary, Vishrav and Celebi, Onur and Auli, Michael and Stoyanov, Ves and Conneau, Alexis},
  year = {2020},
  month = oct,
  abstract = {Unsupervised pre-training has led to much recent progress in natural language understanding. In this paper, we study self-training as another way to leverage unlabeled data through semi-supervised learning. To obtain additional data for a specific task, we introduce SentAugment, a data augmentation method which computes task-specific query embeddings from labeled data to retrieve sentences from a bank of billions of unlabeled sentences crawled from the web. Unlike previous semi-supervised methods, our approach does not require in-domain unlabeled data and is therefore more generally applicable. Experiments show that self-training is complementary to strong RoBERTa baselines on a variety of tasks. Our augmentation approach leads to scalable and effective self-training with improvements of up to 2.6\% on standard text classification benchmarks. Finally, we also show strong gains on knowledge-distillation and few-shot learning.},
  archivePrefix = {arXiv},
  eprint = {2010.02194},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/du et al_2020_self-training improves pre-training for natural language understanding.pdf},
  journal = {arXiv:2010.02194 [cs]},
  primaryClass = {cs}
}

@article{duan19_NGBoostNaturalGradient,
  title = {{{NGBoost}}: {{Natural Gradient Boosting}} for {{Probabilistic Prediction}}},
  shorttitle = {{{NGBoost}}},
  author = {Duan, Tony and Avati, Anand and Ding, Daisy Yi and Basu, Sanjay and Ng, Andrew Y. and Schuler, Alejandro},
  year = {2019},
  month = oct,
  abstract = {We present Natural Gradient Boosting (NGBoost), an algorithm which brings probabilistic prediction capability to gradient boosting in a generic way. Predictive uncertainty estimation is crucial in many applications such as healthcare and weather forecasting. Probabilistic prediction, which is the approach where the model outputs a full probability distribution over the entire outcome space, is a natural way to quantify those uncertainties. Gradient Boosting Machines have been widely successful in prediction tasks on structured input data, but a simple boosting solution for probabilistic prediction of real valued outputs is yet to be made. NGBoost is a gradient boosting approach which uses the \textbackslash emph\{Natural Gradient\} to address technical challenges that makes generic probabilistic prediction hard with existing gradient boosting methods. Our approach is modular with respect to the choice of base learner, probability distribution, and scoring rule. We show empirically on several regression datasets that NGBoost provides competitive predictive performance of both uncertainty estimates and traditional metrics.},
  archivePrefix = {arXiv},
  eprint = {1910.03225},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Duan et al_2019_NGBoost2.pdf;/home/trung/Zotero/storage/XF3LDSY7/1910.html},
  journal = {arXiv:1910.03225 [cs, stat]},
  keywords = {boosting,Computer Science - Machine Learning,gradient,probabilistic prediction,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{duan19_UnsupervisedModelSelection,
  title = {Unsupervised {{Model Selection}} for {{Variational Disentangled Representation Learning}}},
  author = {Duan, Sunny and Matthey, Loic and Saraiva, Andre and Watters, Nicholas and Burgess, Christopher P. and Lerchner, Alexander and Higgins, Irina},
  year = {2019},
  month = sep,
  abstract = {Disentangled representations have recently been shown to improve fairness, data efficiency and generalisation in simple supervised and reinforcement learning tasks. To extend the benefits of disentangled representations to more complex domains and practical applications, it is important to enable hyperparameter tuning and model selection of existing unsupervised approaches without requiring access to ground truth attribute labels, which are not available for most datasets. This paper addresses this problem by introducing a simple yet robust and reliable method for unsupervised disentangled model selection. We show that our approach performs comparably to the existing supervised alternatives across 5400 models from six state of the art unsupervised disentangled representation learning model classes. Furthermore, we show that the ranking produced by our approach correlates well with the final task performance on two different domains.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1905.12614},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/duan et al_2019_unsupervised model selection for variational disentangled representation learning.pdf;/home/trung/Zotero/storage/B2LJA4FA/1905.html},
  journal = {arXiv:1905.12614 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,model selection,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@book{dubois14_MySQLcookbook,
  title = {{{MySQL}} Cookbook},
  author = {DuBois, Paul},
  year = {2014},
  edition = {Third edition},
  publisher = {{O'Reilly}},
  address = {{Beijing ; Sebastopol, CA}},
  annotation = {ZSCC: 0000031},
  file = {/home/trung/GoogleDrive/Zotero/dubois_2014_mysql cookbook.pdf},
  isbn = {978-1-4493-7402-0},
  keywords = {MySQL (Electronic resource),SQL (Computer program language)},
  language = {en},
  lccn = {QA76.73.S67 D587 2014}
}

@article{ducau17_MutualInformationVariational,
  title = {Mutual {{Information}} in {{Variational Autoencoders}}},
  author = {Ducau, Felipe N and Tr{\'e}nous, Sony},
  year = {2017},
  pages = {8},
  abstract = {Motivated by the work of Chen et al. [2], we analyze the role of mutual information in variational autoencoders. We experimentally study the behavior of this model when mutual information between the latent code and the generated data is explicitly enforced as part of its loss function. Furthermore, we make an attempt to formalize the role of MI in the VAE objective. We give an interpretation of a lower bound to the MI as the reconstruction error of a dual VAE.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/ducau et al_2017_mutual information in variational autoencoders.pdf},
  keywords = {information},
  language = {en}
}

@article{dumoulin17_LearnedRepresentationArtistic,
  title = {A {{Learned Representation For Artistic Style}}},
  author = {Dumoulin, Vincent and Shlens, Jonathon and Kudlur, Manjunath},
  year = {2017},
  month = feb,
  abstract = {The diversity of painting styles represents a rich visual vocabulary for the construction of an image. The degree to which one may learn and parsimoniously capture this visual vocabulary measures our understanding of the higher level features of paintings, if not images in general. In this work we investigate the construction of a single, scalable deep network that can parsimoniously capture the artistic style of a diversity of paintings. We demonstrate that such a network generalizes across a diversity of artistic styles by reducing a painting to a point in an embedding space. Importantly, this model permits a user to explore new painting styles by arbitrarily combining the styles learned from individual paintings. We hope that this work provides a useful step towards building rich models of paintings and offers a window on to the structure of the learned representation of artistic style.},
  archivePrefix = {arXiv},
  eprint = {1610.07629},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dumoulin et al_2017_a learned representation for artistic style.pdf},
  journal = {arXiv:1610.07629 [cs]},
  primaryClass = {cs}
}

@article{dumoulin18_guideconvolutionarithmetic,
  title = {A Guide to Convolution Arithmetic for Deep Learning},
  author = {Dumoulin, Vincent and Visin, Francesco},
  year = {2018},
  month = jan,
  abstract = {We introduce a guide to help deep learning practitioners understand and manipulate convolutional neural network architectures. The guide clarifies the relationship between various properties (input shape, kernel shape, zero padding, strides and output shape) of convolutional, pooling and transposed convolutional layers, as well as the relationship between convolutional and transposed convolutional layers. Relationships are derived for various cases, and are illustrated in order to make them intuitive.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1603.07285},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dumoulin et al_2018_a guide to convolution arithmetic for deep learning.pdf;/home/trung/Zotero/storage/N98TC7M8/1603.html},
  journal = {arXiv:1603.07285 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{dunkelau00_FairnessAwareMachineLearning,
  title = {Fairness-{{Aware Machine Learning}}},
  author = {Dunkelau, Jannik and Leuschel, Michael},
  pages = {60},
  abstract = {We provide an overview of the state-of-the-art in fairnessaware machine learning and examine a wide variety of research articles in the area. We survey different fairness notions, algorithms for pre-, in-, and post-processing of the data and models, and provide an overview of available frameworks.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/dunkelau et al_fairness-aware machine learning.pdf},
  language = {en}
}

@article{dupont18_LearningDisentangledJoint,
  title = {Learning {{Disentangled Joint Continuous}} and {{Discrete Representations}}},
  author = {Dupont, Emilien},
  year = {2018},
  month = oct,
  abstract = {We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.},
  annotation = {ZSCC: 0000035},
  archivePrefix = {arXiv},
  eprint = {1804.00104},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dupont_2018_learning disentangled joint continuous and discrete representations.pdf},
  journal = {arXiv:1804.00104 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{durif19_pCMFProbabilisticCount,
  title = {{{pCMF}}: {{Probabilistic Count Matrix Factorization}} for {{Single Cell Expression Data Analysis}}},
  author = {Durif, Ghislain and Modolo, Laurent and Mold, Jeff E. and {Lambert-Lacroix}, Sophie and Picard, Franck},
  year = {2019},
  month = mar,
  abstract = {The development of high throughput single-cell sequencing technologies now allows the investigation of the population level diversity of cellular transcriptomes. This diversity has shown two faces. First, the expression dynamics (gene to gene variability) can be quantified more accurately, thanks to the measurement of lowly-expressed genes. Second, the cell-to-cell variability is high, with a low proportion of cells expressing the same gene at the same time/level. Those emerging patterns appear to be very challenging from the statistical point of view, especially to represent and to provide a summarized view of single-cell expression data. PCA is one of the most powerful framework to provide a suitable representation of high dimensional datasets, by searching for latent directions catching the most variability in the data. Unfortunately, classical PCA is based on Euclidean distances and projections that work poorly in presence of over-dispersed counts that show drop-out events (zero-inflation) like single-cell expression data We propose a probabilistic Count Matrix Factorization (pCMF) approach for single-cell expression data analysis, that relies on a sparse Gamma-Poisson factor model. This hierarchical model is inferred using a variational EM algorithm. It is able to jointly build a low dimensional representation of cells and genes. We show how this probabilistic framework induces a geometry that is suitable for single-cell data visualization, and produces a compression of the data that is very powerful for clustering purposes. Our method is competed against other standard representation methods like t-SNE, and we illustrate its performance for the representation of single-cell data. We especially focus on publicly available single-cell RNA-seq datasets.},
  annotation = {ZSCC: 0000009},
  archivePrefix = {arXiv},
  eprint = {1710.11028},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/durif et al_2019_pcmf.pdf;/home/trung/Zotero/storage/6RVS8RNE/1710.html},
  journal = {arXiv:1710.11028 [stat]},
  primaryClass = {stat}
}

@article{durkan19_NeuralSplineFlows,
  title = {Neural {{Spline Flows}}},
  author = {Durkan, Conor and Bekasov, Artur and Murray, Iain and Papamakarios, George},
  year = {2019},
  month = dec,
  abstract = {A normalizing flow models a complex probability density as an invertible transformation of a simple base density. Flows based on either coupling or autoregressive transforms both offer exact density evaluation and sampling, but rely on the parameterization of an easily invertible elementwise transformation, whose choice determines the flexibility of these models. Building upon recent work, we propose a fully-differentiable module based on monotonic rational-quadratic splines, which enhances the flexibility of both coupling and autoregressive transforms while retaining analytic invertibility. We demonstrate that neural spline flows improve density estimation, variational inference, and generative modeling of images.},
  archivePrefix = {arXiv},
  eprint = {1906.04032},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/durkan et al_2019_neural spline flows.pdf},
  journal = {arXiv:1906.04032 [cs, stat]},
  primaryClass = {cs, stat}
}

@incollection{duvenaud15_ConvolutionalNetworksGraphs,
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 28},
  author = {Duvenaud, David K and Maclaurin, Dougal and Iparraguirre, Jorge and Bombarell, Rafael and Hirzel, Timothy and {Aspuru-Guzik}, Alan and Adams, Ryan P},
  editor = {Cortes, C. and Lawrence, N. D. and Lee, D. D. and Sugiyama, M. and Garnett, R.},
  year = {2015},
  pages = {2224--2232},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/duvenaud et al_2015_convolutional networks on graphs for learning molecular fingerprints.pdf;/home/trung/Zotero/storage/C82TA2Y3/5954-convolutional-networks-on-graphs-for-learning-molecular-fingerprints.html},
  keywords = {graph}
}

@article{duvenaud15_ConvolutionalNetworksGraphsa,
  title = {Convolutional {{Networks}} on {{Graphs}} for {{Learning Molecular Fingerprints}}},
  author = {Duvenaud, David and Maclaurin, Dougal and {Aguilera-Iparraguirre}, Jorge and {G{\'o}mez-Bombarelli}, Rafael and Hirzel, Timothy and {Aspuru-Guzik}, Al{\'a}n and Adams, Ryan P.},
  year = {2015},
  month = nov,
  abstract = {We introduce a convolutional neural network that operates directly on graphs. These networks allow end-to-end learning of prediction pipelines whose inputs are graphs of arbitrary size and shape. The architecture we present generalizes standard molecular feature extraction methods based on circular fingerprints. We show that these data-driven features are more interpretable, and have better predictive performance on a variety of tasks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1509.09292},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/duvenaud et al_2015_convolutional networks on graphs for learning molecular fingerprints2.pdf},
  journal = {arXiv:1509.09292 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dwivedi20_BenchmarkingGraphNeural,
  title = {Benchmarking {{Graph Neural Networks}}},
  author = {Dwivedi, Vijay Prakash and Joshi, Chaitanya K. and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  year = {2020},
  month = mar,
  abstract = {Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. They have been successfully applied to a myriad of domains including chemistry, physics, social sciences, knowledge graphs, recommendation, and neuroscience. As the field grows, it becomes critical to identify the architectures and key mechanisms which generalize across graphs sizes, enabling us to tackle larger, more complex datasets and domains. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new GNNs and compare models in the absence of a standardized benchmark with consistent experimental settings and large datasets. In this paper, we propose a reproducible GNN benchmarking framework6, with the facility for researchers to add new datasets and models conveniently. We apply this benchmarking framework to novel medium-scale graph datasets from mathematical modeling, computer vision, chemistry and combinatorial problems to establish key operations when designing effective GNNs. Precisely, graph convolutions, anisotropic diffusion, residual connections and normalization layers are universal building blocks for developing robust and scalable GNNs.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {2003.00982},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/RMQUFAM6/Dwivedi et al. - 2020 - Benchmarking Graph Neural Networks.pdf},
  journal = {arXiv:2003.00982 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{dwivedi20_BenchmarkingGraphNeurala,
  title = {Benchmarking {{Graph Neural Networks}}},
  author = {Dwivedi, Vijay Prakash and Joshi, Chaitanya K. and Laurent, Thomas and Bengio, Yoshua and Bresson, Xavier},
  year = {2020},
  month = jul,
  abstract = {Graph neural networks (GNNs) have become the standard toolkit for analyzing and learning from data on graphs. As the field grows, it becomes critical to identify key architectures and validate new ideas that generalize to larger, more complex datasets. Unfortunately, it has been increasingly difficult to gauge the effectiveness of new models in the absence of a standardized benchmark with consistent experimental settings. In this paper, we introduce a reproducible GNN benchmarking framework, with the facility for researchers to add new models conveniently for arbitrary datasets. We demonstrate the usefulness of our framework by presenting a principled investigation into the recent Weisfeiler-Lehman GNNs (WL-GNNs) compared to message passing-based graph convolutional networks (GCNs) for a variety of graph tasks, i.e. graph regression/classification and node/link prediction, with medium-scale datasets.},
  archivePrefix = {arXiv},
  eprint = {2003.00982},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dwivedi et al_2020_benchmarking graph neural networks.pdf},
  journal = {arXiv:2003.00982 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{dwivedi20_Revisitingcomplexitybiasvariance,
  title = {Revisiting Complexity and the Bias-Variance Tradeoff},
  author = {Dwivedi, Raaz and Singh, Chandan and Yu, Bin and Wainwright, Martin J.},
  year = {2020},
  month = jun,
  abstract = {The recent success of high-dimensional models, such as deep neural networks (DNNs), has led many to question the validity of the bias-variance tradeoff principle in high dimensions. We reexamine it with respect to two key choices: the model class and the complexity measure. We argue that failing to suitably specify either one can falsely suggest that the tradeoff does not hold. This observation motivates us to seek a valid complexity measure, defined with respect to a reasonably good class of models. Building on Rissanen's principle of minimum description length (MDL), we propose a novel MDL-based complexity (MDL-COMP). We focus on the context of linear models, which have been recently used as a stylized tractable approximation to DNNs in high-dimensions. MDL-COMP is defined via an optimality criterion over the encodings induced by a good Ridge estimator class. We derive closed-form expressions for MDL-COMP and show that for a dataset with n observations and d parameters it is not always equal to d/n, and is a function of the singular values of the design matrix and the signal-to-noise ratio. For random Gaussian design, we find that while MDL-COMP scales linearly with d in lowdimensions (d {$<$} n), for high-dimensions (d {$>$} n) the scaling is exponentially smaller, scaling as log d. We hope that such a slow growth of complexity in highdimensions can help shed light on the good generalization performance of several well-tuned high-dimensional models. Moreover, via an array of simulations and real-data experiments, we show that a data-driven Prac-MDL-COMP can inform hyper-parameter tuning for ridge regression in limited data settings, sometimes improving upon cross-validation.},
  archivePrefix = {arXiv},
  eprint = {2006.10189},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/dwivedi et al_2020_revisiting complexity and the bias-variance tradeoff.pdf},
  journal = {arXiv:2006.10189 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{e.min18_SurveyClusteringDeep,
  title = {A {{Survey}} of {{Clustering With Deep Learning}}: {{From}} the {{Perspective}} of {{Network Architecture}}},
  author = {{E. Min} and {X. Guo} and {Q. Liu} and {G. Zhang} and {J. Cui} and {J. Long}},
  year = {2018},
  volume = {6},
  pages = {39501--39514},
  issn = {2169-3536},
  file = {/home/trung/GoogleDrive/Zotero/e. min et al_2018_a survey of clustering with deep learning.pdf},
  journal = {IEEE Access}
}

@article{e20_MathematicalUnderstandingNeural,
  title = {Towards a {{Mathematical Understanding}} of {{Neural Network}}-{{Based Machine Learning}}: What We Know and What We Don't},
  shorttitle = {Towards a {{Mathematical Understanding}} of {{Neural Network}}-{{Based Machine Learning}}},
  author = {E, Weinan and Ma, Chao and Wojtowytsch, Stephan and Wu, Lei},
  year = {2020},
  month = sep,
  abstract = {The purpose of this article is to review the achievements made in the last few years towards the understanding of the reasons behind the success and subtleties of neural network-based machine learning. In the tradition of good old applied mathematics, we will not only give attention to rigorous mathematical results, but also the insight we have gained from careful numerical experiments as well as the analysis of simplified models. Along the way, we also list the open problems which we believe to be the most important topics for further study. This is not a complete overview over this quickly moving field, but we hope to provide a perspective which may be helpful especially to new researchers in the area.},
  archivePrefix = {arXiv},
  eprint = {2009.10713},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/e et al_2020_towards a mathematical understanding of neural network-based machine learning.pdf},
  journal = {arXiv:2009.10713 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{eastwood18_FrameworkQuantitativeEvaluation,
  title = {A {{Framework}} for the {{Quantitative Evaluation}} of {{Disentangled Representations}}},
  author = {Eastwood, Cian and Williams, Christopher K I},
  year = {2018},
  pages = {15},
  abstract = {Recent AI research has emphasised the importance of learning disentangled representations of the explanatory factors behind data. Despite the growing interest in models which can learn such representations, visual inspection remains the standard evaluation metric. While various desiderata have been implied in recent definitions, it is currently unclear what exactly makes one disentangled representation better than another. In this work we propose a framework for the quantitative evaluation of disentangled representations when the ground-truth latent structure is available. Three criteria are explicitly defined and quantified to elucidate the quality of learnt representations and thus compare models on an equal basis. To illustrate the appropriateness of the framework, we employ it to compare quantitatively the representations learned by recent state-of-the-art models.},
  annotation = {ZSCC: 0000063},
  file = {/home/trung/GoogleDrive/Zotero/eastwood et al_2018_a framework for the quantitative evaluation of disentangled representations.pdf},
  keywords = {disentan,disentanglement,favorite},
  language = {en}
}

@article{ebbers20_AdversarialContrastivePredictive,
  title = {Adversarial {{Contrastive Predictive Coding}} for {{Unsupervised Learning}} of {{Disentangled Representations}}},
  author = {Ebbers, Janek and Kuhlmann, Michael and {Haeb-Umbach}, Reinhold},
  year = {2020},
  month = may,
  abstract = {In this work we tackle disentanglement of speaker and content related variations in speech signals. We propose a fully convolutional variational autoencoder employing two encoders: a content encoder and a speaker encoder. To foster disentanglement we propose adversarial contrastive predictive coding. This new disentanglement method does neither need parallel data nor any supervision, not even speaker labels. With successful disentanglement the model is able to perform voice conversion by recombining content and speaker attributes. Due to the speaker encoder which learns to extract speaker traits from an audio signal, the proposed model not only provides meaningful speaker embeddings but is also able to perform zero-shot voice conversion, i.e. with previously unseen source and target speakers. Compared to state-of-the-art disentanglement approaches we show competitive disentanglement and voice conversion performance for speakers seen during training and superior performance for unseen speakers.},
  archivePrefix = {arXiv},
  eprint = {2005.12963},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ebbers et al_2020_adversarial contrastive predictive coding for unsupervised learning of disentangled representations.pdf},
  journal = {arXiv:2005.12963 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{echraibi20_VariationalPosteriorDirichlet,
  title = {On the {{Variational Posterior}} of {{Dirichlet Process Deep Latent Gaussian Mixture Models}}},
  author = {Echraibi, Amine and {Flocon-Cholet}, Joachim and Gosselin, St{\'e}phane and Vaton, Sandrine},
  year = {2020},
  month = jul,
  abstract = {Thanks to the reparameterization trick, deep latent Gaussian models have shown tremendous success recently in learning latent representations. The ability to couple them however with nonparamet-ric priors such as the Dirichlet Process (DP) hasn't seen similar success due to its non parameteriz-able nature. In this paper, we present an alternative treatment of the variational posterior of the Dirichlet Process Deep Latent Gaussian Mixture Model (DP-DLGMM), where we show that the prior cluster parameters and the variational posteriors of the beta distributions and cluster hidden variables can be updated in closed-form. This leads to a standard reparameterization trick on the Gaussian latent variables knowing the cluster assignments. We demonstrate our approach on standard benchmark datasets, we show that our model is capable of generating realistic samples for each cluster obtained, and manifests competitive performance in a semi-supervised setting.},
  archivePrefix = {arXiv},
  eprint = {2006.08993},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/echraibi et al_2020_on the variational posterior of dirichlet process deep latent gaussian mixture models.pdf},
  journal = {arXiv:2006.08993 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ede20_ReviewDeepLearning,
  title = {Review: {{Deep Learning}} in {{Electron Microscopy}}},
  shorttitle = {Review},
  author = {Ede, Jeffrey M.},
  year = {2020},
  month = sep,
  abstract = {Deep learning is transforming most areas of science and technology, including electron microscopy. This review paper offers a practical perspective aimed at developers with limited familiarity. For context, we review popular applications of deep learning in electron microscopy. Following, we discuss hardware and software needed to get started with deep learning and interface with electron microscopes. We then review neural network components, popular architectures, and their optimization. Finally, we discuss future directions of deep learning in electron microscopy.},
  archivePrefix = {arXiv},
  eprint = {2009.08328},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ede_2020_review.pdf},
  journal = {arXiv:2009.08328 [cond-mat]},
  primaryClass = {cond-mat}
}

@article{eghbal-zadeh18_MixtureDensityGenerative,
  title = {Mixture {{Density Generative Adversarial Networks}}},
  author = {{Eghbal-zadeh}, Hamid and Zellinger, Werner and Widmer, Gerhard},
  year = {2018},
  month = oct,
  abstract = {Generative Adversarial Networks have surprising ability for generating sharp and realistic images, though they are known to suffer from the so-called mode collapse problem. In this paper, we propose a new GAN variant called Mixture Density GAN that while being capable of generating high-quality images, overcomes this problem by encouraging the Discriminator to form clusters in its embedding space, which in turn leads the Generator to exploit these and discover different modes in the data. This is achieved by positioning Gaussian density functions in the corners of a simplex, using the resulting Gaussian mixture as a likelihood function over discriminator embeddings, and formulating an objective function for GAN training that is based on these likelihoods. We show that the optimum of our training objective is attained if and only if the generated and the real distribution match exactly. We further support our theoretical results with empirical evaluations on one synthetic and several real image datasets (CIFAR-10, CelebA, MNIST, and FashionMNIST). We demonstrate empirically (1) the quality of the generated images in Mixture Density GAN and their strong similarity to real images, as measured by the Fr\'echet Inception Distance (FID), which compares very favourably with state-of-the-art methods, and (2) the ability to avoid mode collapse and discover all data modes.},
  archivePrefix = {arXiv},
  eprint = {1811.00152},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/eghbal-zadeh et al_2018_mixture density generative adversarial networks.pdf},
  journal = {arXiv:1811.00152 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{eguchi20_InformationTheoreticApproach,
  title = {An {{Information Theoretic Approach}} to {{Reveal}} the {{Formation}} of {{Shared Representations}}},
  author = {Eguchi, Akihiro and Horii, Takato and Nagai, Takayuki and Kanai, Ryota and Oizumi, Masafumi},
  year = {2020},
  month = jan,
  volume = {14},
  pages = {1},
  issn = {1662-5188},
  doi = {10.3389/fncom.2020.00001},
  abstract = {Modality-invariant categorical representations, i.e., shared representation, is thought to play a key role in learning to categorize multi-modal information. We have investigated how a bimodal autoencoder can form a shared representation in an unsupervised manner with multi-modal data. We explored whether altering the depth of the network and mixing the multi-modal inputs at the input layer affect the development of the shared representations. Based on the activation of units in the hidden layers, we classified them into four different types: visual cells, auditory cells, inconsistent visual and auditory cells, and consistent visual and auditory cells. Our results show that the number and quality of the last type (i.e., shared representation) significantly differ depending on the depth of the network and are enhanced when the network receives mixed inputs as opposed to separate inputs for each modality, as occurs in typical two-stage frameworks. In the present work, we present a way to utilize information theory to understand the abstract representations formed in the hidden layers of the network. We believe that such an information theoretic approach could potentially provide insights into the development of more efficient and cost-effective ways to train neural networks using qualitative measures of the representations that cannot be captured by analyzing only the final outputs of the networks.},
  file = {/home/trung/GoogleDrive/Zotero/eguchi et al_2020_an information theoretic approach to reveal the formation of shared representations.pdf},
  journal = {Frontiers in Computational Neuroscience},
  keywords = {information},
  language = {en}
}

@article{el-arini00_DirichletProcessesGentle,
  title = {Dirichlet {{Processes}}: {{A Gentle Tutorial}}},
  author = {{El-Arini}, Khalid},
  pages = {44},
  file = {/home/trung/GoogleDrive/Zotero/el-arini_dirichlet processes.pdf},
  language = {en}
}

@article{elad19_DirectValidationInformation,
  title = {Direct {{Validation}} of the {{Information Bottleneck Principle}} for {{Deep Nets}}},
  author = {Elad, Adar and Haviv, Doron and Blau, Yochai and Michaeli, Tomer},
  year = {2019},
  pages = {5},
  abstract = {The information bottleneck (IB) has been suggested as a fundamental principle governing performance in deep neural nets (DNNs). This idea sparked research on the information plane dynamics during training with the cross-entropy loss, and on using the IB of some ``bottleneck'' layer as a loss function. However, the claim that reaching the maximal value of the IB Lagrangian in each layer leads to optimal performance, was in fact never directly confirmed. In this paper, we propose a direct way of validating this hypothesis, using layer-by-layer training with the IB loss. In accordance with the original theory, we train each DNN layer explicitly with the IB objective (and without any classification loss), and freeze it before moving on to train the next layer. While mutual information (MI) is generally hard to estimate in high dimensions, we show that in the case of MI between DNN layers, this can be done quite accurately using a modification of the recently proposed mutual information neural estimator [4]. Interestingly, we find that layer-by-layer training with the IB loss leads to accuracy which is on-par with end-to-end training with the cross entropy loss. This is, thus, the first direct experimental illustration of the link between the IB value in each layer, and a net's performance.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/elad et al_2019_direct validation of the information bottleneck principle for deep nets.pdf},
  keywords = {information},
  language = {en}
}

@article{elibol20_VarianceReductionSparse,
  title = {Variance {{Reduction}} with {{Sparse Gradients}}},
  author = {Elibol, Melih and Lei, Lihua and Jordan, Michael I.},
  year = {2020},
  month = jan,
  abstract = {Variance reduction methods such as SVRG (Johnson \& Zhang, 2013) and SpiderBoost (Wang et al., 2018) use a mixture of large and small batch gradients to reduce the variance of stochastic gradients. Compared to SGD (Robbins \& Monro, 1951), these methods require at least double the number of operations per update to model parameters. To reduce the computational cost of these methods, we introduce a new sparsity operator: The random-top-k operator. Our operator reduces computational complexity by estimating gradient sparsity exhibited in a variety of applications by combining the top-k operator (Stich et al., 2018; Aji \& Heafield, 2017) and the randomized coordinate descent operator. With this operator, large batch gradients offer an extra benefit beyond variance reduction: A reliable estimate of gradient sparsity. Theoretically, our algorithm is at least as good as the best algorithm (SpiderBoost), and further excels in performance whenever the random-top-k operator captures gradient sparsity. Empirically, our algorithm consistently outperforms SpiderBoost using various models on various tasks including image classification, natural language processing, and sparse matrix factorization. We also provide empirical evidence to support the intuition behind our algorithm via a simple gradient entropy computation, which serves to quantify gradient sparsity at every iteration.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.09623},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/XCPBAHLM/Elibol et al. - 2020 - Variance Reduction with Sparse Gradients.pdf},
  journal = {arXiv:2001.09623 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{elsayed19_SaccaderImprovingAccuracy,
  title = {Saccader: {{Improving Accuracy}} of {{Hard Attention Models}} for {{Vision}}},
  shorttitle = {Saccader},
  author = {Elsayed, Gamaleldin F. and Kornblith, Simon and Le, Quoc V.},
  year = {2019},
  month = aug,
  abstract = {Although deep convolutional neural networks achieve state-of-the-art performance across nearly all image classification tasks, their decisions are difficult to interpret. One approach that offers some level of interpretability by design is \textbackslash textit\{hard attention\}, which uses only relevant portions of the image. However, training hard attention models with only class label supervision is challenging, and hard attention has proved difficult to scale to complex datasets. Here, we propose a novel hard attention model, which we term Saccader. Key to Saccader is a pretraining step that requires only class labels and provides initial attention locations for policy gradient optimization. Our best models narrow the gap to common ImageNet baselines, achieving \$75\textbackslash\%\$ top-1 and \$91\textbackslash\%\$ top-5 while attending to less than one-third of the image.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.07644},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/elsayed et al_2019_saccader.pdf;/home/trung/Zotero/storage/CKL9FNVY/1908.html},
  journal = {arXiv:1908.07644 [cs, stat]},
  keywords = {attention,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,hard attention,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{elsken19_NeuralArchitectureSearch,
  title = {Neural {{Architecture Search}}: {{A Survey}}},
  shorttitle = {Neural {{Architecture Search}}},
  author = {Elsken, Thomas and Metzen, Jan Hendrik and Hutter, Frank},
  year = {2019},
  month = apr,
  abstract = {Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy.},
  annotation = {ZSCC: 0000038},
  archivePrefix = {arXiv},
  eprint = {1808.05377},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/elsken et al_2019_neural architecture search.pdf;/home/trung/Zotero/storage/LAJHKS94/1808.html},
  journal = {arXiv:1808.05377 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,nas,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{engel17_LatentConstraintsLearning,
  title = {Latent {{Constraints}}: {{Learning}} to {{Generate Conditionally}} from {{Unconditional Generative Models}}},
  shorttitle = {Latent {{Constraints}}},
  author = {Engel, Jesse and Hoffman, Matthew and Roberts, Adam},
  year = {2017},
  month = dec,
  abstract = {Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal "realism" constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function. Code with dedicated cloud instance has been made publicly available (https://goo.gl/STGMGx).},
  archivePrefix = {arXiv},
  eprint = {1711.05772},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/engel et al_2017_latent constraints.pdf},
  journal = {arXiv:1711.05772 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{engel17_NeuralAudioSynthesis,
  title = {Neural {{Audio Synthesis}} of {{Musical Notes}} with {{WaveNet Autoencoders}}},
  author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
  year = {2017},
  month = apr,
  abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
  archivePrefix = {arXiv},
  eprint = {1704.01279},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/engel et al_2017_neural audio synthesis of musical notes with wavenet autoencoders.pdf},
  journal = {arXiv:1704.01279 [cs]},
  primaryClass = {cs}
}

@article{engel18_LatentConstraintsLearning,
  title = {Latent {{Constraints}}: {{Learning}} to {{Generate Conditionally}} from {{Unconditional Generative Models}}},
  author = {Engel, Jesse and Hoffman, Matthew D and Roberts, Adam},
  year = {2018},
  pages = {22},
  abstract = {Deep generative neural networks have proven effective at both conditional and unconditional modeling of complex data distributions. Conditional generation enables interactive control, but creating new controls often requires expensive retraining. In this paper, we develop a method to condition generation without retraining the model. By post-hoc learning latent constraints, value functions that identify regions in latent space that generate outputs with desired attributes, we can conditionally sample from these regions with gradient-based optimization or amortized actor functions. Combining attribute constraints with a universal ``realism'' constraint, which enforces similarity to the data distribution, we generate realistic conditional images from an unconditional variational autoencoder. Further, using gradient-based optimization, we demonstrate identity-preserving transformations that make the minimal adjustment in latent space to modify the attributes of an image. Finally, with discrete sequences of musical notes, we demonstrate zero-shot conditional generation, learning latent constraints in the absence of labeled data or a differentiable reward function.},
  file = {/home/trung/GoogleDrive/Zotero/engel et al_2018_latent constraints.pdf},
  language = {en}
}

@article{enguehard19_NeuralLanguagePriors,
  title = {Neural {{Language Priors}}},
  author = {Enguehard, Joseph and Busbridge, Dan and Zhelezniak, Vitalii and Hammerla, Nils},
  year = {2019},
  month = oct,
  abstract = {The choice of sentence encoder architecture reflects assumptions about how a sentences meaning is composed from its constituent words. We examine the contribution of these architectures by holding them randomly initialised and fixed, effectively treating them as as hand-crafted language priors, and evaluating the resulting sentence encoders on downstream language tasks. We find that even when encoders are presented with additional information that can be used to solve tasks, the corresponding priors do not leverage this information, except in an isolated case. We also find that apparently uninformative priors are just as good as seemingly informative priors on almost all tasks, indicating that learning is a necessary component to leverage information provided by architecture choice.},
  archivePrefix = {arXiv},
  eprint = {1910.03492},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/enguehard et al_2019_neural language priors.pdf},
  journal = {arXiv:1910.03492 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{eraslan18_DCASinglecell,
  title = {{{DCA}}: {{Single}} Cell {{RNA}}-Seq Denoising Using a Deep Count Autoencoder},
  author = {Eraslan, G{\"o}kcen and Simon, Lukas M. and Mircea, Maria and Mueller, Nikola S. and Theis, Fabian J.},
  year = {2018},
  month = apr,
  doi = {10.1101/300681},
  abstract = {Single-cell RNA sequencing (scRNA-seq) has enabled researchers to study gene expression at a cellular resolution. However, noise due to amplification and dropout may obstruct analyses, so scalable denoising methods for increasingly large but sparse scRNAseq data are needed. We propose a deep count autoencoder network (DCA) to denoise scRNA-seq datasets. DCA takes the count distribution, overdispersion and sparsity of the data into account using a zero-inflated negative binomial noise model, and nonlinear gene-gene or gene-dispersion interactions are captured. Our method scales linearly with the number of cells and can therefore be applied to datasets of millions of cells. We demonstrate that DCA denoising improves a diverse set of typical scRNA-seq data analyses using simulated and real datasets. DCA outperforms existing methods for data imputation in quality and speed, enhancing biological discovery.},
  file = {/home/trung/GoogleDrive/Zotero/eraslan et al_2018_dca.pdf},
  keywords = {czi},
  language = {en}
}

@article{ermolov20_WhiteningSelfSupervisedRepresentation,
  title = {Whitening for {{Self}}-{{Supervised Representation Learning}}},
  author = {Ermolov, Aleksandr and Siarohin, Aliaksandr and Sangineto, Enver and Sebe, Nicu},
  year = {2020},
  month = jul,
  abstract = {Recent literature on self-supervised learning is based on the contrastive loss, where image instances which share the same semantic content ("positives") are contrasted with instances extracted from other images ("negatives"). However, in order for the learning to be effective, a lot of negatives should be compared with a positive pair. This is not only computationally demanding, but it also requires that the positive and the negative representations are kept consistent with each other over a long training period. In this paper we propose a different direction and a new loss function for self-supervised learning which is based on the whitening of the latent-space features. The whitening operation has a "scattering" effect on the batch samples, which compensates the lack of a large number of negatives, avoiding degenerate solutions where all the sample representations collapse to a single point. We empirically show that our loss accelerates self-supervised training and the learned representations are much more effective for downstream tasks than previously published work.},
  archivePrefix = {arXiv},
  eprint = {2007.06346},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ermolov et al_2020_whitening for self-supervised representation learning.pdf},
  journal = {arXiv:2007.06346 [cs, stat]},
  keywords = {self-supervised},
  primaryClass = {cs, stat}
}

@article{eslami16_AttendInferRepeat,
  title = {Attend, {{Infer}}, {{Repeat}}: {{Fast Scene Understanding}} with {{Generative Models}}},
  shorttitle = {Attend, {{Infer}}, {{Repeat}}},
  author = {Eslami, S. M. Ali and Heess, Nicolas and Weber, Theophane and Tassa, Yuval and Szepesvari, David and Kavukcuoglu, Koray and Hinton, Geoffrey E.},
  year = {2016},
  month = mar,
  abstract = {We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene - without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1603.08575},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/eslami et al_2016_attend, infer, repeat.pdf;/home/trung/Zotero/storage/55BIJ7I8/1603.html},
  journal = {arXiv:1603.08575 [cs]},
  keywords = {attend,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,generative,infer,reason},
  primaryClass = {cs}
}

@article{esmaeili18_StructuredDisentangledRepresentations,
  title = {Structured {{Disentangled Representations}}},
  author = {Esmaeili, Babak and Wu, Hao and Jain, Sarthak and Bozkurt, Alican and Siddharth, N. and Paige, Brooks and Brooks, Dana H. and Dy, Jennifer and {van de Meent}, Jan-Willem},
  year = {2018},
  month = dec,
  abstract = {Deep latent-variable models learn representations of high-dimensional data in an unsupervised manner. A number of recent efforts have focused on learning representations that disentangle statistically independent axes of variation by introducing modifications to the standard objective function. These approaches generally assume a simple diagonal Gaussian prior and as a result are not able to reliably disentangle discrete factors of variation. We propose a two-level hierarchical objective to control relative degree of statistical independence between blocks of variables and individual variables within blocks. We derive this objective as a generalization of the evidence lower bound, which allows us to explicitly represent the trade-offs between mutual information between data and representation, KL divergence between representation and prior, and coverage of the support of the empirical data distribution. Experiments on a variety of datasets demonstrate that our objective can not only disentangle discrete variables, but that doing so also improves disentanglement of other variables and, importantly, generalization even to unseen combinations of factors.},
  annotation = {ZSCC: 0000024},
  archivePrefix = {arXiv},
  eprint = {1804.02086},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/esmaeili et al_2018_structured disentangled representations.pdf},
  journal = {arXiv:1804.02086 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{esmaeilpour20_ConditioningTrickTraining,
  title = {Conditioning {{Trick}} for {{Training Stable GANs}}},
  author = {Esmaeilpour, Mohammad and Sallo, Raymel Alfonso and {St-Georges}, Olivier and Cardinal, Patrick and Koerich, Alessandro Lameiras},
  year = {2020},
  month = oct,
  abstract = {In this paper we propose a conditioning trick, called difference departure from normality, applied on the generator network in response to instability issues during GAN training. We force the generator to get closer to the departure from normality function of real samples computed in the spectral domain of Schur decomposition. This binding makes the generator amenable to truncation and does not limit exploring all the possible modes. We slightly modify the BigGAN architecture incorporating residual network for synthesizing 2D representations of audio signals which enables reconstructing high quality sounds with some preserved phase information. Additionally, the proposed conditional training scenario makes a trade-off between fidelity and variety for the generated spectrograms. The experimental results on UrbanSound8k and ESC-50 environmental sound datasets and the Mozilla common voice dataset have shown that the proposed GAN configuration with the conditioning trick remarkably outperforms baseline architectures, according to three objective metrics: inception score, Frechet inception distance, and signal-to-noise ratio.},
  archivePrefix = {arXiv},
  eprint = {2010.05844},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/esmaeilpour et al_2020_conditioning trick for training stable gans.pdf},
  journal = {arXiv:2010.05844 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{esser19_UnsupervisedRobustDisentangling,
  title = {Unsupervised {{Robust Disentangling}} of {{Latent Characteristics}} for {{Image Synthesis}}},
  author = {Esser, Patrick and Haux, Johannes and Ommer, Bj{\"o}rn},
  year = {2019},
  month = oct,
  abstract = {Deep generative models come with the promise to learn an explainable representation for visual objects that allows image sampling, synthesis, and selective modification. The main challenge is to learn to properly model the independent latent characteristics of an object, especially its appearance and pose. We present a novel approach that learns disentangled representations of these characteristics and explains them individually. Training requires only pairs of images depicting the same object appearance, but no pose annotations. We propose an additional classifier that estimates the minimal amount of regularization required to enforce disentanglement. Thus both representations together can completely explain an image while being independent of each other. Previous methods based on adversarial approaches fail to enforce this independence, while methods based on variational approaches lead to uninformative representations. In experiments on diverse object categories, the approach successfully recombines pose and appearance to reconstruct and retarget novel synthesized images. We achieve significant improvements over state-of-the-art methods which utilize the same level of supervision, and reach performances comparable to those of pose-supervised approaches. However, we can handle the vast body of articulated object classes for which no pose models/annotations are available.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.10223},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/esser et al_2019_unsupervised robust disentangling of latent characteristics for image synthesis.pdf;/home/trung/Zotero/storage/42PVUSDZ/1910.html},
  journal = {arXiv:1910.10223 [cs]},
  keywords = {autoencoder,Computer Science - Computer Vision and Pattern Recognition,disentanglement},
  primaryClass = {cs}
}

@article{esser20_DisentanglingInvertibleInterpretation,
  title = {A {{Disentangling Invertible Interpretation Network}} for {{Explaining Latent Representations}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  year = {2020},
  month = apr,
  abstract = {Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modifications in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efficient approach to define semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classification and image generation networks as well as to semantically guided image manipulation.},
  archivePrefix = {arXiv},
  eprint = {2004.13166},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/esser et al_2020_a disentangling invertible interpretation network for explaining latent representations.pdf},
  journal = {arXiv:2004.13166 [cs]},
  primaryClass = {cs}
}

@article{esser20_TamingTransformersHighResolution,
  title = {Taming {{Transformers}} for {{High}}-{{Resolution Image Synthesis}}},
  author = {Esser, Patrick and Rombach, Robin and Ommer, Bj{\"o}rn},
  year = {2020},
  month = dec,
  abstract = {Designed to learn long-range interactions on sequential data, transformers continue to show state-of-the-art results on a wide variety of tasks. In contrast to CNNs, they contain no inductive bias that prioritizes local interactions. This makes them expressive, but also computationally infeasible for long sequences, such as high-resolution images. We demonstrate how combining the effectiveness of the inductive bias of CNNs with the expressivity of transformers enables them to model and thereby synthesize high-resolution images. We show how to (i) use CNNs to learn a contextrich vocabulary of image constituents, and in turn (ii) utilize transformers to efficiently model their composition within high-resolution images. Our approach is readily applied to conditional synthesis tasks, where both non-spatial information, such as object classes, and spatial information, such as segmentations, can control the generated image. In particular, we present the first results on semanticallyguided synthesis of megapixel images with transformers. Project page at https://git.io/JLlvY.},
  archivePrefix = {arXiv},
  eprint = {2012.09841},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/esser et al_2020_taming transformers for high-resolution image synthesis.pdf},
  journal = {arXiv:2012.09841 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{esteves20_TheoreticalAspectsGroup,
  title = {Theoretical {{Aspects}} of {{Group Equivariant Neural Networks}}},
  author = {Esteves, Carlos},
  year = {2020},
  month = apr,
  abstract = {Group equivariant neural networks have been explored in the past few years and are interesting from theoretical and practical standpoints. They leverage concepts from group representation theory, non-commutative harmonic analysis and differential geometry that do not often appear in machine learning. In practice, they have been shown to reduce sample and model complexity, notably in challenging tasks where input transformations such as arbitrary rotations are present. We begin this work with an exposition of group representation theory and the machinery necessary to define and evaluate integrals and convolutions on groups. Then, we show applications to recent SO(3) and SE(3) equivariant networks, namely the Spherical CNNs, Clebsch-Gordan Networks, and 3D Steerable CNNs. We proceed to discuss two recent theoretical results. The first, by Kondor and Trivedi (ICML'18), shows that a neural network is group equivariant if and only if it has a convolutional structure. The second, by Cohen et al. (NeurIPS'19), generalizes the first to a larger class of networks, with feature maps as fields on homogeneous spaces.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2004.05154},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/esteves_2020_theoretical aspects of group equivariant neural networks.pdf},
  journal = {arXiv:2004.05154 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{evans18_LearningExplanatoryRules,
  title = {Learning {{Explanatory Rules}} from {{Noisy Data}}},
  author = {Evans, Richard and Grefenstette, Edward},
  year = {2018},
  month = jan,
  abstract = {Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data---which is not necessarily easily obtained---that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve.},
  annotation = {ZSCC: 0000083},
  archivePrefix = {arXiv},
  eprint = {1711.04574},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/evans et al_2018_learning explanatory rules from noisy data.pdf;/home/trung/Zotero/storage/RPJCYKL5/1711.html},
  journal = {arXiv:1711.04574 [cs, math]},
  keywords = {Computer Science - Neural and Evolutionary Computing,Mathematics - Logic},
  primaryClass = {cs, math}
}

@article{evci20_GradientFlowSparse,
  title = {Gradient {{Flow}} in {{Sparse Neural Networks}} and {{How Lottery Tickets Win}}},
  author = {Evci, Utku and Ioannou, Yani A. and Keskin, Cem and Dauphin, Yann},
  year = {2020},
  month = oct,
  abstract = {Sparse Neural Networks (NNs) can match the generalization of dense NNs using a fraction of the compute/storage for inference, and also have the potential to enable efficient training. However, naively training unstructured sparse NNs from random initialization results in significantly worse generalization, with the notable exception of Lottery Tickets (LTs) and Dynamic Sparse Training (DST). In this work, we attempt to answer: (1) why training unstructured sparse networks from random initialization performs poorly and; (2) what makes LTs and DST the exceptions? We show that sparse NNs have poor gradient flow at initialization and propose a modified initialization for unstructured connectivity. Furthermore, we find that DST methods significantly improve gradient flow during training over traditional sparse training methods. Finally, we show that LTs do not improve gradient flow, rather their success lies in re-learning the pruning solution they are derived from - however, this comes at the cost of learning novel solutions.},
  archivePrefix = {arXiv},
  eprint = {2010.03533},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/evci et al_2020_gradient flow in sparse neural networks and how lottery tickets win.pdf},
  journal = {arXiv:2010.03533 [cs]},
  primaryClass = {cs}
}

@article{evci20_RiggingLotteryMaking,
  title = {Rigging the {{Lottery}}: {{Making All Tickets Winners}}},
  shorttitle = {Rigging the {{Lottery}}},
  author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  year = {2020},
  month = jul,
  abstract = {Sparse neural networks have been shown to be more parameter and compute efficient compared to dense networks and in some cases are used to decrease wall clock inference times. There is a large body of work on training dense networks to yield sparse networks for inference (Molchanov et al., 2017; Zhu \& Gupta, 2018; Narang et al., 2017; Li et al., 2016; Guo et al., 2016). This limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. Importantly,by adjusting the topology it can start from any initialization \textendash{} not just ``lucky'' ones. We demonstrate state-of-the-art sparse training results with ResNet-50, MobileNet v1 and MobileNet v2 on the ImageNet-2012 dataset, WideResNets on the CIFAR-10 dataset and RNNs on the WikiText-103 dataset. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static.},
  archivePrefix = {arXiv},
  eprint = {1911.11134},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/evci et al_2020_rigging the lottery.pdf},
  journal = {arXiv:1911.11134 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{faghri20_StudyGradientVariance,
  title = {A {{Study}} of {{Gradient Variance}} in {{Deep Learning}}},
  author = {Faghri, Fartash and Duvenaud, David and Fleet, David J. and Ba, Jimmy},
  year = {2020},
  month = jul,
  abstract = {The impact of gradient noise on training deep models is widely acknowledged but not well understood. In this context, we study the distribution of gradients during training. We introduce a method, Gradient Clustering, to minimize the variance of average mini-batch gradient with stratified sampling. We prove that the variance of average mini-batch gradient is minimized if the elements are sampled from a weighted clustering in the gradient space. We measure the gradient variance on common deep learning benchmarks and observe that, contrary to common assumptions, gradient variance increases during training, and smaller learning rates coincide with higher variance. In addition, we introduce normalized gradient variance as a statistic that better correlates with the speed of convergence compared to gradient variance.},
  archivePrefix = {arXiv},
  eprint = {2007.04532},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/faghri et al_2020_a study of gradient variance in deep learning.pdf},
  journal = {arXiv:2007.04532 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fajtl20_LatentBernoulliAutoencoder,
  title = {Latent {{Bernoulli Autoencoder}}},
  author = {Fajtl, Jiri and Argyriou, Vasileios and Monekosso, Dorothy and Remagnino, Paolo},
  year = {2020},
  pages = {11},
  abstract = {In this work, we pose the question whether it is possible to design and train an autoencoder model in an end-to-end fashion to learn representations in the multivariate Bernoulli latent space, and achieve performance comparable with the state-ofthe-art variational methods. Moreover, we investigate how to generate novel samples and perform smooth interpolation and attributes modification in the binary latent space. To meet our objective, we propose a simplified, deterministic model with a straight-through gradient estimator to learn the binary latents and show its competitiveness with the latest VAE methods. Furthermore, we propose a novel method based on a random hyperplane rounding for sampling and smooth interpolation in the latent space. Our method performs on a par or better than the current state-of-the-art methods on common CelebA, CIFAR-10 and MNIST datasets.},
  file = {/home/trung/GoogleDrive/Zotero/fajtl et al_2020_latent bernoulli autoencoder.pdf},
  language = {en}
}

@article{falcon20_FrameworkContrastiveSelfSupervised,
  title = {A {{Framework For Contrastive Self}}-{{Supervised Learning And Designing A New Approach}}},
  author = {Falcon, William and Cho, Kyunghyun},
  year = {2020},
  month = aug,
  abstract = {Contrastive self-supervised learning (CSL) is an approach to learn useful representations by solving a pretext task that selects and compares anchor, negative and positive (APN) features from an unlabeled dataset. We present a conceptual framework that characterizes CSL approaches in five aspects (1) data augmentation pipeline, (2) encoder selection, (3) representation extraction, (4) similarity measure, and (5) loss function. We analyze three leading CSL approaches--AMDIM, CPC, and SimCLR--, and show that despite different motivations, they are special cases under this framework. We show the utility of our framework by designing Yet Another DIM (YADIM) which achieves competitive results on CIFAR-10, STL-10 and ImageNet, and is more robust to the choice of encoder and the representation extraction strategy. To support ongoing CSL research, we release the PyTorch implementation of this conceptual framework along with standardized implementations of AMDIM, CPC (V2), SimCLR, BYOL, Moco (V2) and YADIM.},
  archivePrefix = {arXiv},
  eprint = {2009.00104},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/falcon et al_2020_a framework for contrastive self-supervised learning and designing a new approach.pdf},
  journal = {arXiv:2009.00104 [cs]},
  keywords = {self-supervised},
  primaryClass = {cs}
}

@article{fan19_GeneratingInteractiveWorlds,
  title = {Generating {{Interactive Worlds}} with {{Text}}},
  author = {Fan, Angela and Urbanek, Jack and Ringshia, Pratik and Dinan, Emily and Qian, Emma and Karamcheti, Siddharth and Prabhumoye, Shrimai and Kiela, Douwe and Rocktaschel, Tim and Szlam, Arthur and Weston, Jason},
  year = {2019},
  month = dec,
  abstract = {Procedurally generating cohesive and interesting game environments is challenging and time-consuming. In order for the relationships between the game elements to be natural, common-sense has to be encoded into arrangement of the elements. In this work, we investigate a machine learning approach for world creation using content from the multi-player text adventure game environment LIGHT. We introduce neural network based models to compositionally arrange locations, characters, and objects into a coherent whole. In addition to creating worlds based on existing elements, our models can generate new game content. Humans can also leverage our models to interactively aid in worldbuilding. We show that the game environments created with our approach are cohesive, diverse, and preferred by human evaluators compared to other machine learning based world construction algorithms.},
  archivePrefix = {arXiv},
  eprint = {1911.09194},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fan et al_2019_generating interactive worlds with text.pdf},
  journal = {arXiv:1911.09194 [cs]},
  primaryClass = {cs}
}

@article{fang14_Collagendoubleedgedsword,
  title = {Collagen as a Double-Edged Sword in Tumor Progression},
  author = {Fang, Min and Yuan, Jingping and Peng, Chunwei and Li, Yan},
  year = {2014},
  month = apr,
  volume = {35},
  pages = {2871--2882},
  issn = {1010-4283},
  doi = {10.1007/s13277-013-1511-7},
  abstract = {It has been recognized that cancer is not merely a disease of tumor cells, but a disease of imbalance, in which stromal cells and tumor microenvironment play crucial roles. Extracellular matrix (ECM) as the most abundant component in tumor microenvironment can regulate tumor cell behaviors and tissue tension homeostasis. Collagen constitutes the scaffold of tumor microenvironment and affects tumor microenvironment such that it regulates ECM remodeling by collagen degradation and re-deposition, and promotes tumor infiltration, angiogenesis, invasion and migration. While collagen was traditionally regarded as a passive barrier to resist tumor cells, it is now evident that collagen is also actively involved in promoting tumor progression. Collagen changes in tumor microenvironment release biomechanical signals, which are sensed by both tumor cells and stromal cells, trigger a cascade of biological events. In this work, we discuss how collagen can be a double-edged sword in tumor progression, both inhibiting and promoting tumor progression at different stages of cancer development.},
  file = {/home/trung/GoogleDrive/Zotero/fang et al_2014_collagen as a double-edged sword in tumor progression.pdf},
  journal = {Tumour Biology},
  number = {4},
  pmcid = {PMC3980040},
  pmid = {24338768}
}

@article{fang19_DynamicsLearningCascaded,
  title = {Dynamics {{Learning}} with {{Cascaded Variational Inference}} for {{Multi}}-{{Step Manipulation}}},
  author = {Fang, Kuan and Zhu, Yuke and Garg, Animesh and Savarese, Silvio and {Fei-Fei}, Li},
  year = {2019},
  month = oct,
  abstract = {The fundamental challenge of planning for multi-step manipulation is to find effective and plausible action sequences that lead to the task goal. We present Cascaded Variational Inference (CAVIN) Planner, a model-based method that hierarchically generates plans by sampling from latent spaces. To facilitate planning over long time horizons, our method learns latent representations that decouple the prediction of high-level effects from the generation of low-level motions through cascaded variational inference. This enables us to model dynamics at two different levels of temporal resolutions for hierarchical planning. We evaluate our approach in three multi-step robotic manipulation tasks in cluttered tabletop environments given high-dimensional observations. Empirical results demonstrate that the proposed method outperforms state-of-the-art model-based methods by strategically interacting with multiple objects.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.13395},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fang et al_2019_dynamics learning with cascaded variational inference for multi-step manipulation.pdf;/home/trung/Zotero/storage/S2CNUHD6/1910.html},
  journal = {arXiv:1910.13395 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,hierarchical,variational},
  primaryClass = {cs}
}

@article{fang20_DiscreteVariationalAttention,
  title = {Discrete {{Variational Attention Models}} for {{Language Generation}}},
  author = {Fang, Xianghong and Bai, Haoli and Xu, Zenglin and Lyu, Michael and King, Irwin},
  year = {2020},
  month = apr,
  abstract = {Variational autoencoders have been widely applied for natural language generation, however, there are two long-standing problems: information underrepresentation and posterior collapse. The former arises from the fact that only the last hidden state from the encoder is transformed to the latent space, which is insufficient to summarize data. The latter comes as a result of the imbalanced scale between the reconstruction loss and the KL divergence in the objective function. To tackle these issues, in this paper we propose the discrete variational attention model with categorical distribution over the attention mechanism owing to the discrete nature in languages. Our approach is combined with an auto-regressive prior to capture the sequential dependency from observations, which can enhance the latent space for language generation. Moreover, thanks to the property of discreteness, the training of our proposed approach does not suffer from posterior collapse. Furthermore, we carefully analyze the superiority of discrete latent space over the continuous space with the common Gaussian distribution. Extensive experiments on language generation demonstrate superior advantages of our proposed approach in comparison with the state-ofthe-art counterparts.},
  archivePrefix = {arXiv},
  eprint = {2004.09764},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fang et al_2020_discrete variational attention models for language generation.pdf},
  journal = {arXiv:2004.09764 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{faradonbeh19_reviewNeuralTuring,
  title = {A Review on {{Neural Turing Machine}}},
  author = {Faradonbeh, Soroor Malekmohammadi and {Safi-Esfahani}, Faramarz},
  year = {2019},
  month = apr,
  abstract = {One of the major objectives of Artificial Intelligence is to design learning algorithms that are executed on a general purposes computational machines such as human brain. Neural Turing Machine (NTM) is a step towards realizing such a computational machine. The attempt is made here to run a systematic review on Neural Turing Machine. First, the mind-map and taxonomy of machine learning, neural networks, and Turing machine are introduced. Next, NTM is inspected in terms of concepts, structure, variety of versions, implemented tasks, comparisons, etc. Finally, the paper discusses on issues and ends up with several future works.},
  archivePrefix = {arXiv},
  eprint = {1904.05061},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/faradonbeh et al_2019_a review on neural turing machine.pdf;/home/trung/Zotero/storage/UREAMX7N/1904.html},
  journal = {arXiv:1904.05061 [cs]},
  keywords = {attention,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,memory,turing machine},
  primaryClass = {cs}
}

@article{farooq20_Latentfeaturesharing,
  title = {Latent Feature Sharing: An Adaptive Approach to Linear Decomposition Models},
  shorttitle = {Latent Feature Sharing},
  author = {Farooq, Adam and Raykov, Yordan P. and Raykov, Petar and Little, Max A.},
  year = {2020},
  month = aug,
  abstract = {Latent feature models are canonical tools for exploratory analysis in classical and modern multivariate statistics. Many high-dimensional data can be approximated using a union of low-dimensional subspaces or factors. The allocation of data points to these latent factors itself typically uncovers key relationships in the input and helps us represent hidden causes explaining the data. A widely adopted view is to model feature allocation with discrete latent variables, where each data point is associated with a binary vector indicating latent features possessed by this data point. In this work we revise some of the issues with existing parametric and Bayesian nonparametric processes for feature allocation modelling and propose a novel framework that can capture wider set of feature allocation distributions. This new framework allows for explicit control over the number of features used to express each point and enables a more flexible set of allocation distributions including feature allocations with different sparsity levels. We use this approach to derive a novel adaptive Factor analysis (aFA), as well as, an adaptive probabilistic principle component analysis (aPPCA) capable of flexible structure discovery and dimensionality reduction in a wide case of scenarios. Motivated by the often prohibitive slowness of feature allocation models, we derive both standard a Gibbs sampler, as well as, an expectation-maximization inference algorithms for aPPCA and aFA that converge orders of magnitude faster to a reasonable point estimate solution. We demonstrate that aFA can handle richer feature distributions, when compared to widely used sparse FA models and Bayesian nonparametric FA models. The utility of the proposed aPPCA model is demonstrated for standard PCA tasks such as feature learning, data visualization and data whitening. We show that aPPCA and aFA can infer interpretable high level features both when applied on raw MNIST and when applied for interpreting autoencoder features. We also demonstrate an application of the aPPCA to more robust blind source separation for functional magnetic resonance imaging (fMRI).},
  archivePrefix = {arXiv},
  eprint = {2006.12369},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/farooq et al_2020_latent feature sharing.pdf},
  journal = {arXiv:2006.12369 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{fazelnia20_MixedMembershipRecurrent,
  title = {Mixed {{Membership Recurrent Neural Networks}} for {{Modeling Customer Purchases}}},
  author = {Fazelnia, Ghazal and Ibrahim, Mark and Modarres, Ceena and Wu, Kevin and Paisley, John},
  year = {2020},
  pages = {8},
  abstract = {Models of sequential data such as the recurrent neural network (RNN) often implicitly treat a sequence of data as having a fixed time interval between observations. They also do not account for group-level effects when multiple sequences are observed generated from separate sources. A simple example is user purchasing behavior, where each user generates a unique sequence of purchases, and the time between purchases is variable. We propose a model for such sequential data based on the RNN that accounts for varying time intervals between observations in a sequence. We do this by learning a group-level ``base'' parameter to which each data-generating object can revert as more time passes before the next observation. This requires modeling assumptions about the data that we argue are typically satisfied by consumer purchasing behavior. Our approach is motivated by the mixed membership framework, with Latent Dirichlet Allocation being the canonical example, which we adapt to our dynamic setting. We demonstrate our approach on two consumer shopping datasets: The Instacart set of 3.4 million online grocery orders made by 206K customers, and a UK retail set consisting of over 500K orders.},
  file = {/home/trung/GoogleDrive/Zotero/fazelnia et al_2020_mixed membership recurrent neural networks for modeling customer purchases.pdf},
  journal = {New York},
  language = {en}
}

@article{federici20_LearningRobustRepresentations,
  title = {Learning {{Robust Representations}} via {{Multi}}-{{View Information Bottleneck}}},
  author = {Federici, Marco and Dutta, Anjan and Forr{\'e}, Patrick and Kushman, Nate and Akata, Zeynep},
  year = {2020},
  month = feb,
  abstract = {The information bottleneck principle provides an information-theoretic method for representation learning, by training an encoder to retain all information which is relevant for predicting the label while minimizing the amount of other, excess information in the representation. The original formulation, however, requires labeled data to identify the superfluous information. In this work, we extend this ability to the multi-view unsupervised setting, where two views of the same underlying entity are provided but the label is unknown. This enables us to identify superfluous information as that not shared by both views. A theoretical analysis leads to the definition of a new multi-view model that produces state-of-the-art results on the Sketchy dataset and label-limited versions of the MIR-Flickr dataset. We also extend our theory to the single-view setting by taking advantage of standard data augmentation techniques, empirically showing better generalization capabilities when compared to common unsupervised approaches for representation learning.},
  archivePrefix = {arXiv},
  eprint = {2002.07017},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/federici et al_2020_learning robust representations via multi-view information bottleneck.pdf},
  journal = {arXiv:2002.07017 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{feige19_Invariantequivariantrepresentationlearning,
  title = {Invariant-Equivariant Representation Learning for Multi-Class Data},
  author = {Feige, Ilya},
  year = {2019},
  month = may,
  abstract = {Representations learnt through deep neural networks tend to be highly informative, but opaque in terms of what information they learn to encode. We introduce an approach to probabilistic modelling that learns to represent data with two separate deep representations: an invariant representation that encodes the information of the class from which the data belongs, and an equivariant representation that encodes the symmetry transformation defining the particular data point within the class manifold (equivariant in the sense that the representation varies naturally with symmetry transformations). This approach is based primarily on the strategic routing of data through the two latent variables, and thus is conceptually transparent, easy to implement, and in-principle generally applicable to any data comprised of discrete classes of continuous distributions (e.g. objects in images, topics in language, individuals in behavioural data). We demonstrate qualitatively compelling representation learning and competitive quantitative performance, in both supervised and semi-supervised settings, versus comparable modelling approaches in the literature with little fine tuning.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1902.03251},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/feige_2019_invariant-equivariant representation learning for multi-class data.pdf;/home/trung/Zotero/storage/UNJGI29I/1902.html},
  journal = {arXiv:1902.03251 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{feldman20_WhatNeuralNetworks,
  title = {What {{Neural Networks Memorize}} and {{Why}}: {{Discovering}} the {{Long Tail}} via {{Influence Estimation}}},
  shorttitle = {What {{Neural Networks Memorize}} and {{Why}}},
  author = {Feldman, Vitaly and Zhang, Chiyuan},
  year = {2020},
  month = aug,
  abstract = {Deep learning algorithms are well-known to have a propensity for fitting the training data very well and often fit even outliers and mislabeled data points. Such fitting requires memorization of training data labels, a phenomenon that has attracted significant research interest but has not been given a compelling explanation so far. A recent work of Feldman [Fel19] proposes a theoretical explanation for this phenomenon based on a combination of two insights. First, natural image and data distributions are (informally) known to be long-tailed, that is have a significant fraction of rare and atypical examples. Second, in a simple theoretical model such memorization is necessary for achieving close-to-optimal generalization error when the data distribution is long-tailed. However, no direct empirical evidence for this explanation or even an approach for obtaining such evidence were given.},
  archivePrefix = {arXiv},
  eprint = {2008.03703},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/feldman et al_2020_what neural networks memorize and why.pdf},
  journal = {arXiv:2008.03703 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{feng20_DualSwapDisentangling,
  title = {Dual {{Swap Disentangling}}},
  author = {Feng, Zunlei and Wang, Xinchao and Ke, Chenglong and Zeng, Anxiang and Tao, Dacheng and Song, Mingli},
  year = {2020},
  month = jan,
  abstract = {Learning interpretable disentangled representations is a crucial yet challenging task. In this paper, we propose a weakly semi-supervised method, termed as Dual Swap Disentangling (DSD), for disentangling using both labeled and unlabeled data. Unlike conventional weakly supervised methods that rely on full annotations on the group of samples, we require only limited annotations on paired samples that indicate their shared attribute like the color. Our model takes the form of a dual autoencoder structure. To achieve disentangling using the labeled pairs, we follow a ``encoding-swap-decoding'' process, where we first swap the parts of their encodings corresponding to the shared attribute, and then decode the obtained hybrid codes to reconstruct the original input pairs. For unlabeled pairs, we follow the ``encodingswap-decoding'' process twice on designated encoding parts and enforce the final outputs to approximate the input pairs. By isolating parts of the encoding and swapping them back and forth, we impose the dimension-wise modularity and portability of the encodings of the unlabeled samples, which implicitly encourages disentangling under the guidance of labeled pairs. This dual swap mechanism, tailored for semi-supervised setting, turns out to be very effective. Experiments on image datasets from a wide domain show that our model yields state-of-the-art disentangling performances.},
  archivePrefix = {arXiv},
  eprint = {1805.10583},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/feng et al_2020_dual swap disentangling.pdf},
  journal = {arXiv:1805.10583 [cs]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs}
}

@article{feng2020good,
  title = {Good Semi-Supervised \{\vphantom\}{{VAE}}\vphantom\{\} Requires Tighter Evidence Lower Bound},
  author = {Feng, Haozhe and Kong, Kezhi and Zhang, Tianye and Xue, Siyue and Chen, Wei},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/feng et al_2020_good semi-supervised vae requires tighter evidence lower bound.pdf}
}

@article{fernando00_DeepLearningApproaches,
  title = {Deep {{Learning Approaches}} to {{Feature Extraction}}, {{Modelling}} and {{Compensation}} for {{Short Duration Language Identification}}},
  author = {Fernando, Sarith},
  pages = {199},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/fernando_deep learning approaches to feature extraction, modelling and compensation for short duration language identification.pdf},
  language = {en}
}

@article{ferrer18_JointProbabilisticLinear,
  title = {Joint {{Probabilistic Linear Discriminant Analysis}}},
  author = {Ferrer, Luciana},
  year = {2018},
  month = jan,
  abstract = {Standard probabilistic linear discriminant analysis (PLDA) for speaker recognition assumes that the sample's features (usually, i-vectors) are given by a sum of three terms: a term that depends on the speaker identity, a term that models the within-speaker variability and is assumed independent across samples, and a final term that models any remaining variability and is also independent across samples. In this work, we propose a generalization of this model where the within-speaker variability is not necessarily assumed independent across samples but dependent on another discrete variable. This variable, which we call the channel variable as in the standard PLDA approach, could be, for example, a discrete category for the channel characteristics, the language spoken by the speaker, the type of speech in the sample (conversational, monologue, read), etc. The value of this variable is assumed to be known during training but not during testing. Scoring is performed, as in standard PLDA, by computing a likelihood ratio between the null hypothesis that the two sides of a trial belong to the same speaker versus the alternative hypothesis that the two sides belong to different speakers. The two likelihoods are computed by marginalizing over two hypothesis about the channels in both sides of a trial: that they are the same and that they are different. This way, we expect that the new model will be better at coping with same-channel versus different-channel trials than standard PLDA, since knowledge about the channel (or language, or speech style) is used during training and implicitly considered during scoring.},
  archivePrefix = {arXiv},
  eprint = {1704.02346},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ferrer_2018_joint probabilistic linear discriminant analysis.pdf},
  journal = {arXiv:1704.02346 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{fertig18_betaVAEscan,
  title = {\$\textbackslash beta\$-{{VAEs}} Can Retain Label Information Even at High Compression},
  author = {Fertig, Emily and Arbabi, Aryan and Alemi, Alexander A.},
  year = {2018},
  month = dec,
  abstract = {In this paper, we investigate the degree to which the encoding of a \$\textbackslash beta\$-VAE captures label information across multiple architectures on Binary Static MNIST and Omniglot. Even though they are trained in a completely unsupervised manner, we demonstrate that a \$\textbackslash beta\$-VAE can retain a large amount of label information, even when asked to learn a highly compressed representation.},
  archivePrefix = {arXiv},
  eprint = {1812.02682},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fertig et al_2018_$-beta$-vaes can retain label information even at high compression.pdf},
  journal = {arXiv:1812.02682 [cs, stat]},
  keywords = {favorite,read},
  primaryClass = {cs, stat}
}

@article{feyisetan19_LeveragingHierarchicalRepresentations,
  title = {Leveraging {{Hierarchical Representations}} for {{Preserving Privacy}} and {{Utility}} in {{Text}}},
  author = {Feyisetan, Oluwaseyi and Diethe, Tom and Drake, Thomas},
  year = {2019},
  month = oct,
  abstract = {Guaranteeing a certain level of user privacy in an arbitrary piece of text is a challenging issue. However, with this challenge comes the potential of unlocking access to vast data stores for training machine learning models and supporting data driven decisions. We address this problem through the lens of dx-privacy, a generalization of Differential Privacy to non Hamming distance metrics. In this work, we explore word representations in Hyperbolic space as a means of preserving privacy in text. We provide a proof satisfying dx-privacy, then we define a probability distribution in Hyperbolic space and describe a way to sample from it in high dimensions. Privacy is provided by perturbing vector representations of words in high dimensional Hyperbolic space to obtain a semantic generalization. We conduct a series of experiments to demonstrate the tradeoff between privacy and utility. Our privacy experiments illustrate protections against an authorship attribution algorithm while our utility experiments highlight the minimal impact of our perturbations on several downstream machine learning models. Compared to the Euclidean baseline, we observe {$>$} 20x greater guarantees on expected privacy against comparable worst case statistics.},
  archivePrefix = {arXiv},
  eprint = {1910.08917},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/feyisetan et al_2019_leveraging hierarchical representations for preserving privacy and utility in text.pdf},
  journal = {arXiv:1910.08917 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{figueroa00_SemisupervisedLearningusing,
  title = {Semi-Supervised {{Learning}} Using {{Deep Generative Models}} and {{Auxiliary Tasks}}},
  author = {Figueroa, Jhosimar Arias},
  pages = {9},
  abstract = {In this work, we propose a semi-supervised approach based on generative models to learn both feature representations and categories in an end-to-end manner. The learning process is guided by our proposed auxiliary task that performs assignments for the unlabeled data and regularizes the feature representations with the use of metric embedding methods. Our model is represented by a Gaussian Mixture Variational Autoencoder (GMVAE), in which, we model our categories with the Gumbel-Softmax distribution and we benefit from the autoencoder's architecture to learn feature representations. Experimental results show the effectiveness of our method on three datasets: MNIST, Fashion-MNIST and SVHN.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/figueroa_semi-supervised learning using deep generative models and auxiliary tasks.pdf},
  language = {en}
}

@article{figueroa00_SemisupervisedLearningusinga,
  title = {Semi-Supervised {{Learning}} Using {{Deep Generative Models}} and {{Auxiliary Tasks}}},
  author = {Figueroa, Jhosimar Arias},
  pages = {9},
  abstract = {In this work, we propose a semi-supervised approach based on generative models to learn both feature representations and categories in an end-to-end manner. The learning process is guided by our proposed auxiliary task that performs assignments for the unlabeled data and regularizes the feature representations with the use of metric embedding methods. Our model is represented by a Gaussian Mixture Variational Autoencoder (GMVAE), in which, we model our categories with the Gumbel-Softmax distribution and we benefit from the autoencoder's architecture to learn feature representations. Experimental results show the effectiveness of our method on three datasets: MNIST, Fashion-MNIST and SVHN.},
  file = {/home/trung/GoogleDrive/Zotero/figueroa_semi-supervised learning using deep generative models and auxiliary tasks2.pdf},
  keywords = {semi-supervised},
  language = {en}
}

@article{figurnov18_ImplicitReparameterizationGradients,
  title = {Implicit {{Reparameterization Gradients}}},
  author = {Figurnov, Michael and Mohamed, Shakir and Mnih, Andriy},
  year = {2018},
  month = may,
  abstract = {By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.},
  archivePrefix = {arXiv},
  eprint = {1805.08498},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/figurnov et al_2018_implicit reparameterization gradients.pdf},
  journal = {arXiv:1805.08498 [cs, stat]},
  keywords = {Computer Science - Machine Learning,favorite,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{finlayson19_Adversarialattacksmedical,
  title = {Adversarial Attacks on Medical Machine Learning},
  author = {Finlayson, Samuel G. and Bowers, John D. and Ito, Joichi and Zittrain, Jonathan L. and Beam, Andrew L. and Kohane, Isaac S.},
  year = {2019},
  month = mar,
  volume = {363},
  pages = {1287--1289},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aaw4399},
  annotation = {ZSCC: 0000055},
  file = {/home/trung/GoogleDrive/Zotero/finlayson et al_2019_adversarial attacks on medical machine learning.pdf},
  journal = {Science},
  language = {en},
  number = {6433}
}

@article{finn00_ProbabilisticModelAgnosticMetaLearning,
  title = {Probabilistic {{Model}}-{{Agnostic Meta}}-{{Learning}}},
  author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  pages = {12},
  abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.},
  annotation = {ZSCC: 0000098},
  file = {/home/trung/Zotero/storage/Y2RGTZI3/Finn et al. - Probabilistic Model-Agnostic Meta-Learning.pdf},
  language = {en}
}

@article{finn16_ConnectionGenerativeAdversarial,
  title = {A {{Connection}} between {{Generative Adversarial Networks}}, {{Inverse Reinforcement Learning}}, and {{Energy}}-{{Based Models}}},
  author = {Finn, Chelsea and Christiano, Paul and Abbeel, Pieter and Levine, Sergey},
  year = {2016},
  month = nov,
  abstract = {Generative adversarial networks (GANs) are a recently proposed class of generative models in which a generator is trained to optimize a cost function that is being simultaneously learned by a discriminator. While the idea of learning cost functions is relatively new to the field of generative modeling, learning costs has long been studied in control and reinforcement learning (RL) domains, typically for imitation learning from demonstrations. In these fields, learning cost function underlying observed behavior is known as inverse reinforcement learning (IRL) or inverse optimal control. While at first the connection between cost learning in RL and cost learning in generative modeling may appear to be a superficial one, we show in this paper that certain IRL methods are in fact mathematically equivalent to GANs. In particular, we demonstrate an equivalence between a sample-based algorithm for maximum entropy IRL and a GAN in which the generator's density can be evaluated and is provided as an additional input to the discriminator. Interestingly, maximum entropy IRL is a special case of an energy-based model. We discuss the interpretation of GANs as an algorithm for training energy-based models, and relate this interpretation to other recent work that seeks to connect GANs and EBMs. By formally highlighting the connection between GANs, IRL, and EBMs, we hope that researchers in all three communities can better identify and apply transferable ideas from one domain to another, particularly for developing more stable and scalable algorithms: a major challenge in all three domains.},
  archivePrefix = {arXiv},
  eprint = {1611.03852},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/finn et al_2016_a connection between generative adversarial networks, inverse reinforcement learning, and energy-based models.pdf},
  journal = {arXiv:1611.03852 [cs]},
  primaryClass = {cs}
}

@article{finn19_ProbabilisticModelAgnosticMetaLearning,
  title = {Probabilistic {{Model}}-{{Agnostic Meta}}-{{Learning}}},
  author = {Finn, Chelsea and Xu, Kelvin and Levine, Sergey},
  year = {2019},
  month = oct,
  abstract = {Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.},
  archivePrefix = {arXiv},
  eprint = {1806.02817},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/finn et al_2019_probabilistic model-agnostic meta-learning.pdf},
  journal = {arXiv:1806.02817 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{finzi20_ProbabilisticNumericConvolutional,
  title = {Probabilistic {{Numeric Convolutional Neural Networks}}},
  author = {Finzi, Marc and Bondesan, Roberto and Welling, Max},
  year = {2020},
  month = oct,
  abstract = {Continuous input signals like images and time series that are irregularly sampled or have missing values are challenging for existing deep learning methods. Coherently defined feature representations must depend on the values in unobserved regions of the input. Drawing from the work in probabilistic numerics, we propose Probabilistic Numeric Convolutional Neural Networks which represent features as Gaussian processes (GPs), providing a probabilistic description of discretization error. We then define a convolutional layer as the evolution of a PDE defined on this GP, followed by a nonlinearity. This approach also naturally admits steerable equivariant convolutions under e.g. the rotation group. In experiments we show that our approach yields a \$3\textbackslash times\$ reduction of error from the previous state of the art on the SuperPixel-MNIST dataset and competitive performance on the medical time series dataset PhysioNet2012.},
  archivePrefix = {arXiv},
  eprint = {2010.10876},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/finzi et al_2020_probabilistic numeric convolutional neural networks.pdf},
  journal = {arXiv:2010.10876 [cs]},
  keywords = {_tablet,favorite},
  primaryClass = {cs}
}

@article{fischer20_CEBImprovesModel,
  title = {{{CEB Improves Model Robustness}}},
  author = {Fischer, Ian and Alemi, Alexander A.},
  year = {2020},
  month = feb,
  abstract = {We demonstrate that the Conditional Entropy Bottleneck (CEB) can improve model robustness. CEB is an easy strategy to implement and works in tandem with data augmentation procedures. We report results of a large scale adversarial robustness study on CIFAR-10, as well as the ImageNet-C Common Corruptions Benchmark, ImageNet-A, and PGD attacks.},
  archivePrefix = {arXiv},
  eprint = {2002.05380},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fischer et al_2020_ceb improves model robustness.pdf},
  journal = {arXiv:2002.05380 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{fischer20_ConditionalEntropyBottleneck,
  title = {The {{Conditional Entropy Bottleneck}}},
  author = {Fischer, Ian},
  year = {2020},
  volume = {22},
  issn = {1099-4300},
  doi = {10.3390/e22090999},
  abstract = {Much of the field of Machine Learning exhibits a prominent set of failure modes, including vulnerability to adversarial examples, poor out-of-distribution (OoD) detection, miscalibration, and willingness to memorize random labelings of datasets. We characterize these as failures of robust generalization, which extends the traditional measure of generalization as accuracy or related metrics on a held-out set. We hypothesize that these failures to robustly generalize are due to the learning systems retaining too much information about the training data. To test this hypothesis, we propose the Minimum Necessary Information (MNI) criterion for evaluating the quality of a model. In order to train models that perform well with respect to the MNI criterion, we present a new objective function, the Conditional Entropy Bottleneck (CEB), which is closely related to the Information Bottleneck (IB). We experimentally test our hypothesis by comparing the performance of CEB models with deterministic models and Variational Information Bottleneck (VIB) models on a variety of different datasets and robustness challenges. We find strong empirical evidence supporting our hypothesis that MNI models improve on these problems of robust generalization.},
  file = {/home/trung/GoogleDrive/Zotero/fischer_2020_the conditional entropy bottleneck.pdf},
  journal = {Entropy},
  keywords = {information,information bottleneck,information theory,machine learning},
  number = {9}
}

@article{fisher19_AllModelsare,
  title = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}: {{Learning}} a {{Variable}}'s {{Importance}} by {{Studying}} an {{Entire Class}} of {{Prediction Models Simultaneously}}},
  shorttitle = {All {{Models}} Are {{Wrong}}, but {{Many}} Are {{Useful}}},
  author = {Fisher, Aaron and Rudin, Cynthia and Dominici, Francesca},
  year = {2019},
  month = dec,
  abstract = {Variable importance (VI) tools describe how much covariates contribute to a prediction model's accuracy. However, important variables for one well-performing model (for example, a linear model \$f(\textbackslash mathbf\{x\})=\textbackslash mathbf\{x\}\^\{T\}\textbackslash beta\$ with a fixed coefficient vector \$\textbackslash beta\$) may be unimportant for another model. In this paper, we propose model class reliance (MCR) as the range of VI values across all well-performing model in a prespecified class. Thus, MCR gives a more comprehensive description of importance by accounting for the fact that many prediction models, possibly of different parametric forms, may fit the data well. In the process of deriving MCR, we show several informative results for permutation-based VI estimates, based on the VI measures used in Random Forests. Specifically, we derive connections between permutation importance estimates for a single prediction model, U-statistics, conditional variable importance, conditional causal effects, and linear model coefficients. We then give probabilistic bounds for MCR, using a novel, generalizable technique. We apply MCR to a public data set of Broward County criminal records to study the reliance of recidivism prediction models on sex and race. In this application, MCR can be used to help inform VI for unknown, proprietary models.},
  archivePrefix = {arXiv},
  eprint = {1801.01489},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fisher et al_2019_all models are wrong, but many are useful.pdf},
  journal = {arXiv:1801.01489 [stat]},
  primaryClass = {stat}
}

@book{forbes11_StatisticaldistributionsCatherine,
  title = {Statistical Distributions: {{Catherine Forbes}} ... [et Al.]},
  shorttitle = {Statistical Distributions},
  editor = {Forbes, C. S.},
  year = {2011},
  edition = {4th ed},
  publisher = {{Wiley}},
  address = {{Hoboken, N.J}},
  file = {/home/trung/GoogleDrive/Zotero/forbes_2011_statistical distributions.pdf},
  isbn = {978-0-470-39063-4},
  keywords = {_tablet},
  language = {en},
  lccn = {QA273.6 .E92 2011}
}

@article{fortuin19_SOMVAEInterpretableDiscrete,
  title = {{{SOM}}-{{VAE}}: {{Interpretable Discrete Representation Learning}} on {{Time Series}}},
  shorttitle = {{{SOM}}-{{VAE}}},
  author = {Fortuin, Vincent and H{\"u}ser, Matthias and Locatello, Francesco and Strathmann, Heiko and R{\"a}tsch, Gunnar},
  year = {2019},
  month = jan,
  abstract = {High-dimensional time series are common in many domains. Since human cognition is not optimized to work well in high-dimensional spaces, these areas could benefit from interpretable low-dimensional representations. However, most representation learning algorithms for time series data are difficult to interpret. This is due to non-intuitive mappings from data features to salient properties of the representation and non-smoothness over time. To address this problem, we propose a new representation learning framework building on ideas from interpretable discrete dimensionality reduction and deep generative modeling. This framework allows us to learn discrete representations of time series, which give rise to smooth and interpretable embeddings with superior clustering performance. We introduce a new way to overcome the non-differentiability in discrete representation learning and present a gradient-based version of the traditional self-organizing map algorithm that is more performant than the original. Furthermore, to allow for a probabilistic interpretation of our method, we integrate a Markov model in the representation space. This model uncovers the temporal transition structure, improves clustering performance even further and provides additional explanatory insights as well as a natural representation of uncertainty.},
  archivePrefix = {arXiv},
  eprint = {1806.02199},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/AN6QK867/Fortuin et al. - 2019 - SOM-VAE Interpretable Discrete Representation Lea.pdf},
  journal = {arXiv:1806.02199 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fortuin20_GPVAEDeepProbabilistic,
  title = {{{GP}}-{{VAE}}: {{Deep Probabilistic Time Series Imputation}}},
  shorttitle = {{{GP}}-{{VAE}}},
  author = {Fortuin, Vincent and Baranchuk, Dmitry and R{\"a}tsch, Gunnar and Mandt, Stephan},
  year = {2020},
  month = feb,
  abstract = {Multivariate time series with missing values are common in areas such as healthcare and finance, and have grown in number and complexity over the years. This raises the question whether deep learning methodologies can outperform classical data imputation methods in this domain. However, na\"ive applications of deep learning fall short in giving reliable confidence estimates and lack interpretability. We propose a new deep sequential latent variable model for dimensionality reduction and data imputation. Our modeling assumption is simple and interpretable: the high dimensional time series has a lower-dimensional representation which evolves smoothly in time according to a Gaussian process. The non-linear dimensionality reduction in the presence of missing data is achieved using a VAE approach with a novel structured variational approximation. We demonstrate that our approach outperforms several classical and deep learning-based data imputation methods on high-dimensional data from the domains of computer vision and healthcare, while additionally improving the smoothness of the imputations and providing interpretable uncertainty estimates.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1907.04155},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/ZCPKWG6S/Fortuin et al. - 2020 - GP-VAE Deep Probabilistic Time Series Imputation.pdf},
  journal = {arXiv:1907.04155 [cs, stat]},
  keywords = {favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@book{foster00_GenerativeDeepLearning,
  title = {Generative {{Deep Learning}}},
  author = {Foster, David},
  annotation = {ZSCC: 0000010},
  file = {/home/trung/GoogleDrive/Zotero/foster_generative deep learning.pdf},
  language = {en}
}

@article{fostiropoulos20_DepthwiseDiscreteRepresentation,
  title = {Depthwise {{Discrete Representation Learning}}},
  author = {Fostiropoulos, Iordanis},
  year = {2020},
  month = apr,
  abstract = {Recent advancements in learning Discrete Representations as opposed to continuous ones have led to state of art results in tasks that involve Language, Audio and Vision. Some latent factors such as words,phonemes and shapes are better represented by discrete latent variables as opposed to continuous. Vector Quantized Variational Autoencoders (VQVAE) have produced remarkable results in multiple domains. VQVAE learns a prior distribution ze along with its mapping to a discrete number of K vectors (Vector Quantization). We propose applying VQ along the feature axis. We hypothesize that by doing so, we are learning a mapping between the codebook vectors and the marginal distribution of the prior feature space. Our approach leads to 33\% improvement as compared to prevous discrete models and has similar performance to state of the art auto-regressive models (e.g. PixelSNAIL). We evaluate our approach on a static prior using an artificial toy dataset (blobs). We further evaluate our approach on benchmarks for CIFAR-10 and ImageNet.},
  archivePrefix = {arXiv},
  eprint = {2004.05462},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fostiropoulos_2020_depthwise discrete representation learning.pdf},
  journal = {arXiv:2004.05462 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fox19_AdvocacyLearningLearning,
  title = {Advocacy {{Learning}}: {{Learning}} through {{Competition}} and {{Class}}-{{Conditional Representations}}},
  shorttitle = {Advocacy {{Learning}}},
  author = {Fox, Ian and Wiens, Jenna},
  year = {2019},
  month = aug,
  abstract = {We introduce advocacy learning, a novel supervised training scheme for attention-based classification problems. Advocacy learning relies on a framework consisting of two connected networks: 1) \$N\$ Advocates (one for each class), each of which outputs an argument in the form of an attention map over the input, and 2) a Judge, which predicts the class label based on these arguments. Each Advocate produces a class-conditional representation with the goal of convincing the Judge that the input example belongs to their class, even when the input belongs to a different class. Applied to several different classification tasks, we show that advocacy learning can lead to small improvements in classification accuracy over an identical supervised baseline. Though a series of follow-up experiments, we analyze when and how such class-conditional representations improve discriminative performance. Though somewhat counter-intuitive, a framework in which subnetworks are trained to competitively provide evidence in support of their class shows promise, in many cases performing on par with standard learning approaches. This provides a foundation for further exploration into competition and class-conditional representations in supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1908.02723},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fox et al_2019_advocacy learning.pdf},
  journal = {arXiv:1908.02723 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{fox19_HierarchicalVariationalImitation,
  title = {Hierarchical {{Variational Imitation Learning}} of {{Control Programs}}},
  author = {Fox, Roy and Shin, Richard and Paul, William and Zou, Yitian and Song, Dawn and Goldberg, Ken and Abbeel, Pieter and Stoica, Ion},
  year = {2019},
  month = dec,
  abstract = {Autonomous agents can learn by imitating teacher demonstrations of the intended behavior. Hierarchical control policies are ubiquitously useful for such learning, having the potential to break down structured tasks into simpler sub-tasks, thereby improving data efficiency and generalization. In this paper, we propose a variational inference method for imitation learning of a control policy represented by parametrized hierarchical procedures (PHP), a program-like structure in which procedures can invoke sub-procedures to perform sub-tasks. Our method discovers the hierarchical structure in a dataset of observation\textendash action traces of teacher demonstrations, by learning an approximate posterior distribution over the latent sequence of procedure calls and terminations. Samples from this learned distribution then guide the training of the hierarchical control policy. We identify and demonstrate a novel benefit of variational inference in the context of hierarchical imitation learning: in decomposing the policy into simpler procedures, inference can leverage acausal information that is unused by other methods. Training PHP with variational inference outperforms LSTM baselines in terms of data efficiency and generalization, requiring less than half as much data to achieve a 24\% error rate in executing the bubble sort algorithm, and to achieve no error in executing Karel programs.},
  archivePrefix = {arXiv},
  eprint = {1912.12612},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/CYJZQHRF/Fox et al. - 2019 - Hierarchical Variational Imitation Learning of Con.pdf},
  journal = {arXiv:1912.12612 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fraccaro16_SequentialNeuralModels,
  title = {Sequential {{Neural Models}} with {{Stochastic Layers}}},
  author = {Fraccaro, Marco and S{\o}nderby, S{\o}ren Kaae and Paquet, Ulrich and Winther, Ole},
  year = {2016},
  month = nov,
  abstract = {How can we efficiently propagate uncertainty in a latent state representation with recurrent neural networks? This paper introduces stochastic recurrent neural networks which glue a deterministic recurrent neural network and a state space model together to form a stochastic and sequential neural generative model. The clear separation of deterministic and stochastic layers allows a structured variational inference network to track the factorization of the model's posterior distribution. By retaining both the nonlinear recursive structure of a recurrent neural network and averaging over the uncertainty in a latent path, like a state space model, we improve the state of the art results on the Blizzard and TIMIT speech modeling data sets by a large margin, while achieving comparable performances to competing methods on polyphonic music modeling.},
  annotation = {ZSCC: 0000183},
  archivePrefix = {arXiv},
  eprint = {1605.07571},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/NM8PTNVE/Fraccaro et al. - 2016 - Sequential Neural Models with Stochastic Layers.pdf},
  journal = {arXiv:1605.07571 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{frankle19_LotteryTicketHypothesis,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.},
  archivePrefix = {arXiv},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/frankle et al_2019_the lottery ticket hypothesis.pdf},
  journal = {arXiv:1803.03635 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{franzese20_ProbabilisticEnsembleDeep,
  title = {Probabilistic {{Ensemble}} of {{Deep Information Networks}}},
  author = {Franzese, Giulio and Visintin, Monica},
  year = {2020},
  volume = {22},
  issn = {1099-4300},
  doi = {10.3390/e22010100},
  abstract = {We describe a classifier made of an ensemble of decision trees, designed using information theory concepts. In contrast to algorithms C4.5 or ID3, the tree is built from the leaves instead of the root. Each tree is made of nodes trained independently of the others, to minimize a local cost function (information bottleneck). The trained tree outputs the estimated probabilities of the classes given the input datum, and the outputs of many trees are combined to decide the class. We show that the system is able to provide results comparable to those of the tree classifier in terms of accuracy, while it shows many advantages in terms of modularity, reduced complexity, and memory requirements.},
  file = {/home/trung/GoogleDrive/Zotero/franzese et al_2020_probabilistic ensemble of deep information networks.pdf},
  journal = {Entropy},
  keywords = {classifier,decision tree,ensemble,information,information bottleneck,information theory},
  number = {1}
}

@article{freeman19_LearningPredictLooking,
  title = {Learning to {{Predict Without Looking Ahead}}: {{World Models Without Forward Prediction}}},
  shorttitle = {Learning to {{Predict Without Looking Ahead}}},
  author = {Freeman, C. Daniel and Metz, Luke and Ha, David},
  year = {2019},
  month = oct,
  abstract = {Much of model-based reinforcement learning involves learning a model of an agent's world, and training an agent to leverage this model to perform a task more efficiently. While these models are demonstrably useful for agents, every naturally occurring model of the world of which we are aware---e.g., a brain---arose as the byproduct of competing evolutionary pressures for survival, not minimization of a supervised forward-predictive loss via gradient descent. That useful models can arise out of the messy and slow optimization process of evolution suggests that forward-predictive modeling can arise as a side-effect of optimization under the right circumstances. Crucially, this optimization process need not explicitly be a forward-predictive loss. In this work, we introduce a modification to traditional reinforcement learning which we call observational dropout, whereby we limit the agents ability to observe the real environment at each timestep. In doing so, we can coerce an agent into learning a world model to fill in the observation gaps during reinforcement learning. We show that the emerged world model, while not explicitly trained to predict the future, can help the agent learn key skills required to perform well in its environment. Videos of our results available at https://learningtopredict.github.io/},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.13038},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/freeman et al_2019_learning to predict without looking ahead.pdf;/home/trung/Zotero/storage/6DDYW4XQ/1910.html},
  journal = {arXiv:1910.13038 [cs]},
  primaryClass = {cs}
}

@article{frenay14_Estimatingmutualinformation,
  title = {Estimating Mutual Information for Feature Selection in the Presence of Label Noise},
  author = {Fr{\'e}nay, Beno{\^i}t and Doquire, Gauthier and Verleysen, Michel},
  year = {2014},
  month = mar,
  volume = {71},
  pages = {832--848},
  issn = {01679473},
  doi = {10.1016/j.csda.2013.05.001},
  abstract = {A way to achieve feature selection for classification problems polluted by label noise is proposed. The performances of traditional feature selection algorithms often decrease sharply when some samples are wrongly labelled. A method based on a probabilistic label noise model combined with a nearest neighbours-based entropy estimator is introduced to robustly evaluate the mutual information, a popular relevance criterion for feature selection. A backward greedy search procedure is used in combination with this criterion to find relevant sets of features. Experiments establish that (i) there is a real need to take a possible label noise into account when selecting features and (ii) the proposed methodology is effectively able to reduce the negative impact of the mislabelled data points on the feature selection process.},
  file = {/home/trung/GoogleDrive/Zotero/frénay et al_2014_estimating mutual information for feature selection in the presence of label noise.pdf},
  journal = {Computational Statistics \& Data Analysis},
  keywords = {information},
  language = {en}
}

@article{frick20_InformalIntroductionMultiplet,
  title = {An {{Informal Introduction}} to {{Multiplet Neural Networks}}},
  author = {Frick, Nathan E.},
  year = {2020},
  month = jun,
  abstract = {In the artificial neuron, I replace the dot product with the weighted Lehmer mean, which may emulate different cases of a generalized mean. The single neuron instance is replaced by a multiplet of neurons which have the same averaging weights. A group of outputs feed forward, in lieu of the single scalar. The generalization parameter is typically set to a different value for each neuron in the multiplet.},
  archivePrefix = {arXiv},
  eprint = {2006.01606},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/frick_2020_an informal introduction to multiplet neural networks.pdf},
  journal = {arXiv:2006.01606 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{fridman18_ArguingMachinesHuman,
  title = {Arguing {{Machines}}: {{Human Supervision}} of {{Black Box AI Systems That Make Life}}-{{Critical Decisions}}},
  shorttitle = {Arguing {{Machines}}},
  author = {Fridman, Lex and Ding, Li and Jenik, Benedikt and Reimer, Bryan},
  year = {2018},
  month = sep,
  abstract = {We consider the paradigm of a black box AI system that makes life-critical decisions. We propose an "arguing machines" framework that pairs the primary AI system with a secondary one that is independently trained to perform the same task. We show that disagreement between the two systems, without any knowledge of underlying system design or operation, is sufficient to arbitrarily improve the accuracy of the overall decision pipeline given human supervision over disagreements. We demonstrate this system in two applications: (1) an illustrative example of image classification and (2) on large-scale real-world semi-autonomous driving data. For the first application, we apply this framework to image classification achieving a reduction from 8.0\% to 2.8\% top-5 error on ImageNet. For the second application, we apply this framework to Tesla Autopilot and demonstrate the ability to predict 90.4\% of system disengagements that were labeled by human annotators as challenging and needing human supervision.},
  archivePrefix = {arXiv},
  eprint = {1710.04459},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fridman et al_2018_arguing machines.pdf;/home/trung/GoogleDrive/Zotero/fridman et al_2018_arguing machines2.pdf;/home/trung/Zotero/storage/CUXICYL7/1710.html},
  journal = {arXiv:1710.04459 [cs]},
  primaryClass = {cs}
}

@article{fridman19_DeepTrafficCrowdsourcedHyperparameter,
  title = {{{DeepTraffic}}: {{Crowdsourced Hyperparameter Tuning}} of {{Deep Reinforcement Learning Systems}} for {{Multi}}-{{Agent Dense Traffic Navigation}}},
  shorttitle = {{{DeepTraffic}}},
  author = {Fridman, Lex and Terwilliger, Jack and Jenik, Benedikt},
  year = {2019},
  month = jan,
  abstract = {We present a traffic simulation named DeepTraffic where the planning systems for a subset of the vehicles are handled by a neural network as part of a model-free, off-policy reinforcement learning process. The primary goal of DeepTraffic is to make the hands-on study of deep reinforcement learning accessible to thousands of students, educators, and researchers in order to inspire and fuel the exploration and evaluation of deep Q-learning network variants and hyperparameter configurations through large-scale, open competition. This paper investigates the crowd-sourced hyperparameter tuning of the policy network that resulted from the first iteration of the DeepTraffic competition where thousands of participants actively searched through the hyperparameter space.},
  archivePrefix = {arXiv},
  eprint = {1801.02805},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fridman et al_2019_deeptraffic.pdf;/home/trung/Zotero/storage/VGT5N3I5/1801.html},
  journal = {arXiv:1801.02805 [cs]},
  primaryClass = {cs}
}

@article{fritschek20_NeuralMutualInformation,
  title = {Neural {{Mutual Information Estimation}} for {{Channel Coding}}: {{State}}-of-the-{{Art Estimators}}, {{Analysis}}, and {{Performance Comparison}}},
  shorttitle = {Neural {{Mutual Information Estimation}} for {{Channel Coding}}},
  author = {Fritschek, Rick and Schaefer, Rafael F. and Wunder, Gerhard},
  year = {2020},
  month = jun,
  abstract = {Deep learning based physical layer design, i.e., using dense neural networks as encoders and decoders, has received considerable interest recently. However, while such an approach is naturally training data-driven, actions of the wireless channel are mimicked using standard channel models, which only partially reflect the physical ground truth. Very recently, neural network based mutual information (MI) estimators have been proposed that directly extract channel actions from the input-output measurements and feed these outputs into the channel encoder. This is a promising direction as such a new design paradigm is fully adaptive and training data-based. This paper implements further recent improvements of such MI estimators, analyzes theoretically their suitability for the channel coding problem, and compares their performance. To this end, a new MI estimator using a \textbackslash emph\{``reverse Jensen''\} approach is proposed.},
  archivePrefix = {arXiv},
  eprint = {2006.16015},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fritschek et al_2020_neural mutual information estimation for channel coding.pdf},
  journal = {arXiv:2006.16015 [cs, math]},
  keywords = {information},
  primaryClass = {cs, math}
}

@book{fromm41_EscapseFreedom,
  title = {Escapse from {{Freedom}}},
  author = {Fromm, Erich},
  year = {1941},
  file = {/home/trung/GoogleDrive/Zotero/fromm_1941_escapse from freedom.pdf}
}

@book{fromm56_ArtLoving,
  title = {The {{Art}} of {{Loving}}},
  author = {Fromm, Erich},
  year = {1956},
  file = {/home/trung/GoogleDrive/Zotero/fromm_1956_the art of loving.pdf}
}

@book{fromm76_HaveBe,
  title = {To {{Have}} or {{To Be}}?},
  author = {Fromm, Erich},
  year = {1976},
  file = {/home/trung/GoogleDrive/Zotero/fromm_1976_to have or to be.pdf}
}

@article{frosst18_DARCCCDetectingAdversaries,
  title = {{{DARCCC}}: {{Detecting Adversaries}} by {{Reconstruction}} from {{Class Conditional Capsules}}},
  shorttitle = {{{DARCCC}}},
  author = {Frosst, Nicholas and Sabour, Sara and Hinton, Geoffrey},
  year = {2018},
  month = nov,
  abstract = {We present a simple technique that allows capsule models to detect adversarial images. In addition to being trained to classify images, the capsule model is trained to reconstruct the images from the pose parameters and identity of the correct top-level capsule. Adversarial images do not look like a typical member of the predicted class and they have much larger reconstruction errors when the reconstruction is produced from the top-level capsule for that class. We show that setting a threshold on the \$l2\$ distance between the input image and its reconstruction from the winning capsule is very effective at detecting adversarial images for three different datasets. The same technique works quite well for CNNs that have been trained to reconstruct the image from all or part of the last hidden layer before the softmax. We then explore a stronger, white-box attack that takes the reconstruction error into account. This attack is able to fool our detection technique but in order to make the model change its prediction to another class, the attack must typically make the "adversarial" image resemble images of the other class.},
  archivePrefix = {arXiv},
  eprint = {1811.06969},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/frosst et al_2018_darccc.pdf;/home/trung/Zotero/storage/7FRYRGD5/1811.html},
  journal = {arXiv:1811.06969 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{fu19_CyclicalAnnealingSchedule,
  title = {Cyclical {{Annealing Schedule}}: {{A Simple Approach}} to {{Mitigating KL Vanishing}}},
  shorttitle = {Cyclical {{Annealing Schedule}}},
  author = {Fu, Hao and Li, Chunyuan and Liu, Xiaodong and Gao, Jianfeng and Celikyilmaz, Asli and Carin, Lawrence},
  year = {2019},
  month = jun,
  abstract = {Variational autoencoders (VAEs) with an auto-regressive decoder have been applied for many natural language processing (NLP) tasks. The VAE objective consists of two terms, (i) reconstruction and (ii) KL regularization, balanced by a weighting hyper-parameter \textbackslash beta. One notorious training difficulty is that the KL term tends to vanish. In this paper we study scheduling schemes for \textbackslash beta, and show that KL vanishing is caused by the lack of good latent codes in training the decoder at the beginning of optimization. To remedy this, we propose a cyclical annealing schedule, which repeats the process of increasing \textbackslash beta multiple times. This new procedure allows the progressive learning of more meaningful latent codes, by leveraging the informative representations of previous cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1903.10145},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/fu et al_2019_cyclical annealing schedule.pdf},
  journal = {arXiv:1903.10145 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@article{fukushima80_Neocognitronselforganizingneural,
  title = {Neocognitron: {{A}} Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition Unaffected by Shift in Position},
  author = {Fukushima, Kunihiko},
  year = {1980},
  month = apr,
  volume = {36},
  pages = {193--202},
  issn = {1432-0770},
  doi = {10.1007/BF00344251},
  abstract = {A neural network model for a mechanism of visual pattern recognition is proposed in this paper. The network is self-organized by ``learning without a teacher'', and acquires an ability to recognize stimulus patterns based on the geometrical similarity (Gestalt) of their shapes without affected by their positions. This network is given a nickname ``neocognitron''. After completion of self-organization, the network has a structure similar to the hierarchy model of the visual nervous system proposed by Hubel and Wiesel. The network consits of an input layer (photoreceptor array) followed by a cascade connection of a number of modular structures, each of which is composed of two layers of cells connected in a cascade. The first layer of each module consists of ``S-cells'', which show characteristics similar to simple cells or lower order hypercomplex cells, and the second layer consists of ``C-cells'' similar to complex cells or higher order hypercomplex cells. The afferent synapses to each S-cell have plasticity and are modifiable. The network has an ability of unsupervised learning: We do not need any ``teacher'' during the process of self-organization, and it is only needed to present a set of stimulus patterns repeatedly to the input layer of the network. The network has been simulated on a digital computer. After repetitive presentation of a set of stimulus patterns, each stimulus pattern has become to elicit an output only from one of the C-cell of the last layer, and conversely, this C-cell has become selectively responsive only to that stimulus pattern. That is, none of the C-cells of the last layer responds to more than one stimulus pattern. The response of the C-cells of the last layer is not affected by the pattern's position at all. Neither is it affected by a small change in shape nor in size of the stimulus pattern.},
  file = {/home/trung/GoogleDrive/Zotero/fukushima_1980_neocognitron.pdf},
  journal = {Biological Cybernetics},
  number = {4}
}

@article{gabardi07_ReviewDietarySupplement,
  title = {A {{Review}} of {{Dietary Supplement}}\textendash{{Induced Renal Dysfunction}}},
  author = {Gabardi, Steven and Munz, Kristin and Ulbricht, Catherine},
  year = {2007},
  month = jul,
  volume = {2},
  pages = {757--765},
  publisher = {{American Society of Nephrology}},
  issn = {1555-9041, 1555-905X},
  doi = {10.2215/CJN.00500107},
  abstract = {Complementary and alternative medicine (CAM) is a multibillion-dollar industry. Almost half of the American population uses some form of CAM, with many using them in addition to prescription medications. Most patients fail to inform their health care providers of their CAM use, and physicians rarely inquire. Annually, thousands of dietary supplement\textendash induced adverse events are reported to Poison Control Centers nationwide. CAM manufacturers are not responsible for proving safety and efficacy, because the Food and Drug Administration does not regulate them. However, concern exists surrounding the safety of CAM. A literature search using MEDLINE and EMBASE was undertaken to explore the impact of CAM on renal function. English-language studies and case reports were selected for inclusion but were limited to those that consisted of human subjects, both adult and pediatric. This review provides details on dietary supplements that have been associated with renal dysfunction and focuses on 17 dietary supplements that have been associated with direct renal injury, CAM-induced immune-mediated nephrotoxicity, nephrolithiasis, rhabdomyolysis with acute renal injury, and hepatorenal syndrome. It is concluded that it is imperative that use of dietary supplements be monitored closely in all patients. Health care practitioners must take an active role in identifying patients who are using CAM and provide appropriate patient education.},
  chapter = {Mini-Review},
  copyright = {Copyright \textcopyright{} 2007 by the American Society of Nephrology},
  file = {/home/trung/GoogleDrive/Zotero/gabardi et al_2007_a review of dietary supplement–induced renal dysfunction.pdf},
  journal = {Clinical Journal of the American Society of Nephrology},
  language = {en},
  number = {4},
  pmid = {17699493}
}

@article{gabardi07_ReviewDietarySupplementa,
  title = {A {{Review}} of {{Dietary Supplement}}\textendash{{Induced Renal Dysfunction}}},
  author = {Gabardi, Steven and Munz, Kristin and Ulbricht, Catherine},
  year = {2007},
  month = jul,
  volume = {2},
  pages = {757--765},
  publisher = {{American Society of Nephrology}},
  issn = {1555-9041, 1555-905X},
  doi = {10.2215/CJN.00500107},
  abstract = {{$<$}p{$>$}Complementary and alternative medicine (CAM) is a multibillion-dollar industry. Almost half of the American population uses some form of CAM, with many using them in addition to prescription medications. Most patients fail to inform their health care providers of their CAM use, and physicians rarely inquire. Annually, thousands of dietary supplement\textendash induced adverse events are reported to Poison Control Centers nationwide. CAM manufacturers are not responsible for proving safety and efficacy, because the Food and Drug Administration does not regulate them. However, concern exists surrounding the safety of CAM. A literature search using MEDLINE and EMBASE was undertaken to explore the impact of CAM on renal function. English-language studies and case reports were selected for inclusion but were limited to those that consisted of human subjects, both adult and pediatric. This review provides details on dietary supplements that have been associated with renal dysfunction and focuses on 17 dietary supplements that have been associated with direct renal injury, CAM-induced immune-mediated nephrotoxicity, nephrolithiasis, rhabdomyolysis with acute renal injury, and hepatorenal syndrome. It is concluded that it is imperative that use of dietary supplements be monitored closely in all patients. Health care practitioners must take an active role in identifying patients who are using CAM and provide appropriate patient education.{$<$}/p{$>$}},
  chapter = {Mini-Review},
  copyright = {Copyright \textcopyright{} 2007 by the American Society of Nephrology},
  file = {/home/trung/GoogleDrive/Zotero/gabardi et al_2007_a review of dietary supplement–induced renal dysfunction2.pdf},
  journal = {Clinical Journal of the American Society of Nephrology},
  language = {en},
  number = {4},
  pmid = {17699493}
}

@article{gabbay20_DemystifyingInterClassDisentanglement,
  title = {Demystifying {{Inter}}-{{Class Disentanglement}}},
  author = {Gabbay, Aviv and Hoshen, Yedid},
  year = {2020},
  month = feb,
  abstract = {Learning to disentangle the hidden factors of variations within a set of observations is a key task for artificial intelligence. We present a unified formulation for class and content disentanglement and use it to illustrate the limitations of current methods. We therefore introduce LORD, a novel method based on Latent Optimization for Representation Disentanglement. We find that latent optimization, along with an asymmetric noise regularization, is superior to amortized inference for achieving disentangled representations. In extensive experiments, our method is shown to achieve better disentanglement performance than both adversarial and non-adversarial methods that use the same level of supervision. We further introduce a clustering-based approach for extending our method for settings that exhibit in-class variation with promising results on the task of domain translation.},
  archivePrefix = {arXiv},
  eprint = {1906.11796},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gabbay et al_2020_demystifying inter-class disentanglement.pdf},
  journal = {arXiv:1906.11796 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{gabriel20_ArtificialIntelligenceValues,
  title = {Artificial {{Intelligence}}, {{Values}} and {{Alignment}}},
  author = {Gabriel, Iason},
  year = {2020},
  month = jan,
  abstract = {This paper looks at philosophical questions that arise in the context of AI alignment. It defends three propositions. First, normative and technical aspects of the AI alignment problem are interrelated, creating space for productive engagement between people working in both domains. Second, it is important to be clear about the goal of alignment. There are significant differences between AI that aligns with instructions, intentions, revealed preferences, ideal preferences, interests and values. A principle-based approach to AI alignment, which combines these elements in a systematic way, has considerable advantages in this context. Third, the central challenge for theorists is not to identify `true' moral principles for AI; rather, it is to identify fair principles for alignment, that receive reflective endorsement despite widespread variation in people's moral beliefs. The final part of the paper explores three ways in which fair principles for AI alignment could potentially be identified.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {2001.09768},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/KQH8WNDU/Gabriel - 2020 - Artificial Intelligence, Values and Alignment.pdf},
  journal = {arXiv:2001.09768 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{gabry19_VisualizationBayesianworkflow,
  title = {Visualization in {{Bayesian}} Workflow},
  author = {Gabry, Jonah and Simpson, Daniel and Vehtari, Aki and Betancourt, Michael and Gelman, Andrew},
  year = {2019},
  month = feb,
  volume = {182},
  pages = {389--402},
  issn = {0964-1998, 1467-985X},
  doi = {10.1111/rssa.12378},
  file = {/home/trung/GoogleDrive/Zotero/gabry et al_2019_visualization in bayesian workflow.pdf},
  journal = {Journal of the Royal Statistical Society: Series A (Statistics in Society)},
  language = {en},
  number = {2}
}

@article{gaier19_WeightAgnosticNeural,
  title = {Weight {{Agnostic Neural Networks}}},
  author = {Gaier, Adam and Ha, David},
  year = {2019},
  month = sep,
  abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
  archivePrefix = {arXiv},
  eprint = {1906.04358},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gaier et al_2019_weight agnostic neural networks.pdf;/home/trung/GoogleDrive/Zotero/gaier et al_2019_weight agnostic neural networks2.pdf;/home/trung/Zotero/storage/N6DYX2WR/1906.html},
  journal = {arXiv:1906.04358 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gal00_UncertaintyDeepLearning,
  title = {Uncertainty in {{Deep Learning}}},
  author = {Gal, Yarin},
  pages = {174},
  annotation = {ZSCC: 0001407},
  file = {/home/trung/GoogleDrive/Zotero/gal_uncertainty in deep learning.pdf},
  language = {en}
}

@article{gal16_DropoutBayesianApproximation,
  title = {Dropout as a {{Bayesian Approximation}}: {{Representing Model Uncertainty}} in {{Deep Learning}}},
  shorttitle = {Dropout as a {{Bayesian Approximation}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  year = {2016},
  month = oct,
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs -- extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and non-linearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  annotation = {ZSCC: 0001433},
  archivePrefix = {arXiv},
  eprint = {1506.02142},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gal et al_2016_dropout as a bayesian approximation.pdf;/home/trung/Zotero/storage/KJS42MZR/1506.html},
  journal = {arXiv:1506.02142 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gale19_StateSparsityDeepa,
  title = {The {{State}} of {{Sparsity}} in {{Deep Neural Networks}}},
  author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
  year = {2019},
  month = feb,
  abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
  archivePrefix = {arXiv},
  eprint = {1902.09574},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gale et al_2019_the state of sparsity in deep neural networks.pdf},
  journal = {arXiv:1902.09574 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gallier00_FundamentalsLinearAlgebra,
  title = {Fundamentals of {{Linear Algebra}} and {{Optimization}}},
  author = {Gallier, Jean and Quaintance, Jocelyn},
  pages = {999},
  file = {/home/trung/GoogleDrive/Zotero/gallier et al_fundamentals of linear algebra and optimization.pdf},
  language = {en}
}

@article{gallier00_NotesDifferentialGeometry,
  title = {Notes on {{Differential Geometry}} and {{Lie Groups}}},
  author = {Gallier, Jean},
  pages = {660},
  file = {/home/trung/GoogleDrive/Zotero/gallier_notes on diﬀerential geometry and lie groups.pdf},
  language = {en}
}

@article{gallier20_AlgebraTopologyDifferential,
  title = {Algebra, {{Topology}}, {{Differential Calculus}}, and {{Optimization Theory For Computer Science}} and {{Machine Learning}}},
  author = {Gallier, Jean and Quaintance, Jocelyn},
  year = {2020},
  pages = {1962},
  file = {/home/trung/GoogleDrive/Zotero/gallier et al_2020_algebra, topology, diﬀerential calculus, and optimization theory for computer science and machine learning.pdf},
  language = {en}
}

@article{gama20_GraphsConvolutionsNeural,
  title = {Graphs, {{Convolutions}}, and {{Neural Networks}}},
  author = {Gama, Fernando and Isufi, Elvin and Leus, Geert and Ribeiro, Alejandro},
  year = {2020},
  month = mar,
  abstract = {Network data can be conveniently modeled as a graph signal, where data values are assigned to nodes of a graph that describes the underlying network topology. Successful learning from network data is built upon methods that effectively exploit this graph structure. In this work, we overview graph convolutional filters, which are linear, local and distributed operations that adequately leverage the graph structure. We then discuss graph neural networks (GNNs), built upon graph convolutional filters, that have been shown to be powerful nonlinear learning architectures. We show that GNNs are permutation equivariant and stable to changes in the underlying graph topology, allowing them to scale and transfer. We also introduce GNN extensions using edgevarying and autoregressive moving average graph filters, and discuss their properties. Finally, we study the use of GNNs in learning decentralized controllers for robot swarm and in addressing the recommender system problem.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {2003.03777},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/JFXH7BDS/Gama et al. - 2020 - Graphs, Convolutions, and Neural Networks.pdf},
  journal = {arXiv:2003.03777 [cs, eess, stat]},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{gamal82_LecturesNetworkInformation,
  title = {Lectures on  {{Network Information Theory}}},
  author = {Gamal, Abbas El},
  year = {1982},
  pages = {160},
  file = {/home/trung/GoogleDrive/Zotero/gamal_1982_lectures on network information theory.pdf},
  journal = {The Early Years},
  keywords = {information},
  language = {en}
}

@article{gammone18_Omega3PolyunsaturatedFatty,
  title = {Omega-3 {{Polyunsaturated Fatty Acids}}: {{Benefits}} and {{Endpoints}} in {{Sport}}},
  shorttitle = {Omega-3 {{Polyunsaturated Fatty Acids}}},
  author = {Gammone, Maria Alessandra and Riccioni, Graziano and Parrinello, Gaspare and D'Orazio, Nicolantonio},
  year = {2018},
  month = dec,
  volume = {11},
  issn = {2072-6643},
  doi = {10.3390/nu11010046},
  abstract = {The influence of nutrition has the potential to substantially affect physical function and body metabolism. Particular attention has been focused on omega-3 polyunsaturated fatty acids (n-3 PUFAs), which can be found both in terrestrial features and in the marine world. They are responsible for numerous cellular functions, such as signaling, cell membrane fluidity, and structural maintenance. They also regulate the nervous system, blood pressure, hematic clotting, glucose tolerance, and inflammatory processes, which may be useful in all inflammatory conditions. Animal models and cell-based models show that n-3 PUFAs can influence skeletal muscle metabolism. Furthermore, recent human studies demonstrate that they can influence not only the exercise and the metabolic response of skeletal muscle, but also the functional response for a period of exercise training. In addition, their potential anti-inflammatory and antioxidant activity may provide health benefits and performance improvement especially in those who practice physical activity, due to their increased reactive oxygen production. This review highlights the importance of n-3 PUFAs in our diet, which focuses on their potential healthy effects in sport.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/gammone et al_2018_omega-3 polyunsaturated fatty acids.pdf},
  journal = {Nutrients},
  number = {1},
  pmcid = {PMC6357022},
  pmid = {30591639}
}

@misc{gans20_JavaScriptDataScience,
  title = {{{JavaScript}} for {{Data Science}}},
  author = {Gans, Maya and Hodges, Toby and Wilson, Greg},
  year = {2020},
  howpublished = {https://js4ds.org/}
}

@article{gao18_AutoEncodingTotalCorrelation,
  title = {Auto-{{Encoding Total Correlation Explanation}}},
  author = {Gao, Shuyang and Brekelmans, Rob and Steeg, Greg Ver and Galstyan, Aram},
  year = {2018},
  month = feb,
  abstract = {Advances in unsupervised learning enable reconstruction and generation of samples from complex distributions, but this success is marred by the inscrutability of the representations learned. We propose an information-theoretic approach to characterizing disentanglement and dependence in representation learning using multivariate mutual information, also called total correlation. The principle of total Cor-relation Ex-planation (CorEx) has motivated successful unsupervised learning applications across a variety of domains, but under some restrictive assumptions. Here we relax those restrictions by introducing a flexible variational lower bound to CorEx. Surprisingly, we find that this lower bound is equivalent to the one in variational autoencoders (VAE) under certain conditions. This information-theoretic view of VAE deepens our understanding of hierarchical VAE and motivates a new algorithm, AnchorVAE, that makes latent codes more interpretable through information maximization and enables generation of richer and more realistic samples.},
  annotation = {ZSCC: 0000026},
  archivePrefix = {arXiv},
  eprint = {1802.05822},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gao et al_2018_auto-encoding total correlation explanation.pdf},
  journal = {arXiv:1802.05822 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{gao18_Hierarchicalattentionnetworks,
  title = {Hierarchical Attention Networks for Information Extraction from Cancer Pathology Reports},
  author = {Gao, Shang and Young, Michael T and Qiu, John X and Yoon, Hong-Jun and Christian, James B and Fearn, Paul A and Tourassi, Georgia D and Ramanthan, Arvind},
  year = {2018},
  month = mar,
  volume = {25},
  pages = {321--330},
  issn = {1067-5027, 1527-974X},
  doi = {10.1093/jamia/ocx131},
  abstract = {Objective: We explored how a deep learning (DL) approach based on hierarchical attention networks (HANs) can improve model performance for multiple information extraction tasks from unstructured cancer pathology reports compared to conventional methods that do not sufficiently capture syntactic and semantic contexts from free-text documents. Materials and Methods: Data for our analyses were obtained from 942 deidentified pathology reports collected by the National Cancer Institute Surveillance, Epidemiology, and End Results program. The HAN was implemented for 2 information extraction tasks: (1) primary site, matched to 12 International Classification of Diseases for Oncology topography codes (7 breast, 5 lung primary sites), and (2) histological grade classification, matched to G1\textendash G4. Model performance metrics were compared to conventional machine learning (ML) approaches including naive Bayes, logistic regression, support vector machine, random forest, and extreme gradient boosting, and other DL models, including a recurrent neural network (RNN), a recurrent neural network with attention (RNN w/A), and a convolutional neural network. Results: Our results demonstrate that for both information tasks, HAN performed significantly better compared to the conventional ML and DL techniques. In particular, across the 2 tasks, the mean micro and macro F-scores for the HAN with pretraining were (0.852,0.708), compared to naive Bayes (0.518, 0.213), logistic regression (0.682, 0.453), support vector machine (0.634, 0.434), random forest (0.698, 0.508), extreme gradient boosting (0.696, 0.522), RNN (0.505, 0.301), RNN w/A (0.637, 0.471), and convolutional neural network (0.714, 0.460). Conclusions: HAN-based DL models show promise in information abstraction tasks within unstructured clinical pathology reports.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/gao et al_2018_hierarchical attention networks for information extraction from cancer pathology reports.pdf},
  journal = {Journal of the American Medical Informatics Association},
  keywords = {attention,heirarchical},
  language = {en},
  number = {3}
}

@article{gao19_AdversarialFeatureDistillation,
  title = {An {{Adversarial Feature Distillation Method}} for {{Audio Classification}}},
  author = {Gao, Liang and Mi, Haibo and Zhu, Boqing and Feng, Dawei and Li, Yicong and Peng, Yuxing},
  year = {2019},
  volume = {7},
  pages = {105319--105330},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2931656},
  abstract = {The audio classification task aims to discriminate between different audio signal types. In this task, deep neural networks have achieved better performance than the traditional shallow architecture-based machine-learning method. However, deep neural networks often require huge computational and storage requirements that hinder the deployment in embedded devices. In this paper, we proposed a distillation method which transfers knowledge from well-trained networks to a small network, and the method can compress model size while improving audio classification precision. The contributions of the proposed method are two folds: a multi-level feature distillation method was proposed and an adversarial learning strategy was employed to improve the knowledge transfer. The extensive experiments are conducted on three audio classification tasks, audio scene classification, general audio tagging, and speech command recognition. The experimental results demonstrate that: the small network can provide better performance while achieves the calculated amount of floating-point operations per second (FLOPS) compression ratio of 76:1 and parameters compression ratio of 3:1.},
  file = {/home/trung/GoogleDrive/Zotero/gao et al_2019_an adversarial feature distillation method for audio classification.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{gao19_FlowContrastiveEstimation,
  title = {Flow {{Contrastive Estimation}} of {{Energy}}-{{Based Models}}},
  author = {Gao, Ruiqi and Nijkamp, Erik and Kingma, Diederik P. and Xu, Zhen and Dai, Andrew M. and Wu, Ying Nian},
  year = {2019},
  month = dec,
  abstract = {This paper studies a training method to jointly estimate an energy-based model and a flow-based model, in which the two models are iteratively updated based on a shared adversarial value function. This joint training method has the following traits. (1) The update of the energy-based model is based on noise contrastive estimation, with the flow model serving as a strong noise distribution. (2) The update of the flow model approximately minimizes the Jensen-Shannon divergence between the flow model and the data distribution. (3) Unlike generative adversarial networks (GAN) which estimates an implicit probability distribution defined by a generator model, our method estimates two explicit probabilistic distributions on the data. Using the proposed method we demonstrate a significant improvement on the synthesis quality of the flow model, and show the effectiveness of unsupervised feature learning by the learned energy-based model. Furthermore, the proposed training method can be easily adapted to semi-supervised learning. We achieve competitive results to the state-of-the-art semi-supervised learning methods.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.00589},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gao et al_2019_flow contrastive estimation of energy-based models.pdf;/home/trung/Zotero/storage/LS3J7HPY/1912.html},
  journal = {arXiv:1912.00589 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gao20_FreeEnergyPrincipleRepresentation,
  title = {A {{Free}}-{{Energy Principle}} for {{Representation Learning}}},
  author = {Gao, Yansong and Chaudhari, Pratik},
  year = {2020},
  month = feb,
  abstract = {This paper employs a formal connection of machine learning with thermodynamics to characterize the quality of learnt representations for transfer learning. We discuss how informationtheoretic functionals such as rate, distortion and classification loss of a model lie on a convex, so-called equilibrium surface. We prescribe dynamical processes to traverse this surface under constraints, e.g., an iso-classification process that trades off rate and distortion to keep the classification loss unchanged. We demonstrate how this process can be used for transferring representations from a source dataset to a target dataset while keeping the classification loss constant. Experimental validation of the theoretical results is provided on standard image-classification datasets.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2002.12406},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/2Z5GWDHI/Gao and Chaudhari - 2020 - A Free-Energy Principle for Representation Learnin.pdf},
  journal = {arXiv:2002.12406 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{garbacea19_LowBitrateSpeech,
  title = {Low {{Bit}}-Rate {{Speech Coding}} with {{VQ}}-{{VAE}} and a {{WaveNet Decoder}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Garbacea, Cristina and van {den Oord}, Aaron and Li, Yazhe and Lim, Felicia S C and Luebs, Alejandro and Vinyals, Oriol and Walters, Thomas C},
  year = {2019},
  month = may,
  pages = {735--739},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683277},
  abstract = {In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosodytransparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.},
  file = {/home/trung/GoogleDrive/Zotero/garbacea et al_2019_low bit-rate speech coding with vq-vae and a wavenet decoder.pdf},
  isbn = {978-1-4799-8131-1},
  language = {en}
}

@inproceedings{garbacea19_LowBitrateSpeecha,
  title = {Low {{Bit}}-Rate {{Speech Coding}} with {{VQ}}-{{VAE}} and a {{WaveNet Decoder}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {G{\^a}rbacea, Cristina and van {den Oord}, A{\"a}ron and Li, Yazhe and Lim, Felicia S C and Luebs, Alejandro and Vinyals, Oriol and Walters, Thomas C},
  year = {2019},
  month = may,
  pages = {735--739},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.2019.8683277},
  abstract = {In order to efficiently transmit and store speech signals, speech codecs create a minimally redundant representation of the input signal which is then decoded at the receiver with the best possible perceptual quality. In this work we demonstrate that a neural network architecture based on VQ-VAE with a WaveNet decoder can be used to perform very low bit-rate speech coding with high reconstruction quality. A prosody-transparent and speaker-independent model trained on the LibriSpeech corpus coding audio at 1.6 kbps exhibits perceptual quality which is around halfway between the MELP codec at 2.4 kbps and AMR-WB codec at 23.05 kbps. In addition, when training on high-quality recorded speech with the test speaker included in the training set, a model coding speech at 1.6 kbps produces output of similar perceptual quality to that generated by AMR-WB at 23.05 kbps.},
  annotation = {ZSCC: 0000005},
  file = {/home/trung/GoogleDrive/Zotero/gârbacea et al_2019_low bit-rate speech coding with vq-vae and a wavenet decoder.pdf;/home/trung/Zotero/storage/EL5M4IPX/8683277.html}
}

@article{garcez20_NeurosymbolicAI3rd,
  title = {Neurosymbolic {{AI}}: {{The}} 3rd {{Wave}}},
  shorttitle = {Neurosymbolic {{AI}}},
  author = {d'Avila Garcez, Artur and Lamb, Luis C.},
  year = {2020},
  month = dec,
  abstract = {Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.},
  archivePrefix = {arXiv},
  eprint = {2012.05876},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/garcez et al_2020_neurosymbolic ai.pdf},
  journal = {arXiv:2012.05876 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{garcia-romero19_xVectorDNNRefinement,
  title = {X-{{Vector DNN Refinement}} with {{Full}}-{{Length Recordings}} for {{Speaker Recognition}}},
  booktitle = {Interspeech 2019},
  author = {{Garcia-Romero}, Daniel and Snyder, David and Sell, Gregory and McCree, Alan and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2019},
  month = sep,
  pages = {1493--1496},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2205},
  abstract = {State-of-the-art text-independent speaker recognition systems for long recordings (a few minutes) are based on deep neural network (DNN) speaker embeddings. Current implementations of this paradigm use short speech segments (a few seconds) to train the DNN. This introduces a mismatch between training and inference when extracting embeddings for long duration recordings. To address this, we present a DNN refinement approach that updates a subset of the DNN parameters with full recordings to reduce this mismatch. At the same time, we also modify the DNN architecture to produce embeddings optimized for cosine distance scoring. This is accomplished using a largemargin strategy with angular softmax. Experimental validation shows that our approach is capable of producing embeddings that achieve record performance on the SITW benchmark.},
  file = {/home/trung/GoogleDrive/Zotero/garcia-romero et al_2019_x-vector dnn refinement with full-length recordings for speaker recognition.pdf},
  language = {en}
}

@article{garg20_GeneralizationRepresentationalLimits,
  title = {Generalization and {{Representational Limits}} of {{Graph Neural Networks}}},
  author = {Garg, Vikas K. and Jegelka, Stefanie and Jaakkola, Tommi},
  year = {2020},
  month = feb,
  abstract = {We address two fundamental questions about graph neural networks (GNNs). First, we prove that several important graph properties cannot be computed by GNNs that rely entirely on local information. Such GNNs include the standard message passing models, and more powerful spatial variants that exploit local graph structure (e.g., via relative orientation of messages, or local port ordering) to distinguish neighbors of each node. Our treatment includes a novel graph-theoretic formalism. Second, we provide the first data dependent generalization bounds for message passing GNNs. This analysis explicitly accounts for the local permutation invariance of GNNs. Our bounds are much tighter than existing VC-dimension based guarantees for GNNs, and are comparable to Rademacher bounds for recurrent neural networks.},
  archivePrefix = {arXiv},
  eprint = {2002.06157},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/garg et al_2020_generalization and representational limits of graph neural networks.pdf},
  journal = {arXiv:2002.06157 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{garnelo16_DeepSymbolicReinforcement,
  title = {Towards {{Deep Symbolic Reinforcement Learning}}},
  author = {Garnelo, Marta and Arulkumaran, Kai and Shanahan, Murray},
  year = {2016},
  month = oct,
  abstract = {Deep reinforcement learning (DRL) brings the power of deep neural networks to bear on the generic task of trial-and-error learning, and its effectiveness has been convincingly demonstrated on tasks such as Atari video games and the game of Go. However, contemporary DRL systems inherit a number of shortcomings from the current generation of deep learning techniques. For example, they require very large datasets to work effectively, entailing that they are slow to learn even when such datasets are available. Moreover, they lack the ability to reason on an abstract level, which makes it difficult to implement high-level cognitive functions such as transfer learning, analogical reasoning, and hypothesis-based reasoning. Finally, their operation is largely opaque to humans, rendering them unsuitable for domains in which verifiability is important. In this paper, we propose an end-to-end reinforcement learning architecture comprising a neural back end and a symbolic front end with the potential to overcome each of these shortcomings. As proof-of-concept, we present a preliminary implementation of the architecture and apply it to several variants of a simple video game. We show that the resulting system -- though just a prototype -- learns effectively, and, by acquiring a set of symbolic rules that are easily comprehensible to humans, dramatically outperforms a conventional, fully neural DRL system on a stochastic variant of the game.},
  archivePrefix = {arXiv},
  eprint = {1609.05518},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/garnelo et al_2016_towards deep symbolic reinforcement learning.pdf;/home/trung/Zotero/storage/GG2W6UCV/1609.html},
  journal = {arXiv:1609.05518 [cs]},
  keywords = {causal,compositional,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,symbolic},
  primaryClass = {cs}
}

@article{garnelo18_NeuralProcesses,
  title = {Neural {{Processes}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  year = {2018},
  month = jul,
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archivePrefix = {arXiv},
  eprint = {1807.01622},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/garnelo et al_2018_neural processes.pdf;/home/trung/Zotero/storage/3C68SAWQ/1807.html},
  journal = {arXiv:1807.01622 [cs, stat]},
  keywords = {Computer Science - Machine Learning,gaussian process,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{garriga-alonso18_DeepConvolutionalNetworks,
  title = {Deep {{Convolutional Networks}} as Shallow {{Gaussian Processes}}},
  author = {{Garriga-Alonso}, Adri{\`a} and Rasmussen, Carl Edward and Aitchison, Laurence},
  year = {2018},
  month = aug,
  abstract = {We show that the output of a (residual) convolutional neural network (CNN) with an appropriate prior over the weights and biases is a Gaussian process (GP) in the limit of infinitely many convolutional filters, extending similar results for dense networks. For a CNN, the equivalent kernel can be computed exactly and, unlike "deep kernels", has very few parameters: only the hyperparameters of the original CNN. Further, we show that this kernel has two properties that allow it to be computed efficiently; the cost of evaluating the kernel for a pair of images is similar to a single forward pass through the original CNN with only one filter per layer. The kernel equivalent to a 32-layer ResNet obtains 0.84\% classification error on MNIST, a new record for GPs with a comparable number of parameters.},
  archivePrefix = {arXiv},
  eprint = {1808.05587},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/garriga-alonso et al_2018_deep convolutional networks as shallow gaussian processes.pdf;/home/trung/Zotero/storage/U4UISJXE/1808.html},
  journal = {arXiv:1808.05587 [cs, stat]},
  keywords = {Computer Science - Machine Learning,gaussian process,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{gast18_LightweightProbabilisticDeep,
  title = {Lightweight {{Probabilistic Deep Networks}}},
  author = {Gast, Jochen and Roth, Stefan},
  year = {2018},
  month = may,
  abstract = {Even though probabilistic treatments of neural networks have a long history, they have not found widespread use in practice. Sampling approaches are often too slow already for simple networks. The size of the inputs and the depth of typical CNN architectures in computer vision only compound this problem. Uncertainty in neural networks has thus been largely ignored in practice, despite the fact that it may provide important information about the reliability of predictions and the inner workings of the network. In this paper, we introduce two lightweight approaches to making supervised learning with probabilistic deep networks practical: First, we suggest probabilistic output layers for classification and regression that require only minimal changes to existing networks. Second, we employ assumed density filtering and show that activation uncertainties can be propagated in a practical fashion through the entire network, again with minor changes. Both probabilistic networks retain the predictive power of the deterministic counterpart, but yield uncertainties that correlate well with the empirical error induced by their predictions. Moreover, the robustness to adversarial examples is significantly increased.},
  archivePrefix = {arXiv},
  eprint = {1805.11327},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gast et al_2018_lightweight probabilistic deep networks.pdf;/home/trung/Zotero/storage/H4VWEPGH/1805.html},
  journal = {arXiv:1805.11327 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{gatopoulos20_SelfSupervisedVariationalAutoEncoders,
  title = {Self-{{Supervised Variational Auto}}-{{Encoders}}},
  author = {Gatopoulos, Ioannis and Tomczak, Jakub M.},
  year = {2020},
  month = oct,
  abstract = {Density estimation, compression and data generation are crucial tasks in artificial intelligence. Variational Auto-Encoders (VAEs) constitute a single framework to achieve these goals. Here, we present a novel class of generative models, called self-supervised Variational Auto-Encoder (selfVAE), that utilizes deterministic and discrete variational posteriors. This class of models allows to perform both conditional and unconditional sampling, while simplifying the objective function. First, we use a single self-supervised transformation as a latent variable, where a transformation is either downscaling or edge detection. Next, we consider a hierarchical architecture, i.e., multiple transformations, and we show its benefits compared to the VAE. The flexibility of selfVAE in data reconstruction finds a particularly interesting use case in data compression tasks, where we can trade-off memory for better data quality, and vice-versa. We present performance of our approach on three benchmark image data (Cifar10, Imagenette64, and CelebA).},
  archivePrefix = {arXiv},
  eprint = {2010.02014},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gatopoulos et al_2020_self-supervised variational auto-encoders.pdf},
  journal = {arXiv:2010.02014 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gattinoni20_Covid19DoesNot,
  title = {Covid-19 {{Does Not Lead}} to a ``{{Typical}}'' {{Acute Respiratory Distress Syndrome}}},
  author = {Gattinoni, Luciano and Coppola, Silvia and Cressoni, Massimo and Busana, Mattia and Rossi, Sandra and Chiumello, Davide},
  year = {2020},
  month = mar,
  pages = {rccm.202003-0817LE},
  issn = {1073-449X, 1535-4970},
  doi = {10.1164/rccm.202003-0817LE},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/gattinoni et al_2020_covid-19 does not lead to a “typical” acute respiratory distress syndrome.pdf},
  journal = {American Journal of Respiratory and Critical Care Medicine},
  language = {en}
}

@article{gatys15_NeuralAlgorithmArtistic,
  title = {A {{Neural Algorithm}} of {{Artistic Style}}},
  author = {Gatys, Leon A. and Ecker, Alexander S. and Bethge, Matthias},
  year = {2015},
  month = sep,
  abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the content and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. However, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks. Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to separate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the striking similarities between performance-optimised artificial neural networks and biological vision, our work offers a path forward to an algorithmic understanding of how humans create and perceive artistic imagery.},
  archivePrefix = {arXiv},
  eprint = {1508.06576},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gatys et al_2015_a neural algorithm of artistic style.pdf},
  journal = {arXiv:1508.06576 [cs, q-bio]},
  primaryClass = {cs, q-bio}
}

@techreport{gayoso19_TotalVIJointModel,
  title = {{{TotalVI}}: {{A Joint Model}} of {{RNA Expression}} and {{Surface Protein Abundance}} in {{Single Cells}}},
  author = {Gayoso, Adam and Lopez, Romain and Steier, Zo{\"e} and Regier, Jeffrey and Streets, Aaron and Yosef, Nir},
  year = {2019},
  month = oct,
  institution = {{Bioinformatics}},
  doi = {10.1101/791947},
  abstract = {Cellular indexing of transcriptomes and epitopes by sequencing (CITE-seq) combines unbiased single-cell transcriptome measurements with surface protein quantification comparable to flow cytometry, the gold standard for cell type identification. However, current analysis pipelines cannot address the two primary challenges of CITE-seq data: combining both modalities in a shared latent space that harnesses the power of the paired measurements, and handling the technical artifacts of the protein measurement, which is obscured by non-negligible background noise. Here we present Total Variational Inference (totalVI), a fully probabilistic end-to-end framework for normalizing and analyzing CITE-seq data, based on a hierarchical Bayesian model. In totalVI, the mRNA and protein measurements for each cell are generated from a low-dimensional latent random variable unique to that cell, representing its cellular state. totalVI uses deep neural networks to specify conditional distributions. By leveraging advances in stochastic variational inference, it scales easily to millions of cells. Explicit modeling of nuisance factors enables totalVI to produce denoised data in both domains, as well as a batch-corrected latent representation of cells for downstream analysis tasks.},
  file = {/home/trung/GoogleDrive/Zotero/gayoso et al_2019_totalvi.pdf},
  language = {en},
  type = {Preprint}
}

@inproceedings{gburrek19_UnsupervisedLearningDisentangled,
  title = {Unsupervised {{Learning}} of a {{Disentangled Speech Representation}} for {{Voice Conversion}}},
  booktitle = {10th {{ISCA Speech Synthesis Workshop}}},
  author = {Gburrek, Tobias and Glarner, Thomas and Ebbers, Janek and {Haeb-Umbach}, Reinhold and Wagner, Petra},
  year = {2019},
  month = sep,
  pages = {81--86},
  publisher = {{ISCA}},
  doi = {10.21437/SSW.2019-15},
  abstract = {This paper presents an approach to voice conversion, which does neither require parallel data nor speaker or phone labels for training. It can convert between speakers which are not in the training set by employing the previously proposed concept of a factorized hierarchical variational autoencoder. Here, linguistic and speaker induced variations are separated upon the notion that content induced variations change at a much shorter time scale, i.e., at the segment level, than speaker induced variations, which vary at the longer utterance level. In this contribution we propose to employ convolutional instead of recurrent network layers in the encoder and decoder blocks, which is shown to achieve better phone recognition accuracy on the latent segment variables at frame-level due to their better temporal resolution. For voice conversion the mean of the utterance variables is replaced with the respective estimated mean of the target speaker. The resulting log-mel spectra of the decoder output are used as local conditions of a WaveNet which is utilized for synthesis of the speech waveforms. Experiments show both good disentanglement properties of the latent space variables, and good voice conversion performance.},
  file = {/home/trung/GoogleDrive/Zotero/gburrek et al_2019_unsupervised learning of a disentangled speech representation for voice conversion.pdf},
  language = {en}
}

@techreport{geddes19_Autoencoderbasedclusterensembles,
  title = {Autoencoder-Based Cluster Ensembles for Single-Cell {{RNA}}-Seq Data Analysis},
  author = {Geddes, Thomas A and Kim, Taiyun and Nan, Lihao and Burchfield, James G and Yang, Jean YH and Tao, Dacheng and Yang, Pengyi},
  year = {2019},
  month = sep,
  institution = {{Bioinformatics}},
  doi = {10.1101/773903},
  abstract = {Background: Single-cell RNA-sequencing (scRNA-seq) is a transformative technology, allowing global transcriptomes of individual cells to be profiled with high accuracy. An essential task in scRNA-seq data analysis is the identification of cell types from complex samples or tissues profiled in an experiment. To this end, clustering has become a key computational technique for grouping cells based on their transcriptome profiles, enabling subsequent cell type identification from each cluster of cells. Due to the high feature-dimensionality of the transcriptome (i.e. the large number of measured genes in each cell) and because only a small fraction of genes are cell type-specific and therefore informative for generating cell type-specific clusters, clustering directly on the original feature/gene dimension may lead to uninformative clusters and hinder correct cell type identification. Results: Here, we propose an autoencoder-based cluster ensemble framework in which we first take random subspace projections from the data, then compress each random projection to a low-dimensional space using an autoencoder artificial neural network, and finally apply ensemble clustering across all encoded datasets for generating clusters of cells. We employ four evaluation metrics to benchmark clustering performance and our experiments demonstrate that the proposed autoencoder-based cluster ensemble can lead to substantially improved cell type-specific clusters when applied with both the standard k-means clustering algorithm and a state-of-the-art kernel-based clustering algorithm (SIMLR) designed specifically for scRNA-seq data. Compared to directly using these clustering algorithms on the original datasets, the performance improvement in some cases is up to 100\%, depending on the evaluation metrics used. Conclusions: Our results suggest that the proposed framework can facilitate more accurate cell type identification as well as other downstream analyses. The code for creating the proposed autoencoder-based cluster ensemble framework is freely available from https://github.com/gedcom/autoencoder\_cluster\_ensemble},
  file = {/home/trung/GoogleDrive/Zotero/geddes et al_2019_autoencoder-based cluster ensembles for single-cell rna-seq data analysis.pdf},
  language = {en},
  type = {Preprint}
}

@article{gehring17_ConvolutionalSequenceSequence,
  title = {Convolutional {{Sequence}} to {{Sequence Learning}}},
  author = {Gehring, Jonas and Auli, Michael and Grangier, David and Yarats, Denis and Dauphin, Yann N.},
  year = {2017},
  month = jul,
  abstract = {The prevalent approach to sequence to sequence learning maps an input sequence to a variable length output sequence via recurrent neural networks. We introduce an architecture based entirely on convolutional neural networks. Compared to recurrent models, computations over all elements can be fully parallelized during training and optimization is easier since the number of non-linearities is fixed and independent of the input length. Our use of gated linear units eases gradient propagation and we equip each decoder layer with a separate attention module. We outperform the accuracy of the deep LSTM setup of Wu et al. (2016) on both WMT'14 English-German and WMT'14 English-French translation at an order of magnitude faster speed, both on GPU and CPU.},
  annotation = {ZSCC: 0000918},
  archivePrefix = {arXiv},
  eprint = {1705.03122},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gehring et al_2017_convolutional sequence to sequence learning.pdf;/home/trung/Zotero/storage/TVUEBCXZ/1705.html},
  journal = {arXiv:1705.03122 [cs]},
  keywords = {Computer Science - Computation and Language,position embedding},
  primaryClass = {cs}
}

@article{geiger20_InformationPlaneAnalyses,
  title = {On {{Information Plane Analyses}} of {{Neural Network Classifiers}} -- {{A Review}}},
  author = {Geiger, Bernhard C.},
  year = {2020},
  month = aug,
  abstract = {We review the current literature concerned with information plane analyses of neural network classifiers. While the underlying information bottleneck theory and the claim that information-theoretic compression is causally linked to generalization are plausible, empirical evidence was found to be both supporting and conflicting. We review this evidence together with a detailed analysis of how the respective information quantities were estimated. Our survey suggests that compression visualized in information planes is not necessarily information-theoretic, but is rather often compatible with geometric compression of the latent representations. This insight gives the information plane a renewed justification. Aside from this, we shed light on the problem of estimating mutual information in deterministic neural networks and its consequences. Specifically, we argue that even in feed-forward neural networks the data processing inequality need not hold for estimates of mutual information. Similarly, while a fitting phase, in which the mutual information between the latent representation and the target increases, is necessary (but not sufficient) for good classification performance, depending on the specifics of mutual information estimation such a fitting phase need not be visible in the information plane.},
  archivePrefix = {arXiv},
  eprint = {2003.09671},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/geiger_2020_on information plane analyses of neural network classifiers -- a review.pdf},
  journal = {arXiv:2003.09671 [cs, math, stat]},
  keywords = {information},
  primaryClass = {cs, math, stat}
}

@article{geirhos18_Generalisationhumansdeep,
  title = {Generalisation in Humans and Deep Neural Networks},
  author = {Geirhos, Robert and Temme, Carlos R. Medina and Rauber, Jonas and Sch{\"u}tt, Heiko H. and Bethge, Matthias and Wichmann, Felix A.},
  year = {2018},
  month = aug,
  abstract = {We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.},
  annotation = {ZSCC: 0000042},
  archivePrefix = {arXiv},
  eprint = {1808.08750},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/geirhos et al_2018_generalisation in humans and deep neural networks.pdf;/home/trung/Zotero/storage/BH56A73E/1808.html},
  journal = {arXiv:1808.08750 [cs, q-bio, stat]},
  keywords = {cognition,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,explain,generalization,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  primaryClass = {cs, q-bio, stat}
}

@article{geirhos20_ShortcutLearningDeep,
  title = {Shortcut {{Learning}} in {{Deep Neural Networks}}},
  author = {Geirhos, Robert and Jacobsen, J{\"o}rn-Henrik and Michaelis, Claudio and Zemel, Richard and Brendel, Wieland and Bethge, Matthias and Wichmann, Felix A.},
  year = {2020},
  month = may,
  abstract = {Deep learning has triggered the current rise of artificial intelligence and is the workhorse of today's machine intelligence. Numerous success stories have rapidly spread all over science, industry and society, but its limitations have only recently come into focus. In this perspective we seek to distil how many of deep learning's problem can be seen as different symptoms of the same underlying problem: shortcut learning. Shortcuts are decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios. Related issues are known in Comparative Psychology, Education and Linguistics, suggesting that shortcut learning may be a common characteristic of learning systems, biological and artificial alike. Based on these observations, we develop a set of recommendations for model interpretation and benchmarking, highlighting recent advances in machine learning to improve robustness and transferability from the lab to real-world applications.},
  archivePrefix = {arXiv},
  eprint = {2004.07780},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/geirhos et al_2020_shortcut learning in deep neural networks.pdf},
  journal = {arXiv:2004.07780 [cs, q-bio]},
  keywords = {_tablet,favorite,representation},
  primaryClass = {cs, q-bio}
}

@article{gelman00_TeachingStatisticsBag,
  title = {Teaching {{Statistics}}: {{A Bag}} of {{Tricks}}},
  author = {Gelman, Andrew},
  pages = {112},
  file = {/home/trung/GoogleDrive/Zotero/gelman_teaching statistics.pdf},
  language = {en}
}

@article{genewein20_AlgorithmsCausalReasoning,
  title = {Algorithms for {{Causal Reasoning}} in {{Probability Trees}}},
  author = {Genewein, Tim and McGrath, Tom and D{\'e}letang, Gr{\'e}goire and Mikulik, Vladimir and Martic, Miljan and Legg, Shane and Ortega, Pedro A.},
  year = {2020},
  month = nov,
  abstract = {Probability trees are one of the simplest models of causal generative processes. They possess clean semantics and -- unlike causal Bayesian networks -- they can represent context-specific causal dependencies, which are necessary for e.g. causal induction. Yet, they have received little attention from the AI and ML community. Here we present concrete algorithms for causal reasoning in discrete probability trees that cover the entire causal hierarchy (association, intervention, and counterfactuals), and operate on arbitrary propositional and causal events. Our work expands the domain of causal reasoning to a very general class of discrete stochastic processes.},
  archivePrefix = {arXiv},
  eprint = {2010.12237},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/genewein et al_2020_algorithms for causal reasoning in probability trees.pdf},
  journal = {arXiv:2010.12237 [cs]},
  keywords = {causal,favorite},
  primaryClass = {cs}
}

@article{gerald20_NodeEmbeddingCommunity,
  title = {From {{Node Embedding To Community Embedding}} : {{A Hyperbolic Approach}}},
  shorttitle = {From {{Node Embedding To Community Embedding}}},
  author = {Gerald, Thomas and Zaatiti, Hadi and Hajri, Hatem and Baskiotis, Nicolas and Schwander, Olivier},
  year = {2020},
  month = mar,
  abstract = {Detecting communities on graphs has received significant interest in recent literature. Current state-of-the-art community embedding approach called \textbackslash textit\{ComE\} tackles this problem by coupling graph embedding with community detection. Considering the success of hyperbolic representations of graph-structured data in last years, an ongoing challenge is to set up a hyperbolic approach for the community detection problem. The present paper meets this challenge by introducing a Riemannian equivalent of \textbackslash textit\{ComE\}. Our proposed approach combines hyperbolic embeddings with Riemannian K-means or Riemannian mixture models to perform community detection. We illustrate the usefulness of this framework through several experiments on real-world social networks and comparisons with \textbackslash textit\{ComE\} and recent hyperbolic-based classification approaches.},
  archivePrefix = {arXiv},
  eprint = {1907.01662},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gerald et al_2020_from node embedding to community embedding.pdf},
  journal = {arXiv:1907.01662 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{germain15_MADEMaskedAutoencoder,
  title = {{{MADE}}: {{Masked Autoencoder}} for {{Distribution Estimation}}},
  shorttitle = {{{MADE}}},
  author = {Germain, Mathieu and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2015},
  month = jun,
  abstract = {There has been a lot of recent interest in designing neural network models to estimate a distribution from a set of examples. We introduce a simple modification for autoencoder neural networks that yields powerful generative models. Our method masks the autoencoder's parameters to respect autoregressive constraints: each input is reconstructed only from previous inputs in a given ordering. Constrained this way, the autoencoder outputs can be interpreted as a set of conditional probabilities, and their product, the full joint probability. We can also train a single network that can decompose the joint probability in multiple different orderings. Our simple framework can be applied to multiple architectures, including deep ones. Vectorized implementations, such as on GPUs, are simple and fast. Experiments demonstrate that this approach is competitive with state-of-the-art tractable distribution estimators. At test time, the method is significantly faster and scales better than other autoregressive estimators.},
  archivePrefix = {arXiv},
  eprint = {1502.03509},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/germain et al_2015_made.pdf},
  journal = {arXiv:1502.03509 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{ghasedidizaji18_SemiSupervisedGenerativeAdversarial,
  title = {Semi-{{Supervised Generative Adversarial Network}} for {{Gene Expression Inference}}},
  booktitle = {Proceedings of the 24th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}},
  author = {Ghasedi Dizaji, Kamran and Wang, Xiaoqian and Huang, Heng},
  year = {2018},
  month = jul,
  pages = {1435--1444},
  publisher = {{ACM}},
  address = {{London United Kingdom}},
  doi = {10.1145/3219819.3220114},
  abstract = {Gene expression profiling provides comprehensive characterization of cellular states under different experimental conditions, thus contributes to the prosperity of many fields of biomedical research. Although the rapid development of gene expression profiling has been observed, genome-wide profiling of large libraries is still expensive and difficult. Due to the fact that there are significant correlations between gene expression patterns, previous studies introduced regression models for predicting the target gene expressions from the landmark gene profiles. These models formulate the gene expression inference in a completely supervised manner, which require a large labeled dataset (i.e. paired landmark and target gene expressions). However, collecting the whole gene expressions is much more expensive than the landmark genes. In order to address this issue and take advantage of cheap unlabeled data (i.e. landmark genes), we propose a novel semi-supervised deep generative model for target gene expression inference. Our model is based on the generative adversarial network (GAN) to approximate the joint distribution of landmark and target genes, and an inference network to learn the conditional distribution of target genes given the landmark genes. We employ the reliable generated data by our GAN model as the extra training pairs to improve the training of our inference model, and utilize the trustworthy predictions of the inference network to enhance the adversarial training of our GAN network. We evaluate our model on the prediction of two types of gene expression data and identify obvious advantage over the counterparts.},
  annotation = {ZSCC: 0000011},
  file = {/home/trung/GoogleDrive/Zotero/ghasedi dizaji et al_2018_semi-supervised generative adversarial network for gene expression inference.pdf},
  isbn = {978-1-4503-5552-0},
  language = {en}
}

@article{ghazvininejad20_SemiAutoregressiveTrainingImproves,
  title = {Semi-{{Autoregressive Training Improves Mask}}-{{Predict Decoding}}},
  author = {Ghazvininejad, Marjan and Levy, Omer and Zettlemoyer, Luke},
  year = {2020},
  month = jan,
  abstract = {The recently proposed mask-predict decoding algorithm has narrowed the performance gap between semi-autoregressive machine translation models and the traditional left-toright approach. We introduce a new training method for conditional masked language models, SMART, which mimics the semiautoregressive behavior of mask-predict, producing training examples that contain model predictions as part of their inputs. Models trained with SMART produce higher-quality translations when using mask-predict decoding, effectively closing the remaining performance gap with fully autoregressive models.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.08785},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/6DA4LYG4/Ghazvininejad et al. - 2020 - Semi-Autoregressive Training Improves Mask-Predict.pdf},
  journal = {arXiv:2001.08785 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ghiasi18_DropBlockregularizationmethod,
  title = {{{DropBlock}}: {{A}} Regularization Method for Convolutional Networks},
  shorttitle = {{{DropBlock}}},
  author = {Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V.},
  year = {2018},
  month = oct,
  abstract = {Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78.13\% accuracy, which is more than 1.6\% improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from 36.8\% to 38.4\%.},
  annotation = {ZSCC: 0000125},
  archivePrefix = {arXiv},
  eprint = {1810.12890},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/S9YF37SP/Ghiasi et al. - 2018 - DropBlock A regularization method for convolution.pdf},
  journal = {arXiv:1810.12890 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{ghojogh20_StochasticNeighborEmbedding,
  title = {Stochastic {{Neighbor Embedding}} with {{Gaussian}} and {{Student}}-t {{Distributions}}: {{Tutorial}} and {{Survey}}},
  shorttitle = {Stochastic {{Neighbor Embedding}} with {{Gaussian}} and {{Student}}-t {{Distributions}}},
  author = {Ghojogh, Benyamin and Ghodsi, Ali and Karray, Fakhri and Crowley, Mark},
  year = {2020},
  month = sep,
  abstract = {Stochastic Neighbor Embedding (SNE) is a manifold learning and dimensionality reduction method with a probabilistic approach. In SNE, every point is consider to be the neighbor of all other points with some probability and this probability is tried to be preserved in the embedding space. SNE considers Gaussian distribution for the probability in both the input and embedding spaces. However, t-SNE uses the Student-t and Gaussian distributions in these spaces, respectively. In this tutorial and survey paper, we explain SNE, symmetric SNE, t-SNE (or Cauchy-SNE), and t-SNE with general degrees of freedom. We also cover the out-of-sample extension and acceleration for these methods. Some simulations to visualize the embeddings are also provided.},
  archivePrefix = {arXiv},
  eprint = {2009.10301},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ghojogh et al_2020_stochastic neighbor embedding with gaussian and student-t distributions.pdf},
  journal = {arXiv:2009.10301 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ghorbani20_NeuronShapleyDiscovering,
  title = {Neuron {{Shapley}}: {{Discovering}} the {{Responsible Neurons}}},
  shorttitle = {Neuron {{Shapley}}},
  author = {Ghorbani, Amirata and Zou, James},
  year = {2020},
  month = nov,
  abstract = {We develop Neuron Shapley as a new framework to quantify the contribution of individual neurons to the prediction and performance of a deep network. By accounting for interactions across neurons, Neuron Shapley is more effective in identifying important filters compared to common approaches based on activation patterns. Interestingly, removing just 30 filters with the highest Shapley scores effectively destroys the prediction accuracy of Inception-v3 on ImageNet. Visualization of these few critical filters provides insights into how the network functions. Neuron Shapley is a flexible framework and can be applied to identify responsible neurons in many tasks. We illustrate additional applications of identifying filters that are responsible for biased prediction in facial recognition and filters that are vulnerable to adversarial attacks. Removing these filters is a quick way to repair models. Enabling all these applications is a new multi-arm bandit algorithm that we developed to efficiently estimate Neuron Shapley values.},
  archivePrefix = {arXiv},
  eprint = {2002.09815},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ghorbani et al_2020_neuron shapley.pdf},
  journal = {arXiv:2002.09815 [cs, stat]},
  keywords = {representation},
  primaryClass = {cs, stat}
}

@article{ghorbani20_Probabilisticcharactermotion,
  title = {Probabilistic Character Motion Synthesis Using a Hierarchical Deep Latent Variable Model},
  author = {Ghorbani, Saeed and Wloka, Calden and Etemad, Ali and Brubaker, Marcus A. and Troje, Nikolaus F.},
  year = {2020},
  publisher = {{The Eurographics Association and John Wiley \& Sons Ltd.}},
  issn = {1467-8659},
  doi = {10.1111/cgf.14116},
  file = {/home/trung/GoogleDrive/Zotero/ghorbani et al_2020_probabilistic character motion synthesis using a hierarchical deep latent variable model.pdf},
  journal = {Computer Graphics Forum}
}

@article{ghose20_Batchnormentropic,
  title = {Batch Norm with Entropic Regularization Turns Deterministic Autoencoders into Generative Models},
  author = {Ghose, Amur and Rashwan, Abdullah and Poupart, Pascal},
  year = {2020},
  month = feb,
  abstract = {The variational autoencoder is a well defined deep generative model that utilizes an encoder-decoder framework where an encoding neural network outputs a non-deterministic code for reconstructing an input. The encoder achieves this by sampling from a distribution for every input, instead of outputting a deterministic code per input. The great advantage of this process is that it allows the use of the network as a generative model for sampling from the data distribution beyond provided samples for training. We show in this work that utilizing batch normalization as a source for non-determinism suffices to turn deterministic autoencoders into generative models on par with variational ones, so long as we add a suitable entropic regularization to the training objective.},
  archivePrefix = {arXiv},
  eprint = {2002.10631},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ghose et al_2020_batch norm with entropic regularization turns deterministic autoencoders into generative models.pdf},
  journal = {arXiv:2002.10631 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ghosh19_VariationalDeterministicAutoencoders,
  title = {From {{Variational}} to {{Deterministic Autoencoders}}},
  author = {Ghosh, Partha and Sajjadi, Mehdi S. M. and Vergari, Antonio and Black, Michael and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = oct,
  abstract = {Variational Autoencoders (VAEs) provide a theoretically-backed and popular framework for deep generative models. However, learning a VAE from data poses still unanswered theoretical questions and considerable practical challenges. In this work, we propose an alternative framework for generative modeling that is simpler, easier to train, and deterministic, yet has many of the advantages of the VAE. We observe that sampling a stochastic encoder in a Gaussian VAE can be interpreted as simply injecting noise into the input of a deterministic decoder. We investigate how substituting this kind of stochasticity, with other explicit and implicit regularization schemes, can lead to an equally smooth and meaningful latent space without having to force it to conform to an arbitrarily chosen prior. To retrieve a generative mechanism to sample new data points, we introduce an ex-post density estimation step that can be readily applied to the proposed framework as well as existing VAEs, improving their sample quality. We show, in a rigorous empirical study, that the proposed regularized deterministic autoencoders are able to generate samples that are comparable to, or better than, those of VAEs and more powerful alternatives when applied to images as well as to structured data such as molecules.},
  annotation = {ZSCC: 0000010},
  archivePrefix = {arXiv},
  eprint = {1903.12436},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ghosh et al_2019_from variational to deterministic autoencoders.pdf},
  journal = {arXiv:1903.12436 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ghosh20_VariationalDeterministicAutoencoders,
  title = {From {{Variational}} to {{Deterministic Autoencoders}}},
  author = {Ghosh, Partha and Sajjadi, Mehdi S M and Vergari, Antonio and Scholkopf, Bernhard},
  year = {2020},
  pages = {25},
  file = {/home/trung/GoogleDrive/Zotero/ghosh et al_2020_from variational to deterministic autoencoders.pdf},
  language = {en}
}

@inproceedings{ghosh20_variationaldeterministicautoencoders,
  title = {From Variational to Deterministic Autoencoders},
  booktitle = {International Conference on Learning Representations},
  author = {Ghosh, Partha and Sajjadi, Mehdi S. M. and Vergari, Antonio and Black, Michael and Scholkopf, Bernhard},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/ghosh et al_2020_from variational to deterministic autoencoders2.pdf},
  keywords = {disentanglement}
}

@misc{giacaglia19_HowTransformersWork,
  title = {How {{Transformers Work}}},
  author = {Giacaglia, Giuliano},
  year = {2019},
  month = apr,
  abstract = {Transformers are a type of neural network architecture that have been gaining popularity. Transformers were recently used by OpenAI in\ldots},
  file = {/home/trung/Zotero/storage/2T4ZHPML/transformers-141e32e69591.html},
  howpublished = {https://towardsdatascience.com/transformers-141e32e69591},
  journal = {Medium},
  language = {en}
}

@article{giannone19_NoRepresentationTransformation,
  title = {No {{Representation}} without {{Transformation}}},
  author = {Giannone, Giorgio and Masci, Jonathan and Osendorfer, Christian},
  year = {2019},
  month = dec,
  abstract = {We propose to extend Latent Variable Models with a simple idea: learn to encode not only samples but also transformations of such samples. This means that the latent space is not only populated by embeddings but also by higher order objects that map between these embeddings. We show how a hierarchical graphical model can be utilized to enforce desirable algebraic properties of such latent mappings. These mappings in turn structure the latent space and hence can have a core impact on downstream tasks that are solved in the latent space. We demonstrate this impact on a set of experiments and also show that the representation of these latent mappings reflects interpretable properties.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.03845},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/giannone et al_2019_no representation without transformation.pdf},
  journal = {arXiv:1912.03845 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{giannone20_NoRepresentationTransformation,
  title = {No {{Representation}} without {{Transformation}}},
  author = {Giannone, Giorgio and Saremi, Saeed and Masci, Jonathan and Osendorfer, Christian},
  year = {2020},
  month = apr,
  abstract = {We extend the framework of variational autoencoders to represent transformations explicitly in the latent space. In the family of hierarchical graphical models that emerges, the latent space is populated by higher order objects that are inferred jointly with the latent representations they act on. To explicitly demonstrate the effect of these higher order objects, we show that the inferred latent transformations reflect interpretable properties in the observation space. Furthermore, the model is structured in such a way that in the absence of transformations, we can run inference and obtain generative capabilities comparable with standard variational autoencoders. Finally, utilizing the trained encoder, we outperform the baselines by a wide margin on a challenging out-of-distribution classification task.},
  archivePrefix = {arXiv},
  eprint = {1912.03845},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/giannone et al_2020_no representation without transformation.pdf},
  journal = {arXiv:1912.03845 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{giannone20_NoRepresentationTransformationa,
  title = {No {{Representation}} without {{Transformation}}},
  author = {Giannone, Giorgio and Saremi, Saeed and Masci, Jonathan and Osendorfer, Christian},
  year = {2020},
  month = apr,
  abstract = {We extend the framework of variational autoencoders to represent transformations explicitly in the latent space. In the family of hierarchical graphical models that emerges, the latent space is populated by higher order objects that are inferred jointly with the latent representations they act on. To explicitly demonstrate the effect of these higher order objects, we show that the inferred latent transformations reflect interpretable properties in the observation space. Furthermore, the model is structured in such a way that in the absence of transformations, we can run inference and obtain generative capabilities comparable with standard variational autoencoders. Finally, utilizing the trained encoder, we outperform the baselines by a wide margin on a challenging out-of-distribution classification task.},
  archivePrefix = {arXiv},
  eprint = {1912.03845},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/giannone et al_2020_no representation without transformation2.pdf},
  journal = {arXiv:1912.03845 [cs, stat]},
  keywords = {auxiliary,representation,vae_issues},
  primaryClass = {cs, stat}
}

@article{gidaris18_UnsupervisedRepresentationLearning,
  title = {Unsupervised {{Representation Learning}} by {{Predicting Image Rotations}}},
  author = {Gidaris, Spyros and Singh, Praveer and Komodakis, Nikos},
  year = {2018},
  month = mar,
  abstract = {Over the last years, deep convolutional neural networks (ConvNets) have transformed the field of computer vision thanks to their unparalleled capacity to learn high level semantic image features. However, in order to successfully learn those features, they usually require massive amounts of manually labeled data, which is both expensive and impractical to scale. Therefore, unsupervised semantic feature learning, i.e., learning without requiring manual annotation effort, is of crucial importance in order to successfully harvest the vast amount of visual data that are available today. In our work we propose to learn image features by training ConvNets to recognize the 2d rotation that is applied to the image that it gets as input. We demonstrate both qualitatively and quantitatively that this apparently simple task actually provides a very powerful supervisory signal for semantic feature learning. We exhaustively evaluate our method in various unsupervised feature learning benchmarks and we exhibit in all of them state-of-the-art performance. Specifically, our results on those benchmarks demonstrate dramatic improvements w.r.t. prior state-of-the-art approaches in unsupervised representation learning and thus significantly close the gap with supervised feature learning. For instance, in PASCAL VOC 2007 detection task our unsupervised pre-trained AlexNet model achieves the state-of-the-art (among unsupervised methods) mAP of 54.4\% that is only 2.4 points lower from the supervised case. We get similarly striking results when we transfer our unsupervised learned features on various other tasks, such as ImageNet classification, PASCAL classification, PASCAL segmentation, and CIFAR-10 classification. The code and models of our paper will be published on: https://github.com/gidariss/FeatureLearningRotNet.},
  archivePrefix = {arXiv},
  eprint = {1803.07728},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gidaris et al_2018_unsupervised representation learning by predicting image rotations.pdf},
  journal = {arXiv:1803.07728 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{gidel19_ImplicitRegularizationDiscrete,
  title = {Implicit {{Regularization}} of {{Discrete Gradient Dynamics}} in {{Linear Neural Networks}}},
  author = {Gidel, Gauthier and Bach, Francis and {Lacoste-Julien}, Simon},
  year = {2019},
  month = dec,
  abstract = {When optimizing over-parameterized models, such as deep neural networks, a large set of parameters can achieve zero training error. In such cases, the choice of the optimization algorithm and its respective hyper-parameters introduces biases that will lead to convergence to specific minimizers of the objective. Consequently, this choice can be considered as an implicit regularization for the training of over-parametrized models. In this work, we push this idea further by studying the discrete gradient dynamics of the training of a two-layer linear network with the least-squares loss. Using a time rescaling, we show that, with a vanishing initialization and a small enough step size, this dynamics sequentially learns the solutions of a reduced-rank regression with a gradually increasing rank.},
  archivePrefix = {arXiv},
  eprint = {1904.13262},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gidel et al_2019_implicit regularization of discrete gradient dynamics in linear neural networks.pdf},
  journal = {arXiv:1904.13262 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{gilmer17_NeuralMessagePassing,
  title = {Neural {{Message Passing}} for {{Quantum Chemistry}}},
  author = {Gilmer, Justin and Schoenholz, Samuel S. and Riley, Patrick F. and Vinyals, Oriol and Dahl, George E.},
  year = {2017},
  month = jun,
  abstract = {Supervised learning on molecules has incredible potential to be useful in chemistry, drug discovery, and materials science. Luckily, several promising and closely related neural network models invariant to molecular symmetries have already been described in the literature. These models learn a message passing algorithm and aggregation procedure to compute a function of their entire input graph. At this point, the next step is to find a particularly effective variant of this general approach and apply it to chemical prediction benchmarks until we either solve them or reach the limits of the approach. In this paper, we reformulate existing models into a single common framework we call Message Passing Neural Networks (MPNNs) and explore additional novel variations within this framework. Using MPNNs we demonstrate state of the art results on an important molecular property prediction benchmark; these results are strong enough that we believe future work should focus on datasets with larger molecules or more accurate ground truth labels.},
  archivePrefix = {arXiv},
  eprint = {1704.01212},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gilmer et al_2017_neural message passing for quantum chemistry.pdf;/home/trung/Zotero/storage/KTA5646K/1704.html},
  journal = {arXiv:1704.01212 [cs]},
  keywords = {Computer Science - Machine Learning,graph,I.2.6},
  primaryClass = {cs}
}

@article{gilpin19_ExplainingExplanationsOverview,
  title = {Explaining {{Explanations}}: {{An Overview}} of {{Interpretability}} of {{Machine Learning}}},
  shorttitle = {Explaining {{Explanations}}},
  author = {Gilpin, Leilani H. and Bau, David and Yuan, Ben Z. and Bajwa, Ayesha and Specter, Michael and Kagal, Lalana},
  year = {2019},
  month = feb,
  abstract = {There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we provide our definition of explainability and show how it can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.},
  archivePrefix = {arXiv},
  eprint = {1806.00069},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gilpin et al_2019_explaining explanations.pdf;/home/trung/Zotero/storage/48U84AFN/1806.html},
  journal = {arXiv:1806.00069 [cs, stat]},
  keywords = {_tablet,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{girin20_DynamicalVariationalAutoencoders,
  title = {Dynamical {{Variational Autoencoders}}: {{A Comprehensive Review}}},
  shorttitle = {Dynamical {{Variational Autoencoders}}},
  author = {Girin, Laurent and Leglaive, Simon and Bie, Xiaoyu and Diard, Julien and Hueber, Thomas and {Alameda-Pineda}, Xavier},
  year = {2020},
  month = aug,
  abstract = {The Variational Autoencoder (VAE) is a powerful deep generative model that is now extensively used to represent high-dimensional complex data via a low-dimensional latent space that is learned in an unsupervised manner. In the original VAE model, input data vectors are processed independently. In the recent years, a series of papers have presented different extensions of the VAE to sequential data, that not only model the latent space, but also model the temporal dependencies within a sequence of data vectors and/or corresponding latent vectors, relying on recurrent neural networks or state space models. In this paper we perform an extensive literature review of these models. Importantly, we introduce and discuss a general class of models called Dynamical Variational Autoencoders (DVAEs) that encompass a large subset of these temporal VAE extensions. Then we present in details seven different instances of DVAE that were recently proposed in the literature, with an effort to homogenize the notations and presentation lines, as well as to relate those models with existing classical temporal models (that are also presented for the sake of completeness). We reimplemented those seven DVAE models and we present the results of an experimental benchmark that we conducted on the speech analysis-resynthesis task (the PyTorch code will be made publicly available). An extensive discussion is presented at the end of the paper, aiming to comment on important issues concerning the DVAE class of models and to describe future research guidelines.},
  archivePrefix = {arXiv},
  eprint = {2008.12595},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/girin et al_2020_dynamical variational autoencoders.pdf},
  journal = {arXiv:2008.12595 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{glick10_Autophagycellularmolecular,
  title = {Autophagy: Cellular and Molecular Mechanisms},
  shorttitle = {Autophagy},
  author = {Glick, Danielle and Barth, Sandra and Macleod, Kay F.},
  year = {2010},
  month = may,
  volume = {221},
  pages = {3--12},
  issn = {0022-3417},
  doi = {10.1002/path.2697},
  abstract = {Autophagy is a self-degradative process that is important for balancing sources of energy at critical times in development and in response to nutrient stress. Autophagy also plays a housekeeping role in removing misfolded or aggregated proteins, clearing damaged organelles, such as mitochondria, endoplasmic reticulum and peroxisomes, as well as eliminating intracellular pathogens. Thus, autophagy is generally thought of as a survival mechanism, although its deregulation has been linked to non-apoptotic cell death. Autophagy can be either non-selective or selective in the removal of specific organelles, ribosomes and protein aggregates, although the mechanisms regulating aspects of selective autophagy are not fully worked out. In addition to elimination of intracellular aggregates and damaged organelles, autophagy promotes cellular senescence and cell surface antigen presentation, protects against genome instability and prevents necrosis, giving it a key role in preventing diseases such as cancer, neurodegeneration, cardiomyopathy, diabetes, liver disease, autoimmune diseases and infections. This review summarizes the most up-to-date findings on how autophagy is executed and regulated at the molecular level and how its disruption can lead to disease.},
  annotation = {ZSCC: 0001652},
  file = {/home/trung/GoogleDrive/Zotero/glick et al_2010_autophagy.pdf},
  journal = {The Journal of pathology},
  number = {1},
  pmcid = {PMC2990190},
  pmid = {20225336}
}

@article{glorot00_Understandingdifficultytraining,
  title = {Understanding the Difficulty of Training Deep Feedforward Neural Networks},
  author = {Glorot, Xavier and Bengio, Yoshua},
  pages = {8},
  abstract = {Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.},
  annotation = {ZSCC: 0006872},
  file = {/home/trung/GoogleDrive/Zotero/glorot et al_understanding the difﬁculty of training deep feedforward neural networks.pdf},
  language = {en}
}

@article{goh17_DeepLearningComputational,
  title = {Deep {{Learning}} for {{Computational Chemistry}}},
  author = {Goh, Garrett B. and Hodas, Nathan O. and Vishnu, Abhinav},
  year = {2017},
  month = jan,
  abstract = {The rise and fall of artificial neural networks is well documented in the scientific literature of both computer science and computational chemistry. Yet almost two decades later, we are now seeing a resurgence of interest in deep learning, a machine learning algorithm based on multilayer neural networks. Within the last few years, we have seen the transformative impact of deep learning in many domains, particularly in speech recognition and computer vision, to the extent that the majority of expert practitioners in those field are now regularly eschewing prior established models in favor of deep learning models. In this review, we provide an introductory overview into the theory of deep neural networks and their unique properties that distinguish them from traditional machine learning algorithms used in cheminformatics. By providing an overview of the variety of emerging applications of deep neural networks, we highlight its ubiquity and broad applicability to a wide range of challenges in the field, including QSAR, virtual screening, protein structure prediction, quantum chemistry, materials design and property prediction. In reviewing the performance of deep neural networks, we observed a consistent outperformance against non-neural networks state-of-the-art models across disparate research topics, and deep neural network based models often exceeded the "glass ceiling" expectations of their respective tasks. Coupled with the maturity of GPU-accelerated computing for training deep neural networks and the exponential growth of chemical data on which to train these networks on, we anticipate that deep learning algorithms will be a valuable tool for computational chemistry.},
  annotation = {ZSCC: 0000171},
  archivePrefix = {arXiv},
  eprint = {1701.04503},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/goh et al_2017_deep learning for computational chemistry.pdf;/home/trung/Zotero/storage/P426QTWX/1701.html},
  journal = {arXiv:1701.04503 [physics, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Engineering; Finance; and Science,Computer Science - Machine Learning,Physics - Chemical Physics,Statistics - Machine Learning},
  primaryClass = {physics, stat}
}

@article{goldberg19_AssessingBERTSyntactic,
  title = {Assessing {{BERT}}'s {{Syntactic Abilities}}},
  author = {Goldberg, Yoav},
  year = {2019},
  month = jan,
  abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
  annotation = {ZSCC: 0000023},
  archivePrefix = {arXiv},
  eprint = {1901.05287},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/goldberg_2019_assessing bert's syntactic abilities.pdf;/home/trung/Zotero/storage/T5DNU9FT/1901.html},
  journal = {arXiv:1901.05287 [cs]},
  keywords = {bert,Computer Science - Computation and Language,explain,nlp},
  primaryClass = {cs}
}

@article{goldt20_Modelinginfluencedata,
  title = {Modeling the Influence of Data Structure on Learning in Neural Networks: {{The}} Hidden Manifold Model},
  author = {Goldt, Sebastian and M{\'e}zard, Marc and Krzakala, Florent and Zdeborov{\'a}, Lenka},
  year = {2020},
  month = dec,
  volume = {10},
  pages = {041044},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevX.10.041044},
  file = {/home/trung/GoogleDrive/Zotero/goldt et al_2020_modeling the influence of data structure on learning in neural networks.pdf},
  journal = {Physical Review X},
  number = {4}
}

@article{golikov18_Embeddingreparameterizationproceduremanifoldvalued,
  title = {Embedding-Reparameterization Procedure for Manifold-Valued Latent Variables in Generative Models},
  author = {Golikov, Eugene and Kretov, Maksim},
  year = {2018},
  month = dec,
  abstract = {Conventional prior for Variational Auto-Encoder (VAE) is a Gaussian distribution. Recent works demonstrated that choice of prior distribution affects learning capacity of VAE models. We propose a general technique (embedding-reparameterization procedure, or ER) for introducing arbitrary manifold-valued variables in VAE model. We compare our technique with a conventional VAE on a toy benchmark problem. This is work in progress.},
  archivePrefix = {arXiv},
  eprint = {1812.02769},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/golikov et al_2018_embedding-reparameterization procedure for manifold-valued latent variables in generative models.pdf},
  journal = {arXiv:1812.02769 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{golikov18_Embeddingreparameterizationproceduremanifoldvalueda,
  title = {Embedding-Reparameterization Procedure for Manifold-Valued Latent Variables in Generative Models},
  author = {Golikov, Eugene and Kretov, Maksim},
  year = {2018},
  month = dec,
  abstract = {Conventional prior for Variational Auto-Encoder (VAE) is a Gaussian distribution. Recent works demonstrated that choice of prior distribution affects learning capacity of VAE models. We propose a general technique (embedding-reparameterization procedure, or ER) for introducing arbitrary manifold-valued variables in VAE model. We compare our technique with a conventional VAE on a toy benchmark problem. This is work in progress.},
  archivePrefix = {arXiv},
  eprint = {1812.02769},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/false},
  journal = {arXiv:1812.02769 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gondal19_TransferInductiveBias,
  title = {On the {{Transfer}} of {{Inductive Bias}} from {{Simulation}} to the {{Real World}}: A {{New Disentanglement Dataset}}},
  shorttitle = {On the {{Transfer}} of {{Inductive Bias}} from {{Simulation}} to the {{Real World}}},
  author = {Gondal, Muhammad Waleed and W{\"u}thrich, Manuel and Miladinovi{\'c}, {\DJ}or{\dj}e and Locatello, Francesco and Breidt, Martin and Volchkov, Valentin and Akpo, Joel and Bachem, Olivier and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  year = {2019},
  month = jun,
  abstract = {Learning meaningful and compact representations with structurally disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over 450'000 images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world.},
  archivePrefix = {arXiv},
  eprint = {1906.03292},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gondal et al_2019_on the transfer of inductive bias from simulation to the real world2.pdf;/home/trung/Zotero/storage/NLAX84VJ/1906.html},
  journal = {arXiv:1906.03292 [cs, stat]},
  keywords = {Computer Science - Machine Learning,dataset,disentanglement,Statistics - Machine Learning,transfer learning},
  primaryClass = {cs, stat}
}

@inproceedings{Gondaletal19,
  title = {On the Transfer of Inductive Bias from Simulation to the Real World: A New Disentanglement Dataset},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Gondal, M. W. and Wuthrich, M. and Miladinovic, D. and Locatello, F. and Breidt, M. and Volchkov, V. and Akpo, J. and Bachem, O. and Sch{\"o}lkopf, B. and Bauer, S. and Wallach, H. and Larochelle, H. and Beygelzimer, A. and {d'Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  month = dec,
  pages = {15714--15725},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/gondal et al_2019_on the transfer of inductive bias from simulation to the real world.pdf},
  month_numeric = {12}
}

@article{gong20_Learningpartiallyobservedmultimodal,
  title = {Learning from Partially-Observed Multimodal Data with Variational Autoencoders},
  author = {Gong, Yu and Hajimirsadeghi, Hossein and He, Jiawei and Nawhal, Megha and Durand, Thibaut and Mori, Greg},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/gong et al_2020_learning from partially-observed multimodal data with variational autoencoders.pdf}
}

@article{gonzalez-soto19_CausalGamesCausal,
  title = {Causal {{Games}} and {{Causal Nash Equilibrium}}},
  author = {{Gonzalez-Soto}, Mauricio and Sucar, Luis E. and Escalante, Hugo J.},
  year = {2019},
  month = oct,
  abstract = {Classical results of Decision Theory, and its extension to a multi-agent setting: Game Theory, operate only at the associative level of information; this is, classical decision makers only take into account probabilities of events; we go one step further and consider causal information: in this work, we define Causal Decision Problems and extend them to a multi-agent decision problem, which we call a causal game. For such games, we study belief updating in a class of strategic games in which any player's action causes some consequence via a causal model, which is unknown by all players; for this reason, the most suitable model is Harsanyi's Bayesian Game. We propose a probability updating for the Bayesian Game in such a way that the knowledge of any player in terms of probabilistic beliefs about the causal model, as well as what is caused by her actions as well as the actions of every other player are taken into account. Based on such probability updating we define a Nash equilibria for Causal Games.},
  archivePrefix = {arXiv},
  eprint = {1910.06729},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gonzalez-soto et al_2019_causal games and causal nash equilibrium.pdf},
  journal = {arXiv:1910.06729 [cs]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs}
}

@article{gonzalez19_SolvingInverseProblems,
  title = {Solving {{Inverse Problems}} by {{Joint Posterior Maximization}} with a {{VAE Prior}}},
  author = {Gonz{\'a}lez, Mario and Almansa, Andr{\'e}s and Delbracio, Mauricio and Mus{\'e}, Pablo and Tan, Pauline},
  year = {2019},
  month = nov,
  abstract = {In this paper we address the problem of solving ill-posed inverse problems in imaging where the prior is a neural generative model. Specifically we consider the decoupled case where the prior is trained once and can be reused for many different log-concave degradation models without retraining. Whereas previous MAP-based approaches to this problem lead to highly non-convex optimization algorithms, our approach computes the joint (space-latent) MAP that naturally leads to alternate optimization algorithms and to the use of a stochastic encoder to accelerate computations. The resulting technique is called JPMAP because it performs Joint Posterior Maximization using an Autoencoding Prior. We show theoretical and experimental evidence that the proposed objective function is quite close to bi-convex. Indeed it satisfies a weak bi-convexity property which is sufficient to guarantee that our optimization scheme converges to a stationary point. Experimental results also show the higher quality of the solutions obtained by our JPMAP approach with respect to other non-convex MAP approaches which more often get stuck in spurious local optima.},
  archivePrefix = {arXiv},
  eprint = {1911.06379},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/FTRKAXVZ/González et al. - 2019 - Solving Inverse Problems by Joint Posterior Maximi.pdf},
  journal = {arXiv:1911.06379 [cs, eess, math, stat]},
  language = {en},
  primaryClass = {cs, eess, math, stat}
}

@article{goodfellow00_casedynamicdefenses,
  title = {The Case for Dynamic Defenses against Adversarial Examples},
  author = {Goodfellow, Ian},
  pages = {22},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/goodfellow_the case for dynamic defenses against adversarial examples.pdf},
  language = {en}
}

@article{goodfellow00_GenerativeAdversarialNets,
  title = {Generative {{Adversarial Nets}}},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  pages = {9},
  file = {/home/trung/GoogleDrive/Zotero/goodfellow et al_generative adversarial nets.pdf},
  language = {en}
}

@article{goodfellow00_LargeScaleFeatureLearning,
  title = {Large-{{Scale Feature Learning With Spike}}-and-{{Slab Sparse Coding}}},
  author = {Goodfellow, Ian J and Courville, Aaron and Bengio, Yoshua},
  pages = {8},
  abstract = {We consider the problem of object recognition with a large number of classes. In order to overcome the low amount of labeled examples available in this setting, we introduce a new feature learning and extraction procedure based on a factor model we call spike-and-slab sparse coding (S3C). Prior work on S3C has not prioritized the ability to exploit parallel architectures and scale S3C to the enormous problem sizes needed for object recognition. We present a novel inference procedure for appropriate for use with GPUs which allows us to dramatically increase both the training set size and the amount of latent factors that S3C may be trained with. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the spike-and-slab Restricted Boltzmann Machine (ssRBM) on the CIFAR-10 dataset. We use the CIFAR-100 dataset to demonstrate that our method scales to large numbers of classes better than previous methods. Finally, we use our method to win the NIPS 2011 Workshop on Challenges In Learning Hierarchical Models' Transfer Learning Challenge.},
  file = {/home/trung/GoogleDrive/Zotero/goodfellow et al_large-scale feature learning with spike-and-slab sparse coding.pdf},
  language = {en}
}

@article{goodfellow00_MeasuringInvariancesDeep,
  title = {Measuring {{Invariances}} in {{Deep Networks}}},
  author = {Goodfellow, Ian J and Le, Quoc V and Saxe, Andrew M and Lee, Honglak and Ng, Andrew Y},
  pages = {9},
  abstract = {For many pattern recognition tasks, the ideal input feature would be invariant to multiple confounding properties (such as illumination and viewing angle, in computer vision applications). Recently, deep architectures trained in an unsupervised manner have been proposed as an automatic method for extracting useful features. However, it is difficult to evaluate the learned features by any means other than using them in a classifier. In this paper, we propose a number of empirical tests that directly measure the degree to which these learned features are invariant to different input transformations. We find that stacked autoencoders learn modestly increasingly invariant features with depth when trained on natural images. We find that convolutional deep belief networks learn substantially more invariant features in each layer. These results further justify the use of ``deep'' vs. ``shallower'' representations, but suggest that mechanisms beyond merely stacking one autoencoder on top of another may be important for achieving invariance. Our evaluation metrics can also be used to evaluate future work in deep learning, and thus help the development of future algorithms.},
  annotation = {ZSCC: 0000373},
  file = {/home/trung/GoogleDrive/Zotero/goodfellow et al_measuring invariances in deep networks.pdf},
  language = {en}
}

@book{goodfellow12_Spikeandslabsparsecoding,
  title = {Spike-and-Slab Sparse Coding for Unsupervised Feature Discovery. {{arXiv}} Preprint {{arXiv}}:1201.3382},
  shorttitle = {Spike-and-Slab Sparse Coding for Unsupervised Feature Discovery. {{arXiv}} Preprint {{arXiv}}},
  author = {Goodfellow, Ian J. and Courville, Aaron and Bengio, Yoshua},
  year = {2012},
  abstract = {We introduce spike-and-slab sparse coding (S3C), an unsupervised feature dis-covery algorithm. S3C is based on a generative model that resembles both the spike-and-slab RBM and sparse coding. Since exact inference in this model is intractable, we derive a structured variational inference procedure and employ a variational EM training algorithm. We demonstrate that this approach improves upon the supervised learning capabilities of both sparse coding and the ssRBM on the CIFAR-10 dataset. We evaluate our approach's potential for semi-supervised learning on subsets of CIFAR-10. We use our method to win the NIPS 2011 Work-shop on Challenges In Learning Hierarchical Models ' Transfer Learning Chal-lenge. 1 The S3C model The S3C model consists of latent binary spike variables h {$\in$} \{0, 1\}N, latent real-valued slab vari-ables s {$\in$} RN, and real-valued D-dimensional visible vector v {$\in$} RD generated according to this},
  file = {/home/trung/GoogleDrive/Zotero/goodfellow et al_2012_spike-and-slab sparse coding for unsupervised feature discovery.pdf}
}

@article{goodfellow17_NIPS2016Tutorial,
  title = {{{NIPS}} 2016 {{Tutorial}}: {{Generative Adversarial Networks}}},
  shorttitle = {{{NIPS}} 2016 {{Tutorial}}},
  author = {Goodfellow, Ian},
  year = {2017},
  month = apr,
  abstract = {This report summarizes the tutorial presented by the author at NIPS 2016 on generative adversarial networks (GANs). The tutorial describes: (1) Why generative modeling is a topic worth studying, (2) how generative models work, and how GANs compare to other generative models, (3) the details of how GANs work, (4) research frontiers in GANs, and (5) state-of-the-art image models that combine GANs with other methods. Finally, the tutorial contains three exercises for readers to complete, and the solutions to these exercises.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1701.00160},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/goodfellow_2017_nips 2016 tutorial.pdf;/home/trung/Zotero/storage/ZLXGHIGY/1701.html},
  journal = {arXiv:1701.00160 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{goodman20_AIWargaming,
  title = {{{AI}} and {{Wargaming}}},
  author = {Goodman, James and Risi, Sebastian and Lucas, Simon},
  year = {2020},
  month = sep,
  abstract = {Recent progress in Game AI has demonstrated that given enough data from human gameplay, or experience gained via simulations, machines can rival or surpass the most skilled human players in classic games such as Go, or commercial computer games such as Starcraft. We review the current state-of-the-art through the lens of wargaming, and ask firstly what features of wargames distinguish them from the usual AI testbeds, and secondly which recent AI advances are best suited to address these wargame-specific features.},
  archivePrefix = {arXiv},
  eprint = {2009.08922},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/goodman et al_2020_ai and wargaming.pdf},
  journal = {arXiv:2009.08922 [cs]},
  primaryClass = {cs}
}

@article{gorbach17_ScalableVariationalInference,
  title = {Scalable {{Variational Inference}} for {{Dynamical Systems}}},
  author = {Gorbach, Nico S. and Bauer, Stefan and Buhmann, Joachim M.},
  year = {2017},
  month = may,
  abstract = {Gradient matching is a promising tool for learning parameters and state dynamics of ordinary differential equations. It is a grid free inference approach, which, for fully observable systems is at times competitive with numerical integration. However, for many real-world applications, only sparse observations are available or even unobserved variables are included in the model description. In these cases most gradient matching methods are difficult to apply or simply do not provide satisfactory results. That is why, despite the high computational cost, numerical integration is still the gold standard in many applications. Using an existing gradient matching approach, we propose a scalable variational inference framework which can infer states and parameters simultaneously, offers computational speedups, improved accuracy and works well even under model misspecifications in a partially observable system.},
  archivePrefix = {arXiv},
  eprint = {1705.07079},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gorbach et al_2017_scalable variational inference for dynamical systems.pdf;/home/trung/Zotero/storage/NB2XPMSD/1705.html},
  journal = {arXiv:1705.07079 [stat]},
  keywords = {Statistics - Machine Learning,variational},
  primaryClass = {stat}
}

@article{gordon-rodriguez20_continuouscategoricalnovel,
  title = {The Continuous Categorical: A Novel Simplex-Valued Exponential Family},
  shorttitle = {The Continuous Categorical},
  author = {{Gordon-Rodriguez}, Elliott and {Loaiza-Ganem}, Gabriel and Cunningham, John P.},
  year = {2020},
  month = jun,
  abstract = {Simplex-valued data appear throughout statistics and machine learning, for example in the context of transfer learning and compression of deep networks. Existing models for this class of data rely on the Dirichlet distribution or other related loss functions; here we show these standard choices suffer systematically from a number of limitations, including bias and numerical issues that frustrate the use of flexible network models upstream of these distributions. We resolve these limitations by introducing a novel exponential family of distributions for modeling simplex-valued data - the continuous categorical, which arises as a nontrivial multivariate generalization of the recently discovered continuous Bernoulli. Unlike the Dirichlet and other typical choices, the continuous categorical results in a well-behaved probabilistic loss function that produces unbiased estimators, while preserving the mathematical simplicity of the Dirichlet. As well as exploring its theoretical properties, we introduce sampling methods for this distribution that are amenable to the reparameterization trick, and evaluate their performance. Lastly, we demonstrate that the continuous categorical outperforms standard choices empirically, across a simulation study, an applied example on multi-party elections, and a neural network compression task.},
  archivePrefix = {arXiv},
  eprint = {2002.08563},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gordon-rodriguez et al_2020_the continuous categorical.pdf},
  journal = {arXiv:2002.08563 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{gordon-rodriguez20_UsesAbusesCrossEntropy,
  title = {Uses and {{Abuses}} of the {{Cross}}-{{Entropy Loss}}: {{Case Studies}} in {{Modern Deep Learning}}},
  shorttitle = {Uses and {{Abuses}} of the {{Cross}}-{{Entropy Loss}}},
  author = {{Gordon-Rodriguez}, Elliott and {Loaiza-Ganem}, Gabriel and Pleiss, Geoff and Cunningham, John P.},
  year = {2020},
  month = nov,
  abstract = {Modern deep learning is primarily an experimental science, in which empirical advances occasionally come at the expense of probabilistic rigor. Here we focus on one such example; namely the use of the categorical cross-entropy loss to model data that is not strictly categorical, but rather takes values on the simplex. This practice is standard in neural network architectures with label smoothing and actor-mimic reinforcement learning, amongst others. Drawing on the recently discovered continuous-categorical distribution, we propose probabilistically-inspired alternatives to these models, providing an approach that is more principled and theoretically appealing. Through careful experimentation, including an ablation study, we identify the potential for outperformance in these models, thereby highlighting the importance of a proper probabilistic treatment, as well as illustrating some of the failure modes thereof.},
  archivePrefix = {arXiv},
  eprint = {2011.05231},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gordon-rodriguez et al_2020_uses and abuses of the cross-entropy loss.pdf},
  journal = {arXiv:2011.05231 [cs, stat]},
  keywords = {_tablet,favorite,representation},
  primaryClass = {cs, stat}
}

@article{gordon17_BayesianSemisupervisedLearning,
  title = {Bayesian {{Semisupervised Learning}} with {{Deep Generative Models}}},
  author = {Gordon, Jonathan and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2017},
  month = jun,
  abstract = {Neural network based generative models with discriminative components are a powerful approach for semi-supervised learning. However, these techniques a) cannot account for model uncertainty in the estimation of the model's discriminative component and b) lack flexibility to capture complex stochastic patterns in the label generation process. To avoid these problems, we first propose to use a discriminative component with stochastic inputs for increased noise flexibility. We show how an efficient Gibbs sampling procedure can marginalize the stochastic inputs when inferring missing labels in this model. Following this, we extend the discriminative component to be fully Bayesian and produce estimates of uncertainty in its parameter values. This opens the door for semi-supervised Bayesian active learning.},
  archivePrefix = {arXiv},
  eprint = {1706.09751},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gordon et al_2017_bayesian semisupervised learning with deep generative models.pdf},
  journal = {arXiv:1706.09751 [stat]},
  primaryClass = {stat}
}

@article{gordon18_MorphNetFastSimple,
  title = {{{MorphNet}}: {{Fast}} \& {{Simple Resource}}-{{Constrained Structure Learning}} of {{Deep Networks}}},
  shorttitle = {{{MorphNet}}},
  author = {Gordon, Ariel and Eban, Elad and Nachum, Ofir and Chen, Bo and Wu, Hao and Yang, Tien-Ju and Choi, Edward},
  year = {2018},
  month = apr,
  abstract = {We present MorphNet, an approach to automate the design of neural network structures. MorphNet iteratively shrinks and expands a network, shrinking via a resource-weighted sparsifying regularizer on activations and expanding via a uniform multiplicative factor on all layers. In contrast to previous approaches, our method is scalable to large networks, adaptable to specific resource constraints (e.g. the number of floating-point operations per inference), and capable of increasing the network's performance. When applied to standard network architectures on a wide variety of datasets, our approach discovers novel structures in each domain, obtaining higher performance while respecting the resource constraint.},
  annotation = {ZSCC: 0000089},
  archivePrefix = {arXiv},
  eprint = {1711.06798},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gordon et al_2018_morphnet.pdf;/home/trung/Zotero/storage/JGDFEY9M/1711.html},
  journal = {arXiv:1711.06798 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gordon20_Combiningdeepgenerative,
  title = {Combining Deep Generative and Discriminative Models for {{Bayesian}} Semi-Supervised Learning},
  author = {Gordon, Jonathan and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2020},
  month = apr,
  volume = {100},
  pages = {107156},
  issn = {00313203},
  doi = {10.1016/j.patcog.2019.107156},
  abstract = {Generative models can be used for a wide range of tasks, and have the appealing ability to learn from both labelled and unlabelled data. In contrast, discriminative models cannot learn from unlabelled data, but tend to outperform their generative counterparts in supervised tasks. We develop a framework to jointly train deep generative and discriminative models, enjoying the benefits of both. The framework allows models to learn from labelled and unlabelled data, as well as naturally account for uncertainty in predictive distributions, providing the first Bayesian approach to semi-supervised learning with deep generative models. We demonstrate that our blended discriminative and generative models outperform purely generative models in both predictive performance and uncertainty calibration in a number of semi-supervised learning tasks.},
  file = {/home/trung/GoogleDrive/Zotero/gordon et al_2020_combining deep generative and discriminative models for bayesian semi-supervised learning.pdf},
  journal = {Pattern Recognition},
  language = {en}
}

@book{gorelick14_HighperformancePython,
  title = {High Performance {{Python}}},
  author = {Gorelick, Micha and Ozsvald, Ian},
  year = {2014},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  abstract = {"If you're an experienced Python programmer, High Performance Python will guide you through the various routes of code optimization. You'll learn how to use smarter algorithms and leverage peripheral technologies, such as numpy, cython, cpython, and various multi-threaded and multi-node strategies. There's a lack of good learning and reference material available if you want to learn Python for highly computational tasks. Because of it, fields from physics to biology and systems infrastructure to data science are hitting barriers. They need the fast prototyping nature of Python, but too few people know how to wield it"--Publisher's description},
  annotation = {OCLC: ocn868080968},
  file = {/home/trung/GoogleDrive/Zotero/gorelick et al_2014_high performance python.pdf},
  isbn = {978-1-4493-6159-4},
  keywords = {Python (Computer program language)},
  language = {en},
  lccn = {QA76.73.P98 G67 2014}
}

@article{gorgen00_algebraiccharacterisationstaged,
  title = {An Algebraic Characterisation of Staged Trees: Their Geometry and Causal Implications},
  author = {G{\"o}rgen, Christiane},
  pages = {159},
  file = {/home/trung/GoogleDrive/Zotero/görgen_an algebraic characterisation of staged trees.pdf},
  language = {en}
}

@article{gorinova19_AutomaticReparameterisationProbabilistic,
  title = {Automatic {{Reparameterisation}} of {{Probabilistic Programs}}},
  author = {Gorinova, Maria I. and Moore, Dave and Hoffman, Matthew D.},
  year = {2019},
  month = jun,
  abstract = {Probabilistic programming has emerged as a powerful paradigm in statistics, applied science, and machine learning: by decoupling modelling from inference, it promises to allow modellers to directly reason about the processes generating data. However, the performance of inference algorithms can be dramatically affected by the parameterisation used to express a model, requiring users to transform their programs in non-intuitive ways. We argue for automating these transformations, and demonstrate that mechanisms available in recent modeling frameworks can implement non-centring and related reparameterisations. This enables new inference algorithms, and we propose two: a simple approach using interleaved sampling and a novel variational formulation that searches over a continuous space of parameterisations. We show that these approaches enable robust inference across a range of models, and can yield more efficient samplers than the best fixed parameterisation.},
  archivePrefix = {arXiv},
  eprint = {1906.03028},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gorinova et al_2019_automatic reparameterisation of probabilistic programs.pdf},
  journal = {arXiv:1906.03028 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{gottipati20_LearningNavigateSynthetically,
  title = {Learning {{To Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning}}},
  author = {Gottipati, Sai Krishna and Sattarov, Boris and Niu, Sufeng and Pathak, Yashaswi and Wei, Haoran and Liu, Shengchao and Thomas, Karam M. J. and Blackburn, Simon and Coley, Connor W. and Tang, Jian and Chandar, Sarath and Bengio, Yoshua},
  year = {2020},
  month = may,
  abstract = {Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in deep generative models. However, current generative approaches exhibit a significant challenge as they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel forward synthesis framework powered by reinforcement learning (RL) for de novo drug design, Policy Gradient for Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting commercially available small molecule building blocks to valid chemical reactions at every time step of the iterative virtual multi-step synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and penalized clogP. Moreover, we validate PGFS in an in-silico proof-of-concept associated with three HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.},
  archivePrefix = {arXiv},
  eprint = {2004.12485},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gottipati et al_2020_learning to navigate the synthetically accessible chemical space using reinforcement learning.pdf},
  journal = {arXiv:2004.12485 [cs]},
  primaryClass = {cs}
}

@article{gottipati20_LearningNavigateSyntheticallya,
  title = {Learning {{To Navigate The Synthetically Accessible Chemical Space Using Reinforcement Learning}}},
  author = {Gottipati, Sai Krishna and Sattarov, Boris and Niu, Sufeng and Pathak, Yashaswi and Wei, Haoran and Liu, Shengchao and Thomas, Karam M. J. and Blackburn, Simon and Coley, Connor W. and Tang, Jian and Chandar, Sarath and Bengio, Yoshua},
  year = {2020},
  month = may,
  abstract = {Over the last decade, there has been significant progress in the field of machine learning for de novo drug design, particularly in deep generative models. However, current generative approaches exhibit a significant challenge as they do not ensure that the proposed molecular structures can be feasibly synthesized nor do they provide the synthesis routes of the proposed small molecules, thereby seriously limiting their practical applicability. In this work, we propose a novel forward synthesis framework powered by reinforcement learning (RL) for de novo drug design, Policy Gradient for Forward Synthesis (PGFS), that addresses this challenge by embedding the concept of synthetic accessibility directly into the de novo drug design system. In this setup, the agent learns to navigate through the immense synthetically accessible chemical space by subjecting commercially available small molecule building blocks to valid chemical reactions at every time step of the iterative virtual multi-step synthesis process. The proposed environment for drug discovery provides a highly challenging test-bed for RL algorithms owing to the large state space and high-dimensional continuous action space with hierarchical actions. PGFS achieves state-of-the-art performance in generating structures with high QED and penalized clogP. Moreover, we validate PGFS in an in-silico proof-of-concept associated with three HIV targets. Finally, we describe how the end-to-end training conceptualized in this study represents an important paradigm in radically expanding the synthesizable chemical space and automating the drug discovery process.},
  archivePrefix = {arXiv},
  eprint = {2004.12485},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gottipati et al_2020_learning to navigate the synthetically accessible chemical space using reinforcement learning2.pdf},
  journal = {arXiv:2004.12485 [cs]},
  primaryClass = {cs}
}

@article{gou19_MethodConstructingSupervised,
  title = {A {{Method}} for {{Constructing Supervised Topic Model Based}} on {{Term Frequency}}-{{Inverse Topic Frequency}}},
  author = {Gou, Zhinan and Huo, Zheng and Liu, Yuanzhen and Yang, Yi},
  year = {2019},
  month = dec,
  volume = {11},
  pages = {1486},
  issn = {2073-8994},
  doi = {10.3390/sym11121486},
  abstract = {Supervised topic modeling has been successfully applied in the fields of document classification and tag recommendation in recent years. However, most existing models neglect the fact that topic terms have the ability to distinguish topics. In this paper, we propose a term frequency-inverse topic frequency (TF-ITF) method for constructing a supervised topic model, in which the weight of each topic term indicates the ability to distinguish topics. We conduct a series of experiments with not only the symmetric Dirichlet prior parameters but also the asymmetric Dirichlet prior parameters. Experimental results demonstrate that the result of introducing TF-ITF into a supervised topic model outperforms several state-of-the-art supervised topic models.},
  file = {/home/trung/GoogleDrive/Zotero/gou et al_2019_a method for constructing supervised topic model based on term frequency-inverse topic frequency.pdf},
  journal = {Symmetry},
  language = {en},
  number = {12}
}

@article{govalkar00_ComparisonRecentNeural,
  title = {A {{Comparison}} of {{Recent Neural Vocoders}} for {{Speech Signal Reconstruction}}},
  author = {Govalkar, Prachi and Fischer, Johannes and Zalkow, Frank and Dittmar, Christian},
  pages = {6},
  abstract = {In recent years, text-to-speech (TTS) synthesis has benefited from advanced machine learning approaches. Most prominently, since the introduction of the WaveNet architecture, neural vocoders have exhibited superior performance in terms of the naturalness of synthesized speech signals in comparison to traditional vocoders. In this paper, a fair comparison of recent neural vocoders is presented in a signal reconstruction scenario. That means we use such techniques to resynthesize speech waveforms from mel-scaled spectrograms, a compact and generally non-invertible representation of the underlying audio signal. In that context, we conduct listening tests according to the well established MUSHRA standard and compare the attained results to similar studies. Weighing off the perceptual quality to the computational requirements, our findings shall serve as a guideline to both practitioners and researchers in speech synthesis.},
  file = {/home/trung/GoogleDrive/Zotero/govalkar et al_a comparison of recent neural vocoders for speech signal reconstruction.pdf},
  language = {en}
}

@article{goyal19_InfoBotTransferExploration,
  title = {{{InfoBot}}: {{Transfer}} and {{Exploration}} via the {{Information Bottleneck}}},
  shorttitle = {{{InfoBot}}},
  author = {Goyal, Anirudh and Islam, Riashat and Strouse, Daniel and Ahmed, Zafarali and Botvinick, Matthew and Larochelle, Hugo and Bengio, Yoshua and Levine, Sergey},
  year = {2019},
  month = jan,
  abstract = {A central challenge in reinforcement learning is discovering effective policies for tasks where rewards are sparsely distributed. We postulate that in the absence of useful reward signals, an effective exploration strategy should seek out \{\textbackslash it decision states\}. These states lie at critical junctions in the state space from where the agent can transition to new, potentially unexplored regions. We propose to learn about decision states from prior experience. By training a goal-conditioned policy with an information bottleneck, we can identify decision states by examining where the model actually leverages the goal state. We find that this simple mechanism effectively identifies decision states, even in partially observed settings. In effect, the model learns the sensory cues that correlate with potential subgoals. In new environments, this model can then identify novel subgoals for further exploration, guiding the agent through a sequence of potential decision states and through new regions of the state space.},
  archivePrefix = {arXiv},
  eprint = {1901.10902},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/goyal et al_2019_infobot.pdf;/home/trung/Zotero/storage/CUTIDN23/1901.html},
  journal = {arXiv:1901.10902 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,transfer learning},
  primaryClass = {cs, stat}
}

@article{goyal19_RecurrentIndependentMechanisms,
  title = {Recurrent {{Independent Mechanisms}}},
  author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = sep,
  abstract = {Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.},
  archivePrefix = {arXiv},
  eprint = {1909.10893},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/goyal et al_2019_recurrent independent mechanisms.pdf;/home/trung/Zotero/storage/GKAK8ESC/1909.html},
  journal = {arXiv:1909.10893 [cs, stat]},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{goyal20_ClinicalCharacteristicsCovid19,
  title = {Clinical {{Characteristics}} of {{Covid}}-19 in {{New York City}}},
  author = {Goyal, Parag and Choi, Justin J. and Pinheiro, Laura C. and Schenck, Edward J. and Chen, Ruijun and Jabri, Assem and Satlin, Michael J. and Campion, Thomas R. and Nahid, Musarrat and Ringel, Joanna B. and Hoffman, Katherine L. and Alshak, Mark N. and Li, Han A. and Wehmeyer, Graham T. and Rajan, Mangala and Reshetnyak, Evgeniya and Hupert, Nathaniel and Horn, Evelyn M. and Martinez, Fernando J. and Gulick, Roy M. and Safford, Monika M.},
  year = {2020},
  month = apr,
  volume = {0},
  pages = {null},
  publisher = {{Massachusetts Medical Society}},
  issn = {0028-4793},
  doi = {10.1056/NEJMc2010419},
  annotation = {ZSCC: NoCitationData[s0]  \_eprint: https://doi.org/10.1056/NEJMc2010419},
  file = {/home/trung/GoogleDrive/Zotero/goyal et al_2020_clinical characteristics of covid-19 in new york city.pdf;/home/trung/Zotero/storage/RLPZYE96/nejmc2010419.html},
  journal = {New England Journal of Medicine},
  number = {0}
}

@article{goyal20_RecurrentIndependentMechanisms,
  title = {Recurrent {{Independent Mechanisms}}},
  author = {Goyal, Anirudh and Lamb, Alex and Hoffmann, Jordan and Sodhani, Shagun and Levine, Sergey and Bengio, Yoshua and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = jul,
  abstract = {Learning modular structures which reflect the dynamics of the environment can lead to better generalization and robustness to changes which only affect a few of the underlying causes. We propose Recurrent Independent Mechanisms (RIMs), a new recurrent architecture in which multiple groups of recurrent cells operate with nearly independent transition dynamics, communicate only sparingly through the bottleneck of attention, and are only updated at time steps where they are most relevant. We show that this leads to specialization amongst the RIMs, which in turn allows for dramatically improved generalization on tasks where some factors of variation differ systematically between training and evaluation.},
  archivePrefix = {arXiv},
  eprint = {1909.10893},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/goyal et al_2020_recurrent independent mechanisms.pdf},
  journal = {arXiv:1909.10893 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@inproceedings{goyal20_variationalbandwidthbottleneck,
  title = {The Variational Bandwidth Bottleneck: {{Stochastic}} Evaluation on an Information Budget},
  booktitle = {International Conference on Learning Representations},
  author = {Goyal, Anirudh and Bengio, Yoshua and Botvinick, Matthew and Levine, Sergey},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/goyal et al_2020_the variational bandwidth bottleneck.pdf},
  keywords = {information}
}

@techreport{granja19_singlecellframework,
  title = {A Single Cell Framework for Multi-Omic Analysis of Disease Identifies Malignant Regulatory Signatures in Mixed Phenotype Acute Leukemia},
  author = {Granja, Jeffrey M. and Klemm, Sandy and McGinnis, Lisa M. and Kathiria, Arwa S. and Mezger, Anja and Parks, Benjamin and Gars, Eric and Liedtke, Michaela and Zheng, Grace X.Y. and Chang, Howard Y. and Majeti, Ravindra and Greenleaf, William J.},
  year = {2019},
  month = jul,
  institution = {{Genomics}},
  doi = {10.1101/696328},
  abstract = {Abstract                        In order to identify the molecular determinants of human diseases, such as cancer, that arise from a diverse range of tissue, it is necessary to accurately distinguish normal and pathogenic cellular programs.             1\textendash 3             Here we present a novel approach for single-cell multi-omic deconvolution of healthy and pathological molecular signatures within phenotypically heterogeneous malignant cells. By first creating immunophenotypic, transcriptomic and epigenetic single-cell maps of hematopoietic development from healthy peripheral blood and bone marrow mononuclear cells, we identify cancer-specific transcriptional and chromatin signatures from single cells in a cohort of mixed phenotype acute leukemia (MPAL) clinical samples. MPALs are a high-risk subtype of acute leukemia characterized by a heterogeneous malignant cell population expressing both myeloid and lymphoid lineage-specific markers.             4, 5             Our results reveal widespread heterogeneity in the pathogenetic gene regulatory and expression programs across patients, yet relatively consistent changes within patients even across malignant cells occupying diverse portions of the hematopoietic lineage. An integrative analysis of transcriptomic and epigenetic maps identifies 91,601 putative gene-regulatory interactions and classifies a number of transcription factors that regulate leukemia specific genes, including             RUNX1             -linked regulatory elements proximal to             CD69             . This work provides a template for integrative, multi-omic analysis for the interpretation of pathogenic molecular signatures in the context of developmental origin.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/granja et al_2019_a single cell framework for multi-omic analysis of disease identifies malignant regulatory signatures in mixed phenotype acute leukemia.pdf},
  language = {en},
  type = {Preprint}
}

@article{granja19_Singlecellmultiomicanalysis,
  title = {Single-Cell Multiomic Analysis Identifies Regulatory Programs in Mixed-Phenotype Acute Leukemia},
  author = {Granja, Jeffrey M. and Klemm, Sandy and McGinnis, Lisa M. and Kathiria, Arwa S. and Mezger, Anja and Corces, M. Ryan and Parks, Benjamin and Gars, Eric and Liedtke, Michaela and Zheng, Grace X. Y. and Chang, Howard Y. and Majeti, Ravindra and Greenleaf, William J.},
  year = {2019},
  month = dec,
  volume = {37},
  pages = {1458--1465},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-019-0332-7},
  abstract = {Analyzing DNA accessibility, transcriptome and protein expression in single cells uncovers cancer-regulatory programs.},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  file = {/home/trung/Zotero/storage/KTEW686H/s41587-019-0332-7.html},
  journal = {Nature Biotechnology},
  language = {en},
  number = {12}
}

@article{granja19_Singlecellmultiomicanalysisa,
  title = {Single-Cell Multiomic Analysis Identifies Regulatory Programs in Mixed-Phenotype Acute Leukemia},
  author = {Granja, Jeffrey M. and Klemm, Sandy and McGinnis, Lisa M. and Kathiria, Arwa S. and Mezger, Anja and Corces, M. Ryan and Parks, Benjamin and Gars, Eric and Liedtke, Michaela and Zheng, Grace X. Y. and Chang, Howard Y. and Majeti, Ravindra and Greenleaf, William J.},
  year = {2019},
  month = dec,
  volume = {37},
  pages = {1458--1465},
  issn = {1087-0156, 1546-1696},
  doi = {10.1038/s41587-019-0332-7},
  abstract = {Identifying the causes of human diseases requires deconvolution of abnormal molecular phenotypes spanning DNA accessibility, gene expression and protein abundance1\textendash 3. We present a single-cell framework that integrates highly multiplexed protein quantification, transcriptome profiling and analysis of chromatin accessibility. Using this approach, we establish a normal epigenetic baseline for healthy blood development, which we then use to deconvolve aberrant molecular features within blood from patients with mixed-phenotype acute leukemia4,5. Despite widespread epigenetic heterogeneity within the patient cohort, we observe common malignant signatures across patients as well as patient-specific regulatory features that are shared across phenotypic compartments of individual patients. Integrative analysis of transcriptomic and chromatin-accessibility maps identified 91,601 putative peak-to-gene linkages and transcription factors that regulate leukemia-specific genes, such as RUNX1-linked regulatory elements proximal to the marker gene CD69. These results demonstrate how integrative, multiomic analysis of single cells within the framework of normal development can reveal both distinct and shared molecular mechanisms of disease from patient samples.},
  file = {/home/trung/GoogleDrive/Zotero/granja et al_2019_single-cell multiomic analysis identifies regulatory programs in mixed-phenotype acute leukemia.pdf},
  journal = {Nature Biotechnology},
  language = {en},
  number = {12}
}

@article{grathwohl19_YourClassifierSecretly,
  title = {Your {{Classifier}} Is {{Secretly}} an {{Energy Based Model}} and {{You Should Treat}} It {{Like One}}},
  author = {Grathwohl, Will and Wang, Kuan-Chieh and Jacobsen, J{\"o}rn-Henrik and Duvenaud, David and Norouzi, Mohammad and Swersky, Kevin},
  year = {2019},
  month = dec,
  abstract = {We propose to reinterpret a standard discriminative classifier of p(y|x) as an energy based model for the joint distribution p(x, y). In this setting, the standard class probabilities can be easily computed as well as unnormalized values of p(x) and p(x|y). Within this framework, standard discriminative architectures may be used and the model can also be trained on unlabeled data. We demonstrate that energy based training of the joint distribution improves calibration, robustness, and out-of-distribution detection while also enabling our models to generate samples rivaling the quality of recent GAN approaches. We improve upon recently proposed techniques for scaling up the training of energy based models and present an approach which adds little overhead compared to standard classification training. Our approach is the first to achieve performance rivaling the state-of-the-art in both generative and discriminative learning within one hybrid model.},
  annotation = {ZSCC: 0000008},
  archivePrefix = {arXiv},
  eprint = {1912.03263},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/grathwohl et al_2019_your classifier is secretly an energy based model and you should treat it like one.pdf},
  journal = {arXiv:1912.03263 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{grathwohl20_NoMCMCme,
  title = {No {{MCMC}} for Me: {{Amortized}} Sampling for Fast and Stable Training of Energy-Based Models},
  shorttitle = {No {{MCMC}} for Me},
  author = {Grathwohl, Will and Kelly, Jacob and Hashemi, Milad and Norouzi, Mohammad and Swersky, Kevin and Duvenaud, David},
  year = {2020},
  month = oct,
  abstract = {Energy-Based Models (EBMs) present a flexible and appealing way to represent uncertainty. Despite recent advances, training EBMs on high-dimensional data remains a challenging problem as the state-of-the-art approaches are costly, unstable, and require considerable tuning and domain expertise to apply successfully. In this work, we present a simple method for training EBMs at scale which uses an entropy-regularized generator to amortize the MCMC sampling typically used in EBM training. We improve upon prior MCMC-based entropy regularization methods with a fast variational approximation. We demonstrate the effectiveness of our approach by using it to train tractable likelihood models. Next, we apply our estimator to the recently proposed Joint Energy Model (JEM), where we match the original performance with faster and stable training. This allows us to extend JEM models to semi-supervised classification on tabular data from a variety of continuous domains.},
  archivePrefix = {arXiv},
  eprint = {2010.04230},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/grathwohl et al_2020_no mcmc for me.pdf},
  journal = {arXiv:2010.04230 [cs]},
  primaryClass = {cs}
}

@inproceedings{graves11_Practicalvariationalinference,
  title = {Practical Variational Inference for Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Graves, Alex},
  editor = {{Shawe-Taylor}, J. and Zemel, R. and Bartlett, P. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  volume = {24},
  pages = {2348--2356},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/graves_2011_practical variational inference for neural networks.pdf}
}

@article{graves14_GeneratingSequencesRecurrent,
  title = {Generating {{Sequences With Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2014},
  month = jun,
  abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1308.0850},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/graves_2014_generating sequences with recurrent neural networks.pdf;/home/trung/Zotero/storage/DQ3XTTYV/1308.html},
  journal = {arXiv:1308.0850 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,generative,recurrent,sequence},
  primaryClass = {cs}
}

@article{graves14_NeuralTuringMachines,
  title = {Neural {{Turing Machines}}},
  author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
  year = {2014},
  month = oct,
  abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
  annotation = {ZSCC: 0001148},
  archivePrefix = {arXiv},
  eprint = {1410.5401},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/graves et al_2014_neural turing machines.pdf;/home/trung/Zotero/storage/KKYMSUAS/1410.html},
  journal = {arXiv:1410.5401 [cs]},
  keywords = {attention,Computer Science - Neural and Evolutionary Computing,memory},
  primaryClass = {cs}
}

@techreport{graving20_VAESNEdeepgenerative,
  title = {{{VAE}}-{{SNE}}: A Deep Generative Model for Simultaneous Dimensionality Reduction and Clustering},
  shorttitle = {{{VAE}}-{{SNE}}},
  author = {Graving, Jacob M. and Couzin, Iain D.},
  year = {2020},
  month = jul,
  institution = {{Animal Behavior and Cognition}},
  doi = {10.1101/2020.07.17.207993},
  abstract = {Scientific datasets are growing rapidly in scale and complexity. Consequently, the task of understanding these data to answer scientific questions increasingly requires the use of compression algorithms that reduce dimensionality by combining correlated features and cluster similar observations to summarize large datasets. Here we introduce a method for both dimension reduction and clustering called VAE-SNE 6 (variational autoencoder stochastic neighbor embedding). Our model combines elements from deep learning, probabilistic inference, and manifold learning to produce interpretable compressed 8 representations while also readily scaling to tens-of-millions of observations. Unlike existing methods, VAE-SNE simultaneously compresses high-dimensional data and automatically learns a distribution of clusters within the data \textemdash{} without the need to manually select the number of clusters. This naturally creates a multi-scale representation, which makes it straightforward to generate coarse-grained descriptions for large subsets of related observations and select specific regions of interest for further analysis. VAE-SNE can also quickly and easily embed new samples, detect outliers, and can be optimized with small batches of data, which makes it possible to compress datasets that are otherwise too large to fit into memory. We evaluate VAE-SNE as a general purpose method for dimensionality reduction by 6 applying it to multiple real-world datasets and by comparing its performance with existing methods for dimensionality reduction. We find that VAE-SNE produces high-quality compressed representations with 8 results that are on par with existing nonlinear dimensionality reduction algorithms. As a practical example, we demonstrate how the cluster distribution learned by VAE-SNE can be used for unsupervised action recognition to detect and classify repeated motifs of stereotyped behavior in high-dimensional timeseries data. Finally, we also introduce variants of VAE-SNE for embedding data in polar (spherical) coordinates and for embedding image data from raw pixels. VAE-SNE is a robust, feature-rich, and scalable method with broad applicability to a range of datasets in the life sciences and beyond.},
  file = {/home/trung/GoogleDrive/Zotero/graving et al_2020_vae-sne.pdf},
  language = {en},
  type = {Preprint}
}

@article{greene13_FishOilsCoronary,
  title = {Fish {{Oils}}, {{Coronary Heart Disease}}, and the {{Environment}}},
  author = {Greene, Jonathan and Ashburn, Sarah M. and Razzouk, Louai and Smith, Donald A},
  year = {2013},
  month = sep,
  volume = {103},
  pages = {1568--1576},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2012.300959},
  abstract = {Clinical trials continue to produce conflicting results on the effectiveness of fish oils for the primary and secondary prevention of coronary heart disease. Despite many large, well-performed studies, questions still remain, made even more complex by the addition of early revascularization and statins in our coronary heart disease armamentarium. This is complicated by the reality that fish oil production has a measureable impact on reducing fish populations, which in turn has a negative impact on creating a sustainable product. We review the current data for fish oil usage in the primary and secondary prevention of coronary heart disease with an eye toward future studies, and the effects fish oil production has on the environment and efforts that are currently under way to mitigate these effects.},
  file = {/home/trung/GoogleDrive/Zotero/greene et al_2013_fish oils, coronary heart disease, and the environment.pdf},
  journal = {American Journal of Public Health},
  number = {9},
  pmcid = {PMC3780665},
  pmid = {23409906}
}

@article{greenhill20_BayesianOptimizationAdaptive,
  title = {Bayesian {{Optimization}} for {{Adaptive Experimental Design}}: {{A Review}}},
  shorttitle = {Bayesian {{Optimization}} for {{Adaptive Experimental Design}}},
  author = {Greenhill, Stewart and Rana, Santu and Gupta, Sunil and Vellanki, Pratibha and Venkatesh, Svetha},
  year = {2020},
  volume = {8},
  pages = {13937--13948},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2966228},
  abstract = {Bayesian optimisation is a statistical method that efficiently models and optimises expensive ``black-box'' functions. This review considers the application of Bayesian optimisation to experimental design, in comparison to existing Design of Experiments (DOE) methods. Solutions are surveyed for a range of core issues in experimental design including: the incorporation of prior knowledge, high dimensional optimisation, constraints, batch evaluation, multiple objectives, multi-fidelity data, and mixed variable types.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/QHL64NPS/Greenhill et al. - 2020 - Bayesian Optimization for Adaptive Experimental De.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{greff17_NeuralExpectationMaximization,
  title = {Neural {{Expectation Maximization}}},
  author = {Greff, Klaus and {van Steenkiste}, Sjoerd and Schmidhuber, J{\"u}rgen},
  year = {2017},
  month = nov,
  abstract = {Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network. Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities. We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects. We demonstrate that the learned representations are useful for next-step prediction.},
  archivePrefix = {arXiv},
  eprint = {1708.03498},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/greff et al_2017_neural expectation maximization.pdf;/home/trung/Zotero/storage/ZJPSPMHW/1708.html},
  journal = {arXiv:1708.03498 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{gregor18_TemporalDifferenceVariational,
  title = {Temporal {{Difference Variational Auto}}-{{Encoder}}},
  author = {Gregor, Karol and Papamakarios, George and Besse, Frederic and Buesing, Lars and Weber, Theophane},
  year = {2018},
  month = jun,
  abstract = {To act and plan in complex environments, we posit that agents should have a mental simulator of the world with three characteristics: (a) it should build an abstract state representing the condition of the world; (b) it should form a belief which represents uncertainty on the world; (c) it should go beyond simple step-by-step simulation, and exhibit temporal abstraction. Motivated by the absence of a model satisfying all these requirements, we propose TD-VAE, a generative sequence model that learns representations containing explicit beliefs about states several steps into the future, and that can be rolled out directly without single-step transitions. TD-VAE is trained on pairs of temporally separated time points, using an analogue of temporal difference learning used in reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1806.03107},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gregor et al_2018_temporal difference variational auto-encoder.pdf;/home/trung/Zotero/storage/ELHF3ZX2/1806.html},
  journal = {arXiv:1806.03107 [cs, stat]},
  keywords = {Computer Science - Machine Learning,sequential,Statistics - Machine Learning,temporal,variational},
  primaryClass = {cs, stat}
}

@article{gresele00_IncompleteRosettaStone,
  title = {The {{Incomplete Rosetta Stone Problem}}: {{Identifiability Results}} for {{Multi}}-{{View Nonlinear ICA}}},
  author = {Gresele, Luigi and Rubenstein, Paul K and Mehrjou, Arash and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  pages = {11},
  abstract = {We consider the problem of recovering a common latent source with independent components from multiple views. This applies to settings in which a variable is measured with multiple experimental modalities, and where the goal is to synthesize the disparate measurements into a single unified representation. We consider the case that the observed views are a nonlinear mixing of component-wise corruptions of the sources. When the views are considered separately, this reduces to nonlinear Independent Component Analysis (ICA) for which it is provably impossible to undo the mixing. We present novel identifiability proofs that this is possible when the multiple views are considered jointly, showing that the mixing can theoretically be undone using function approximators such as deep neural networks. In contrast to known identifiability results for nonlinear ICA, we prove that independent latent sources with arbitrary mixing can be recovered as long as multiple, sufficiently different noisy views are available.},
  file = {/home/trung/GoogleDrive/Zotero/gresele et al_the incomplete rosetta stone problem.pdf},
  language = {en}
}

@article{gresele19_IncompleteRosettaStone,
  title = {The {{Incomplete Rosetta Stone Problem}}: {{Identifiability Results}} for {{Multi}}-{{View Nonlinear ICA}}},
  shorttitle = {The {{Incomplete Rosetta Stone Problem}}},
  author = {Gresele, Luigi and Rubenstein, Paul K. and Mehrjou, Arash and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = may,
  abstract = {We consider the problem of recovering a common latent source with independent components from multiple views. This applies to settings in which a variable is measured with multiple experimental modalities, and where the goal is to synthesize the disparate measurements into a single unified representation. We consider the case that the observed views are a nonlinear mixing of component-wise corruptions of the sources. When the views are considered separately, this reduces to nonlinear Independent Component Analysis (ICA) for which it is provably impossible to undo the mixing. We present novel identifiability proofs that this is possible when the multiple views are considered jointly, showing that the mixing can theoretically be undone using function approximators such as deep neural networks. In contrast to known identifiability results for nonlinear ICA, we prove that independent latent sources with arbitrary mixing can be recovered as long as multiple, sufficiently different noisy views are available.},
  archivePrefix = {arXiv},
  eprint = {1905.06642},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gresele et al_2019_the incomplete rosetta stone problem.pdf;/home/trung/Zotero/storage/KXMJIE73/1905.html},
  journal = {arXiv:1905.06642 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{gresele19_IncompleteRosettaStonea,
  title = {The {{Incomplete Rosetta Stone Problem}}: {{Identifiability Results}} for {{Multi}}-{{View Nonlinear ICA}}},
  shorttitle = {The {{Incomplete Rosetta Stone Problem}}},
  author = {Gresele, Luigi and Rubenstein, Paul K. and Mehrjou, Arash and Locatello, Francesco and Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = aug,
  abstract = {We consider the problem of recovering a common latent source with independent components from multiple views. This applies to settings in which a variable is measured with multiple experimental modalities, and where the goal is to synthesize the disparate measurements into a single unified representation. We consider the case that the observed views are a nonlinear mixing of component-wise corruptions of the sources. When the views are considered separately, this reduces to nonlinear Independent Component Analysis (ICA) for which it is provably impossible to undo the mixing. We present novel identifiability proofs that this is possible when the multiple views are considered jointly, showing that the mixing can theoretically be undone using function approximators such as deep neural networks. In contrast to known identifiability results for nonlinear ICA, we prove that independent latent sources with arbitrary mixing can be recovered as long as multiple, sufficiently different noisy views are available.},
  archivePrefix = {arXiv},
  eprint = {1905.06642},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/IHSR8AVB/Gresele et al. - 2019 - The Incomplete Rosetta Stone Problem Identifiabil.pdf},
  journal = {arXiv:1905.06642 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{gretton00_Interpretablecomparisondistributions,
  title = {Interpretable Comparison of Distributions and Models},
  author = {Gretton, Arthur and Sutherland, Dougal and Jitkrittum, Wittawat},
  pages = {306},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/gretton et al_interpretable comparison of distributions and models.pdf},
  language = {en}
}

@article{gretton08_KernelMethodTwoSample,
  title = {A {{Kernel Method}} for the {{Two}}-{{Sample Problem}}},
  author = {Gretton, Arthur and Borgwardt, Karsten and Rasch, Malte J. and Scholkopf, Bernhard and Smola, Alexander J.},
  year = {2008},
  month = may,
  abstract = {We propose a framework for analyzing and comparing distributions, allowing us to design statistical tests to determine if two samples are drawn from different distributions. Our test statistic is the largest difference in expectations over functions in the unit ball of a reproducing kernel Hilbert space (RKHS). We present two tests based on large deviation bounds for the test statistic, while a third is based on the asymptotic distribution of this statistic. The test statistic can be computed in quadratic time, although efficient linear time approximations are available. Several classical metrics on distributions are recovered when the function space used to compute the difference in expectations is allowed to be more general (eg. a Banach space). We apply our two-sample tests to a variety of problems, including attribute matching for databases using the Hungarian marriage method, where they perform strongly. Excellent performance is also obtained when comparing distributions over graphs, for which these are the first such tests.},
  annotation = {ZSCC: 0000120},
  archivePrefix = {arXiv},
  eprint = {0805.2368},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gretton et al_2008_a kernel method for the two-sample problem.pdf},
  journal = {arXiv:0805.2368 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{greydanus19_HamiltonianNeuralNetworks,
  title = {Hamiltonian {{Neural Networks}}},
  author = {Greydanus, Sam and Dzamba, Misko and Yosinski, Jason},
  year = {2019},
  month = sep,
  abstract = {Even though neural networks enjoy widespread use, they still struggle to learn the basic laws of physics. How might we endow them with better inductive biases? In this paper, we draw inspiration from Hamiltonian mechanics to train models that learn and respect exact conservation laws in an unsupervised manner. We evaluate our models on problems where conservation of energy is important, including the two-body problem and pixel observations of a pendulum. Our model trains faster and generalizes better than a regular neural network. An interesting side effect is that our model is perfectly reversible in time.},
  archivePrefix = {arXiv},
  eprint = {1906.01563},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/greydanus et al_2019_hamiltonian neural networks.pdf},
  journal = {arXiv:1906.01563 [cs]},
  primaryClass = {cs}
}

@article{griesar02_Nicotineeffectsalertness,
  title = {Nicotine Effects on Alertness and Spatial Attention in Non-Smokers},
  author = {Griesar, William S. and Zajdel, Daniel P. and Oken, Barry S.},
  year = {2002},
  month = may,
  volume = {4},
  pages = {185--194},
  issn = {1462-2203},
  doi = {10.1080/14622200210123617},
  abstract = {Nicotine reportedly improves covert orienting of spatial attention, but enhanced alertness may also play a role. The present study explored nicotine effects on measures of spatial attention and alertness in non-smokers. Nicotine was delivered to 17 non-smokers (data from 12 subjects were analyzed) by a 7-mg transdermal patch (one patch in a low-nicotine condition; two patches in a high-nicotine condition). We examined nicotine's effects on spatial attention using a covert orienting task with central, predictive cue stimuli. Nicotine effects on alertness were examined with EEG and subjective questionnaires. Blood was drawn and serum levels of nicotine are reported. Nicotine decreased overall reaction times in the covert orienting task. There was no change in the validity effect, the reaction time difference between validly and invalidly cued targets. However, nicotine significantly improved both EEG and self-rated measures of alertness. We conclude that nicotine increases alertness in non-smokers, but we found no improvement in spatial attention using a covert orienting task.},
  journal = {Nicotine \& Tobacco Research: Official Journal of the Society for Research on Nicotine and Tobacco},
  language = {eng},
  number = {2},
  pmid = {12028851}
}

@article{grill20_Bootstrapyourown,
  title = {Bootstrap Your Own Latent: {{A}} New Approach to Self-Supervised {{Learning}}},
  shorttitle = {Bootstrap Your Own Latent},
  author = {Grill, Jean-Bastien and Strub, Florian and Altch{\'e}, Florent and Tallec, Corentin and Richemond, Pierre H. and Buchatskaya, Elena and Doersch, Carl and Pires, Bernardo Avila and Guo, Zhaohan Daniel and Azar, Mohammad Gheshlaghi and Piot, Bilal and Kavukcuoglu, Koray and Munos, R{\'e}mi and Valko, Michal},
  year = {2020},
  month = sep,
  abstract = {We introduce Bootstrap Your Own Latent (BYOL), a new approach to self-supervised image representation learning. BYOL relies on two neural networks, referred to as online and target networks, that interact and learn from each other. From an augmented view of an image, we train the online network to predict the target network representation of the same image under a different augmented view. At the same time, we update the target network with a slow-moving average of the online network. While state-of-the art methods rely on negative pairs, BYOL achieves a new state of the art without them. BYOL reaches \$74.3\textbackslash\%\$ top-1 classification accuracy on ImageNet using a linear evaluation with a ResNet-50 architecture and \$79.6\textbackslash\%\$ with a larger ResNet. We show that BYOL performs on par or better than the current state of the art on both transfer and semi-supervised benchmarks. Our implementation and pretrained models are given on GitHub.},
  archivePrefix = {arXiv},
  eprint = {2006.07733},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/grill et al_2020_bootstrap your own latent.pdf},
  journal = {arXiv:2006.07733 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{gritsenko20_SpectralEnergyDistance,
  title = {A {{Spectral Energy Distance}} for {{Parallel Speech Synthesis}}},
  author = {Gritsenko, Alexey A. and Salimans, Tim and van den Berg, Rianne and Snoek, Jasper and Kalchbrenner, Nal},
  year = {2020},
  month = oct,
  abstract = {Speech synthesis is an important practical generative modeling problem that has seen great progress over the last few years, with likelihood-based autoregressive neural models now outperforming traditional concatenative systems. A downside of such autoregressive models is that they require executing tens of thousands of sequential operations per second of generated audio, making them ill-suited for deployment on specialized deep learning hardware. Here, we propose a new learning method that allows us to train highly parallel models of speech, without requiring access to an analytical likelihood function. Our approach is based on a generalized energy distance between the distributions of the generated and real audio. This spectral energy distance is a proper scoring rule with respect to the distribution over magnitude-spectrograms of the generated waveform audio and offers statistical consistency guarantees. The distance can be calculated from minibatches without bias, and does not involve adversarial learning, yielding a stable and consistent method for training implicit generative models. Empirically, we achieve state-of-the-art generation quality among implicit generative models, as judged by the recently-proposed cFDSD metric. When combining our method with adversarial techniques, we also improve upon the recently-proposed GAN-TTS model in terms of Mean Opinion Score as judged by trained human evaluators.},
  archivePrefix = {arXiv},
  eprint = {2008.01160},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gritsenko et al_2020_a spectral energy distance for parallel speech synthesis.pdf},
  journal = {arXiv:2008.01160 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{gronbech00_Deeplearningsinglecell,
  title = {Deep Learning for Single-Cell Transcript Counts},
  author = {Gr{\o}nbech, Christopher Heje and Vording, Maximillian Fornitz},
  pages = {65},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/grønbech et al_deep learning for single-cell transcript counts.pdf},
  language = {en}
}

@article{gronbech18_scVAEVariationalautoencoders,
  title = {{{scVAE}}: {{Variational}} Auto-Encoders for Single-Cell Gene Expression Data},
  shorttitle = {{{scVAE}}},
  author = {Gr{\o}nbech, Christopher Heje and Vording, Maximillian Fornitz and Timshel, Pascal N and S{\o}nderby, Casper Kaae and Pers, Tune Hannes and Winther, Ole},
  year = {2018},
  month = may,
  doi = {10.1101/318295},
  abstract = {We propose a novel variational auto-encoder-based method for analysis of single-cell RNA sequencing (scRNA-seq) data. It avoids data preprocessing by using raw count data as input and can robustly estimate the expected gene expression levels and a latent representation for each cell. We show for several scRNAseq data sets that our method outperforms recently proposed scRNA-seq methods in clustering cells. Our software tool scVAE has support for several count likelihood functions and a variant of the variational auto-encoder has a priori clustering in the latent space.},
  file = {/home/trung/GoogleDrive/Zotero/grønbech et al_2018_scvae.pdf},
  keywords = {czi},
  language = {en}
}

@article{grunwald04_Gametheorymaximum,
  title = {Game Theory, Maximum Entropy, Minimum Discrepancy and Robust {{Bayesian}} Decision Theory},
  author = {Gr{\"u}nwald, Peter D. and Dawid, A. Philip},
  year = {2004},
  month = aug,
  volume = {32},
  pages = {1367--1433},
  issn = {0090-5364},
  doi = {10.1214/009053604000000553},
  file = {/home/trung/GoogleDrive/Zotero/grünwald et al_2004_game theory, maximum entropy, minimum discrepancy and robust bayesian decision theory.pdf},
  journal = {The Annals of Statistics},
  keywords = {_tablet,favorite},
  language = {en},
  number = {4}
}

@article{gu19_DistributedMachineLearning,
  title = {Distributed {{Machine Learning}} on {{Mobile Devices}}: {{A Survey}}},
  shorttitle = {Distributed {{Machine Learning}} on {{Mobile Devices}}},
  author = {Gu, Renjie and Yang, Shuo and Wu, Fan},
  year = {2019},
  month = sep,
  abstract = {In recent years, mobile devices have gained increasingly development with stronger computation capability and larger storage. Some of the computation-intensive machine learning and deep learning tasks can now be run on mobile devices. To take advantage of the resources available on mobile devices and preserve users' privacy, the idea of mobile distributed machine learning is proposed. It uses local hardware resources and local data to solve machine learning sub-problems on mobile devices, and only uploads computation results instead of original data to contribute to the optimization of the global model. This architecture can not only relieve computation and storage burden on servers, but also protect the users' sensitive information. Another benefit is the bandwidth reduction, as various kinds of local data can now participate in the training process without being uploaded to the server. In this paper, we provide a comprehensive survey on recent studies of mobile distributed machine learning. We survey a number of widely-used mobile distributed machine learning methods. We also present an in-depth discussion on the challenges and future directions in this area. We believe that this survey can demonstrate a clear overview of mobile distributed machine learning and provide guidelines on applying mobile distributed machine learning to real applications.},
  archivePrefix = {arXiv},
  eprint = {1909.08329},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gu et al_2019_distributed machine learning on mobile devices.pdf;/home/trung/Zotero/storage/LGA4HH49/1909.html},
  journal = {arXiv:1909.08329 [cs, stat]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,distributed,I.2.6,mobile,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{gu20_ImprovedDeepNeural,
  title = {An {{Improved Deep Neural Network}} for {{Modeling Speaker Characteristics}} at {{Different Temporal Scales}}},
  author = {Gu, Bin and Guo, Wu},
  year = {2020},
  month = jan,
  abstract = {This paper presents an improved deep embedding learning method based on convolutional neural network (CNN) for text-independent speaker verification. Two improvements are proposed for x-vector embedding learning: (1) Multi-scale convolution (MSCNN) is adopted in frame-level layers to capture complementary speaker information in different receptive fields. (2) A Baum-Welch statistics attention (BWSA) mechanism is applied in pooling-layer, which can integrate more useful long-term speaker characteristics in the temporal pooling layer. Experiments are carried out on the NIST SRE16 evaluation set. The results demonstrate the effectiveness of MSCNN and show the proposed BWSA can further improve the performance of the DNN embedding system},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.04584},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gu et al_2020_an improved deep neural network for modeling speaker characteristics at different temporal scales.pdf;/home/trung/Zotero/storage/JD59GBEI/2001.html},
  journal = {arXiv:2001.04584 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing},
  primaryClass = {cs, eess}
}

@inproceedings{guan19_DeepUnifiedUnderstanding,
  title = {Towards a {{Deep}} and {{Unified Understanding}} of {{Deep Neural Models}} in {{NLP}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Guan, Chaoyu and Wang, Xiting and Zhang, Quanshi and Chen, Runjin and He, Di and Xie, Xing},
  year = {2019},
  month = may,
  pages = {2454--2463},
  abstract = {We define a unified information-based measure to provide quantitative explanations on how intermediate layers of deep Natural Language Processing (NLP) models leverage information of input words. O...},
  file = {/home/trung/GoogleDrive/Zotero/guan et al_2019_towards a deep and unified understanding of deep neural models in nlp.pdf;/home/trung/Zotero/storage/GF63UDP8/guan19a.html},
  keywords = {nlp},
  language = {en}
}

@article{guedj19_PrimerPACBayesianLearning,
  title = {A {{Primer}} on {{PAC}}-{{Bayesian Learning}}},
  author = {Guedj, Benjamin},
  year = {2019},
  month = may,
  abstract = {Generalised Bayesian learning algorithms are increasingly popular in machine learning, due to their PAC generalisation properties and flexibility. The present paper aims at providing a self-contained survey on the resulting PAC-Bayes framework and some of its main theoretical and algorithmic developments.},
  annotation = {ZSCC: 0000011},
  archivePrefix = {arXiv},
  eprint = {1901.05353},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/guedj_2019_a primer on pac-bayesian learning.pdf;/home/trung/Zotero/storage/LUYMV9FH/1901.html},
  journal = {arXiv:1901.05353 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{guerguiev19_Spikebasedcausalinference,
  title = {Spike-Based Causal Inference for Weight Alignment},
  author = {Guerguiev, Jordan and Kording, Konrad P. and Richards, Blake A.},
  year = {2019},
  month = oct,
  abstract = {In artificial neural networks trained with gradient descent, the weights used for processing stimuli are also used during backward passes to calculate gradients. For the real brain to approximate gradients, gradient information would have to be propagated separately, such that one set of synaptic weights is used for processing and another set is used for backward passes. This produces the so-called "weight transport problem" for biological models of learning, where the backward weights used to calculate gradients need to mirror the forward weights used to process stimuli. This weight transport problem has been considered so hard that popular proposals for biological learning assume that the backward weights are simply random, as in the feedback alignment algorithm. However, such random weights do not appear to work well for large networks. Here we show how the discontinuity introduced in a spiking system can lead to a solution to this problem. The resulting algorithm is a special case of an estimator used for causal inference in econometrics, regression discontinuity design. We show empirically that this algorithm rapidly makes the backward weights approximate the forward weights. As the backward weights become correct, this improves learning performance over feedback alignment on tasks such as Fashion-MNIST and CIFAR-10. Our results demonstrate that a simple learning rule in a spiking network can allow neurons to produce the right backward connections and thus solve the weight transport problem.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1910.01689},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/guerguiev et al_2019_spike-based causal inference for weight alignment.pdf;/home/trung/Zotero/storage/Q99UYSB5/1910.html},
  journal = {arXiv:1910.01689 [cs, q-bio]},
  keywords = {causal,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  primaryClass = {cs, q-bio}
}

@article{gui18_LongShortTermMemory,
  title = {Long {{Short}}-{{Term Memory}} with {{Dynamic Skip Connections}}},
  author = {Gui, Tao and Zhang, Qi and Zhao, Lujun and Lin, Yaosong and Peng, Minlong and Gong, Jingjing and Huang, Xuanjing},
  year = {2018},
  month = nov,
  abstract = {In recent years, long short-term memory (LSTM) has been successfully used to model sequential data of variable length. However, LSTM can still experience difficulty in capturing long-term dependencies. In this work, we tried to alleviate this problem by introducing a dynamic skip connection, which can learn to directly connect two dependent words. Since there is no dependency information in the training data, we propose a novel reinforcement learning-based method to model the dependency relationship and connect dependent words. The proposed model computes the recurrent transition functions based on the skip connections, which provides a dynamic skipping advantage over RNNs that always tackle entire sentences sequentially. Our experimental results on three natural language processing tasks demonstrate that the proposed method can achieve better performance than existing methods. In the number prediction experiment, the proposed model outperformed LSTM with respect to accuracy by nearly 20\%.},
  archivePrefix = {arXiv},
  eprint = {1811.03873},
  eprinttype = {arxiv},
  journal = {arXiv:1811.03873 [cs]},
  keywords = {attention,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{guimaraes13_Sesameflaxseedoil,
  title = {Sesame and Flaxseed Oil: Nutritional Quality and Effects on Serum Lipids and Glucose in Rats},
  shorttitle = {Sesame and Flaxseed Oil},
  author = {Guimar{\~a}es, Rita de C{\'a}ssia Avellaneda and Macedo, Maria L{\'i}gia Rodrigues and Munhoz, Cl{\'a}udia Leite and Filiu, Wander and Viana, Lu{\'i}s Henrique and Nozaki, Vanessa Ta{\'i}s and Hiane, Priscila Aiko},
  year = {2013},
  month = mar,
  volume = {33},
  pages = {209--217},
  publisher = {{SBCTA}},
  issn = {0101-2061},
  doi = {10.1590/S0101-20612013005000029},
  file = {/home/trung/GoogleDrive/Zotero/guimarães et al_2013_sesame and flaxseed oil.pdf},
  journal = {Food Science and Technology},
  language = {en},
  number = {1}
}

@article{guimera20_Bayesianmachinescientist,
  title = {A {{Bayesian}} Machine Scientist to Aid in the Solution of Challenging Scientific Problems},
  author = {Guimer{\`a}, Roger and Reichardt, Ignasi and {Aguilar-Mogas}, Antoni and Massucci, Francesco A. and Miranda, Manuel and Pallar{\`e}s, Jordi and {Sales-Pardo}, Marta},
  year = {2020},
  month = jan,
  volume = {6},
  pages = {eaav6971},
  issn = {2375-2548},
  doi = {10.1126/sciadv.aav6971},
  abstract = {Closed-form, interpretable mathematical models have been instrumental for advancing our understanding of the world; with the data revolution, we may now be in a position to uncover new such models for many systems from physics to the social sciences. However, to deal with increasing amounts of data, we need ``machine scientists'' that are able to extract these models automatically from data. Here, we introduce a Bayesian machine scientist, which establishes the plausibility of models using explicit approximations to the exact marginal posterior over models and establishes its prior expectations about models by learning from a large empirical corpus of mathematical expressions. It explores the space of models using Markov chain Monte Carlo. We show that this approach uncovers accurate models for synthetic and real data and provides out-of-sample predictions that are more accurate than those of existing approaches and of other nonparametric methods.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/QB669RKF/Guimerà et al. - 2020 - A Bayesian machine scientist to aid in the solutio.pdf},
  journal = {Science Advances},
  language = {en},
  number = {5}
}

@article{guizzo00_EssentialMessageClaude,
  title = {The {{Essential Message}}: {{Claude Shannon}} and the {{Making}} of {{Information Theory}}},
  author = {Guizzo, Erico Marui},
  pages = {77},
  abstract = {In 1948, Claude Shannon, a young engineer and mathematician working at the Bell Telephone Laboratories, published "A Mathematical Theory of Communication," a seminal paper that marked the birth of information theory. In that paper, Shannon defined what the once fuzzy concept of "information" meant for communication engineers and proposed a precise way to quantify it-in his theory, the fundamental unit of information is the bit. He also showed how data could be "compressed" before transmission and how virtually error-free communication could be achieved. The concepts Shannon developed in his paper are at the heart of today's digital information technology. CDs, DVDs, cell phones, fax machines, modems, computer networks, hard drives, memory chips, encryption schemes, MP3 music, optical communication, high-definition television-all these things embody many of Shannon's ideas and others inspired by him.},
  file = {/home/trung/GoogleDrive/Zotero/guizzo_the essential message.pdf},
  keywords = {favorite},
  language = {en}
}

@article{gulrajani16_PixelVAELatentVariable,
  title = {{{PixelVAE}}: {{A Latent Variable Model}} for {{Natural Images}}},
  shorttitle = {{{PixelVAE}}},
  author = {Gulrajani, Ishaan and Kumar, Kundan and Ahmed, Faruk and Taiga, Adrien Ali and Visin, Francesco and Vazquez, David and Courville, Aaron},
  year = {2016},
  month = nov,
  abstract = {Natural image modeling is a landmark challenge of unsupervised learning. Variational Autoencoders (VAEs) learn a useful latent representation and model global structure well but have difficulty capturing small details. PixelCNN models details very well, but lacks a latent code and is difficult to scale for capturing large structures. We present PixelVAE, a VAE model with an autoregressive decoder based on PixelCNN. Our model requires very few expensive autoregressive layers compared to PixelCNN and learns latent codes that are more compressed than a standard VAE while still capturing most non-trivial structure. Finally, we extend our model to a hierarchy of latent variables at different scales. Our model achieves state-of-the-art performance on binarized MNIST, competitive performance on 64x64 ImageNet, and high-quality samples on the LSUN bedrooms dataset.},
  archivePrefix = {arXiv},
  eprint = {1611.05013},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2016/false;/home/trung/Zotero/storage/PMVCBLSQ/1611.html},
  journal = {arXiv:1611.05013 [cs]},
  keywords = {Computer Science - Machine Learning,discrete,variational},
  primaryClass = {cs}
}

@techreport{gunady19_scGAINSingleCell,
  title = {{{scGAIN}}: {{Single Cell RNA}}-Seq {{Data Imputation}} Using {{Generative Adversarial Networks}}},
  shorttitle = {{{scGAIN}}},
  author = {Gunady, Mohamed K. and Kancherla, Jayaram and Bravo, H{\'e}ctor Corrada and Feizi, Soheil},
  year = {2019},
  month = nov,
  institution = {{Bioinformatics}},
  doi = {10.1101/837302},
  abstract = {Single cell RNA sequencing (scRNA-seq) provides a rich view into the heterogeneity underlying a cell population. However single-cell data are usually noisy and very sparse due to the presence of dropout genes. In this work we propose an approach to impute missing gene expressions in single cell data using generative adversarial networks (GANs). By learning an approximate distribution of the data, our approach, scGAIN, can impute dropouts in simulated and real single cell data. The work in this paper discusses how to adopt GAIN training model into the domain of imputing single cell data. Experiments show that scGAIN gives competitive results compared to the state-of-the-art approaches while showing superiority in various aspects in simulation and real data. Imputation by scGAIN successfully recovers the underlying clustering of different subpopulations, provides sharp estimates around true mean expressions and increase the correspondence with matched bulk RNAseq experiments.},
  file = {/home/trung/GoogleDrive/Zotero/gunady et al_2019_scgain.pdf},
  language = {en},
  type = {Preprint}
}

@incollection{gunasekar17_ImplicitRegularizationMatrix,
  title = {Implicit {{Regularization}} in {{Matrix Factorization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Gunasekar, Suriya and Woodworth, Blake E and Bhojanapalli, Srinadh and Neyshabur, Behnam and Srebro, Nati},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {6151--6159},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/gunasekar et al_2017_implicit regularization in matrix factorization.pdf}
}

@incollection{gunasekar18_ImplicitBiasGradient,
  title = {Implicit {{Bias}} of {{Gradient Descent}} on {{Linear Convolutional Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Gunasekar, Suriya and Lee, Jason D and Soudry, Daniel and Srebro, Nati},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {9461--9471},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/gunasekar et al_2018_implicit bias of gradient descent on linear convolutional networks.pdf}
}

@article{guo17_BoostingVariationalInference,
  title = {Boosting {{Variational Inference}}},
  author = {Guo, Fangjian and Wang, Xiangyu and Fan, Kai and Broderick, Tamara and Dunson, David B.},
  year = {2017},
  month = mar,
  abstract = {Variational inference (VI) provides fast approximations of a Bayesian posterior in part because it formulates posterior approximation as an optimization problem: to find the closest distribution to the exact posterior over some family of distributions. For practical reasons, the family of distributions in VI is usually constrained so that it does not include the exact posterior, even as a limit point. Thus, no matter how long VI is run, the resulting approximation will not approach the exact posterior. We propose to instead consider a more flexible approximating family consisting of all possible finite mixtures of a parametric base distribution (e.g., Gaussian). For efficient inference, we borrow ideas from gradient boosting to develop an algorithm we call boosting variational inference (BVI). BVI iteratively improves the current approximation by mixing it with a new component from the base distribution family and thereby yields progressively more accurate posterior approximations as more computing time is spent. Unlike a number of common VI variants including mean-field VI, BVI is able to capture multimodality, general posterior covariance, and nonstandard posterior shapes.},
  archivePrefix = {arXiv},
  eprint = {1611.05559},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/guo et al_2017_boosting variational inference.pdf},
  journal = {arXiv:1611.05559 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{guo19_MAANetMultiviewAware,
  title = {{{MAANet}}: {{Multi}}-View {{Aware Attention Networks}} for {{Image Super}}-{{Resolution}}},
  shorttitle = {{{MAANet}}},
  author = {Guo, Jingcai and Ma, Shiheng and Guo, Song},
  year = {2019},
  month = apr,
  abstract = {In most recent years, deep convolutional neural networks (DCNNs) based image super-resolution (SR) has gained increasing attention in multimedia and computer vision communities, focusing on restoring the high-resolution (HR) image from a low-resolution (LR) image. However, one nonnegligible flaw of DCNNs based methods is that most of them are not able to restore high-resolution images containing sufficient high-frequency information from low-resolution images with low-frequency information redundancy. Worse still, as the depth of DCNNs increases, the training easily encounters the problem of vanishing gradients, which makes the training more difficult. These problems hinder the effectiveness of DCNNs in image SR task. To solve these problems, we propose the Multi-view Aware Attention Networks (MAANet) for image SR task. Specifically, we propose the local aware (LA) and global aware (GA) attention to deal with LR features in unequal manners, which can highlight the high-frequency components and discriminate each feature from LR images in the local and the global views, respectively. Furthermore, we propose the local attentive residual-dense (LARD) block, which combines the LA attention with multiple residual and dense connections, to fit a deeper yet easy to train architecture. The experimental results show that our proposed approach can achieve remarkable performance compared with other state-of-the-art methods.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1904.06252},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/guo et al_2019_maanet.pdf},
  journal = {arXiv:1904.06252 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{guo20_CardiovascularImplicationsFatal,
  title = {Cardiovascular {{Implications}} of {{Fatal Outcomes}} of {{Patients With Coronavirus Disease}} 2019 ({{COVID}}-19)},
  author = {Guo, Tao and Fan, Yongzhen and Chen, Ming and Wu, Xiaoyan and Zhang, Lin and He, Tao and Wang, Hairong and Wan, Jing and Wang, Xinghuan and Lu, Zhibing},
  year = {2020},
  month = mar,
  issn = {2380-6583},
  doi = {10.1001/jamacardio.2020.1017},
  abstract = {OBJECTIVE To evaluate the association of underlying cardiovascular disease (CVD) and myocardial injury with fatal outcomes in patients with COVID-19. DESIGN, SETTING, AND PARTICIPANTS This retrospective single-center case series analyzed patients with COVID-19 at the Seventh Hospital of Wuhan City, China, from January 23, 2020, to February 23, 2020. Analysis began February 25, 2020. MAIN OUTCOMES AND MEASURES Demographic data, laboratory findings, comorbidities, and treatments were collected and analyzed in patients with and without elevation of troponin T (TnT) levels. RESULT Among 187 patients with confirmed COVID-19, 144 patients (77\%) were discharged and 43 patients (23\%) died. The mean (SD) age was 58.50 (14.66) years. Overall, 66 (35.3\%) had underlying CVD including hypertension, coronary heart disease, and cardiomyopathy, and 52 (27.8\%) exhibited myocardial injury as indicated by elevated TnT levels. The mortality during hospitalization was 7.62\% (8 of 105) for patients without underlying CVD and normal TnT levels, 13.33\% (4 of 30) for those with underlying CVD and normal TnT levels, 37.50\% (6 of 16) for those without underlying CVD but elevated TnT levels, and 69.44\% (25 of 36) for those with underlying CVD and elevated TnTs. Patients with underlying CVD were more likely to exhibit elevation of TnT levels compared with the patients without CVD (36 [54.5\%] vs 16 [13.2\%]). Plasma TnT levels demonstrated a high and significantly positive linear correlation with plasma high-sensitivity C-reactive protein levels ({$\beta$} = 0.530, P {$<$} .001) and N-terminal pro\textendash brain natriuretic peptide (NT-proBNP) levels ({$\beta$} = 0.613, P {$<$} .001). Plasma TnT and NT-proBNP levels during hospitalization (median [interquartile range (IQR)], 0.307 [0.094-0.600]; 1902.00 [728.35-8100.00]) and impending death (median [IQR], 0.141 [0.058-0.860]; 5375 [1179.50-25695.25]) increased significantly compared with admission values (median [IQR], 0.0355 [0.015-0.102]; 796.90 [401.93-1742.25]) in patients who died (P = .001; P {$<$} .001), while no significant dynamic changes of TnT (median [IQR], 0.010 [0.007-0.019]; 0.013 [0.007-0.022]; 0.011 [0.007-0.016]) and NT-proBNP (median [IQR], 352.20 [174.70-636.70]; 433.80 [155.80-1272.60]; 145.40 [63.4-526.50]) was observed in survivors (P = .96; P = .16). During hospitalization, patients with elevated TnT levels had more frequent malignant arrhythmias, and the use of glucocorticoid therapy (37 [71.2\%] vs 69 [51.1\%]) and mechanical ventilation (41 [59.6\%] vs 14 [10.4\%]) were higher compared with patients with normal TnT levels. The mortality rates of patients with and without use of angiotensin-converting enzyme inhibitors/angiotensin receptor blockers was 36.8\% (7 of 19) and 25.6\% (43 of 168). CONCLUSIONS AND RELEVANCE Myocardial injury is significantly associated with fatal outcome of COVID-19, while the prognosis of patients with underlying CVD but without myocardial injury is relatively favorable. Myocardial injury is associated with cardiac dysfunction and arrhythmias. Inflammation may be a potential mechanism for myocardial injury. Aggressive treatment may be considered for patients at high risk of myocardial injury.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/GoogleDrive/Zotero/guo et al_2020_cardiovascular implications of fatal outcomes of patients with coronavirus disease 2019 (covid-19).pdf},
  journal = {JAMA Cardiology},
  language = {en}
}

@article{guo20_GraphCodeBERTPretrainingCode,
  title = {{{GraphCodeBERT}}: {{Pre}}-Training {{Code Representations}} with {{Data Flow}}},
  shorttitle = {{{GraphCodeBERT}}},
  author = {Guo, Daya and Ren, Shuo and Lu, Shuai and Feng, Zhangyin and Tang, Duyu and Liu, Shujie and Zhou, Long and Duan, Nan and Yin, Jian and Jiang, Daxin and Zhou, Ming},
  year = {2020},
  month = sep,
  abstract = {Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of "where-the-value-comes-from" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.},
  archivePrefix = {arXiv},
  eprint = {2009.08366},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/guo et al_2020_graphcodebert.pdf},
  journal = {arXiv:2009.08366 [cs]},
  primaryClass = {cs}
}

@article{guo20_interpretableneuralnetwork,
  title = {An Interpretable Neural Network Model through Piecewise Linear Approximation},
  author = {Guo, Mengzhuo and Zhang, Qingpeng and Liao, Xiuwu and Zeng, Daniel Dajun},
  year = {2020},
  month = jan,
  abstract = {Most existing interpretable methods explain a black-box model in a post-hoc manner, which uses simpler models or data analysis techniques to interpret the predictions after the model is learned. However, they (a) may derive contradictory explanations on the same predictions given different methods and data samples, and (b) focus on using simpler models to provide higher descriptive accuracy at the sacrifice of prediction accuracy. To address these issues, we propose a hybrid interpretable model that combines a piecewise linear component and a nonlinear component. The first component describes the explicit feature contributions by piecewise linear approximation to increase the expressiveness of the model. The other component uses a multi-layer perceptron to capture feature interactions and implicit nonlinearity, and increase the prediction performance. Different from the post-hoc approaches, the interpretability is obtained once the model is learned in the form of feature shapes. We also provide a variant to explore higher-order interactions among features to demonstrate that the proposed model is flexible for adaptation. Experiments demonstrate that the proposed model can achieve good interpretability by describing feature shapes while maintaining state-of-theart accuracy.},
  archivePrefix = {arXiv},
  eprint = {2001.07119},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/PLA7ZYZI/Guo et al. - 2020 - An interpretable neural network model through piec.pdf},
  journal = {arXiv:2001.07119 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{guo20_Salviamiltiorrhizaimproves,
  title = {Salvia Miltiorrhiza Improves {{Alzheimer}}'s Disease},
  author = {Guo, Ying and Dong, Xing and Zhang, Renyan and Zhong, Yanmei and Yang, Peng and Zhang, San Ying},
  year = {2020},
  month = sep,
  volume = {99},
  issn = {0025-7974},
  doi = {10.1097/MD.0000000000021924},
  abstract = {Background: Alzheimer's disease (AD) is an age-related neurodegenerative disease that is slowly becoming a global problem. Salvia miltiorrhiza (SM) has a history of thousands of years of use in China. In recent years, SM has been reported to have the effect of improving Alzheimer's disease. However, there is no systematic review of its efficacy and safety yet. Therefore, we propose a systematic review to evaluate the efficacy and safety of SM for AD patients. Methods: Six databases will be searched: China National Knowledge Infrastructure (CNKI), China Biological Medicine (CBM), China Scientific Journals Database (CSJD), Wanfang database, PubMed, and EMBASE. The information is searched from January 2010 to July 2020. Languages are limited to English and Chinese. The primary outcomes include changes in the Mini-Mental State Examination (MMSE), Alzheimer's Disease Assessment Scale-cognitive subscale (ADAS-Cog) and Activities of Daily Living scale (ADL). Additional outcomes include clinical effective rate and adverse event rate. The Grading of Recommendations Assessment, Development and Evaluation (GRADE) system will be used to assess the strength of the evidence. Results: This systematic review will evaluate the efficacy and safety of SM in the treatment of Alzheimer's disease. Conclusion: This systematic review provides evidence as to whether SM is effective and safe for Alzheimer's disease patients. Systematic review registration: INPLASY202070066.},
  file = {/home/trung/GoogleDrive/Zotero/guo et al_2020_salvia miltiorrhiza improves alzheimer's disease.pdf},
  journal = {Medicine},
  number = {36},
  pmcid = {PMC7478559},
  pmid = {32899026}
}

@article{guo20_SurveyKnowledgeGraphBased,
  title = {A {{Survey}} on {{Knowledge Graph}}-{{Based Recommender Systems}}},
  author = {Guo, Qingyu and Zhuang, Fuzhen and Qin, Chuan and Zhu, Hengshu and Xie, Xing and Xiong, Hui and He, Qing},
  year = {2020},
  month = feb,
  abstract = {To solve the information explosion problem and enhance user experience in various online applications, recommender systems have been developed to model users preferences. Although numerous efforts have been made toward more personalized recommendations, recommender systems still suffer from several challenges, such as data sparsity and cold start. In recent years, generating recommendations with the knowledge graph as side information has attracted considerable interest. Such an approach can not only alleviate the abovementioned issues for a more accurate recommendation, but also provide explanations for recommended items. In this paper, we conduct a systematical survey of knowledge graph-based recommender systems. We collect recently published papers in this field and summarize them from two perspectives. On the one hand, we investigate the proposed algorithms by focusing on how the papers utilize the knowledge graph for accurate and explainable recommendation. On the other hand, we introduce datasets used in these works. Finally, we propose several potential research directions in this field.},
  archivePrefix = {arXiv},
  eprint = {2003.00911},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/guo et al_2020_a survey on knowledge graph-based recommender systems.pdf},
  journal = {arXiv:2003.00911 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{guo20_VariationalAutoencoderOptimizing,
  title = {Variational {{Autoencoder With Optimizing Gaussian Mixture Model Priors}}},
  author = {Guo, Chunsheng and Zhou, Jialuo and Chen, Huahua and Ying, Na and Zhang, Jianwu and Zhou, Di},
  year = {2020},
  volume = {8},
  pages = {43992--44005},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2977671},
  abstract = {The latent variable prior of the variational autoencoder (VAE) often utilizes a standard Gaussian distribution because of the convenience in calculation, but has an underfitting problem. This paper proposes a variational autoencoder with optimizing Gaussian mixture model priors. This method utilizes a Gaussian mixture model to construct prior distribution, and utilizes the Kullback-Leibler (KL) distance between posterior and prior distribution to implement an iterative optimization of the prior distribution based on the data. The greedy algorithm is used to solve the KL distance for defining the approximate variational lower bound solution of the loss function, and for realizing the VAE with optimizing Gaussian mixture model priors. Compared with the standard VAE method, the proposed method obtains state-of-the-art results on MNIST, Omniglot, and Frey Face datasets, which shows that the VAE with optimizing Gaussian mixture model priors can learn a better model.},
  file = {/home/trung/GoogleDrive/Zotero/guo et al_2020_variational autoencoder with optimizing gaussian mixture model priors.pdf},
  journal = {IEEE Access}
}

@inproceedings{gupta17_CharacterizingImprovingStability,
  title = {Characterizing and {{Improving Stability}} in {{Neural Style Transfer}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Gupta, Agrim and Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  year = {2017},
  month = oct,
  pages = {4087--4096},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2017.438},
  abstract = {Recent progress in style transfer on images has focused on improving the quality of stylized images and speed of methods. However, real-time methods are highly unstable resulting in visible flickering when applied to videos. In this work we characterize the instability of these methods by examining the solution set of the style transfer objective. We show that the trace of the Gram matrix representing style is inversely related to the stability of the method. Then, we present a recurrent convolutional network for real-time video style transfer which incorporates a temporal consistency loss and overcomes the instability of prior methods. Our networks can be applied at any resolution, do not require optical flow at test time, and produce high quality, temporally consistent stylized videos in real-time.},
  file = {/home/trung/GoogleDrive/Zotero/gupta et al_2017_characterizing and improving stability in neural style transfer.pdf}
}

@article{gupta19_DocumentInformedNeural,
  title = {Document {{Informed Neural Autoregressive Topic Models}} with {{Distributional Prior}}},
  author = {Gupta, Pankaj and Chaudhary, Yatin and Buettner, Florian and Sch{\"u}tze, Hinrich},
  year = {2019},
  month = jan,
  abstract = {We address two challenges in topic models: (1) Context information around words helps in determining their actual meaning, e.g., "networks" used in the contexts "artificial neural networks" vs. "biological neuron networks". Generative topic models infer topic-word distributions, taking no or only little context into account. Here, we extend a neural autoregressive topic model to exploit the full context information around words in a document in a language modeling fashion. The proposed model is named as iDocNADE. (2) Due to the small number of word occurrences (i.e., lack of context) in short text and data sparsity in a corpus of few documents, the application of topic models is challenging on such texts. Therefore, we propose a simple and efficient way of incorporating external knowledge into neural autoregressive topic models: we use embeddings as a distributional prior. The proposed variants are named as DocNADEe and iDocNADEe. We present novel neural autoregressive topic model variants that consistently outperform state-of-the-art generative topic models in terms of generalization, interpretability (topic coherence) and applicability (retrieval and classification) over 7 long-text and 8 short-text datasets from diverse domains.},
  archivePrefix = {arXiv},
  eprint = {1809.06709},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gupta et al_2019_document informed neural autoregressive topic models with distributional prior.pdf},
  journal = {arXiv:1809.06709 [cs]},
  primaryClass = {cs}
}

@article{gupta19_NeuralModuleNetworks,
  title = {Neural {{Module Networks}} for {{Reasoning}} over {{Text}}},
  author = {Gupta, Nitish and Lin, Kevin and Roth, Dan and Singh, Sameer and Gardner, Matt},
  year = {2019},
  month = dec,
  abstract = {Answering compositional questions that require multiple steps of reasoning against text is challenging, especially when they involve discrete, symbolic operations. Neural module networks (NMNs) learn to parse such questions as executable programs composed of learnable modules, performing well on synthetic visual QA domains. However, we find that it is challenging to learn these models for non-synthetic questions on open-domain text, where a model needs to deal with the diversity of natural language and perform a broader range of reasoning. We extend NMNs by: (a) introducing modules that reason over a paragraph of text, performing symbolic reasoning (such as arithmetic, sorting, counting) over numbers and dates in a probabilistic and differentiable manner; and (b) proposing an unsupervised auxiliary loss to help extract arguments associated with the events in text. Additionally, we show that a limited amount of heuristically-obtained question program and intermediate module output supervision provides sufficient inductive bias for accurate learning. Our proposed model significantly outperforms state-of-the-art models on a subset of the DROP dataset that poses a variety of reasoning challenges that are covered by our modules.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.04971},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gupta et al_2019_neural module networks for reasoning over text.pdf;/home/trung/Zotero/storage/CT6R3ZEZ/1912.html},
  journal = {arXiv:1912.04971 [cs]},
  primaryClass = {cs}
}

@inproceedings{gupta20_Unsupervisedclusteringusing,
  title = {Unsupervised Clustering Using Pseudo-Semi-Supervised Learning},
  booktitle = {International Conference on Learning Representations},
  author = {Gupta, Divam and Ramjee, Ramachandran and Kwatra, Nipun and Sivathanu, Muthian},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/gupta et al_2020_unsupervised clustering using pseudo-semi-supervised learning.pdf}
}

@article{gustafsson20_HowTrainYour,
  title = {How to {{Train Your Energy}}-{{Based Model}} for {{Regression}}},
  author = {Gustafsson, Fredrik K. and Danelljan, Martin and Timofte, Radu and Sch{\"o}n, Thomas B.},
  year = {2020},
  month = aug,
  abstract = {Energy-based models (EBMs) have become increasingly popular within computer vision in recent years. While they are commonly employed for generative image modeling, recent work has applied EBMs also for regression tasks, achieving state-of-the-art performance on object detection and visual tracking. Training EBMs is however known to be challenging. While a variety of different techniques have been explored for generative modeling, the application of EBMs to regression is not a well-studied problem. How EBMs should be trained for best possible regression performance is thus currently unclear. We therefore accept the task of providing the first detailed study of this problem. To that end, we propose a simple yet highly effective extension of noise contrastive estimation, and carefully compare its performance to six popular methods from literature on the tasks of 1D regression and object detection. The results of this comparison suggest that our training method should be considered the go-to approach. We also apply our method to the visual tracking task, achieving state-of-the-art performance on five datasets. Notably, our tracker achieves 63.7\% AUC on LaSOT and 78.7\% Success on TrackingNet. Code is available at https://github.com/fregu856/ebms\_regression.},
  archivePrefix = {arXiv},
  eprint = {2005.01698},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/gustafsson et al_2020_how to train your energy-based model for regression.pdf},
  journal = {arXiv:2005.01698 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{habib19_SemiSupervisedGenerativeModeling,
  title = {Semi-{{Supervised Generative Modeling}} for {{Controllable Speech Synthesis}}},
  author = {Habib, Raza and Mariooryad, Soroosh and Shannon, Matt and Battenberg, Eric and {Skerry-Ryan}, R. J. and Stanton, Daisy and Kao, David and Bagby, Tom},
  year = {2019},
  month = oct,
  abstract = {We present a novel generative model that combines state-of-the-art neural text-to-speech (TTS) with semi-supervised probabilistic latent variable models. By providing partial supervision to some of the latent variables, we are able to force them to take on consistent and interpretable purposes, which previously hasn't been possible with purely unsupervised TTS models. We demonstrate that our model is able to reliably discover and control important but rarely labelled attributes of speech, such as affect and speaking rate, with as little as 1\% (30 minutes) supervision. Even at such low supervision levels we do not observe a degradation of synthesis quality compared to a state-of-the-art baseline. Audio samples are available on the web.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.01709},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/habib et al_2019_semi-supervised generative modeling for controllable speech synthesis.pdf;/home/trung/Zotero/storage/7YJUUZMK/1910.html},
  journal = {arXiv:1910.01709 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{hadad00_TwoStepDisentanglementMethod,
  title = {A {{Two}}-{{Step Disentanglement Method}}},
  author = {Hadad, Naama and Wolf, Lior and Shahar, Moni},
  pages = {9},
  abstract = {We address the problem of disentanglement of factors that generate a given data into those that are correlated with the labeling and those that are not. Our solution is simpler than previous solutions and employs adversarial training. First, the part of the data that is correlated with the labels is extracted by training a classifier. Then, the other part is extracted such that it enables the reconstruction of the original data but does not contain label information. The utility of the new method is demonstrated on visual datasets as well as on financial data. Our code is available at https://github.com/naamahadad/ A-Two-Step-Disentanglement-Method.},
  file = {/home/trung/GoogleDrive/Zotero/hadad et al_a two-step disentanglement method.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{hadad20_TwoStepDisentanglementMethod,
  title = {A {{Two}}-{{Step Disentanglement Method}}},
  author = {Hadad, Naama and Wolf, Lior and Shahar, Moni},
  year = {2020},
  month = jan,
  abstract = {We address the problem of disentanglement of factors that generate a given data into those that are correlated with the labeling and those that are not. Our solution is simpler than previous solutions and employs adversarial training. First, the part of the data that is correlated with the labels is extracted by training a classifier. Then, the other part is extracted such that it enables the reconstruction of the original data but does not contain label information. The utility of the new method is demonstrated on visual datasets as well as on financial data. Our code is available at https://github.com/naamahadad/ A-Two-Step-Disentanglement-Method.},
  archivePrefix = {arXiv},
  eprint = {1709.00199},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hadad et al_2020_a two-step disentanglement method.pdf},
  journal = {arXiv:1709.00199 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hadjeres20_VectorQuantizedContrastive,
  title = {Vector {{Quantized Contrastive Predictive Coding}} for {{Template}}-Based {{Music Generation}}},
  author = {Hadjeres, Ga{\"e}tan and Crestel, L{\'e}opold},
  year = {2020},
  month = apr,
  abstract = {In this work, we propose a flexible method for generating variations of discrete sequences in which tokens can be grouped into basic units, like sentences in a text or bars in music. More precisely, given a template sequence, we aim at producing novel sequences sharing perceptible similarities with the original template without relying on any annotation; so our problem of generating variations is intimately linked to the problem of learning relevant high-level representations without supervision. Our contribution is two-fold: First, we propose a self-supervised encoding technique, named Vector Quantized Contrastive Predictive Coding which allows to learn a meaningful assignment of the basic units over a discrete set of codes, together with mechanisms allowing to control the information content of these learnt discrete representations. Secondly, we show how these compressed representations can be used to generate variations of a template sequence by using an appropriate attention pattern in the Transformer architecture. We illustrate our approach on the corpus of J.S. Bach chorales where we discuss the musical meaning of the learnt discrete codes and show that our proposed method allows to generate coherent and high-quality variations of a given template.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2004.10120},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/6D398EME/Hadjeres and Crestel - 2020 - Vector Quantized Contrastive Predictive Coding for.pdf},
  journal = {arXiv:2004.10120 [cs, eess]},
  keywords = {favorite},
  language = {en},
  primaryClass = {cs, eess}
}

@article{haeb-umbach19_SpeechProcessingDigital,
  title = {Speech {{Processing}} for {{Digital Home Assistants}}: {{Combining Signal Processing With Deep}}-{{Learning Techniques}}},
  shorttitle = {Speech {{Processing}} for {{Digital Home Assistants}}},
  author = {{Haeb-Umbach}, Reinhold and Watanabe, Shinji and Nakatani, Tomohiro and Bacchiani, Michiel and Hoffmeister, Bjorn and Seltzer, Michael L. and Zen, Heiga and Souden, Mehrez},
  year = {2019},
  month = nov,
  volume = {36},
  pages = {111--124},
  issn = {1053-5888, 1558-0792},
  doi = {10.1109/MSP.2019.2918706},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/Zotero/storage/4W9S47P8/Haeb-Umbach et al. - 2019 - Speech Processing for Digital Home Assistants Com.pdf},
  journal = {IEEE Signal Processing Magazine},
  language = {en},
  number = {6}
}

@article{hafez-kolahi19_InformationBottleneckits,
  title = {Information {{Bottleneck}} and Its {{Applications}} in {{Deep Learning}}},
  author = {{Hafez-Kolahi}, Hassan and Kasaei, Shohreh},
  year = {2019},
  month = apr,
  abstract = {Information Theory (IT) has been used in Machine Learning (ML) from early days of this field. In the last decade, advances in Deep Neural Networks (DNNs) have led to surprising improvements in many applications of ML. The result has been a paradigm shift in the community toward revisiting previous ideas and applications in this new framework. Ideas from IT are no exception. One of the ideas which is being revisited by many researchers in this new era, is Information Bottleneck (IB); a formulation of information extraction based on IT. The IB is promising in both analyzing and improving DNNs. The goal of this survey is to review the IB concept and demonstrate its applications in deep learning. The information theoretic nature of IB, makes it also a good candidate in showing the more general concept of how IT can be used in ML. Two important concepts are highlighted in this narrative on the subject, i) the concise and universal view that IT provides on seemingly unrelated methods of ML, demonstrated by explaining how IB relates to minimal sufficient statistics, stochastic gradient descent, and variational auto-encoders, and ii) the common technical mistakes and problems caused by applying ideas from IT, which is discussed by a careful study of some recent methods suffering from them.},
  archivePrefix = {arXiv},
  eprint = {1904.03743},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hafez-kolahi et al_2019_information bottleneck and its applications in deep learning.pdf},
  journal = {arXiv:1904.03743 [cs, math, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{hafner18_ReliableUncertaintyEstimates,
  title = {Reliable {{Uncertainty Estimates}} in {{Deep Neural Networks}} Using {{Noise Contrastive Priors}}},
  author = {Hafner, Danijar and Tran, Dustin and Lillicrap, Timothy and Irpan, Alex and Davidson, James},
  year = {2018},
  month = jul,
  abstract = {Obtaining reliable uncertainty estimates of neural network predictions is a long standing challenge. Bayesian neural networks have been proposed as a solution, but it remains open how to specify their prior. In particular, the common practice of a standard normal prior in weight space imposes only weak regularities, causing the function posterior to possibly generalize in unforeseen ways on inputs outside of the training distribution. We propose noise contrastive priors (NCPs) to obtain reliable uncertainty estimates. The key idea is to train the model to output high uncertainty for data points outside of the training distribution. NCPs do so using an input prior, which adds noise to the inputs of the current mini batch, and an output prior, which is a wide distribution given these inputs. NCPs are compatible with any model that can output uncertainty estimates, are easy to scale, and yield reliable uncertainty estimates throughout training. Empirically, we show that NCPs prevent overfitting outside of the training distribution and result in uncertainty estimates that are useful for active learning. We demonstrate the scalability of our method on the flight delays data set, where we significantly improve upon previously published results.},
  archivePrefix = {arXiv},
  eprint = {1807.09289},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hafner et al_2018_reliable uncertainty estimates in deep neural networks using noise contrastive priors.pdf},
  journal = {arXiv:1807.09289 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hafner19_DreamControlLearning,
  title = {Dream to {{Control}}: {{Learning Behaviors}} by {{Latent Imagination}}},
  shorttitle = {Dream to {{Control}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  year = {2019},
  month = dec,
  abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1912.01603},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hafner et al_2019_dream to control.pdf;/home/trung/Zotero/storage/HIMNK796/1912.html},
  journal = {arXiv:1912.01603 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Robotics,imagination,latent},
  primaryClass = {cs}
}

@article{hafner20_DreamControlLearning,
  title = {Dream to {{Control}}: {{Learning Behaviors}} by {{Latent Imagination}}},
  shorttitle = {Dream to {{Control}}},
  author = {Hafner, Danijar and Lillicrap, Timothy and Ba, Jimmy and Norouzi, Mohammad},
  year = {2020},
  month = mar,
  abstract = {Learned world models summarize an agent's experience to facilitate learning complex behaviors. While learning world models from high-dimensional sensory inputs is becoming feasible through deep learning, there are many potential ways for deriving behaviors from them. We present Dreamer, a reinforcement learning agent that solves long-horizon tasks from images purely by latent imagination. We efficiently learn behaviors by propagating analytic gradients of learned state values back through trajectories imagined in the compact state space of a learned world model. On 20 challenging visual control tasks, Dreamer exceeds existing approaches in data-efficiency, computation time, and final performance.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1912.01603},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/AGYSR2LX/Hafner et al. - 2020 - Dream to Control Learning Behaviors by Latent Ima.pdf},
  journal = {arXiv:1912.01603 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{haghighatlari00_ChemMLMachineLearning,
  title = {{{ChemML}}: {{A Machine Learning}} and {{Informatics Program Package}} for the {{Analysis}}, {{Mining}}, and {{Modeling}} of {{Chemical}} and {{Materials Data}}},
  author = {Haghighatlari, Mojtaba and Vishwakarma, Gaurav and Altarawy, Doaa and Subramanian, Ramachandran and Kota, Bhargava Urala and Sonpal, Aditya and Setlur, Srirangaraj and Hachmann, Johannes},
  pages = {8},
  annotation = {ZSCC: 0000007},
  file = {/home/trung/GoogleDrive/Zotero/haghighatlari et al_chemml.pdf},
  language = {en}
}

@misc{hagiwara20_mhagiwara100nlppapers,
  title = {Mhagiwara/100-Nlp-Papers},
  author = {Hagiwara, Masato},
  year = {2020},
  month = sep,
  abstract = {100 Must-Read NLP Papers. Contribute to mhagiwara/100-nlp-papers development by creating an account on GitHub.}
}

@misc{hagiwara20_mhagiwararealworldnlp,
  title = {Mhagiwara/Realworldnlp},
  author = {Hagiwara, Masato},
  year = {2020},
  month = aug,
  abstract = {Example code for "Real-World Natural Language Processing"}
}

@article{hahn20_TheoreticalLimitationsSelfAttention,
  title = {Theoretical {{Limitations}} of {{Self}}-{{Attention}} in {{Neural Sequence Models}}},
  author = {Hahn, Michael},
  year = {2020},
  month = jul,
  volume = {8},
  pages = {156--171},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00306},
  abstract = {Transformers are emerging as the new workhorse of NLP, showing great success across tasks. Unlike LSTMs, transformers process input sequences entirely through self-attention. Previous work has suggested that the computational capabilities of self-attention to process hierarchical structures are limited. In this work, we mathematically investigate the computational power of self-attention to model formal languages. Across both soft and hard attention, we show strong theoretical limitations of the computational abilities of selfattention, finding that it cannot model periodic finite-state languages, nor hierarchical structure, unless the number of layers or heads increases with input length. These limitations seem surprising given the practical success of self-attention and the prominent role assigned to hierarchical structure in linguistics, suggesting that natural language can be approximated well with models that are too weak for the formal languages typically assumed in theoretical linguistics.},
  file = {/home/trung/GoogleDrive/Zotero/hahn_2020_theoretical limitations of self-attention in neural sequence models.pdf},
  journal = {Transactions of the Association for Computational Linguistics},
  language = {en}
}

@article{hairer14_theoryregularitystructures,
  title = {A Theory of Regularity Structures},
  author = {Hairer, Martin},
  year = {2014},
  month = nov,
  volume = {198},
  pages = {269--504},
  issn = {0020-9910, 1432-1297},
  doi = {10.1007/s00222-014-0505-4},
  abstract = {We introduce a new notion of ``regularity structure'' that provides an algebraic framework allowing to describe functions and / or distributions via a kind of ``jet'' or local Taylor expansion around each point. The main novel idea is to replace the classical polynomial model which is suitable for describing smooth functions by arbitrary models that are purpose-built for the problem at hand. In particular, this allows to describe the local behaviour not only of functions but also of large classes of distributions. We then build a calculus allowing to perform the various operations (multiplication, composition with smooth functions, integration against singular kernels) necessary to formulate fixed point equations for a very large class of semilinear PDEs driven by some very singular (typically random) input. This allows, for the first time, to give a mathematically rigorous meaning to many interesting stochastic PDEs arising in physics. The theory comes with convergence results that allow to interpret the solutions obtained in this way as limits of classical solutions to regularised problems, possibly modified by the addition of diverging counterterms. These counterterms arise naturally through the action of a ``renormalisation group'' which is defined canonically in terms of the regularity structure associated to the given class of PDEs.},
  archivePrefix = {arXiv},
  eprint = {1303.5113},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hairer_2014_a theory of regularity structures.pdf},
  journal = {Inventiones mathematicae},
  language = {en},
  number = {2}
}

@article{hajiramezanali00_VariationalGraphRecurrent,
  title = {Variational {{Graph Recurrent Neural Networks}}},
  author = {Hajiramezanali, Ehsan and Hasanzadeh, Arman and Narayanan, Krishna and Duffield, Nick and Zhou, Mingyuan and Qian, Xiaoning},
  pages = {11},
  abstract = {Representation learning over graph structured data has been mostly studied in static graph settings while efforts for modeling dynamic graphs are still scant. In this paper, we develop a novel hierarchical variational model that introduces additional latent random variables to jointly model the hidden states of a graph recurrent neural network (GRNN) to capture both topology and node attribute changes in dynamic graphs. We argue that the use of high-level latent random variables in this variational GRNN (VGRNN) can better capture potential variability observed in dynamic graphs as well as the uncertainty of node latent representation. With semi-implicit variational inference developed for this new VGRNN architecture (SIVGRNN), we show that flexible non-Gaussian latent representations can further help dynamic graph analytic tasks. Our experiments with multiple real-world dynamic graph datasets demonstrate that SI-VGRNN and VGRNN consistently outperform the existing baseline and state-of-the-art methods by a significant margin in dynamic link prediction.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/GoogleDrive/Zotero/hajiramezanali et al_variational graph recurrent neural networks.pdf},
  language = {en}
}

@article{halva20_HiddenMarkovNonlinear,
  title = {Hidden {{Markov Nonlinear ICA}}: {{Unsupervised Learning}} from {{Nonstationary Time Series}}},
  shorttitle = {Hidden {{Markov Nonlinear ICA}}},
  author = {H{\"a}lv{\"a}, Hermanni and Hyv{\"a}rinen, Aapo},
  year = {2020},
  month = jun,
  abstract = {Recent advances in nonlinear Independent Component Analysis (ICA) provide a principled framework for unsupervised feature learning and disentanglement. The central idea in such works is that the latent components are assumed to be independent conditional on some observed auxiliary variables, such as the time-segment index. This requires manual segmentation of data into non-stationary segments which is computationally expensive, inaccurate and often impossible. These models are thus not fully unsupervised. We remedy these limitations by combining nonlinear ICA with a Hidden Markov Model, resulting in a model where a latent state acts in place of the observed segment-index. We prove identifiability of the proposed model for a general mixing nonlinearity, such as a neural network. We also show how maximum likelihood estimation of the model can be done using the expectation-maximization algorithm. Thus, we achieve a new nonlinear ICA framework which is unsupervised, more efficient, as well as able to model underlying temporal dynamics.},
  archivePrefix = {arXiv},
  eprint = {2006.12107},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hälvä et al_2020_hidden markov nonlinear ica.pdf},
  journal = {arXiv:2006.12107 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{hamilton00_GraphRepresentationLearning,
  title = {Graph {{Representation Learning}}},
  author = {Hamilton, Petar Veli{\v c}kovi{\'c} {and} William L.},
  abstract = {NeurIPS 2019 Workshop},
  annotation = {ZSCC: 0000101},
  file = {/home/trung/Zotero/storage/9KNY8W4F/grlearning.github.io.html},
  howpublished = {grlearning.github.io/}
}

@article{hamilton00_InductiveRepresentationLearning,
  title = {Inductive {{Representation Learning}} on {{Large Graphs}}},
  author = {Hamilton, Will and Ying, Zhitao and Leskovec, Jure},
  pages = {11},
  abstract = {Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions.},
  file = {/home/trung/GoogleDrive/Zotero/hamilton et al_inductive representation learning on large graphs.pdf},
  language = {en}
}

@article{hamilton18_RepresentationLearningGraphs,
  title = {Representation {{Learning}} on {{Graphs}}: {{Methods}} and {{Applications}}},
  shorttitle = {Representation {{Learning}} on {{Graphs}}},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  year = {2018},
  month = apr,
  abstract = {Machine learning on graphs is an important and ubiquitous task with applications ranging from drug design to friendship recommendation in social networks. The primary challenge in this domain is finding a way to represent, or encode, graph structure so that it can be easily exploited by machine learning models. Traditionally, machine learning approaches relied on user-defined heuristics to extract features encoding structural information about a graph (e.g., degree statistics or kernel functions). However, recent years have seen a surge in approaches that automatically learn to encode graph structure into low-dimensional embeddings, using techniques based on deep learning and nonlinear dimensionality reduction. Here we provide a conceptual review of key advancements in this area of representation learning on graphs, including matrix factorization-based methods, random-walk based algorithms, and graph neural networks. We review methods to embed individual nodes as well as approaches to embed entire (sub)graphs. In doing so, we develop a unified framework to describe these recent approaches, and we highlight a number of important applications and directions for future work.},
  archivePrefix = {arXiv},
  eprint = {1709.05584},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hamilton et al_2018_representation learning on graphs.pdf},
  journal = {arXiv:1709.05584 [cs]},
  language = {en},
  primaryClass = {cs}
}

@misc{hamilton20_GraphRepresentationLearning,
  title = {Graph {{Representation Learning Book}}},
  author = {Hamilton, William L.},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/hamilton_2020_graph representation learning book.pdf},
  howpublished = {https://www.cs.mcgill.ca/\textasciitilde wlh/grl\_book/?fbclid=IwAR1H-CjsHbgTw3GMiOSonDgp0nxYnQtxockKo4m8Nn5Lw\_NjZXputuuIFt0},
  keywords = {_tablet}
}

@article{hamilton20_ItLikelyThat,
  title = {It {{Is Likely That Your Loss Should}} Be a {{Likelihood}}},
  author = {Hamilton, Mark and Shelhamer, Evan and Freeman, William T.},
  year = {2020},
  month = jul,
  abstract = {We recall that certain common losses are simplified likelihoods and instead argue for optimizing full likelihoods that include their parameters, such as the variance of the normal distribution and the temperature of the softmax distribution. Joint optimization of likelihood and model parameters can adaptively tune the scales and shapes of losses and the weights of regularizers. We survey and systematically evaluate how to parameterize and apply likelihood parameters for robust modeling and re-calibration. Additionally, we propose adaptively tuning L2 and L1 weights by fitting the scale parameters of normal and Laplace priors and introduce more flexible element-wise regularizers.},
  archivePrefix = {arXiv},
  eprint = {2007.06059},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hamilton et al_2020_it is likely that your loss should be a likelihood.pdf},
  journal = {arXiv:2007.06059 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hamrick20_LevelsAnalysisMachine,
  title = {Levels of {{Analysis}} for {{Machine Learning}}},
  author = {Hamrick, Jessica and Mohamed, Shakir},
  year = {2020},
  month = apr,
  abstract = {Machine learning is currently involved in some of the most vigorous debates it has ever seen. Such debates often seem to go around in circles, reaching no conclusion or resolution. This is perhaps unsurprising given that researchers in machine learning come to these discussions with very different frames of reference, making it challenging for them to align perspectives and find common ground. As a remedy for this dilemma, we advocate for the adoption of a common conceptual framework which can be used to understand, analyze, and discuss research. We present one such framework which is popular in cognitive science and neuroscience and which we believe has great utility in machine learning as well: Marr's levels of analysis. Through a series of case studies, we demonstrate how the levels facilitate an understanding and dissection of several methods from machine learning. By adopting the levels of analysis in one's own work, we argue that researchers can be better equipped to engage in the debates necessary to drive forward progress in our field.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2004.05107},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/GX9CATGY/Hamrick and Mohamed - 2020 - Levels of Analysis for Machine Learning.pdf},
  journal = {arXiv:2004.05107 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{han16_DeepCompressionCompressing,
  title = {Deep {{Compression}}: {{Compressing Deep Neural Networks}} with {{Pruning}}, {{Trained Quantization}} and {{Huffman Coding}}},
  shorttitle = {Deep {{Compression}}},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  year = {2016},
  month = feb,
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce "deep compression", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1510.00149},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/han et al_2016_deep compression.pdf;/home/trung/Zotero/storage/VEH9I9VA/1510.html},
  journal = {arXiv:1510.00149 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{han19_StatisticalInferenceMeanField,
  title = {Statistical {{Inference}} in {{Mean}}-{{Field Variational Bayes}}},
  author = {Han, Wei and Yang, Yun},
  year = {2019},
  month = nov,
  abstract = {We conduct non-asymptotic analysis on the mean-field variational inference for approximating posterior distributions in complex Bayesian models that may involve latent variables. We show that the mean-field approximation to the posterior can be well-approximated relative to the Kullback-Leibler divergence discrepancy measure by a normal distribution whose center is the maximum likelihood estimator (MLE). In particular, our results imply that the center of the mean-field approximation matches the MLE up to higher-order terms and there is essentially no loss of efficiency in using it as a point estimator for the parameter in any regular parametric model with latent variables. We also propose a new class of variational weighted likelihood bootstrap (VWLB) methods for quantifying the uncertainty in the mean-field variational inference. The proposed VWLB can be viewed as a new sampling scheme that produces independent samples for approximating the posterior. Comparing with traditional sampling algorithms such Markov Chain Monte Carlo, VWLB can be implemented in parallel and is free of tuning.},
  archivePrefix = {arXiv},
  eprint = {1911.01525},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/han et al_2019_statistical inference in mean-field variational bayes.pdf},
  journal = {arXiv:1911.01525 [math, stat]},
  primaryClass = {math, stat}
}

@inproceedings{han20_JointTrainingVariational,
  title = {Joint {{Training}} of {{Variational Auto}}-{{Encoder}} and {{Latent Energy}}-{{Based Model}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Han, Tian and Nijkamp, Erik and Zhou, Linqi and Pang, Bo and Zhu, Song-Chun and Wu, Ying Nian},
  year = {2020},
  month = jun,
  pages = {7975--7984},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00800},
  abstract = {This paper proposes a joint training method to learn both the variational auto-encoder (VAE) and the latent energybased model (EBM). The joint training of VAE and latent EBM are based on an objective function that consists of three Kullback-Leibler divergences between three joint distributions on the latent vector and the image, and the objective function is of an elegant symmetric and anti-symmetric form of divergence triangle that seamlessly integrates variational and adversarial learning. In this joint training scheme, the latent EBM serves as a critic of the generator model, while the generator model and the inference model in VAE serve as the approximate synthesis sampler and inference sampler of the latent EBM. Our experiments show that the joint training greatly improves the synthesis quality of the VAE. It also enables learning of an energy function that is capable of detecting out of sample examples for anomaly detection.},
  file = {/home/trung/GoogleDrive/Zotero/han et al_2020_joint training of variational auto-encoder and latent energy-based model.pdf},
  isbn = {978-1-72817-168-5},
  language = {en}
}

@article{han20_notsoBigGANGeneratingHighFidelity,
  title = {Not-so-{{BigGAN}}: {{Generating High}}-{{Fidelity Images}} on a {{Small Compute Budget}}},
  shorttitle = {Not-so-{{BigGAN}}},
  author = {Han, Seungwook and Srivastava, Akash and Hurwitz, Cole and Sattigeri, Prasanna and Cox, David D.},
  year = {2020},
  month = sep,
  abstract = {BigGAN is the state-of-the-art in high-resolution image generation, successfully leveraging advancements in scalable computing and theoretical understanding of generative adversarial methods to set new records in conditional image generation. A major part of BigGAN's success is due to its use of large mini-batch sizes during training in high dimensions. While effective, this technique requires an incredible amount of compute resources and/or time (256 TPU-v3 Cores), putting the model out of reach for the larger research community. In this paper, we present not-so-BigGAN, a simple and scalable framework for training deep generative models on high-dimensional natural images. Instead of modelling the image in pixel space like in BigGAN, not-so-BigGAN uses wavelet transformations to bypass the curse of dimensionality, reducing the overall compute requirement significantly. Through extensive empirical evaluation, we demonstrate that for a fixed compute budget, not-so-BigGAN converges several times faster than BigGAN, reaching competitive image quality with an order of magnitude lower compute budget (4 Telsa-V100 GPUs).},
  archivePrefix = {arXiv},
  eprint = {2009.04433},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/han et al_2020_not-so-biggan.pdf},
  journal = {arXiv:2009.04433 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{hand13_LatentVariableModels,
  title = {Latent {{Variable Models}} and {{Factor Analysis}}: {{A Unified Approach}}, {{Third Edition}} by {{David J}}. {{Bartholomew}}, {{Martin Knott}}, {{Irini Moustaki}}},
  author = {Hand, David J.},
  year = {2013},
  volume = {81},
  pages = {333--334},
  journal = {International Statistical Review},
  number = {2}
}

@inproceedings{hannun19_SequencetoSequenceSpeechRecognition,
  title = {Sequence-to-{{Sequence Speech Recognition}} with {{Time}}-{{Depth Separable Convolutions}}},
  booktitle = {Interspeech 2019},
  author = {Hannun, Awni and Lee, Ann and Xu, Qiantong and Collobert, Ronan},
  year = {2019},
  month = sep,
  pages = {3785--3789},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2460},
  abstract = {We propose a fully convolutional sequence-to-sequence encoder architecture with a simple and efficient decoder. Our model improves WER on LibriSpeech while being an order of magnitude more efficient than a strong RNN baseline. Key to our approach is a time-depth separable convolution block which dramatically reduces the number of parameters in the model while keeping the receptive field large. We also give a stable and efficient beam search inference procedure which allows us to effectively integrate a language model. Coupled with a convolutional language model, our time-depth separable convolution architecture improves by more than 22\% relative WER over the best previously reported sequence-to-sequence results on the noisy LibriSpeech test set.},
  file = {/home/trung/GoogleDrive/Zotero/hannun et al_2019_sequence-to-sequence speech recognition with time-depth separable convolutions.pdf},
  language = {en}
}

@misc{hao00_Shouldselfdrivingcar,
  title = {Should a Self-Driving Car Kill the Baby or the Grandma? {{Depends}} on Where You're from.},
  shorttitle = {Should a Self-Driving Car Kill the Baby or the Grandma?},
  author = {Hao, Karen},
  abstract = {The infamous ``trolley problem'' was put to millions of people in a global study, revealing how much ethics diverge across cultures.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/Zotero/storage/QLD4D5EP/a-global-ethics-study-aims-to-help-ai-solve-the-self-driving-trolley-problem.html},
  howpublished = {https://www.technologyreview.com/s/612341/a-global-ethics-study-aims-to-help-ai-solve-the-self-driving-trolley-problem/},
  journal = {MIT Technology Review},
  language = {en-US}
}

@article{haque18_ConditionalEndtoEndAudio,
  title = {Conditional {{End}}-to-{{End Audio Transforms}}},
  author = {Haque, Albert and Guo, Michelle and Verma, Prateek},
  year = {2018},
  month = jun,
  abstract = {We present an end-to-end method for transforming audio from one style to another. For the case of speech, by conditioning on speaker identities, we can train a single model to transform words spoken by multiple people into multiple target voices. For the case of music, we can specify musical instruments and achieve the same result. Architecturally, our method is a fully-differentiable sequence-to-sequence model based on convolutional and hierarchical recurrent neural networks. It is designed to capture long-term acoustic dependencies, requires minimal post-processing, and produces realistic audio transforms. Ablation studies confirm that our model can separate speaker and instrument properties from acoustic content at different receptive fields. Empirically, our method achieves competitive performance on community-standard datasets.},
  archivePrefix = {arXiv},
  eprint = {1804.00047},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/haque et al_2018_conditional end-to-end audio transforms.pdf},
  journal = {arXiv:1804.00047 [cs, eess]},
  primaryClass = {cs, eess}
}

@techreport{harikumar20_PersonalizedSingleCellNetworks,
  title = {Personalized {{Single}}-{{Cell Networks}}: {{A Framework}} to {{Predict The Response}} of {{Any Gene To Any Drug For Any Patient}}},
  shorttitle = {Personalized {{Single}}-{{Cell Networks}}},
  author = {Harikumar, Haripriya and Quinn, Thomas P and Rana, Santu and Gupta, Sunil and Venkatesh, Svetha},
  year = {2020},
  month = aug,
  institution = {{In Review}},
  doi = {10.21203/rs.3.rs-57913/v1},
  abstract = {Background: The last decade has seen a major increase in the availability of genomic data. This includes expert-curated databases that describe the biological activity of genes, as well as high-throughput assays that measure the gene expression of bulk tissue and single cells. Integrating these heterogeneous data sources can generate new hypotheses about biological systems. Our primary objective is to combine population-level drug-response data with patient-level single-cell expression data to predict how any gene will respond to any drug for any patient. Methods: We use a ``dual-channel'' random walk with restart algorithm to perform 3 analyses. First, we use glioblastoma single cells from 5 individual patients to discover genes whose functions differ between cancers. Second, we use drug screening data from the Library of Integrated Network-Based Cellular Signatures (LINCS) to show how a cell-specific drug-response signature can be accurately predicted from a baseline (drug-free) gene co-expression network. Finally, we combine both data streams to show how we can predict how any gene will respond to any drug for each of the 5 glioblastoma patients. Conclusions: Our manuscript introduces two innovations to the integration of heterogeneous biological data. First, we use a ``dual-channel'' method to predict up-regulation and down-regulation separately. Second, we use individualized single-cell gene co-expression networks to make personalized predictions. These innovations let us predict gene function and drug response for individual patients. When applied to real data, we identify a number of genes that exhibit a patient-specific drug response, including the pan-cancer oncogene EGFR.},
  file = {/home/trung/GoogleDrive/Zotero/harikumar et al_2020_personalized single-cell networks.pdf},
  language = {en},
  type = {Preprint}
}

@article{harris20_ArrayprogrammingNumPy,
  title = {Array Programming with {{NumPy}}},
  author = {Harris, Charles R. and Millman, K. Jarrod and {van der Walt}, St{\'e}fan J. and Gommers, Ralf and Virtanen, Pauli and Cournapeau, David and Wieser, Eric and Taylor, Julian and Berg, Sebastian and Smith, Nathaniel J. and Kern, Robert and Picus, Matti and Hoyer, Stephan and {van Kerkwijk}, Marten H. and Brett, Matthew and Haldane, Allan and {del R{\'i}o}, Jaime Fern{\'a}ndez and Wiebe, Mark and Peterson, Pearu and {G{\'e}rard-Marchant}, Pierre and Sheppard, Kevin and Reddy, Tyler and Weckesser, Warren and Abbasi, Hameer and Gohlke, Christoph and Oliphant, Travis E.},
  year = {2020},
  month = sep,
  volume = {585},
  pages = {357--362},
  issn = {1476-4687},
  doi = {10.1038/s41586-020-2649-2},
  abstract = {Array programming provides a powerful, compact and expressive syntax for accessing, manipulating and operating on data in vectors, matrices and higher-dimensional arrays. NumPy is the primary array programming library for the Python language. It has an essential role in research analysis pipelines in fields as diverse as physics, chemistry, astronomy, geoscience, biology, psychology, materials science, engineering, finance and economics. For example, in astronomy, NumPy was an important part of the software stack used in the discovery of gravitational waves1 and in the first imaging of a black hole2. Here we review how a few fundamental array concepts lead to a simple and powerful programming paradigm for organizing, exploring and analysing scientific data. NumPy is the foundation upon which the scientific Python ecosystem is constructed. It is so pervasive that several projects, targeting audiences with specialized needs, have developed their own NumPy-like interfaces and array objects. Owing to its central position in the ecosystem, NumPy increasingly acts as an interoperability layer between such array computation libraries and, together with its application programming interface (API), provides a flexible framework to support the next decade of scientific and industrial analysis.},
  file = {/home/trung/GoogleDrive/Zotero/harris et al_2020_array programming with numpy.pdf},
  journal = {Nature},
  number = {7825}
}

@article{hartmann20_FlexiblePriorElicitation,
  title = {Flexible {{Prior Elicitation}} via the {{Prior Predictive Distribution}}},
  author = {Hartmann, Marcelo and Agiashvili, Georgi and B{\"u}rkner, Paul and Klami, Arto},
  year = {2020},
  month = mar,
  abstract = {The prior distribution for the unknown model parameters plays a crucial role in the process of statistical inference based on Bayesian methods. However, specifying suitable priors is often difficult even when detailed prior knowledge is available in principle. The challenge is to express quantitative information in the form of a probability distribution. Prior elicitation addresses this question by extracting subjective information from an expert and transforming it into a valid prior. Most existing methods, however, require information to be provided on the unobservable parameters, whose effect on the data generating process is often complicated and hard to understand. We propose an alternative approach that only requires knowledge about the observable outcomes \textendash{} knowledge which is often much easier for experts to provide. Building upon a principled statistical framework, our approach utilizes the prior predictive distribution implied by the model to automatically transform experts judgements about plausible outcome values to suitable priors on the parameters. We also provide computational strategies to perform inference and guidelines to facilitate practical use.},
  archivePrefix = {arXiv},
  eprint = {2002.09868},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hartmann et al_2020_flexible prior elicitation via the prior predictive distribution.pdf},
  journal = {arXiv:2002.09868 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{hasan20_Learninglatentstochastic,
  title = {Learning Latent Stochastic Differential Equations with Variational Auto-Encoders},
  author = {Hasan, Ali and Pereira, Jo{\~a}o M. and Farsiu, Sina and Tarokh, Vahid},
  year = {2020},
  month = jul,
  abstract = {We present a method for learning latent stochastic differential equations (SDEs) from high dimensional time series data. Given a time series generated from a lower dimensional It\textbackslash\^\{o\} process, the proposed method uncovers the relevant parameters of the SDE through a self-supervised learning approach. Using the framework of variational autoencoders (VAEs), we consider a conditional generative model for the data based on the Euler-Maruyama approximation of SDE solutions. Furthermore, we use recent results on identifiability of semi-supervised learning to show that our model can recover not only the underlying SDE parameters, but also the original latent space, up to an isometry, in the limit of infinite data. We validate the model through a series of different simulated video processing tasks where the underlying SDE is known. Our results suggest that the proposed method effectively learns the underlying SDE, as predicted by the theory.},
  archivePrefix = {arXiv},
  eprint = {2007.06075},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hasan et al_2020_learning latent stochastic differential equations with variational auto-encoders.pdf},
  journal = {arXiv:2007.06075 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{hattingh00_UsingAsyncioPython,
  title = {Using {{Asyncio}} in {{Python}}},
  author = {Hattingh, Caleb},
  pages = {166},
  file = {/home/trung/GoogleDrive/Zotero/hattingh_using asyncio in python.pdf},
  language = {en}
}

@article{hauberg19_OnlyBayesshould,
  title = {Only {{Bayes}} Should Learn a Manifold (on the Estimation of Differential Geometric Structure from Data)},
  author = {Hauberg, S{\o}ren},
  year = {2019},
  month = sep,
  abstract = {We investigate learning of the differential geometric structure of a data manifold embedded in a high-dimensional Euclidean space. We first analyze kernel-based algorithms and show that under the usual regularizations, non-probabilistic methods cannot recover the differential geometric structure, but instead find mostly linear manifolds or spaces equipped with teleports. To properly learn the differential geometric structure, non-probabilistic methods must apply regularizations that enforce large gradients, which go against common wisdom. We repeat the analysis for probabilistic methods and find that under reasonable priors, the geometric structure can be recovered. Fully exploiting the recovered structure, however, requires the development of stochastic extensions to classic Riemannian geometry. We take early steps in that regard. Finally, we partly extend the analysis to modern models based on neural networks, thereby highlighting geometric and probabilistic shortcomings of current deep generative models.},
  archivePrefix = {arXiv},
  eprint = {1806.04994},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hauberg_2019_only bayes should learn a manifold (on the estimation of differential geometric structure from data).pdf},
  journal = {arXiv:1806.04994 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{hautamaki20_WhyDidxVector,
  title = {Why {{Did}} the X-{{Vector System Miss}} a {{Target Speaker}}? {{Impact}} of {{Acoustic Mismatch Upon Target Score}} on {{VoxCeleb Data}}},
  shorttitle = {Why {{Did}} the X-{{Vector System Miss}} a {{Target Speaker}}?},
  author = {Hautam{\"a}ki, Rosa Gonz{\'a}lez and Kinnunen, Tomi},
  year = {2020},
  month = aug,
  abstract = {Modern automatic speaker verification (ASV) relies heavily on machine learning implemented through deep neural networks. It can be difficult to interpret the output of these black boxes. In line with interpretative machine learning, we model the dependency of ASV detection score upon acoustic mismatch of the enrollment and test utterances. We aim to identify mismatch factors that explain target speaker misses (false rejections). We use distance in the first- and second-order statistics of selected acoustic features as the predictors in a linear mixed effects model, while a standard Kaldi x-vector system forms our ASV black-box. Our results on the VoxCeleb data reveal the most prominent mismatch factor to be in F0 mean, followed by mismatches associated with formant frequencies. Our findings indicate that x-vector systems lack robustness to intra-speaker variations.},
  archivePrefix = {arXiv},
  eprint = {2008.04578},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hautamäki et al_2020_why did the x-vector system miss a target speaker.pdf},
  journal = {arXiv:2008.04578 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{havasi20_Trainingindependentsubnetworks,
  title = {Training Independent Subnetworks for Robust Prediction},
  author = {Havasi, Marton and Jenatton, Rodolphe and Fort, Stanislav and Liu, Jeremiah Zhe and Snoek, Jasper and Lakshminarayanan, Balaji and Dai, Andrew M. and Tran, Dustin},
  year = {2020},
  month = oct,
  abstract = {Recent approaches to efficiently ensemble neural networks have shown that strong robustness and uncertainty performance can be achieved with a negligible gain in parameters over the original network. However, these methods still require multiple forward passes for prediction, leading to a significant computational cost. In this work, we show a surprising result: the benefits of using multiple predictions can be achieved `for free' under a single model's forward pass. In particular, we show that, using a multi-input multi-output (MIMO) configuration, one can utilize a single model's capacity to train multiple subnetworks that independently learn the task at hand. By ensembling the predictions made by the subnetworks, we improve model robustness without increasing compute. We observe a significant improvement in negative log-likelihood, accuracy, and calibration error on CIFAR10, CIFAR100, ImageNet, and their out-of-distribution variants compared to previous methods.},
  archivePrefix = {arXiv},
  eprint = {2010.06610},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/havasi et al_2020_training independent subnetworks for robust prediction.pdf},
  journal = {arXiv:2010.06610 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{havrylov20_PreventingPosteriorCollapse,
  title = {Preventing {{Posterior Collapse}} with {{Levenshtein Variational Autoencoder}}},
  author = {Havrylov, Serhii and Titov, Ivan},
  year = {2020},
  month = apr,
  abstract = {Variational autoencoders (VAEs) are a standard framework for inducing latent variable models that have been shown effective in learning text representations as well as in text generation. The key challenge with using VAEs is the posterior collapse problem: learning tends to converge to trivial solutions where the generators ignore latent variables. In our Levenstein VAE, we propose to replace the evidence lower bound (ELBO) with a new objective which is simple to optimize and prevents posterior collapse. Intuitively, it corresponds to generating a sequence from the autoencoder and encouraging the model to predict an optimal continuation according to the Levenshtein distance (LD) with the reference sentence at each time step in the generated sequence. We motivate the method from the probabilistic perspective by showing that it is closely related to optimizing a bound on the intractable Kullback-Leibler divergence of an LD-based kernel density estimator from the model distribution. With this objective, any generator disregarding latent variables will incur large penalties and hence posterior collapse does not happen. We relate our approach to policy distillation (Ross et al., 2011) and dynamic oracles (Goldberg and Nivre, 2012). By considering Yelp and SNLI benchmarks, we show that Levenstein VAE produces more informative latent representations than alternative approaches to preventing posterior collapse.},
  archivePrefix = {arXiv},
  eprint = {2004.14758},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/havrylov et al_2020_preventing posterior collapse with levenshtein variational autoencoder.pdf},
  journal = {arXiv:2004.14758 [cs, stat]},
  keywords = {vae_issues},
  language = {en},
  primaryClass = {cs, stat}
}

@article{he15_DeepResidualLearning,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2015_deep residual learning for image recognition.pdf;/home/trung/Zotero/storage/G9ZYEXR6/1512.html},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{he15_DelvingDeepRectifiers,
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = feb,
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\%). To our knowledge, our result is the first to surpass human-level performance (5.1\%, Russakovsky et al.) on this visual recognition challenge.},
  annotation = {ZSCC: 0006289},
  archivePrefix = {arXiv},
  eprint = {1502.01852},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2015_delving deep into rectifiers.pdf;/home/trung/Zotero/storage/3X4WRFJB/1502.html},
  journal = {arXiv:1502.01852 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{he17_AttGANFacialAttribute,
  title = {{{AttGAN}}: {{Facial Attribute Editing}} by {{Only Changing What You Want}}},
  shorttitle = {{{AttGAN}}},
  author = {He, Zhenliang and Zuo, Wangmeng and Kan, Meina and Shan, Shiguang and Chen, Xilin},
  year = {2017},
  month = nov,
  abstract = {Facial attribute editing aims to manipulate single or multiple attributes of a face image, i.e., to generate a new face with desired attributes while preserving other details. Recently, generative adversarial net (GAN) and encoder-decoder architecture are usually incorporated to handle this task with promising results. Based on the encoder-decoder architecture, facial attribute editing is achieved by decoding the latent representation of the given face conditioned on the desired attributes. Some existing methods attempt to establish an attributeindependent latent representation for further attribute editing. However, such attribute-independent constraint on the latent representation is excessive because it restricts the capacity of the latent representation and may result in information loss, leading to over-smooth and distorted generation. Instead of imposing constraints on the latent representation, in this work we apply an attribute classification constraint to the generated image to just guarantee the correct change of desired attributes, i.e., to ``change what you want''. Meanwhile, the reconstruction learning is introduced to preserve attribute-excluding details, in other words, to ``only change what you want''. Besides, the adversarial learning is employed for visually realistic editing. These three components cooperate with each other forming an effective framework for high quality facial attribute editing, referred as AttGAN. Furthermore, our method is also directly applicable for attribute intensity control and can be naturally extended for attribute style manipulation. Experiments on CelebA dataset show that our method outperforms the state-of-the-arts on realistic attribute editing with facial details well preserved.},
  archivePrefix = {arXiv},
  eprint = {1711.10678},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2017_attgan.pdf},
  journal = {arXiv:1711.10678 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{he18_SequenceSequenceMixture,
  title = {Sequence to {{Sequence Mixture Model}} for {{Diverse Machine Translation}}},
  author = {He, Xuanli and Haffari, Gholamreza and Norouzi, Mohammad},
  year = {2018},
  month = oct,
  abstract = {Sequence to sequence (SEQ2SEQ) models often lack diversity in their generated translations. This can be attributed to the limitation of SEQ2SEQ models in capturing lexical and syntactic variations in a parallel corpus resulting from different styles, genres, topics, or ambiguity of the translation process. In this paper, we develop a novel sequence to sequence mixture (S2SMIX) model that improves both translation diversity and quality by adopting a committee of specialized translation models rather than a single translation model. Each mixture component selects its own training dataset via optimization of the marginal loglikelihood, which leads to a soft clustering of the parallel corpus. Experiments on four language pairs demonstrate the superiority of our mixture model compared to a SEQ2SEQ baseline with standard or diversity-boosted beam search. Our mixture model uses negligible additional parameters and incurs no extra computation cost during decoding.},
  archivePrefix = {arXiv},
  eprint = {1810.07391},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2018_sequence to sequence mixture model for diverse machine translation.pdf;/home/trung/Zotero/storage/84HSCWWI/1810.html},
  journal = {arXiv:1810.07391 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{he19_LaggingInferenceNetworks,
  title = {Lagging {{Inference Networks}} and {{Posterior Collapse}} in {{Variational Autoencoders}}},
  author = {He, Junxian and Spokoyny, Daniel and Neubig, Graham and {Berg-Kirkpatrick}, Taylor},
  year = {2019},
  month = jan,
  abstract = {The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as "posterior collapse" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.},
  archivePrefix = {arXiv},
  eprint = {1901.05534},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2019_lagging inference networks and posterior collapse in variational autoencoders.pdf},
  journal = {arXiv:1901.05534 [cs, stat]},
  keywords = {favorite,vae_issues},
  primaryClass = {cs, stat}
}

@article{he19_Learningpredictcosmological,
  title = {Learning to Predict the Cosmological Structure Formation},
  author = {He, Siyu and Li, Yin and Feng, Yu and Ho, Shirley and Ravanbakhsh, Siamak and Chen, Wei and P{\'o}czos, Barnab{\'a}s},
  year = {2019},
  month = jul,
  volume = {116},
  pages = {13825--13832},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1821458116},
  abstract = {Matter evolved under the influence of gravity from minuscule density fluctuations. Nonperturbative structure formed hierarchically over all scales and developed non-Gaussian features in the Universe, known as the cosmic web. To fully understand the structure formation of the Universe is one of the holy grails of modern astrophysics. Astrophysicists survey large volumes of the Universe and use a large ensemble of computer simulations to compare with the observed data to extract the full information of our own Universe. However, to evolve billions of particles over billions of years, even with the simplest physics, is a daunting task. We build a deep neural network, the Deep Density Displacement Model (                                                                                               D                                                                 3                                                           M                                               ), which learns from a set of prerun numerical simulations, to predict the nonlinear large-scale structure of the Universe with the Zel'dovich Approximation (ZA), an analytical approximation based on perturbation theory, as the input. Our extensive analysis demonstrates that                                                                                               D                                                                 3                                                           M                                               outperforms the second-order perturbation theory (2LPT), the commonly used fast-approximate simulation method, in predicting cosmic structure in the nonlinear regime. We also show that                                                                                               D                                                                 3                                                           M                                               is able to accurately extrapolate far beyond its training data and predict structure formation for significantly different cosmological parameters. Our study proves that deep learning is a practical and accurate alternative to approximate 3D simulations of the gravitational structure formation of the Universe.},
  annotation = {ZSCC: 0000006},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2019_learning to predict the cosmological structure formation.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  language = {en},
  number = {28}
}

@article{he19_MomentumContrastUnsupervised,
  title = {Momentum {{Contrast}} for {{Unsupervised Visual Representation Learning}}},
  author = {He, Kaiming and Fan, Haoqi and Wu, Yuxin and Xie, Saining and Girshick, Ross},
  year = {2019},
  month = nov,
  abstract = {We present Momentum Contrast (MoCo) for unsupervised visual representation learning. From a perspective on contrastive learning as dictionary look-up, we build a dynamic dictionary with a queue and a moving-averaged encoder. This enables building a large and consistent dictionary on-the-fly that facilitates contrastive unsupervised learning. MoCo provides competitive results under the common linear protocol on ImageNet classification. More importantly, the representations learned by MoCo transfer well to downstream tasks. MoCo can outperform its supervised pre-training counterpart in 7 detection/segmentation tasks on PASCAL VOC, COCO, and other datasets, sometimes surpassing it by large margins. This suggests that the gap between unsupervised and supervised representation learning has been largely closed in many vision tasks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.05722},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2019_momentum contrast for unsupervised visual representation learning.pdf;/home/trung/Zotero/storage/2LXRC68Z/1911.html},
  journal = {arXiv:1911.05722 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{he19_ProbganProbabilisticGan,
  title = {Probgan: {{Towards Probabilistic Gan}} with {{Theoretical Guarantees}}},
  author = {He, Hao and Wang, Hao and Lee, Guang-He and Tian, Yonglong},
  year = {2019},
  pages = {28},
  abstract = {Probabilistic modelling is a principled framework to perform model aggregation, which has been a primary mechanism to combat mode collapse in the context of Generative Adversarial Networks (GAN). In this paper, we propose a novel probabilistic framework for GANs, ProbGAN, which iteratively learns a distribution over generators with a carefully crafted prior. Learning is efficiently triggered by a tailored stochastic gradient Hamiltonian Monte Carlo with a novel gradient approximation to perform Bayesian inference. Our theoretical analysis further reveals that our treatment is the first probabilistic framework that yields an equilibrium where generator distributions are faithful to the data distribution. Empirical evidence on synthetic high-dimensional multi-modal data and image databases (CIFAR-10, STL-10, and ImageNet) demonstrates the superiority of our method over both start-of-the-art multi-generator GANs and other probabilistic treatment for GANs.},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2019_probgan.pdf},
  language = {en}
}

@article{he20_BayesianDeepEnsembles,
  title = {Bayesian {{Deep Ensembles}} via the {{Neural Tangent Kernel}}},
  author = {He, Bobby and Lakshminarayanan, Balaji and Teh, Yee Whye},
  year = {2020},
  month = jul,
  abstract = {We explore the link between deep ensembles and Gaussian processes (GPs) through the lens of the Neural Tangent Kernel (NTK): a recent development in understanding the training dynamics of wide neural networks (NNs). Previous work has shown that even in the infinite width limit, when NNs become GPs, there is no GP posterior interpretation to a deep ensemble trained with squared error loss. We introduce a simple modification to standard deep ensembles training, through addition of a computationally-tractable, randomised and untrainable function to each ensemble member, that enables a posterior interpretation in the infinite width limit. When ensembled together, our trained NNs give an approximation to a posterior predictive distribution, and we prove that our Bayesian deep ensembles make more conservative predictions than standard deep ensembles in the infinite width limit. Finally, using finite width NNs we demonstrate that our Bayesian deep ensembles faithfully emulate the analytic posterior predictive when available, and can outperform standard deep ensembles in various out-of-distribution settings, for both regression and classification tasks.},
  archivePrefix = {arXiv},
  eprint = {2007.05864},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2020_bayesian deep ensembles via the neural tangent kernel.pdf},
  journal = {arXiv:2007.05864 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{he20_Celltypecompositionanalysis,
  title = {Cell-Type Composition Analysis of {{scRNA}}-Seq Data with Deep Convolution Neural Network},
  author = {He, Ma and Fang, Zhihao and Liu, Zongbin and Chen, Yan},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/he et al_2020_cell-type composition analysis of scrna-seq data with deep convolution neural network.pdf},
  journal = {BMC Bioinformatics},
  keywords = {_tablet}
}

@book{hearty16_Advancedmachinelearning,
  title = {Advanced Machine Learning with {{Python}}: Solve Challenging Data Science Problems by Mastering Cutting-Edge Machine Learning Techniques in {{Phyton}}},
  shorttitle = {Advanced Machine Learning with {{Python}}},
  author = {Hearty, John},
  year = {2016},
  publisher = {{Packt Publishing}},
  address = {{Birmingham, UK Mumbai}},
  file = {/home/trung/Zotero/storage/YAQM2VGL/Hearty - 2016 - Advanced machine learning with Python solve chall.pdf},
  isbn = {978-1-78439-863-7},
  language = {en},
  series = {Packt Open Source {{Community}} Experience Distilled}
}

@article{hebert17_TreatmentsInflammatoryArthritis,
  title = {Treatments for {{Inflammatory Arthritis}}: {{Potential But Unproven Role}} of {{Topical Copaiba}}},
  shorttitle = {Treatments for {{Inflammatory Arthritis}}},
  author = {Hebert, Patricia and Barice, E. Joan and Park, Juyoung and Dyess, Susan MacLeod and McCaffrey, Ruth and Hennekens, Charles H.},
  year = {2017},
  month = apr,
  volume = {16},
  pages = {40--42},
  issn = {1546-993X},
  abstract = {Traditional medicines for inflammatory arthritis (IA) include nonsteroidal anti-inflammatory drugs (NSAIDs) and cyclooxygenase-2 inhibitors (COXIBs), which have variable clinical benefits and serious side effects. In large-scale randomized, controlled trials (RCTs) in IA, they have demonstrated significant decreases in pain and inflammation but also significant increases in gastrointestinal symptoms, serious bleeding, and cardiovascular events. Copaiba, an essential oil used topically, has potential but unproven benefits, with few to no side effects. Basic research supports its mechanisms of benefit, but human data are sparse and include 1 case series and 1 small RCT examining its benefits for another inflammatory condition, not IA. Providing effective and safe pain relief for patients with IA presents clinical, public health, and research challenges. The clinical challenge is to maximize the benefits of treatment and minimize its risks. Sales of copaiba are increasing and may continue to do so even in the absence of reliable evidence from RCTs, providing a public health challenge. Thus, the research challenge is to test topical copaiba versus a placebo for IA patients against a background of usual care in RCTs of sufficient size, dose, and duration. If such trials show positive results, a logical next step might be head-to-head comparisons against NSAIDs and COXIBs. Evidence from RCTs may support more widespread use or, to paraphrase Huxley, conclude that copaiba is yet another beautiful hypothesis slain by ugly facts.},
  file = {/home/trung/GoogleDrive/Zotero/hebert et al_2017_treatments for inflammatory arthritis.pdf},
  journal = {Integrative Medicine: A Clinician's Journal},
  number = {2},
  pmcid = {PMC6413642},
  pmid = {30881236}
}

@article{hebiri20_LayerSparsityNeural,
  title = {Layer {{Sparsity}} in {{Neural Networks}}},
  author = {Hebiri, Mohamed and Lederer, Johannes},
  year = {2020},
  month = jun,
  abstract = {Sparsity has become popular in machine learning, because it can save computational resources, facilitate interpretations, and prevent overfitting. In this paper, we discuss sparsity in the framework of neural networks. In particular, we formulate a new notion of sparsity that concerns the networks' layers and, therefore, aligns particularly well with the current trend toward deep networks. We call this notion layer sparsity. We then introduce corresponding regularization and refitting schemes that can complement standard deep-learning pipelines to generate more compact and accurate networks.},
  archivePrefix = {arXiv},
  eprint = {2006.15604},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hebiri et al_2020_layer sparsity in neural networks.pdf},
  journal = {arXiv:2006.15604 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{heckel20_EarlyStoppingDeep,
  title = {Early {{Stopping}} in {{Deep Networks}}: {{Double Descent}} and {{How}} to {{Eliminate}} It},
  shorttitle = {Early {{Stopping}} in {{Deep Networks}}},
  author = {Heckel, Reinhard and Yilmaz, Fatih Furkan},
  year = {2020},
  month = sep,
  abstract = {Over-parameterized models, such as large deep networks, often exhibit a double descent phenomenon, where as a function of model size, error first decreases, increases, and decreases at last. This intriguing double descent behavior also occurs as a function of training epochs, and has been conjectured to arise because training epochs control the model complexity. In this paper, we show that such epoch-wise double descent arises for a different reason: It is caused by a superposition of two or more bias-variance tradeoffs that arise because different parts of the network are learned at different epochs, and eliminating this by proper scaling of stepsizes can significantly improve the early stopping performance. We show this analytically for i) linear regression, where differently scaled features give rise to a superposition of bias-variance tradeoffs, and for ii) a two-layer neural network, where the first and second layer each govern a bias-variance tradeoff. Inspired by this theory, we study two standard convolutional networks empirically, and show that eliminating epoch-wise double descent through adjusting stepsizes of different layers improves the early stopping performance significantly.},
  archivePrefix = {arXiv},
  eprint = {2007.10099},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/heckel et al_2020_early stopping in deep networks.pdf},
  journal = {arXiv:2007.10099 [cs, stat]},
  keywords = {early stopping},
  language = {en},
  primaryClass = {cs, stat}
}

@article{heishman10_Metaanalysisacuteeffects,
  title = {Meta-Analysis of the Acute Effects of Nicotine and Smoking on Human Performance},
  author = {Heishman, Stephen J. and Kleykamp, Bethea A. and Singleton, Edward G.},
  year = {2010},
  month = jul,
  volume = {210},
  pages = {453--469},
  issn = {1432-2072},
  doi = {10.1007/s00213-010-1848-1},
  abstract = {RATIONALE AND OBJECTIVE: Empirical studies indicate that nicotine enhances some aspects of attention and cognition, suggesting a role in the maintenance of tobacco dependence. The purpose of this review was to update the literature since our previous review (Heishman et al. Exp Clin Psychopharmacol 2:345-395, 1994) and to determine which aspects of human performance were most sensitive to the effects of nicotine and smoking. METHODS: We conducted a meta-analysis on the outcome measures of 41 double-blind, placebo-controlled laboratory studies published from 1994 to 2008. In all studies, nicotine was administered, and performance was assessed in healthy adult nonsmokers or smokers who were not tobacco-deprived or minimally deprived (},
  file = {/home/trung/GoogleDrive/Zotero/heishman et al_2010_meta-analysis of the acute effects of nicotine and smoking on human performance.pdf},
  journal = {Psychopharmacology},
  language = {eng},
  number = {4},
  pmcid = {PMC3151730},
  pmid = {20414766}
}

@article{hellstrom20_BiasMachineLearning,
  title = {Bias in {{Machine Learning What}} Is It {{Good}} (and {{Bad}}) For?},
  author = {Hellstr{\"o}m, Thomas and Dignum, Virginia and Bensch, Suna},
  year = {2020},
  month = apr,
  abstract = {In public media as well as in scientific publications, the term \textbackslash emph\{bias\} is used in conjunction with machine learning in many different contexts, and with many different meanings. This paper proposes a taxonomy of these different meanings, terminology, and definitions by surveying the, primarily scientific, literature on machine learning. In some cases, we suggest extensions and modifications to promote a clear terminology and completeness. The survey is followed by an analysis and discussion on how different types of biases are connected and depend on each other. We conclude that there is a complex relation between bias occurring in the machine learning pipeline that leads to a model, and the eventual bias of the model (which is typically related to social discrimination). The former bias may or may not influence the latter, in a sometimes bad, and sometime good way.},
  archivePrefix = {arXiv},
  eprint = {2004.00686},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hellström et al_2020_bias in machine learning what is it good (and bad) for.pdf},
  journal = {arXiv:2004.00686 [cs]},
  primaryClass = {cs}
}

@article{hendrycks19_AugMixSimpleData,
  title = {{{AugMix}}: {{A Simple Data Processing Method}} to {{Improve Robustness}} and {{Uncertainty}}},
  shorttitle = {{{AugMix}}},
  author = {Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D. and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  abstract = {Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.},
  archivePrefix = {arXiv},
  eprint = {1912.02781},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Hendrycks et al_2019_AugMix2.pdf;/home/trung/GoogleDrive/Zotero/2019/Hendrycks et al_2019_AugMix3.pdf;/home/trung/GoogleDrive/Zotero/2019/Hendrycks et al_2019_AugMix4.pdf;/home/trung/GoogleDrive/Zotero/hendrycks et al_2019_augmix.pdf},
  journal = {arXiv:1912.02781 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hendrycks19_AugMixSimpleDataa,
  title = {{{AugMix}}: {{A Simple Data Processing Method}} to {{Improve Robustness}} and {{Uncertainty}}},
  shorttitle = {{{AugMix}}},
  author = {Hendrycks, Dan and Mu, Norman and Cubuk, Ekin D. and Zoph, Barret and Gilmer, Justin and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  abstract = {Modern deep neural networks can achieve high accuracy when the training distribution and test distribution are identically distributed, but this assumption is frequently violated in practice. When the train and test distributions are mismatched, accuracy can plummet. Currently there are few techniques that improve robustness to unforeseen data shifts encountered during deployment. In this work, we propose a technique to improve the robustness and uncertainty estimates of image classifiers. We propose AUGMIX, a data processing technique that is simple to implement, adds limited computational overhead, and helps models withstand unforeseen corruptions. AUGMIX significantly improves robustness and uncertainty measures on challenging image classification benchmarks, closing the gap between previous methods and the best possible performance in some cases by more than half.},
  archivePrefix = {arXiv},
  eprint = {1912.02781},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/V5PU4QNB/Hendrycks et al. - 2019 - AugMix A Simple Data Processing Method to Improve.pdf},
  journal = {arXiv:1912.02781 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hensman00_GaussianProcessesBig,
  title = {Gaussian {{Processes}} for {{Big Data}}},
  author = {Hensman, James and Fusi, Nicolo and Lawrence, Neil D},
  pages = {9},
  abstract = {We introduce stochastic variational inference for Gaussian process models. This enables the application of Gaussian process (GP) models to data sets containing millions of data points. We show how GPs can be variationally decomposed to depend on a set of globally relevant inducing variables which factorize the model in the necessary manner to perform variational inference. Our approach is readily extended to models with non-Gaussian likelihoods and latent variable models based around Gaussian processes. We demonstrate the approach on a simple toy problem and two real world data sets.},
  file = {/home/trung/GoogleDrive/Zotero/hensman et al_gaussian processes for big data.pdf},
  keywords = {variational},
  language = {en}
}

@article{hensman20_VariationalFourierFeatures,
  title = {Variational {{Fourier Features}} for {{Gaussian Processes}}},
  author = {Hensman, James and Durrande, Nicolas and Solin, Arno},
  year = {2020},
  pages = {52},
  abstract = {This work brings together two powerful concepts in Gaussian processes: the variational approach to sparse approximation and the spectral representation of Gaussian processes. This gives rise to an approximation that inherits the benefits of the variational approach but with the representational power and computational scalability of spectral representations. The work hinges on a key result that there exist spectral features related to a finite domain of the Gaussian process which exhibit almost-independent covariances. We derive these expressions for Mat\textasciiacute ern kernels in one dimension, and generalize to more dimensions using kernels with specific structures. Under the assumption of additive Gaussian noise, our method requires only a single pass through the data set, making for very fast and accurate computation. We fit a model to 4 million training points in just a few minutes on a standard laptop. With non-conjugate likelihoods, our MCMC scheme reduces the cost of computation from O(N M 2) (for a sparse Gaussian process) to O(N M ) per iteration, where N is the number of data and M is the number of features.},
  file = {/home/trung/GoogleDrive/Zotero/hensman et al_2020_variational fourier features for gaussian processes.pdf},
  language = {en}
}

@article{hernan19_CausalInferenceWhat,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hern{\'a}n, Miguel A and Robins, James M},
  year = {2019},
  pages = {310},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/hernán et al_causal inference.pdf},
  keywords = {causal},
  language = {en}
}

@article{hernan20_CausalInferenceWhat,
  title = {Causal {{Inference}}: {{What If}}},
  author = {Hern{\'a}n, Miguel A and Robins, James M},
  year = {2020},
  pages = {311},
  file = {/home/trung/GoogleDrive/Zotero/hernán et al_2020_causal inference2.pdf},
  language = {en}
}

@article{hestness17_DeepLearningScaling,
  title = {Deep {{Learning Scaling}} Is {{Predictable}}, {{Empirically}}},
  author = {Hestness, Joel and Narang, Sharan and Ardalani, Newsha and Diamos, Gregory and Jun, Heewoo and Kianinejad, Hassan and Patwary, Md Mostofa Ali and Yang, Yang and Zhou, Yanqi},
  year = {2017},
  month = dec,
  abstract = {Deep learning (DL) creates impactful advances following a virtuous recipe: model architecture search, creating large training data sets, and scaling computation. It is widely believed that growing training sets and models should improve accuracy and result in better products. As DL application domains grow, we would like a deeper understanding of the relationships between training set size, computational scale, and model accuracy improvements to advance the state-of-the-art. This paper presents a large scale empirical characterization of generalization error and model size growth as training sets grow. We introduce a methodology for this measurement and test four machine learning domains: machine translation, language modeling, image processing, and speech recognition. Our empirical results show power-law generalization error scaling across a breadth of factors, resulting in power-law exponents---the "steepness" of the learning curve---yet to be explained by theoretical work. Further, model improvements only shift the error but do not appear to affect the power-law exponent. We also show that model size scales sublinearly with data size. These scaling relationships have significant implications on deep learning research, practice, and systems. They can assist model debugging, setting accuracy targets, and decisions about data set growth. They can also guide computing system design and underscore the importance of continued computational scaling.},
  annotation = {ZSCC: 0000055},
  archivePrefix = {arXiv},
  eprint = {1712.00409},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hestness et al_2017_deep learning scaling is predictable, empirically.pdf;/home/trung/Zotero/storage/AFC4AGAR/1712.html},
  journal = {arXiv:1712.00409 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{heusel17_GANsTrainedTwo,
  title = {{{GANs Trained}} by a {{Two Time}}-{{Scale Update Rule Converge}} to a {{Local Nash Equilibrium}}},
  author = {Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas and Nessler, Bernhard and Hochreiter, Sepp},
  year = {2017},
  month = jun,
  abstract = {Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the `Fr\'echet Inception Distance'' (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.},
  archivePrefix = {arXiv},
  eprint = {1706.08500},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/heusel et al_2017_gans trained by a two time-scale update rule converge to a local nash equilibrium.pdf},
  journal = {arXiv:1706.08500 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{higgins17_betaVAELearningBasic,
  title = {Beta-{{VAE}}: {{Learning Basic Visual Concepts}} with a {{Constrained Variational Framework}}},
  author = {Higgins, Irina and Matthey, Loic and Pal, Arka and {et al.}},
  year = {2017},
  file = {/home/trung/GoogleDrive/Zotero/higgins et al_2017_beta-vae.pdf},
  journal = {ICLR},
  keywords = {disentanglement,favorite}
}

@article{higgins18_DefinitionDisentangledRepresentations,
  title = {Towards a {{Definition}} of {{Disentangled Representations}}},
  author = {Higgins, Irina and Amos, David and Pfau, David and Racaniere, Sebastien and Matthey, Loic and Rezende, Danilo and Lerchner, Alexander},
  year = {2018},
  month = dec,
  abstract = {How can intelligent agents solve a diverse set of tasks in a data-efficient manner? The disentangled representation learning approach posits that such an agent would benefit from separating out (disentangling) the underlying structure of the world into disjoint parts of its representation. However, there is no generally agreed-upon definition of disentangling, not least because it is unclear how to formalise the notion of world structure beyond toy datasets with a known ground truth generative process. Here we propose that a principled solution to characterising disentangled representations can be found by focusing on the transformation properties of the world. In particular, we suggest that those transformations that change only some properties of the underlying world state, while leaving all other properties invariant, are what gives exploitable structure to any kind of data. Similar ideas have already been successfully applied in physics, where the study of symmetry transformations has revolutionised the understanding of the world structure. By connecting symmetry transformations to vector representations using the formalism of group and representation theory we arrive at the first formal definition of disentangled representations. Our new definition is in agreement with many of the current intuitions about disentangling, while also providing principled resolutions to a number of previous points of contention. While this work focuses on formally defining disentangling - as opposed to solving the learning problem - we believe that the shift in perspective to studying data transformations can stimulate the development of better representation learning algorithms.},
  annotation = {ZSCC: 0000033},
  archivePrefix = {arXiv},
  eprint = {1812.02230},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/higgins et al_2018_towards a definition of disentangled representations.pdf;/home/trung/Zotero/storage/MDP4Q6W2/1812.html},
  journal = {arXiv:1812.02230 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{hill20_GroundedLanguageLearning,
  title = {Grounded {{Language Learning Fast}} and {{Slow}}},
  author = {Hill, Felix and Tieleman, Olivier and {von Glehn}, Tamara and Wong, Nathaniel and Merzic, Hamza and Clark, Stephen},
  year = {2020},
  month = sep,
  abstract = {Recent work has shown that large text-based neural language models, trained with conventional supervised learning objectives, acquire a surprising propensity for few- and one-shot learning. Here, we show that an embodied agent situated in a simulated 3D world, and endowed with a novel dual-coding external memory, can exhibit similar one-shot word learning when trained with conventional reinforcement learning algorithms. After a single introduction to a novel object via continuous visual perception and a language prompt ("This is a dax"), the agent can re-identify the object and manipulate it as instructed ("Put the dax on the bed"). In doing so, it seamlessly integrates short-term, within-episode knowledge of the appropriate referent for the word "dax" with long-term lexical and motor knowledge acquired across episodes (i.e. "bed" and "putting"). We find that, under certain training conditions and with a particular memory writing mechanism, the agent's one-shot word-object binding generalizes to novel exemplars within the same ShapeNet category, and is effective in settings with unfamiliar numbers of objects. We further show how dual-coding memory can be exploited as a signal for intrinsic motivation, stimulating the agent to seek names for objects that may be useful for later executing instructions. Together, the results demonstrate that deep neural networks can exploit meta-learning, episodic memory and an explicitly multi-modal environment to account for 'fast-mapping', a fundamental pillar of human cognitive development and a potentially transformative capacity for agents that interact with human users.},
  archivePrefix = {arXiv},
  eprint = {2009.01719},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hill et al_2020_grounded language learning fast and slow.pdf},
  journal = {arXiv:2009.01719 [cs]},
  primaryClass = {cs}
}

@incollection{hinton11_TransformingAutoEncoders,
  title = {Transforming {{Auto}}-{{Encoders}}},
  booktitle = {Artificial {{Neural Networks}} and {{Machine Learning}} \textendash{} {{ICANN}} 2011},
  author = {Hinton, Geoffrey E. and Krizhevsky, Alex and Wang, Sida D.},
  editor = {Honkela, Timo and Duch, W{\l}odzis{\l}aw and Girolami, Mark and Kaski, Samuel},
  year = {2011},
  volume = {6791},
  pages = {44--51},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-21735-7_6},
  abstract = {The artificial neural networks that are used to recognize shapes typically use one or more layers of learned feature detectors that produce scalar outputs. By contrast, the computer vision community uses complicated, hand-engineered features, like SIFT [6], that produce a whole vector of outputs including an explicit representation of the pose of the feature. We show how neural networks can be used to learn features that output a whole vector of instantiation parameters and we argue that this is a much more promising way of dealing with variations in position, orientation, scale and lighting than the methods currently employed in the neural networks community. It is also more promising than the handengineered features currently used in computer vision because it provides an efficient way of adapting the features to the domain.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/hinton et al_2011_transforming auto-encoders.pdf},
  isbn = {978-3-642-21734-0 978-3-642-21735-7},
  language = {en}
}

@article{hinton12_Improvingneuralnetworks,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  month = jul,
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  annotation = {ZSCC: 0004291},
  archivePrefix = {arXiv},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hinton et al_2012_improving neural networks by preventing co-adaptation of feature detectors.pdf;/home/trung/GoogleDrive/Zotero/hinton et al_2012_improving neural networks by preventing co-adaptation of feature detectors2.pdf;/home/trung/Zotero/storage/33H2BPK8/1207.html;/home/trung/Zotero/storage/ZUPJN56U/1207.html},
  journal = {arXiv:1207.0580 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,dropout,normalization},
  primaryClass = {cs}
}

@article{hinton18_MATRIXCAPSULESEM,
  title = {{{MATRIX CAPSULES WITH EM ROUTING}}},
  author = {Hinton, Geoffrey and Sabour, Sara and Frosst, Nicholas},
  year = {2018},
  pages = {15},
  abstract = {A capsule is a group of neurons whose outputs represent different properties of the same entity. Each layer in a capsule network contains many capsules. We describe a version of capsules in which each capsule has a logistic unit to represent the presence of an entity and a 4x4 matrix which could learn to represent the relationship between that entity and the viewer (the pose). A capsule in one layer votes for the pose matrix of many different capsules in the layer above by multiplying its own pose matrix by trainable viewpoint-invariant transformation matrices that could learn to represent part-whole relationships. Each of these votes is weighted by an assignment coefficient. These coefficients are iteratively updated for each image using the Expectation-Maximization algorithm such that the output of each capsule is routed to a capsule in the layer above that receives a cluster of similar votes. The transformation matrices are trained discriminatively by backpropagating through the unrolled iterations of EM between each pair of adjacent capsule layers. On the smallNORB benchmark, capsules reduce the number of test errors by 45\% compared to the state-of-the-art. Capsules also show far more resistance to white box adversarial attacks than our baseline convolutional neural network.},
  file = {/home/trung/GoogleDrive/Zotero/hinton et al_2018_matrix capsules with em routing.pdf},
  language = {en}
}

@article{hinz12_5HTPefficacycontraindications,
  title = {5-{{HTP}} Efficacy and Contraindications},
  author = {Hinz, Marty and Stein, Alvin and Uncini, Thomas},
  year = {2012},
  volume = {8},
  pages = {323--328},
  issn = {1176-6328},
  doi = {10.2147/NDT.S33259},
  abstract = {L-5-hydroxytryptophan (5-HTP) is the immediate precursor of serotonin. It is readily synthesized into serotonin without biochemical feedback. This nutrient has a large and strong following who advocate exaggerated and inaccurate claims relating to its effectiveness in the treatment of depression and a number of other serotonin-related diseases. These assertions are not supported by the science. Under close examination, 5-HTP may be contraindicated for depression in some of the very patients for whom promoters of 5-HTP advocate its use.},
  file = {/home/trung/GoogleDrive/Zotero/hinz et al_2012_5-htp efficacy and contraindications.pdf},
  journal = {Neuropsychiatric Disease and Treatment},
  pmcid = {PMC3415362},
  pmid = {22888252}
}

@article{hirsch20_StatisticalStoryVisual,
  title = {A {{Statistical Story}} of {{Visual Illusions}}},
  author = {Hirsch, Elad and Tal, Ayellet},
  year = {2020},
  month = may,
  abstract = {This paper explores the wholly empirical paradigm of visual illusions, which was introduced two decades ago in Neuro-Science. This data-driven approach attempts to explain visual illusions by the likelihood of patches in real-world images. Neither the data, nor the tools, existed at the time to extensively support this paradigm. In the era of big data and deep learning, at last, it becomes possible. This paper introduces a tool that computes the likelihood of patches, given a large dataset to learn from. Given this tool, we present an approach that manages to support the paradigm and explain visual illusions in a unified manner. Furthermore, we show how to generate (or enhance) visual illusions in natural images, by applying the same principles (and tool) reversely.},
  archivePrefix = {arXiv},
  eprint = {2005.08772},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hirsch et al_2020_a statistical story of visual illusions.pdf},
  journal = {arXiv:2005.08772 [cs]},
  primaryClass = {cs}
}

@article{hjelm19_Learningdeeprepresentations,
  title = {Learning Deep Representations by Mutual Information Estimation and Maximization},
  author = {Hjelm, R. Devon and Fedorov, Alex and {Lavoie-Marchildon}, Samuel and Grewal, Karan and Bachman, Phil and Trischler, Adam and Bengio, Yoshua},
  year = {2019},
  month = feb,
  abstract = {In this work, we perform unsupervised learning of representations by maximizing mutual information between an input and the output of a deep neural network encoder. Importantly, we show that structure matters: incorporating knowledge about locality of the input to the objective can greatly influence a representation's suitability for downstream tasks. We further control characteristics of the representation by matching to a prior distribution adversarially. Our method, which we call Deep InfoMax (DIM), outperforms a number of popular unsupervised learning methods and competes with fully-supervised learning on several classification tasks. DIM opens new avenues for unsupervised learning of representations and is an important step towards flexible formulations of representation-learning objectives for specific end-goals.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1808.06670},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hjelm et al_2019_learning deep representations by mutual information estimation and maximization.pdf;/home/trung/Zotero/storage/337HC3S5/1808.html},
  journal = {arXiv:1808.06670 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{ho20_DenoisingDiffusionProbabilistic,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  month = jun,
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion.},
  archivePrefix = {arXiv},
  eprint = {2006.11239},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ho et al_2020_denoising diffusion probabilistic models.pdf},
  journal = {arXiv:2006.11239 [cs, stat]},
  keywords = {_tablet},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ho20_InstabilityComputationalEfficiency,
  title = {Instability, {{Computational Efficiency}} and {{Statistical Accuracy}}},
  author = {Ho, Nhat and Khamaru, Koulik and Dwivedi, Raaz and Wainwright, Martin J. and Jordan, Michael I. and Yu, Bin},
  year = {2020},
  month = may,
  abstract = {Many statistical estimators are defined as the fixed point of a data-dependent operator, with estimators based on minimizing a cost function being an important special case. The limiting performance of such estimators depends on the properties of the population-level operator in the idealized limit of infinitely many samples. We develop a general framework that yields bounds on statistical accuracy based on the interplay between the deterministic convergence rate of the algorithm at the population level, and its degree of (in)stability when applied to an empirical object based on \$n\$ samples. Using this framework, we analyze both stable forms of gradient descent and some higher-order and unstable algorithms, including Newton's method and its cubic-regularized variant, as well as the EM algorithm. We provide applications of our general results to several concrete classes of models, including Gaussian mixture estimation, single-index models, and informative non-response models. We exhibit cases in which an unstable algorithm can achieve the same statistical accuracy as a stable algorithm in exponentially fewer steps---namely, with the number of iterations being reduced from polynomial to logarithmic in sample size \$n\$.},
  archivePrefix = {arXiv},
  eprint = {2005.11411},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ho et al_2020_instability, computational efficiency and statistical accuracy.pdf},
  journal = {arXiv:2005.11411 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{hoffman00_ELBOsurgeryanother,
  title = {{{ELBO}} Surgery: Yet Another Way to Carve up the Variational Evidence Lower Bound},
  author = {Hoffman, Matthew D and Johnson, Matthew J},
  pages = {4},
  abstract = {We rewrite the variational evidence lower bound objective (ELBO) of variational autoencoders in a way that highlights the role of the encoded data distribution. This perspective suggests that to improve our variational bounds we should improve our priors and not just the encoder and decoder.},
  file = {/home/trung/GoogleDrive/Zotero/hoffman et al_elbo surgery.pdf},
  keywords = {elbo broken,favorite,vae_issues,variational},
  language = {en}
}

@article{hoffman00_StochasticVariationalInference,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matthew D},
  pages = {45},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  annotation = {ZSCC: 0001509},
  file = {/home/trung/GoogleDrive/Zotero/hoffman_stochastic variational inference.pdf;/home/trung/GoogleDrive/Zotero/hoffman_stochastic variational inference2.pdf},
  language = {en}
}

@article{hoffman04_ProteinWhichBest,
  title = {Protein \textendash{} {{Which}} Is {{Best}}?},
  author = {Hoffman, Jay R. and Falvo, Michael J.},
  year = {2004},
  month = sep,
  volume = {3},
  pages = {118--130},
  issn = {1303-2968},
  abstract = {Protein intake that exceeds the recommended daily allowance is widely accepted for both endurance and power athletes. However, considering the variety of proteins that are available much less is known concerning the benefits of consuming one protein versus another. The purpose of this paper is to identify and analyze key factors in order to make responsible recommendations to both the general and athletic populations. Evaluation of a protein is fundamental in determining its appropriateness in the human diet. Proteins that are of inferior content and digestibility are important to recognize and restrict or limit in the diet. Similarly, such knowledge will provide an ability to identify proteins that provide the greatest benefit and should be consumed. The various techniques utilized to rate protein will be discussed. Traditionally, sources of dietary protein are seen as either being of animal or vegetable origin. Animal sources provide a complete source of protein (i.e. containing all essential amino acids), whereas vegetable sources generally lack one or more of the essential amino acids. Animal sources of dietary protein, despite providing a complete protein and numerous vitamins and minerals, have some health professionals concerned about the amount of saturated fat common in these foods compared to vegetable sources. The advent of processing techniques has shifted some of this attention and ignited the sports supplement marketplace with derivative products such as whey, casein and soy. Individually, these products vary in quality and applicability to certain populations. The benefits that these particular proteins possess are discussed. In addition, the impact that elevated protein consumption has on health and safety issues (i.e. bone health, renal function) are also reviewed.,                                        Key Points                                                          Higher protein needs are seen in athletic populations.                                               Animal proteins is an important source of protein, however potential health concerns do exist from a diet of protein consumed from primarily animal sources.                                               With a proper combination of sources, vegetable proteins may provide similar benefits as protein from animal sources.                                               Casein protein supplementation may provide the greatest benefit for increases in protein synthesis for a prolonged duration.},
  annotation = {ZSCC: 0000545},
  file = {/home/trung/GoogleDrive/Zotero/hoffman et al_2004_protein – which is best.pdf},
  journal = {Journal of Sports Science \& Medicine},
  number = {3},
  pmcid = {PMC3905294},
  pmid = {24482589}
}

@article{hoffman10_effectsacuteprolonged,
  title = {The Effects of Acute and Prolonged {{CRAM}} Supplementation on Reaction Time and Subjective Measures of Focus and Alertness in Healthy College Students},
  author = {Hoffman, Jay R and Ratamess, Nicholas A and Gonzalez, Adam and Beller, Noah A and Hoffman, Mattan W and Olson, Mark and Purpura, Martin and J{\"a}ger, Ralf},
  year = {2010},
  month = dec,
  volume = {7},
  pages = {39},
  issn = {1550-2783},
  doi = {10.1186/1550-2783-7-39},
  abstract = {Background The purpose of this study was to examine the effect of acute and prolonged (4-weeks) ingestion of a supplement designed to improve reaction time and subjective measures of alertness, energy, fatigue, and focus compared to placebo. Methods Nineteen physically-active subjects (17 men and 2 women) were randomly assigned to a group that either consumed a supplement (21.1 {$\pm$} 0.6 years; body mass: 80.6 {$\pm$} 9.4 kg) or placebo (21.3 {$\pm$} 0.8 years; body mass: 83.4 {$\pm$} 18.5 kg). During the initial testing session (T1), subjects were provided 1.5 g of the supplement (CRAM; {$\alpha$}-glycerophosphocholine, choline bitartrate, phosphatidylserine, vitamins B3, B6, and B12, folic acid, L-tyrosine, anhydrous caffeine, acetyl-L-carnitine, and naringin) or a placebo (PL), and rested quietly for 10-minutes before completing a questionnaire on subjective feelings of energy, fatigue, alertness and focus (PRE). Subjects then performed a 4-minute quickness and reaction test followed by a 10-min bout of exhaustive exercise. The questionnaire and reaction testing sequence was then repeated (POST). Subjects reported back to the lab (T2) following 4-weeks of supplementation and repeated the testing sequence. Results Reaction time significantly declined (p = 0.050) between PRE and POST at T1 in subjects consuming PL, while subjects under CRAM supplementation were able to maintain (p = 0.114) their performance. Significant performance declines were seen in both groups from PRE to POST at T2. Elevations in fatigue were seen for CRAM at both T1 and T2 (p = 0.001 and p = 0.000, respectively), but only at T2 for PL (p = 0.029). Subjects in CRAM maintained focus between PRE and POST during both T1 and T2 trials (p = 0.152 and p = 0.082, respectively), whereas significant declines in focus were observed between PRE and POST in PL at both trials (p = 0.037 and p = 0.014, respectively). No difference in alertness was seen at T1 between PRE and POST for CRAM (p = 0.083), but a significant decline was recorded at T2 (p = 0.005). Alertness was significantly lower at POST at both T1 and T2 for PL (p = 0.040 and p = 0.33, respectively). No differences in any of these subjective measures were seen between the groups at any time point. Conclusion Results indicate that acute ingestion of CRAM can maintain reaction time, and subjective feelings of focus and alertness to both visual and auditory stimuli in healthy college students following exhaustive exercise. However, some habituation may occur following 4-weeks of supplementation.},
  file = {/home/trung/GoogleDrive/Zotero/hoffman et al_2010_the effects of acute and prolonged cram supplementation on reaction time and subjective measures of focus and alertness in healthy college students.pdf},
  journal = {Journal of the International Society of Sports Nutrition},
  pmcid = {PMC3009695},
  pmid = {21156078}
}

@article{hoffman11_NoUTurnSamplerAdaptively,
  title = {The {{No}}-{{U}}-{{Turn Sampler}}: {{Adaptively Setting Path Lengths}} in {{Hamiltonian Monte Carlo}}},
  shorttitle = {The {{No}}-{{U}}-{{Turn Sampler}}},
  author = {Hoffman, Matthew D. and Gelman, Andrew},
  year = {2011},
  month = nov,
  abstract = {Hamiltonian Monte Carlo (HMC) is a Markov chain Monte Carlo (MCMC) algorithm that avoids the random walk behavior and sensitivity to correlated parameters that plague many MCMC methods by taking a series of steps informed by first-order gradient information. These features allow it to converge to high-dimensional target distributions much more quickly than simpler methods such as random walk Metropolis or Gibbs sampling. However, HMC's performance is highly sensitive to two user-specified parameters: a step size \{\textbackslash epsilon\} and a desired number of steps L. In particular, if L is too small then the algorithm exhibits undesirable random walk behavior, while if L is too large the algorithm wastes computation. We introduce the No-U-Turn Sampler (NUTS), an extension to HMC that eliminates the need to set a number of steps L. NUTS uses a recursive algorithm to build a set of likely candidate points that spans a wide swath of the target distribution, stopping automatically when it starts to double back and retrace its steps. Empirically, NUTS perform at least as efficiently as and sometimes more efficiently than a well tuned standard HMC method, without requiring user intervention or costly tuning runs. We also derive a method for adapting the step size parameter \{\textbackslash epsilon\} on the fly based on primal-dual averaging. NUTS can thus be used with no hand-tuning at all. NUTS is also suitable for applications such as BUGS-style automatic inference engines that require efficient "turnkey" sampling algorithms.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1111.4246},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hoffman et al_2011_the no-u-turn sampler.pdf;/home/trung/Zotero/storage/2HZ5KX7L/1111.html},
  journal = {arXiv:1111.4246 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Computation},
  primaryClass = {cs, stat}
}

@article{hoffmann20_AlgebraNets,
  title = {{{AlgebraNets}}},
  author = {Hoffmann, Jordan and Schmitt, Simon and Osindero, Simon and Simonyan, Karen and Elsen, Erich},
  year = {2020},
  month = jun,
  abstract = {Neural networks have historically been built layerwise from the set of functions in \$\{f: \textbackslash mathbb\{R\}\^n \textbackslash to \textbackslash mathbb\{R\}\^m \}\$, i.e. with activations and weights/parameters represented by real numbers, \$\textbackslash mathbb\{R\}\$. Our work considers a richer set of objects for activations and weights, and undertakes a comprehensive study of alternative algebras as number representations by studying their performance on two challenging problems: large-scale image classification using the ImageNet dataset and language modeling using the enwiki8 and WikiText-103 datasets. We denote this broader class of models as AlgebraNets. Our findings indicate that the conclusions of prior work, which explored neural networks constructed from \$\textbackslash mathbb\{C\}\$ (complex numbers) and \$\textbackslash mathbb\{H\}\$ (quaternions) on smaller datasets, do not always transfer to these challenging settings. However, our results demonstrate that there are alternative algebras which deliver better parameter and computational efficiency compared with \$\textbackslash mathbb\{R\}\$. We consider \$\textbackslash mathbb\{C\}\$, \$\textbackslash mathbb\{H\}\$, \$M\_\{2\}(\textbackslash mathbb\{R\})\$ (the set of \$2\textbackslash times2\$ real-valued matrices), \$M\_\{2\}(\textbackslash mathbb\{C\})\$, \$M\_\{3\}(\textbackslash mathbb\{R\})\$ and \$M\_\{4\}(\textbackslash mathbb\{R\})\$. Additionally, we note that multiplication in these algebras has higher compute density than real multiplication, a useful property in situations with inherently limited parameter reuse such as auto-regressive inference and sparse neural networks. We therefore investigate how to induce sparsity within AlgebraNets. We hope that our strong results on large-scale, practical benchmarks will spur further exploration of these unconventional architectures which challenge the default choice of using real numbers for neural network weights and activations.},
  archivePrefix = {arXiv},
  eprint = {2006.07360},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hoffmann et al_2020_algebranets.pdf},
  journal = {arXiv:2006.07360 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{honchar19_GenerativeAIKey,
  title = {Generative {{AI}}: {{A Key}} to {{Machine Intelligence}}?},
  shorttitle = {Generative {{AI}}},
  author = {Honchar, Alexandr},
  year = {2019},
  month = sep,
  abstract = {We're living in the age of the next industrial revolution: the very first three had freed most of the humans from the hard handwork labor\ldots},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/IZKPHQZR/generative-ai-a-key-to-machine-intelligence-674c89a81bc.html},
  howpublished = {https://medium.com/cantors-paradise/generative-ai-a-key-to-machine-intelligence-674c89a81bc},
  journal = {Medium},
  language = {en}
}

@article{hong00_HowGenerativeAdversarial,
  title = {How {{Generative Adversarial Networks}} and {{Their Variants Work}}},
  author = {Hong, Yongjun and Hwang, Uiwon and Yoo, Jaeyoon and Yoon, Sungroh},
  volume = {52},
  pages = {43},
  annotation = {ZSCC: 0000027},
  file = {/home/trung/GoogleDrive/Zotero/hong et al_how generative adversarial networks and their variants work.pdf},
  journal = {ACM Computing Surveys},
  language = {en},
  number = {1}
}

@article{honke20_Representationlearningimproved,
  title = {Representation Learning for Improved Interpretability and Classification Accuracy of Clinical Factors from {{EEG}}},
  author = {Honke, Garrett and Higgins, Irina and Thigpen, Nina and Miskovic, Vladimir and Link, Katie and Gupta, Pramod and Klawohn, Julia and Hajcak, Greg},
  year = {2020},
  month = oct,
  abstract = {Despite extensive standardization, diagnostic interviews for mental health disorders encompass substantial subjective judgment. Previous studies have demonstrated that EEG-based neural measures can function as reliable objective correlates of depression, or even predictors of depression and its course. However, their clinical utility has not been fully realized because of 1) the lack of automated ways to deal with the inherent noise associated with EEG data at scale, and 2) the lack of knowledge of which aspects of the EEG signal may be markers of a clinical disorder. Here we adapt an unsupervised pipeline from the recent deep representation learning literature to address these problems by 1) learning a disentangled representation using \$\textbackslash beta\$-VAE to denoise the signal, and 2) extracting interpretable features associated with a sparse set of clinical labels using a Symbol-Concept Association Network (SCAN). We demonstrate that our method is able to outperform the canonical hand-engineered baseline classification method on a number of factors, including participant age and depression diagnosis. Furthermore, our method recovers a representation that can be used to automatically extract denoised Event Related Potentials (ERPs) from novel, single EEG trajectories, and supports fast supervised re-mapping to various clinical labels, allowing clinicians to re-use a single EEG representation regardless of updates to the standardized diagnostic system. Finally, single factors of the learned disentangled representations often correspond to meaningful markers of clinical factors, as automatically detected by SCAN, allowing for human interpretability and post-hoc expert analysis of the recommendations made by the model.},
  archivePrefix = {arXiv},
  eprint = {2010.15274},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/honke et al_2020_representation learning for improved interpretability and classification accuracy of clinical factors from eeg.pdf},
  journal = {arXiv:2010.15274 [cs, eess]},
  keywords = {_tablet,disentanglement,representation},
  primaryClass = {cs, eess}
}

@article{hoogeboom20_LearningDiscreteDistributions,
  title = {Learning {{Discrete Distributions}} by {{Dequantization}}},
  author = {Hoogeboom, Emiel and Cohen, Taco S. and Tomczak, Jakub M.},
  year = {2020},
  month = jan,
  abstract = {Media is generally stored digitally and is therefore discrete. Many successful deep distribution models in deep learning learn a density, i.e., the distribution of a continuous random variable. Na\textasciidieresis\i ve optimization on discrete data leads to arbitrarily high likelihoods, and instead, it has become standard practice to add noise to datapoints. In this paper, we present a general framework for dequantization that captures existing methods as a special case. We derive two new dequantization objectives: importance-weighted (iw) dequantization and Re\textasciiacute nyi dequantization. In addition, we introduce autoregressive dequantization (ARD) for more flexible dequantization distributions. Empirically we find that iw and Re\textasciiacute nyi dequantization considerably improve performance for uniform dequantization distributions. ARD achieves a negative log-likelihood of 3.06 bits per dimension on CIFAR10, which to the best of our knowledge is state-of-the-art among distribution models that do not require autoregressive inverses for sampling.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.11235},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/V8WKH3DQ/Hoogeboom et al. - 2020 - Learning Discrete Distributions by Dequantization.pdf},
  journal = {arXiv:2001.11235 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{hooker19_benchmarkinterpretabilitymethods,
  title = {A Benchmark for Interpretability Methods in Deep Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Hooker, Sara and Erhan, Dumitru and Kindermans, Pieter-Jan and Kim, Been},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {9737--9748},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/hooker et al_2019_a benchmark for interpretability methods in deep neural networks.pdf;/home/trung/GoogleDrive/Zotero/hooker et al_2019_a benchmark for interpretability methods in deep neural networks2.pdf}
}

@article{hooker19_PleaseStopPermuting,
  title = {Please {{Stop Permuting Features}}: {{An Explanation}} and {{Alternatives}}},
  shorttitle = {Please {{Stop Permuting Features}}},
  author = {Hooker, Giles and Mentch, Lucas},
  year = {2019},
  month = may,
  abstract = {This paper advocates against permute-and-predict (PaP) methods for interpreting black box functions. Methods such as the variable importance measures proposed for random forests, partial dependence plots, and individual conditional expectation plots remain popular because of their ability to provide model-agnostic measures that depend only on the pre-trained model output. However, numerous studies have found that these tools can produce diagnostics that are highly misleading, particularly when there is strong dependence among features. Rather than simply add to this growing literature by further demonstrating such issues, here we seek to provide an explanation for the observed behavior. In particular, we argue that breaking dependencies between features in hold-out data places undue emphasis on sparse regions of the feature space by forcing the original model to extrapolate to regions where there is little to no data. We explore these effects through various settings where a ground-truth is understood and find support for previous claims in the literature that PaP metrics tend to over-emphasize correlated features both in variable importance and partial dependence plots, even though applying permutation methods to the ground-truth models do not. As an alternative, we recommend more direct approaches that have proven successful in other settings: explicitly removing features, conditional permutations, or model distillation methods.},
  archivePrefix = {arXiv},
  eprint = {1905.03151},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hooker et al_2019_please stop permuting features.pdf},
  journal = {arXiv:1905.03151 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{hooker20_CharacterizingMitigatingBias,
  title = {Characterizing and {{Mitigating Bias}} in {{Compact Models}}},
  author = {Hooker, Sara and Moorosi, Nyalleng and Clark, Gregory and Bengio, Samy and Denton, Emily},
  year = {2020},
  pages = {10},
  abstract = {The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. Using these techniques to achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy) is possible. However, overall accuracy hides disproportionately high errors on a small subset of examples. We call this subset Compression Identified Exemplars (CIE). We establish that compression disproportionately impacts error on this underrepresented subset and amplifies existing algorithmic bias. We propose CIE as a human-in-the-loop auditing tool to surface a tractable subset of the dataset for further inspection or annotation by a domain expert. We demonstrate that CIE can both characterize and mitigate bias. Fine-tuning with upweighted CIE is sufficient to mitigate the harm caused by compression and even improves upon the baseline model performance.},
  file = {/home/trung/GoogleDrive/Zotero/hooker et al_2020_characterizing and mitigating bias in compact models.pdf},
  language = {en}
}

@article{hooker20_HardwareLottery,
  title = {The {{Hardware Lottery}}},
  author = {Hooker, Sara},
  year = {2020},
  month = sep,
  abstract = {Hardware, systems and algorithms research communities have historically had different incentive structures and fluctuating motivation to engage with each other explicitly. This historical treatment is odd given that hardware and software have frequently determined which research ideas succeed (and fail). This essay introduces the term hardware lottery to describe when a research idea wins because it is suited to the available software and hardware and not because the idea is superior to alternative research directions. Examples from early computer science history illustrate how hardware lotteries can delay research progress by casting successful ideas as failures. These lessons are particularly salient given the advent of domain specialized hardware which makes it increasingly costly to stray off of the beaten path of research ideas.},
  archivePrefix = {arXiv},
  eprint = {2009.06489},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hooker_2020_the hardware lottery.pdf},
  journal = {arXiv:2009.06489 [cs]},
  primaryClass = {cs}
}

@article{hooker20_WhatCompressedDeep,
  title = {What {{Do Compressed Deep Neural Networks Forget}}?},
  author = {Hooker, Sara and Courville, Aaron and Clark, Gregory and Dauphin, Yann and Frome, Andrea},
  year = {2020},
  month = jul,
  abstract = {Deep neural network pruning and quantization techniques have demonstrated it is possible to achieve high levels of compression with surprisingly little degradation to test set accuracy. However, this measure of performance conceals significant differences in how different classes and images are impacted by model compression techniques. We find that models with radically different numbers of weights have comparable top-line performance metrics but diverge considerably in behavior on a narrow subset of the dataset. This small subset of data points, which we term Pruning Identified Exemplars (PIEs) are systematically more impacted by the introduction of sparsity. Compression disproportionately impacts model performance on the underrepresented long-tail of the data distribution. PIEs over-index on atypical or noisy images that are far more challenging for both humans and algorithms to classify. Our work provides intuition into the role of capacity in deep neural networks and the trade-offs incurred by compression. An understanding of this disparate impact is critical given the widespread deployment of compressed models in the wild.},
  archivePrefix = {arXiv},
  eprint = {1911.05248},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hooker et al_2020_what do compressed deep neural networks forget.pdf},
  journal = {arXiv:1911.05248 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{horita19_UnseenFoodCreation,
  title = {Unseen {{Food Creation}} by {{Mixing Existing Food Images}} with {{Conditional StyleGAN}}},
  booktitle = {Proceedings of the 5th {{International Workshop}} on {{Multimedia Assisted Dietary Management}}  - {{MADiMa}} '19},
  author = {Horita, Daichi and Shimoda, Wataru and Yanai, Keiji},
  year = {2019},
  pages = {19--24},
  publisher = {{ACM Press}},
  address = {{Nice, France}},
  doi = {10.1145/3347448.3357166},
  abstract = {In recent years, thanks to the development of generative adversarial networks (GAN), it has become possible to generate food images. However, the quality is still low and it is difficult to generate appetizing and delicious-looking images. In the latest GAN study, StyleGAN [12] enabled high-level feature separation and stochastic variation of generated images by unsupervised learning. However, to manipulate any style, it is necessary to understand the representation of the latent space and to input reference images. In this paper, we propose a conditional version of StyleGAN to control probabilistic fluctuations of disentangled features. The conditional style-based generator can manipulate the style of any domain by providing conditional vectors. By applying the conditional StyleGAN to the food image domain, we successfully have generated higher quality food images than before. In addition, introducing conditional vectors enabled us to control the food categories of generated images and to generate unseen foods by mixing multiple kinds of foods. In the experiment, to show the result of the proposed method explicitly, Food13 dataset is constructed and evaluated by both qualitative evaluation and quantitative evaluation.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/horita et al_2019_unseen food creation by mixing existing food images with conditional stylegan.pdf},
  isbn = {978-1-4503-6916-9},
  keywords = {conditional,food generation,gan},
  language = {en}
}

@article{hosoya18_simpleprobabilisticdeep,
  title = {A Simple Probabilistic Deep Generative Model for Learning Generalizable Disentangled Representations from Grouped Data},
  author = {Hosoya, Haruo},
  year = {2018},
  month = sep,
  abstract = {The disentangling problem is to discover multiple complex factors of variations hidden in data. One recent approach is to take a dataset with grouping structure and separately estimate a factor common within a group (content) and a factor specific to each group member (transformation). Notably, this approach can learn to represent a continuous space of contents, which allows for generalization to data with unseen contents. In this study, we aim at cultivating this approach within probabilistic deep generative models. Motivated by technical complication in existing groupbased methods, we propose a simpler probabilistic method, called group-contrastive variational autoencoders. Despite its simplicity, our approach achieves reasonable disentanglement with generalizability for three grouped datasets of 3D object images. In comparison with a previous model, although conventional qualitative evaluation shows little difference, our qualitative evaluation using few-shot classification exhibits superior performances for some datasets. We analyze the content representations from different methods and discuss their transformation-dependency and potential performance impacts.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1809.02383},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/XEBCKIIN/Hosoya - 2018 - A simple probabilistic deep generative model for l.pdf},
  journal = {arXiv:1809.02383 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{hosoya19_GroupbasedLearningDisentangled,
  title = {Group-Based {{Learning}} of {{Disentangled Representations}} with {{Generalizability}} for {{Novel Contents}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Hosoya, Haruo},
  year = {2019},
  month = aug,
  pages = {2506--2513},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/348},
  abstract = {Sensory data are often comprised of independent content and transformation factors. For example, face images may have shapes as content and poses as transformation. To infer separately these factors from given data, various ``disentangling'' models have been proposed. However, many of these are supervised or semi-supervised, either requiring attribute labels that are often unavailable or disallowing for generalization over new contents. In this study, we introduce a novel deep generative model, called group-based variational autoencoders. In this, we assume no explicit labels, but a weaker form of structure that groups together data instances having the same content but transformed differently; we thereby separately estimate a group-common factor as content and an instance-specific factor as transformation. This approach allows for learning to represent a general continuous space of contents, which can accommodate unseen contents. Despite the simplicity, our model succeeded in learning, from five datasets, content representations that are highly separate from the transformation representation and generalizable to data with novel contents. We further provide detailed analysis of the latent content code and show insight into how our model obtains the notable transformation invariance and content generalizability.},
  annotation = {ZSCC: 0000003},
  file = {/home/trung/GoogleDrive/Zotero/hosoya_2019_group-based learning of disentangled representations with generalizability for novel contents.pdf},
  isbn = {978-0-9992411-4-1},
  keywords = {disentanglement},
  language = {en}
}

@article{hospedales20_MetaLearningNeuralNetworks,
  title = {Meta-{{Learning}} in {{Neural Networks}}: {{A Survey}}},
  shorttitle = {Meta-{{Learning}} in {{Neural Networks}}},
  author = {Hospedales, Timothy and Antoniou, Antreas and Micaelli, Paul and Storkey, Amos},
  year = {2020},
  month = apr,
  abstract = {The field of meta-learning, or learning-to-learn, has seen a dramatic rise in interest in recent years. Contrary to conventional approaches to AI where a given task is solved from scratch using a fixed learning algorithm, meta-learning aims to improve the learning algorithm itself, given the experience of multiple learning episodes. This paradigm provides an opportunity to tackle many of the conventional challenges of deep learning, including data and computation bottlenecks, as well as the fundamental issue of generalization. In this survey we describe the contemporary meta-learning landscape. We first discuss definitions of meta-learning and position it with respect to related fields, such as transfer learning, multi-task learning, and hyperparameter optimization. We then propose a new taxonomy that provides a more comprehensive breakdown of the space of meta-learning methods today. We survey promising applications and successes of meta-learning including few-shot learning, reinforcement learning and architecture search. Finally, we discuss outstanding challenges and promising areas for future research.},
  archivePrefix = {arXiv},
  eprint = {2004.05439},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hospedales et al_2020_meta-learning in neural networks.pdf},
  journal = {arXiv:2004.05439 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{hou18_RandomizedControlledTrial,
  title = {A {{Randomized Controlled Trial}} to {{Compare}} the {{Effect}} of {{Peanuts}} and {{Almonds}} on the {{Cardio}}-{{Metabolic}} and {{Inflammatory Parameters}} in {{Patients}} with {{Type}} 2 {{Diabetes Mellitus}}},
  author = {Hou, Yun-Ying and Ojo, Omorogieva and Wang, Li-Li and Wang, Qi and Jiang, Qing and Shao, Xin-Yu and Wang, Xiao-Hua},
  year = {2018},
  month = oct,
  volume = {10},
  issn = {2072-6643},
  doi = {10.3390/nu10111565},
  abstract = {A low carbohydrate diet (LCD), with some staple food being replaced with nuts, has been shown to reduce weight, improve blood glucose, and regulate blood lipid in patients with type 2 diabetes mellitus (T2DM). These nuts include tree nuts and ground nuts. Tree nut consumption is associated with improved cardio-vascular and inflammatory parameters. However, the consumption of tree nuts is difficult to promote in patients with diabetes because of their high cost. As the main ground nut, peanuts contain a large number of beneficial nutrients, are widely planted, and are affordable for most patients. However, whether peanuts and tree nuts in combination with LCD have similar benefits in patients with T2DM remains unknown; although almonds are the most consumed and studied tree nut. This study sought to compare the effect of peanuts and almonds, incorporated into a LCD, on cardio-metabolic and inflammatory measures in patients with T2DM. Of the 32 T2DM patients that were recruited, 17 were randomly allocated to the Peanut group (n = 17) and 15 to the Almond group (n = 15) in a parallel design. The patients consumed a LCD with part of the starchy staple food being replaced with peanuts (Peanut group) or almonds (Almond group). The follow-up duration was three months. The indicators for glycemic control, other cardio-metabolic, and inflammatory parameters were collected and compared between the two groups. Twenty-five patients completed the study. There were no significant differences in the self-reported dietary compliance between the two groups. Compared with the baseline, the fasting blood glucose (FBG) and postprandial 2-h blood glucose (PPG) decreased in both the Peanut and Almond groups (p {$<$} 0.05). After the intervention, no statistically significant differences were found between the Peanut group and the Almond group with respect to the FBG and PPG levels. A decrease in the glycated hemoglobin A1c (HbA1c) level from the baseline in the Almond group was found (p {$<$} 0.05). However, no significant difference was found between the two groups with respect to the HbA1c level at the third month. The peanut and almond consumption did not increase the body mass index (BMI) and had no effect on the blood lipid profile or interleukin-6 (IL-6).In conclusion, incorporated into a LCD, almonds and peanuts have a similar effect on improving fasting and postprandial blood glucose among patients with T2DM. However, more studies are required to fully establish the effect of almond on the improvement of HbA1c.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/hou et al_2018_a randomized controlled trial to compare the effect of peanuts and almonds on the cardio-metabolic and inflammatory parameters in patients with type 2 diabetes mellitus.pdf},
  journal = {Nutrients},
  language = {eng},
  number = {11},
  pmcid = {PMC6267433},
  pmid = {30360498}
}

@techreport{hou20_SystematicEvaluationSinglecell,
  title = {A {{Systematic Evaluation}} of {{Single}}-Cell {{RNA}}-Sequencing {{Imputation Methods}}},
  author = {Hou, Wenpin and Ji, Zhicheng and Ji, Hongkai and Hicks, Stephanie C.},
  year = {2020},
  month = jan,
  institution = {{Genomics}},
  doi = {10.1101/2020.01.29.925974},
  abstract = {ABSTRACT           The rapid development of single-cell RNA-sequencing (scRNA-seq) technology, with increased sparsity compared to bulk RNA-sequencing (RNA-seq), has led to the emergence of many methods for preprocessing, including imputation methods. Here, we systematically evaluate the performance of 18 state-of-the-art scRNA-seq imputation methods using cell line and tissue data measured across experimental protocols. Specifically, we assess the similarity of imputed cell profiles to bulk samples as well as investigate whether methods recover relevant biological signals or introduce spurious noise in three downstream analyses: differential expression, unsupervised clustering, and inferring pseudotemporal trajectories. Broadly, we found significant variability in the performance of the methods across evaluation settings. While most scRNA-seq imputation methods recover biological expression observed in bulk RNA-seq data, the majority of the methods do not improve performance in downstream analyses compared to no imputation, in particular for clustering and trajectory analysis, and thus should be used with caution. Furthermore, we find that the performance of scRNA-seq imputation methods depends on many factors including the experimental protocol, the sparsity of the data, the number of cells in the dataset, and the magnitude of the effect sizes. We summarize our results and provide a key set of recommendations for users and investigators to navigate the current space of scRNA-seq imputation methods.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/Zotero/storage/4SW6ZJMM/Hou et al. - 2020 - A Systematic Evaluation of Single-cell RNA-sequenc.pdf},
  language = {en},
  type = {Preprint}
}

@article{howard17_MobileNetsEfficientConvolutional,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  annotation = {ZSCC: 0002981},
  archivePrefix = {arXiv},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/howard et al_2017_mobilenets.pdf;/home/trung/Zotero/storage/YLCW84XP/1704.html},
  journal = {arXiv:1704.04861 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{howard18_UniversalLanguageModel,
  title = {Universal {{Language Model Fine}}-Tuning for {{Text Classification}}},
  author = {Howard, Jeremy and Ruder, Sebastian},
  year = {2018},
  month = jan,
  abstract = {Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24\% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.},
  archivePrefix = {arXiv},
  eprint = {1801.06146},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/howard et al_2018_universal language model fine-tuning for text classification.pdf;/home/trung/Zotero/storage/86NRUN35/1801.html},
  journal = {arXiv:1801.06146 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{hoyer19_Neuralreparameterizationimproves,
  title = {Neural Reparameterization Improves Structural Optimization},
  author = {Hoyer, Stephan and {Sohl-Dickstein}, Jascha and Greydanus, Sam},
  year = {2019},
  month = sep,
  abstract = {Structural optimization is a popular method for designing objects such as bridge trusses, airplane wings, and optical devices. Unfortunately, the quality of solutions depends heavily on how the problem is parameterized. In this paper, we propose using the implicit bias over functions induced by neural networks to improve the parameterization of structural optimization. Rather than directly optimizing densities on a grid, we instead optimize the parameters of a neural network which outputs those densities. This reparameterization leads to different and often better solutions. On a selection of 116 structural optimization tasks, our approach produces the best design 50\% more often than the best baseline method.},
  archivePrefix = {arXiv},
  eprint = {1909.04240},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hoyer et al_2019_neural reparameterization improves structural optimization.pdf;/home/trung/GoogleDrive/Zotero/hoyer et al_2019_neural reparameterization improves structural optimization2.pdf;/home/trung/Zotero/storage/BLN9UGTZ/1909.html;/home/trung/Zotero/storage/C34D7GLU/1909.html},
  journal = {arXiv:1909.04240 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{hsu17_UnsupervisedLearningDisentangled,
  title = {Unsupervised {{Learning}} of {{Disentangled}} and {{Interpretable Representations}} from {{Sequential Data}}},
  author = {Hsu, Wei-Ning and Zhang, Yu and Glass, James},
  year = {2017},
  month = sep,
  abstract = {We present a factorized hierarchical variational autoencoder, which learns disentangled and interpretable representations from sequential data without supervision. Specifically, we exploit the multi-scale nature of information in sequential data by formulating it explicitly within a factorized hierarchical graphical model that imposes sequence-dependent priors and sequence-independent priors to different sets of latent variables. The model is evaluated on two speech corpora to demonstrate, qualitatively, its ability to transform speakers or linguistic content by manipulating different sets of latent variables; and quantitatively, its ability to outperform an i-vector baseline for speaker verification and reduce the word error rate by as much as 35\% in mismatched train/test scenarios for automatic speech recognition tasks.},
  annotation = {ZSCC: 0000111},
  archivePrefix = {arXiv},
  eprint = {1709.07902},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hsu et al_2017_unsupervised learning of disentangled and interpretable representations from sequential data.pdf;/home/trung/Zotero/storage/YPTUZXFI/1709.html},
  journal = {arXiv:1709.07902 [cs, eess, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, eess, stat}
}

@article{hsu19_RobustNeuralVocoding,
  title = {Towards {{Robust Neural Vocoding}} for {{Speech Generation}}: {{A Survey}}},
  shorttitle = {Towards {{Robust Neural Vocoding}} for {{Speech Generation}}},
  author = {Hsu, Po-chun and Wang, Chun-hsuan and Liu, Andy T. and Lee, Hung-yi},
  year = {2019},
  month = dec,
  abstract = {Recently, neural vocoders have been widely used in speech synthesis tasks, including text-to-speech and voice conversion. However, in the encounter of data distribution mismatch between training and inference, neural vocoders trained on real data often degrade in voice quality for unseen scenarios. In this paper, we train three commonly used neural vocoders, including WaveNet, WaveRNN, and WaveGlow, alternately on five different datasets. To study the robustness of neural vocoders, we evaluate the models using acoustic features from seen/unseen speakers, seen/unseen languages, a text-to-speech model, and a voice conversion model. In this work, we found that WaveNet is more robust than WaveRNN, especially in the face of inconsistency between training and testing data. Through our experiments, we show that WaveNet is more suitable for text-to-speech models, and WaveRNN more suitable for voice conversion applications. Furthermore, we present results with considerable reference value of subjective human evaluation for future studies.},
  archivePrefix = {arXiv},
  eprint = {1912.02461},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/2LR5F9LX/Hsu et al. - 2019 - Towards Robust Neural Vocoding for Speech Generati.pdf},
  journal = {arXiv:1912.02461 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{hu00_LearningDataManipulation,
  title = {Learning {{Data Manipulation}} for {{Augmentation}} and {{Weighting}}},
  author = {Hu, Zhiting and Tan, Bowen and Salakhutdinov, Ruslan and Mitchell, Tom and Xing, Eric P},
  pages = {10},
  abstract = {Manipulating data, such as weighting data examples or augmenting with new instances, has been increasingly used to improve model training. Previous work has studied various rule- or learning-based approaches designed for specific types of data manipulation. In this work, we propose a new method that supports learning different manipulation schemes with the same gradient-based algorithm. Our approach builds upon a recent connection of supervised learning and reinforcement learning (RL), and adapts an off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training. Different parameterization of the ``reward'' function instantiates different manipulation schemes. We showcase data augmentation that learns a text transformation network, and data weighting that dynamically adapts the data sample importance. Experiments show the resulting algorithms significantly improve the image and text classification performance in low data regime and class-imbalance problems.},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_learning data manipulation for augmentation and weighting.pdf;/home/trung/GoogleDrive/Zotero/hu et al_learning data manipulation for augmentation and weighting2.pdf;/home/trung/GoogleDrive/Zotero/hu et al_learning data manipulation for augmentation and weighting3.pdf},
  language = {en}
}

@inproceedings{hu17_SemisupervisedMaxmarginTopic,
  title = {Semi-Supervised {{Max}}-Margin {{Topic Model}} with {{Manifold Posterior Regularization}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Sixth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Hu, Wenbo and Zhu, Jun and Su, Hang and Zhuo, Jingwei and Zhang, Bo},
  year = {2017},
  month = aug,
  pages = {1865--1871},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Melbourne, Australia}},
  doi = {10.24963/ijcai.2017/259},
  abstract = {Supervised topic models leverage label information to learn discriminative latent topic representations. As collecting a fully labeled dataset is often time-consuming, semi-supervised learning is of high interest. In this paper, we present an effective semi-supervised max-margin topic model by naturally introducing manifold posterior regularization to a regularized Bayesian topic model, named LapMedLDA. The model jointly learns latent topics and a related classifier with only a small fraction of labeled documents. To perform the approximate inference, we derive an efficient stochastic gradient MCMC method. Unlike the previous semi-supervised topic models, our model adopts a tight coupling between the generative topic model and the discriminative classifier. Extensive experiments demonstrate that such tight coupling brings significant benefits in quantitative and qualitative performance.},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_2017_semi-supervised max-margin topic model with manifold posterior regularization.pdf},
  isbn = {978-0-9992411-0-3},
  language = {en}
}

@article{hu18_DeepGenerativeModels,
  title = {Deep {{Generative Models}} with {{Learnable Knowledge Constraints}}},
  author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Liang, Xiaodan and Qin, Lianhui and Dong, Haoye and Xing, Eric},
  year = {2018},
  month = jun,
  abstract = {The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.},
  archivePrefix = {arXiv},
  eprint = {1806.09764},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_2018_deep generative models with learnable knowledge constraints.pdf;/home/trung/Zotero/storage/GVKZ5V6K/1806.html},
  journal = {arXiv:1806.09764 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hu18_DisentanglingFactorsVariation,
  title = {Disentangling {{Factors}} of {{Variation}} by {{Mixing Them}}},
  author = {Hu, Qiyang and Szab{\'o}, Attila and Portenier, Tiziano and Zwicker, Matthias and Favaro, Paolo},
  year = {2018},
  month = mar,
  abstract = {We propose an approach to learn image representations that consist of disentangled factors of variation without exploiting any manual labeling or data domain knowledge. A factor of variation corresponds to an image attribute that can be discerned consistently across a set of images, such as the pose or color of objects. Our disentangled representation consists of a concatenation of feature chunks, each chunk representing a factor of variation. It supports applications such as transferring attributes from one image to another, by simply mixing and unmixing feature chunks, and classification or retrieval based on one or several attributes, by considering a user-specified subset of feature chunks. We learn our representation without any labeling or knowledge of the data domain, using an autoencoder architecture with two novel training objectives: first, we propose an invariance objective to encourage that encoding of each attribute, and decoding of each chunk, are invariant to changes in other attributes and chunks, respectively; second, we include a classification objective, which ensures that each chunk corresponds to a consistently discernible attribute in the represented image, hence avoiding degenerate feature mappings where some chunks are completely ignored. We demonstrate the effectiveness of our approach on the MNIST, Sprites, and CelebA datasets.},
  archivePrefix = {arXiv},
  eprint = {1711.07410},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_2018_disentangling factors of variation by mixing them.pdf},
  journal = {arXiv:1711.07410 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@techreport{hu18_Parametertuningkey,
  title = {Parameter Tuning Is a Key Part of Dimensionality Reduction via Deep Variational Autoencoders for Single Cell {{RNA}} Transcriptomics},
  author = {Hu, Qiwen and Greene, Casey S},
  year = {2018},
  month = aug,
  institution = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/385534},
  abstract = {Single-cell RNA sequencing (scRNA-seq) is a powerful tool to profile the transcriptomes of a large number of individual cells at a high resolution. These data usually contain measurements of gene expression for many genes in thousands or tens of thousands of cells, though some datasets now reach the million-cell mark. Projecting high-dimensional scRNA-seq data into a low dimensional space aids downstream analysis and data visualization. Many recent preprints accomplish this using variational autoencoders (VAE), generative models that learn underlying structure of data by compress it into a constrained, low dimensional space. The low dimensional spaces generated by VAEs have revealed complex patterns and novel biological signals from large-scale gene expression data and drug response predictions. Here, we evaluate a simple VAE approach for gene expression data, Tybalt, by training and measuring its performance on sets of simulated scRNA-seq data. We find a number of counter-intuitive performance features: i.e., deeper neural networks can struggle when datasets contain more observations under some parameter configurations. We show that these methods are highly sensitive to parameter tuning: when tuned, the performance of the Tybalt model, which was not optimized for scRNA-seq data, outperforms other popular dimension reduction approaches - PCA, ZIFA, UMAP and t-SNE. On the other hand, without tuning performance can also be remarkably poor on the same data. Our results should discourage authors and reviewers from relying on self-reported performance comparisons to evaluate the relative value of contributions in this area at this time. Instead, we recommend that attempts to compare or benchmark autoencoder methods for scRNA-seq data be performed by disinterested third parties or by methods developers only on unseen benchmark data that are provided to all participants simultaneously because the potential for performance differences due to unequal parameter tuning is so high.},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_2018_parameter tuning is a key part of dimensionality reduction via deep variational autoencoders for single cell rna transcriptomics.pdf},
  language = {en}
}

@inproceedings{hu18_unifyingdeepgenerative,
  title = {On Unifying Deep Generative Models},
  booktitle = {International Conference on Learning Representations},
  author = {Hu, Zhiting and Yang, Zichao and Salakhutdinov, Ruslan and Xing, Eric P.},
  year = {2018},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_2018_on unifying deep generative models.pdf}
}

@article{hu19_VariationalConditionalGAN,
  title = {Variational {{Conditional GAN}} for {{Fine}}-Grained {{Controllable Image Generation}}},
  author = {Hu, Mingqi and Zhou, Deyu and He, Yulan},
  year = {2019},
  month = sep,
  abstract = {In this paper, we propose a novel variational generator framework for conditional GANs to catch semantic details for improving the generation quality and diversity. Traditional generators in conditional GANs simply concatenate the conditional vector with the noise as the input representation, which is directly employed for upsampling operations. However, the hidden condition information is not fully exploited, especially when the input is a class label. Therefore, we introduce a variational inference into the generator to infer the posterior of latent variable only from the conditional input, which helps achieve a variable augmented representation for image generation. Qualitative and quantitative experimental results show that the proposed method outperforms the state-of-the-art approaches and achieves the realistic controllable images.},
  archivePrefix = {arXiv},
  eprint = {1909.09979},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_2019_variational conditional gan for fine-grained controllable image generation.pdf;/home/trung/Zotero/storage/JQKBHCRH/1909.html},
  journal = {arXiv:1909.09979 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,controllable,gan,variational},
  primaryClass = {cs}
}

@techreport{hu20_SIMPLEssinglecellRNA,
  title = {{{SIMPLEs}}: A Single-Cell {{RNA}} Sequencing Imputation Strategy Preserving Gene Modules and Cell Clusters Variation},
  shorttitle = {{{SIMPLEs}}},
  author = {Hu, Zhirui and Zu, Songpeng and Liu, Jun S.},
  year = {2020},
  month = jan,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.01.13.904649},
  abstract = {A main challenge in analyzing single-cell RNA sequencing (scRNASeq) data is to reduce technical variations yet retain cell heterogeneity. Due to low mRNAs content per cell and molecule losses during the experiment (called ``dropout''), the gene expression matrix has substantial zero read counts. Existing imputation methods either treat each cell or each gene identically and independently, which oversimplifies the gene correlation and cell type structure. We propose a statistical model-based approach, called SIMPLEs, which iteratively identifies correlated gene modules and cell clusters and imputes dropouts customized for individual gene module and cell type. Simultaneously, it quantifies the uncertainty of imputation and cell clustering. Optionally, SIMPLEs can integrate bulk RNASeq data for estimating dropout rates. In simulations, SIMPLEs performed significantly better than prevailing scRNASeq imputation methods by various metrics. By applying SIMPLEs to several real data sets, we discovered gene modules that can further classify subtypes of cells. Our imputations successfully recovered the expression trends of marker genes in stem cell differentiation and can discover putative pathways regulating biological processes.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/4Y85DE6X/Hu et al. - 2020 - SIMPLEs a single-cell RNA sequencing imputation s.pdf},
  language = {en},
  type = {Preprint}
}

@article{hu20_StrategiesPretrainingGraph,
  title = {Strategies for {{Pre}}-Training {{Graph Neural Networks}}},
  author = {Hu, Weihua and Liu, Bowen and Gomes, Joseph and Zitnik, Marinka and Liang, Percy and Pande, Vijay and Leskovec, Jure},
  year = {2020},
  month = feb,
  abstract = {Many applications of machine learning require a model to make accurate predictions on test examples that are distributionally different from training ones, while task-specific labels are scarce during training. An effective approach to this challenge is to pre-train a model on related tasks where data is abundant, and then fine-tune it on a downstream task of interest. While pre-training has been effective in many language and vision domains, it remains an open question how to effectively use pre-training on graph datasets. In this paper, we develop a new strategy and self-supervised methods for pre-training Graph Neural Networks (GNNs). The key to the success of our strategy is to pre-train an expressive GNN at the level of individual nodes as well as entire graphs so that the GNN can learn useful local and global representations simultaneously. We systematically study pre-training on multiple graph classification datasets. We find that na\"ive strategies, which pre-train GNNs at the level of either entire graphs or individual nodes, give limited improvement and can even lead to negative transfer on many downstream tasks. In contrast, our strategy avoids negative transfer and improves generalization significantly across downstream tasks, leading up to 9.4\% absolute improvements in ROC-AUC over non-pre-trained models and achieving state-of-the-art performance for molecular property prediction and protein function prediction.},
  archivePrefix = {arXiv},
  eprint = {1905.12265},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hu et al_2020_strategies for pre-training graph neural networks.pdf},
  journal = {arXiv:1905.12265 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{huang00_Probabilitydistillationcaveat,
  title = {Probability Distillation: {{A}} Caveat and Alternatives},
  author = {Huang, Chin-Wei and Ahmed, Faruk and Kumar, Kundan and Lacoste, Alexandre and Courville, Aaron},
  pages = {10},
  abstract = {Due to Van den Oord et al. (2018), probability distillation has recently been of interest to deep learning practitioners, where, as a practical workaround for deploying autoregressive models in real-time applications, a student network is used to obtain quality samples in parallel. We identify a pathological optimization issue with the adopted stochastic minimization of the reverse-KL divergence: the curse of dimensionality results in a skewed gradient distribution that renders training inefficient. This means that KL-based ``evaluative'' training can be susceptible to poor exploration if the target distribution is highly structured. We then explore alternative principles for distillation, including one with an ``instructive'' signal, and show that it is possible to achieve qualitatively better results than with KL minimization.},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_probability distillation.pdf},
  language = {en}
}

@article{huang00_TimbreTronWaveNetCycleGAN,
  title = {{{TimbreTron}}: {{A WaveNet}}({{CycleGAN}}({{CQT}}({{Audio}}))) {{Pipeline}} for {{Musical Timbre Transfer}}},
  author = {Huang, Sicong and Li, Qiyang and Anil, Cem and Oore, Sageev and Grosse, Roger B},
  pages = {17},
  annotation = {ZSCC: 0000012},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_timbretron.pdf},
  language = {en}
}

@article{huang16_DeepNetworksStochastic,
  title = {Deep {{Networks}} with {{Stochastic Depth}}},
  author = {Huang, Gao and Sun, Yu and Liu, Zhuang and Sedra, Daniel and Weinberger, Kilian},
  year = {2016},
  month = mar,
  abstract = {Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91\% on CIFAR-10).},
  archivePrefix = {arXiv},
  eprint = {1603.09382},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2016_deep networks with stochastic depth.pdf},
  journal = {arXiv:1603.09382 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,dropout,residual network},
  language = {en},
  primaryClass = {cs}
}

@article{huang18_DifficultiesProbabilityDistillation,
  title = {On {{Difficulties}} of {{Probability Distillation}}},
  author = {Huang, Chin-Wei and Ahmed, Faruk and Kumar, Kundan and Lacoste, Alexandre and Courville, Aaron},
  year = {2018},
  month = sep,
  abstract = {Probability distillation has recently been of interest to deep learning practitioners as it presents a practical solution for sampling from autoregressive models for deployment in real-time...},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2018_on difficulties of probability distillation.pdf}
}

@article{huang18_SAVERgeneexpression,
  title = {{{SAVER}}: Gene Expression Recovery for Single-Cell {{RNA}} Sequencing},
  shorttitle = {{{SAVER}}},
  author = {Huang, Mo and Wang, Jingshu and Torre, Eduardo and Dueck, Hannah and Shaffer, Sydney and Bonasio, Roberto and Murray, John I. and Raj, Arjun and Li, Mingyao and Zhang, Nancy R.},
  year = {2018},
  month = jul,
  volume = {15},
  pages = {539--542},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-018-0033-z},
  annotation = {ZSCC: 0000089},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2018_saver.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {7}
}

@article{huang18_TLVANEtwolevelvariation,
  title = {{{TLVANE}}: A Two-Level Variation Model for Attributed Network Embedding},
  shorttitle = {{{TLVANE}}},
  author = {Huang, Zhichao and Li, Xutao and Ye, Yunming and Li, Feng and Liu, Feng and Yao, Yuan},
  year = {2018},
  month = nov,
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-018-3875-5},
  abstract = {Network embedding aims to learn low-dimensional representations for nodes in social networks, which can serve many applications, such as node classification, link prediction and visualization. Most of network embedding methods focus on learning the representations solely from the topological structure. Recently, attributed network embedding, which utilizes both the topological structure and node content to jointly learn latent representations, becomes a hot topic. However, previous studies obtain the joint representations by directly concatenating the one from each aspect, which may lose the correlations between the topological structure and node content. In this paper, we propose a new attributed network embedding method, TLVANE, which can address the drawback by exploiting the deep variational autoencoders (VAEs). Particularly, a two-level VAE model is built, where the first-level accounts for the joint representations while the second for the embeddings of each aspect. Extensive experiments on three real-world datasets have been conducted, and the results demonstrate the superiority of the proposed method against state-of-the-art competitors.},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2018_tlvane.pdf},
  journal = {Neural Computing and Applications},
  keywords = {disentanglement},
  language = {en}
}

@article{huang19_ClinicalBERTModelingClinical,
  title = {{{ClinicalBERT}}: {{Modeling Clinical Notes}} and {{Predicting Hospital Readmission}}},
  shorttitle = {{{ClinicalBERT}}},
  author = {Huang, Kexin and Altosaar, Jaan and Ranganath, Rajesh},
  year = {2019},
  month = apr,
  abstract = {Clinical notes contain information about patients that goes beyond structured data like lab values and medications. However, clinical notes have been underused relative to structured data, because notes are high-dimensional and sparse. This work develops and evaluates representations of clinical notes using bidirectional transformers (ClinicalBERT). ClinicalBERT uncovers high-quality relationships between medical concepts as judged by humans. ClinicalBert outperforms baselines on 30-day hospital readmission prediction using both discharge summaries and the first few days of notes in the intensive care unit. Code and model parameters are available.},
  annotation = {ZSCC: 0000009},
  archivePrefix = {arXiv},
  eprint = {1904.05342},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/WX7JSUGE/Huang et al. - 2019 - ClinicalBERT Modeling Clinical Notes and Predicti.pdf},
  journal = {arXiv:1904.05342 [cs]},
  language = {en},
  primaryClass = {cs}
}

@techreport{huang19_EvaluationCellType,
  title = {Evaluation of {{Cell Type Deconvolution R Packages}} on {{Single Cell RNA}}-Seq {{Data}}},
  author = {Huang, Qianhui and Liu, Yu and Du, Yuheng and Garmire, Lana},
  year = {2019},
  month = nov,
  institution = {{Bioinformatics}},
  doi = {10.1101/827139},
  abstract = {Annotating cell types is a critical step in single cell RNA-Seq (scRNA-Seq) data analysis. Some deconvolution methods have recently emerged to enable automated cell type identification. However, comprehensive evaluations of these methods are lacking to provide practical guidelines. Moreover, it is not clear whether some deconvolution methods originally designed for analyzing other omics data are adaptable to scRNA-Seq analysis. In this study, we evaluated ten cell-type deconvolution methods publicly available as R packages. Eight of them are popular methods developed specifically for single cell research (Seurat, scmap, SingleR, CHETAH, SingleCellNet, scID, Garnett, SCINA). The other two methods are repurposed from deconvoluting DNA methylation data: Linear Constrained Projection (CP) and Robust Partial Correlations (RPC). We conducted systematic comparisons on a wide variety of public scRNA-seq datasets as well as simulation data. We assessed the accuracy through intra-dataset and inter-dataset predictions, the robustness over practical challenges such as gene filtering and high similarity among cell types, as well as the capabilities on rare and unknown cell-type detection. Overall, methods such as Seurat, SingleR, CP, RPC and SingleCellNet performed well, with Seurat being the best at annotating major cell types. Also, Seurat, SingleR and CP are more robust against down-sampling. However, Seurat does have a major drawback at predicting rare cell populations, and it is suboptimal at differentiating cell types that are highly similar to each other, while SingleR and CP are much better in these aspects.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2019_evaluation of cell type deconvolution r packages on single cell rna-seq data.pdf},
  language = {en},
  type = {Preprint}
}

@article{huang19_MusicTransformerGenerating,
  title = {Music {{Transformer}}: {{Generating Music}} with {{Long}}-{{Term Structure}}},
  author = {Huang, Cheng-Zhi Anna and Uszkoreit, Ashish Vaswani Jakob and Shazeer, Noam and Hawthorne, Ian Simon Curtis and Dai, Andrew M and Hoffman, Matthew D and Eck, Monica Dinculescu Douglas},
  year = {2019},
  pages = {15},
  abstract = {Music relies heavily on repetition to build structure and meaning. Self-reference occurs on multiple timescales, from motifs to phrases to reusing of entire sections of music, such as in pieces with ABA structure. The Transformer (Vaswani et al., 2017), a sequence model based on self-attention, has achieved compelling results in many generation tasks that require maintaining long-range coherence. This suggests that self-attention might also be well-suited to modeling music. In musical composition and performance, however, relative timing is critically important. Existing approaches for representing relative positional information in the Transformer modulate attention based on pairwise distance (Shaw et al., 2018). This is impractical for long sequences such as musical compositions because their memory complexity for intermediate relative information is quadratic in the sequence length. We propose an algorithm that reduces their intermediate memory requirement to linear in the sequence length. This enables us to demonstrate that a Transformer with our modified relative attention mechanism can generate minute-long compositions (thousands of steps) with compelling structure, generate continuations that coherently elaborate on a given motif, and in a seq2seq setup generate accompaniments conditioned on melodies1. We evaluate the Transformer with our relative attention mechanism on two datasets, JSB Chorales and Maestro, and obtain state-of-the-art results on the latter.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2019_music transformer.pdf},
  language = {en}
}

@article{huang19_Notebiasvariance,
  title = {Note on the Bias and Variance of Variational Inference},
  author = {Huang, Chin-Wei and Courville, Aaron},
  year = {2019},
  month = jun,
  abstract = {In this note, we study the relationship between the variational gap and the variance of the (log) likelihood ratio. We show that the gap can be upper bounded by some form of dispersion measure of the likelihood ratio, which suggests the bias of variational inference can be reduced by making the distribution of the likelihood ratio more concentrated, such as via averaging and variance reduction.},
  archivePrefix = {arXiv},
  eprint = {1906.03708},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2019_note on the bias and variance of variational inference.pdf;/home/trung/Zotero/storage/2V33HFM8/1906.html},
  journal = {arXiv:1906.03708 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{huang20_AugmentedNormalizingFlows,
  title = {Augmented {{Normalizing Flows}}: {{Bridging}} the {{Gap Between Generative Flows}} and {{Latent Variable Models}}},
  shorttitle = {Augmented {{Normalizing Flows}}},
  author = {Huang, Chin-Wei and Dinh, Laurent and Courville, Aaron},
  year = {2020},
  month = feb,
  abstract = {In this work, we propose a new family of generative flows on an augmented data space, with an aim to improve expressivity without drastically increasing the computational cost of sampling and evaluation of a lower bound on the likelihood. Theoretically, we prove the proposed flow can approximate a Hamiltonian ODE as a universal transport map. Empirically, we demonstrate state-of-the-art performance on standard benchmarks of flow-based generative modeling.},
  archivePrefix = {arXiv},
  eprint = {2002.07101},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2020_augmented normalizing flows.pdf},
  journal = {arXiv:2002.07101 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{huang20_NormalizationTechniquesTraining,
  title = {Normalization {{Techniques}} in {{Training DNNs}}: {{Methodology}}, {{Analysis}} and {{Application}}},
  shorttitle = {Normalization {{Techniques}} in {{Training DNNs}}},
  author = {Huang, Lei and Qin, Jie and Zhou, Yi and Zhu, Fan and Liu, Li and Shao, Ling},
  year = {2020},
  month = sep,
  abstract = {Normalization techniques are essential for accelerating the training and improving the generalization of deep neural networks (DNNs), and have successfully been used in various applications. This paper reviews and comments on the past, present and future of normalization methods in the context of DNN training. We provide a unified picture of the main motivation behind different approaches from the perspective of optimization, and present a taxonomy for understanding the similarities and differences between them. Specifically, we decompose the pipeline of the most representative normalizing activation methods into three components: the normalization area partitioning, normalization operation and normalization representation recovery. In doing so, we provide insight for designing new normalization technique. Finally, we discuss the current progress in understanding normalization methods, and provide a comprehensive review of the applications of normalization for particular tasks, in which it can effectively solve the key issues.},
  archivePrefix = {arXiv},
  eprint = {2009.12836},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2020_normalization techniques in training dnns.pdf},
  journal = {arXiv:2009.12836 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{huang20_SNDCNNSelfnormalizingdeep,
  title = {{{SNDCNN}}: {{Self}}-Normalizing Deep {{CNNs}} with Scaled Exponential Linear Units for Speech Recognition},
  shorttitle = {{{SNDCNN}}},
  author = {Huang, Zhen and Ng, Tim and Liu, Leo and Mason, Henry and Zhuang, Xiaodan and Liu, Daben},
  year = {2020},
  month = mar,
  abstract = {Very deep CNNs achieve state-of-the-art results in both computer vision and speech recognition, but are difficult to train. The most popular way to train very deep CNNs is to use shortcut connections (SC) together with batch normalization (BN). Inspired by SelfNormalizing Neural Networks, we propose the self-normalizing deep CNN (SNDCNN) based acoustic model topology, by removing the SC/BN and replacing the typical RELU activations with scaled exponential linear unit (SELU) in ResNet-50. SELU activations make the network self-normalizing and remove the need for both shortcut connections and batch normalization. Compared to ResNet50, we can achieve the same or lower (up to 4.5\% relative) word error rate (WER) while boosting both training and inference speed by 60\%-80\%. We also explore other model inference optimization schemes to further reduce latency for production use.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.01992},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/huang et al_2020_sndcnn.pdf},
  journal = {arXiv:1910.01992 [cs, eess, stat]},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{huch97_Osteoarthritisankleknee,
  title = {Osteoarthritis in Ankle and Knee Joints},
  author = {Huch, Klaus and Kuettner, Klaus E. and Dieppe, Paul},
  year = {1997},
  month = feb,
  volume = {26},
  pages = {667--674},
  issn = {00490172},
  doi = {10.1016/S0049-0172(97)80002-9},
  annotation = {ZSCC: 0000171},
  file = {/home/trung/Zotero/storage/ZCTXUM9W/Huch et al. - 1997 - Osteoarthritis in ankle and knee joints.pdf},
  journal = {Seminars in Arthritis and Rheumatism},
  language = {en},
  number = {4}
}

@article{hughes00_SemiSupervisedPredictionConstrainedTopic,
  title = {Semi-{{Supervised Prediction}}-{{Constrained Topic Models}}},
  author = {Hughes, Michael C and Hope, Gabriel and Weiner, Leah and McCoy, Thomas H and Perlis, Roy H and Sudderth, Erik and {Doshi-Velez}, Finale},
  pages = {10},
  abstract = {Supervisory signals can help topic models discover low-dimensional data representations which are useful for a specific prediction task. We propose a framework for training supervised latent Dirichlet allocation that balances two goals: faithful generative explanations of high-dimensional data and accurate prediction of associated class labels. Existing approaches fail to balance these goals by not properly handling a fundamental asymmetry: the intended application is always predicting labels from data, not data from labels. Our new prediction-constrained objective for training generative models coherently integrates supervisory signals even when only a small fraction of training examples are labeled. We demonstrate improved prediction quality compared to previous supervised topic models, achieving results competitive with highdimensional logistic regression on text analysis and electronic health records tasks while simultaneously learning interpretable topics.},
  file = {/home/trung/GoogleDrive/Zotero/hughes et al_semi-supervised prediction-constrained topic models.pdf},
  language = {en}
}

@article{hupkes19_compositionalityneuralnetworks,
  title = {The Compositionality of Neural Networks: Integrating Symbolism and Connectionism},
  shorttitle = {The Compositionality of Neural Networks},
  author = {Hupkes, Dieuwke and Dankers, Verna and Mul, Mathijs and Bruni, Elia},
  year = {2019},
  month = aug,
  abstract = {Despite a multitude of empirical studies, little consensus exists on whether neural networks are able to generalise compositionally, a controversy that, in part, stems from a lack of agreement about what it means for a neural model to be compositional. As a response to this controversy, we present a set of tests that provide a bridge between, on the one hand, the vast amount of linguistic and philosophical theory about compositionality and, on the other, the successful neural models of language. We collect different interpretations of compositionality and translate them into five theoretically grounded tests that are formulated on a task-independent level. In particular, we provide tests to investigate (i) if models systematically recombine known parts and rules (ii) if models can extend their predictions beyond the length they have seen in the training data (iii) if models' composition operations are local or global (iv) if models' predictions are robust to synonym substitutions and (v) if models favour rules or exceptions during training. To demonstrate the usefulness of this evaluation paradigm, we instantiate these five tests on a highly compositional data set which we dub PCFG SET and apply the resulting tests to three popular sequence-tosequence models: a recurrent, a convolution based and a transformer model. We provide an in depth analysis of the results, that uncover the strengths and weaknesses of these three architectures and point to potential areas of improvement.},
  archivePrefix = {arXiv},
  eprint = {1908.08351},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/5VTMULHM/Hupkes et al. - 2019 - The compositionality of neural networks integrati.pdf},
  journal = {arXiv:1908.08351 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{huszar00_ScoringrulesDivergences,
  title = {Scoring Rules, {{Divergences}} and {{Information}} in {{Bayesian Machine Learning}}},
  author = {Huszar, Ferenc},
  pages = {166},
  file = {/home/trung/GoogleDrive/Zotero/huszar_scoring rules, divergences and information in bayesian machine learning.pdf},
  language = {en}
}

@article{huszar17_VariationalInferenceusing,
  title = {Variational {{Inference}} Using {{Implicit Distributions}}},
  author = {Husz{\'a}r, Ferenc},
  year = {2017},
  month = feb,
  abstract = {Generative adversarial networks (GANs) have given us a great tool to fit implicit generative models to data. Implicit distributions are ones we can sample from easily, and take derivatives of samples with respect to model parameters. These models are highly expressive and we argue they can prove just as useful for variational inference (VI) as they are for generative modelling. Several papers have proposed GAN-like algorithms for inference, however, connections to the theory of VI are not always well understood. This paper provides a unifying review of existing algorithms establishing connections between variational autoencoders, adversarially learned inference, operator VI, GAN-based image reconstruction, and more. Secondly, the paper provides a framework for building new algorithms: depending on the way the variational bound is expressed we introduce prior-contrastive and joint-contrastive methods, and show practical inference algorithms based on either density ratio estimation or denoising.},
  archivePrefix = {arXiv},
  eprint = {1702.08235},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/huszár_2017_variational inference using implicit distributions.pdf;/home/trung/Zotero/storage/BXDDR2TC/1702.html},
  journal = {arXiv:1702.08235 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@misc{huyen20_chiphuyenpythoniscool,
  title = {Chiphuyen/Python-Is-Cool},
  author = {Huyen, Chip},
  year = {2020},
  month = oct,
  abstract = {Cool Python features for machine learning that I used to be too afraid to use. Will be updated as I have more time / learn more.}
}

@misc{huyen20_chiphuyenstanfordtensorflowtutorials,
  title = {Chiphuyen/Stanford-Tensorflow-Tutorials},
  author = {Huyen, Chip},
  year = {2020},
  month = oct,
  abstract = {This repository contains code examples for the Stanford's course: TensorFlow for Deep Learning Research.},
  copyright = {MIT License         ,                 MIT License}
}

@misc{hwalsuklee20_hwalsukleetensorflowgenerativemodelcollections,
  title = {Hwalsuklee/Tensorflow-Generative-Model-Collections},
  author = {{hwalsuklee}},
  year = {2020},
  month = feb,
  abstract = {Collection of generative models in Tensorflow. Contribute to hwalsuklee/tensorflow-generative-model-collections development by creating an account on GitHub.},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {Apache-2.0}
}

@article{hwang20_benchmarkstudyreliable,
  title = {A Benchmark Study on Reliable Molecular Supervised Learning via {{Bayesian}} Learning},
  author = {Hwang, Doyeong and Lee, Grace and Jo, Hanseok and Yoon, Seyoul and Ryu, Seongok},
  year = {2020},
  month = jun,
  abstract = {Virtual screening aims to find desirable compounds from chemical library by using computational methods. For this purpose with machine learning, model outputs that can be interpreted as predictive probability will be beneficial, in that a high prediction score corresponds to high probability of correctness. In this work, we present a study on the prediction performance and reliability of graph neural networks trained with the recently proposed Bayesian learning algorithms. Our work shows that Bayesian learning algorithms allow well-calibrated predictions for various GNN architectures and classification tasks. Also, we show the implications of reliable predictions on virtual screening, where Bayesian learning may lead to higher success in finding hit compounds.},
  archivePrefix = {arXiv},
  eprint = {2006.07021},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hwang et al_2020_a benchmark study on reliable molecular supervised learning via bayesian learning.pdf},
  journal = {arXiv:2006.07021 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{hyvarinen00_Independentcomponentanalysis,
  title = {Independent Component Analysis: Algorithms and Applications},
  shorttitle = {Independent Component Analysis},
  author = {Hyv{\"a}rinen, A. and Oja, E.},
  year = {2000},
  month = jun,
  volume = {13},
  pages = {411--430},
  issn = {08936080},
  doi = {10.1016/S0893-6080(00)00026-5},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of nongaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  annotation = {ZSCC: 0018652},
  file = {/home/trung/GoogleDrive/Zotero/hyvärinen et al_2000_independent component analysis.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-5}
}

@article{hyvarinen00_Independentcomponentanalysisa,
  title = {Independent Component Analysis: Algorithms and Applications},
  shorttitle = {Independent Component Analysis},
  author = {Hyv{\"a}rinen, A. and Oja, E.},
  year = {2000},
  month = jun,
  volume = {13},
  pages = {411--430},
  issn = {08936080},
  doi = {10.1016/S0893-6080(00)00026-5},
  abstract = {A fundamental problem in neural network research, as well as in many other disciplines, is finding a suitable representation of multivariate data, i.e. random vectors. For reasons of computational and conceptual simplicity, the representation is often sought as a linear transformation of the original data. In other words, each component of the representation is a linear combination of the original variables. Well-known linear transformation methods include principal component analysis, factor analysis, and projection pursuit. Independent component analysis (ICA) is a recently developed method in which the goal is to find a linear representation of nongaussian data so that the components are statistically independent, or as independent as possible. Such a representation seems to capture the essential structure of the data in many applications, including feature extraction and signal separation. In this paper, we present the basic theory and applications of ICA, and our recent work on the subject.},
  file = {/home/trung/GoogleDrive/Zotero/hyvärinen et al_2000_independent component analysis2.pdf},
  journal = {Neural Networks},
  language = {en},
  number = {4-5}
}

@article{hyvarinen00_Nonlinearindependentcomponent,
  title = {Nonlinear Independent Component Analysis:  {{A}} Principled Framework for  Unsupervised Deep Learning},
  author = {Hyv{\"a}rinen, Aapo},
  pages = {65},
  file = {/home/trung/GoogleDrive/Zotero/hyvärinen_nonlinear independent component analysis.pdf},
  journal = {Independent component analysis},
  language = {en}
}

@article{hyvarinen01_TopographicIndependentComponent,
  title = {Topographic {{Independent Component Analysis}}},
  author = {Hyv{\"a}rinen, Aapo and Hoyer, Patrik O and Inki, Mika},
  year = {2001},
  pages = {28},
  abstract = {In ordinary independent component analysis, the components are assumed to be completely independent, and they do not necessarily have any meaningful order relationships. In practice, however, the estimated ``independent'' components are often not at all independent. We propose that this residual dependence structure could be used to define a topographic order for the components. In particular, a distance between two components could be defined using their higher-order correlations, and this distance could be used to create a topographic representation. Thus we obtain a linear decomposition into approximately independent components, where the dependence of two components is approximated by the proximity of the components in the topographic representation.},
  file = {/home/trung/GoogleDrive/Zotero/hyvärinen et al_2001_topographic independent component analysis.pdf},
  language = {en}
}

@article{hyvarinen05_Estimationnonnormalizedstatistical,
  title = {Estimation of Non-Normalized Statistical Models by Score Matching},
  author = {Hyv{\"a}rinen, Aapo},
  year = {2005},
  month = dec,
  volume = {6},
  pages = {695--709},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {One often wants to estimate statistical models where the probability density function is known only up to a multiplicative normalization constant. Typically, one then has to resort to Markov Chain Monte Carlo methods, or approximations of the normalization constant. Here, we propose that such models can be estimated by minimizing the expected squared distance between the gradient of the log-density given by the model and the gradient of the log-density of the observed data. While the estimation of the gradient of log-density function is, in principle, a very difficult non-parametric problem, we prove a surprising result that gives a simple formula for this objective function. The density function of the observed data does not appear in this formula, which simplifies to a sample average of a sum of some derivatives of the log-density given by the model. The validity of the method is demonstrated on multivariate Gaussian and independent component analysis models, and by estimating an overcomplete filter set for natural image data.},
  file = {/home/trung/GoogleDrive/Zotero/hyvärinen_2005_estimation of non-normalized statistical models by score matching.pdf},
  issue_date = {12/1/2005},
  journal = {Journal of Machine Learning Research}
}

@article{hyvarinen13_Independentcomponentanalysis,
  title = {Independent Component Analysis: Recent Advances},
  author = {Hyv{\"a}rinen, Aapo},
  year = {2013},
  month = feb,
  volume = {371},
  pages = {20110534},
  publisher = {{Royal Society}},
  doi = {10.1098/rsta.2011.0534},
  file = {/home/trung/GoogleDrive/Zotero/hyvärinen_2013_independent component analysis.pdf},
  journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  number = {1984}
}

@incollection{hyvarinen16_Unsupervisedfeatureextraction,
  title = {Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear {{ICA}}},
  booktitle = {Advances in Neural Information Processing Systems 29},
  author = {Hyvarinen, Aapo and Morioka, Hiroshi},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3765--3773},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/hyvarinen et al_2016_unsupervised feature extraction by time-contrastive learning and nonlinear ica.pdf}
}

@inproceedings{hyvarinen17_NonlinearICAtemporally,
  title = {Nonlinear {{ICA}} of Temporally Dependent Stationary Sources},
  author = {Hyvarinen, Aapo and Morioka, Hiroshi},
  editor = {Singh, Aarti and Zhu, Jerry},
  year = {2017},
  month = apr,
  volume = {54},
  pages = {460--469},
  publisher = {{PMLR}},
  address = {{Fort Lauderdale, FL, USA}},
  abstract = {We develop a nonlinear generalization of independent component analysis (ICA) or blind source separation, based on temporal dependencies (e.g. autocorrelations). We introduce a nonlinear generative model where the independent sources are assumed to be temporally dependent, non-Gaussian, and stationary, and we observe arbitrarily nonlinear mixtures of them. We develop a method for estimating the model (i.e. separating the sources) based on logistic regression in a neural network which learns to discriminate between a short temporal window of the data vs. a temporal window of temporally permuted data. We prove that the method estimates the sources for general smooth mixing nonlinearities, assuming the sources have sufficiently strong temporal dependencies, and these dependencies are in a certain way different from dependencies found in Gaussian processes. For Gaussian (and similar) sources, the method estimates the nonlinear part of the mixing. We thus provide the first rigorous and general proof of identifiability of nonlinear ICA for temporally dependent sources, together with a practical method for its estimation.},
  file = {/home/trung/GoogleDrive/Zotero/hyvarinen et al_2017_nonlinear ica of temporally dependent stationary sources.pdf;/home/trung/GoogleDrive/Zotero/hyvarinen et al_2017_nonlinear ica of temporally dependent stationary sources2.pdf},
  pdf = {http://proceedings.mlr.press/v54/hyvarinen17a/hyvarinen17a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{hyvarinen19_NonlinearICAUsing,
  title = {Nonlinear {{ICA Using Auxiliary Variables}} and {{Generalized Contrastive Learning}}},
  author = {Hyvarinen, Aapo and Sasaki, Hiroaki and Turner, Richard E.},
  year = {2019},
  month = feb,
  abstract = {Nonlinear ICA is a fundamental problem for unsupervised representation learning, emphasizing the capacity to recover the underlying latent variables generating the data (i.e., identifiability). Recently, the very first identifiability proofs for nonlinear ICA have been proposed, leveraging the temporal structure of the independent components. Here, we propose a general framework for nonlinear ICA, which, as a special case, can make use of temporal structure. It is based on augmenting the data by an auxiliary variable, such as the time index, the history of the time series, or any other available information. We propose to learn nonlinear ICA by discriminating between true augmented data, or data in which the auxiliary variable has been randomized. This enables the framework to be implemented algorithmically through logistic regression, possibly in a neural network. We provide a comprehensive proof of the identifiability of the model as well as the consistency of our estimation method. The approach not only provides a general theoretical framework combining and generalizing previously proposed nonlinear ICA models and algorithms, but also brings practical advantages.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1805.08651},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/hyvarinen et al_2019_nonlinear ica using auxiliary variables and generalized contrastive learning.pdf},
  journal = {arXiv:1805.08651 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{hyvarinen99_Nonlinearindependentcomponent,
  title = {Nonlinear Independent Component Analysis: {{Existence}} and Uniqueness Results},
  author = {Hyv{\"a}rinen, Aapo and Pajunen, Petteri},
  year = {1999},
  month = apr,
  volume = {12},
  pages = {429--439},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00140-3},
  abstract = {The question of existence and uniqueness of solutions for nonlinear independent component analysis is addressed. It is shown that if the space of mixing functions is not limited there exists always an infinity of solutions. In particular, it is shown how to construct parameterized families of solutions. The indeterminacies involved are not trivial, as in the linear case. Next, it is shown how to utilize some results of complex analysis to obtain uniqueness of solutions. We show that for two dimensions, the solution is unique up to a rotation, if the mixing function is constrained to be a conformal mapping together with some other assumptions. We also conjecture that the solution is strictly unique except in some degenerate cases, as the indeterminacy implied by the rotation is essentially similar to estimating the model of linear ICA.},
  file = {/home/trung/GoogleDrive/Zotero/hyvärinen et al_1999_nonlinear independent component analysis.pdf},
  journal = {Neural Networks},
  keywords = {Blind source separation,Feature extraction,Independent component analysis,Redundancy reduction},
  number = {3}
}

@article{iandola16_SqueezeNetAlexNetlevelaccuracy,
  title = {{{SqueezeNet}}: {{AlexNet}}-Level Accuracy with 50x Fewer Parameters and {$<$}0.{{5MB}} Model Size},
  shorttitle = {{{SqueezeNet}}},
  author = {Iandola, Forrest N. and Han, Song and Moskewicz, Matthew W. and Ashraf, Khalid and Dally, William J. and Keutzer, Kurt},
  year = {2016},
  month = nov,
  abstract = {Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). The SqueezeNet architecture is available for download here: https://github.com/DeepScale/SqueezeNet},
  annotation = {ZSCC: 0001896},
  archivePrefix = {arXiv},
  eprint = {1602.07360},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/iandola et al_2016_squeezenet.pdf;/home/trung/Zotero/storage/HS27UNUK/1602.html},
  journal = {arXiv:1602.07360 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@misc{igor16_NuitBlancheCourse,
  title = {Nuit {{Blanche}}: {{Course}}: {{Stat212b}}: {{Topics Course}} on {{Deep Learning}} by {{Joan Bruna}}, {{UC Berkeley}}, {{Stats Department}}. {{Spring}} 2016.},
  shorttitle = {Nuit {{Blanche}}},
  author = {Igor},
  year = {2016},
  month = oct,
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/6L5FQ33K/course-stat212b-topics-course-on-deep.html},
  journal = {Nuit Blanche}
}

@article{iida00_MultiHopAttentionRNN,
  title = {A {{Multi}}-{{Hop Attention}} for {{RNN}} Based {{Neural Machine Translation}}},
  author = {Iida, Shohei and Kimura, Ryuichiro and Cui, Hongyi and Hung, Po-Hsuan and Utsuro, Takehito and Nagata, Masaaki},
  pages = {8},
  abstract = {Among recent progresses of neural machine translation models, the invention of the Transformer model is one of the most important progresses. It is well-known that the key technologies of the Transformer include multi-head attention mechanism. This paper introduces the multi-head attention mechanism into the traditional RNNbased neural machine translation model. Moreover, inspired by the existing multihop architectures such as end-to-end memory networks and convolutional sequence to sequence learning model, this paper proposes an RNN based NMT model with a multi-hop attention mechanism. The proposed multi-hop attention model has two heads, where for each head, a context vector is calculated based on the states of the encoder and the decoder. Then, in the second turn of the context vector calculation, those context vectors are updated depending not only on one's own context vector but also on the context vector of the other head. Experimental results show that the proposed model significantly outperforms the baseline in BLEU score in Japanese-to-English/English-toJapanese machine translation tasks with and without extended context.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/iida et al_a multi-hop attention for rnn based neural machine translation.pdf},
  keywords = {attention},
  language = {en}
}

@article{ilse18_AttentionbasedDeepMultiple,
  title = {Attention-Based {{Deep Multiple Instance Learning}}},
  author = {Ilse, Maximilian and Tomczak, Jakub M. and Welling, Max},
  year = {2018},
  month = feb,
  abstract = {Multiple instance learning (MIL) is a variation of supervised learning where a single class label is assigned to a bag of instances. In this paper, we state the MIL problem as learning the Bernoulli distribution of the bag label where the bag label probability is fully parameterized by neural networks. Furthermore, we propose a neural network-based permutation-invariant aggregation operator that corresponds to the attention mechanism. Notably, an application of the proposed attention-based operator provides insight into the contribution of each instance to the bag label. We show empirically that our approach achieves comparable performance to the best MIL methods on benchmark MIL datasets and it outperforms other methods on a MNIST-based MIL dataset and two real-life histopathology datasets without sacrificing interpretability.},
  annotation = {ZSCC: 0000077},
  archivePrefix = {arXiv},
  eprint = {1802.04712},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ilse et al_2018_attention-based deep multiple instance learning.pdf;/home/trung/Zotero/storage/4Y2KNTMV/1802.html},
  journal = {arXiv:1802.04712 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ilyas00_AdversarialExamplesAre,
  title = {Adversarial {{Examples Are Not Bugs}}, {{They Are Features}}},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tran, Brandon and Tsipras, Dimitris and Engstrom, Logan and Ma, Aleksander},
  pages = {37},
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features derived from patterns in the data distribution that are highly predictive, yet brittle and incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  file = {/home/trung/GoogleDrive/Zotero/ilyas et al_adversarial examples are not bugs, they are features.pdf},
  language = {en}
}

@article{ilyas19_AdversarialExamplesAre,
  title = {Adversarial {{Examples Are Not Bugs}}, {{They Are Features}}},
  author = {Ilyas, Andrew and Santurkar, Shibani and Tsipras, Dimitris and Engstrom, Logan and Tran, Brandon and Madry, Aleksander},
  year = {2019},
  month = aug,
  abstract = {Adversarial examples have attracted significant attention in machine learning, but the reasons for their existence and pervasiveness remain unclear. We demonstrate that adversarial examples can be directly attributed to the presence of non-robust features: features (derived from patterns in the data distribution) that are highly predictive, yet brittle and (thus) incomprehensible to humans. After capturing these features within a theoretical framework, we establish their widespread existence in standard datasets. Finally, we present a simple setting where we can rigorously tie the phenomena we observe in practice to a misalignment between the (human-specified) notion of robustness and the inherent geometry of the data.},
  archivePrefix = {arXiv},
  eprint = {1905.02175},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/RAZ3HICA/Ilyas et al. - 2019 - Adversarial Examples Are Not Bugs, They Are Featur.pdf},
  journal = {arXiv:1905.02175 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{imran19_MultiAdversarialVariationalAutoencoder,
  title = {Multi-{{Adversarial Variational Autoencoder Networks}}},
  author = {Imran, Abdullah-Al-Zubaer and Terzopoulos, Demetri},
  year = {2019},
  month = jun,
  abstract = {The unsupervised training of GANs and VAEs has enabled them to generate realistic images mimicking real-world distributions and perform image-based unsupervised clustering or semi-supervised classification. Combining the power of these two generative models, we introduce Multi-Adversarial Variational autoEncoder Networks (MAVENs), a novel network architecture that incorporates an ensemble of discriminators in a VAE-GAN network, with simultaneous adversarial learning and variational inference. We apply MAVENs to the generation of synthetic images and propose a new distribution measure to quantify the quality of the generated images. Our experimental results using datasets from the computer vision and medical imaging domains---Street View House Numbers, CIFAR-10, and Chest X-Ray datasets---demonstrate competitive performance against state-of-the-art semi-supervised models both in image generation and classification tasks.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1906.06430},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/imran et al_2019_multi-adversarial variational autoencoder networks.pdf;/home/trung/Zotero/storage/4KPE9LRH/1906.html},
  journal = {arXiv:1906.06430 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{india19_SelfMultiHeadAttention,
  title = {Self {{Multi}}-{{Head Attention}} for {{Speaker Recognition}}},
  author = {India, Miquel and Safari, Pooyan and Hernando, Javier},
  year = {2019},
  month = jul,
  abstract = {Most state-of-the-art Deep Learning (DL) approaches for speaker recognition work on a short utterance level. Given the speech signal, these algorithms extract a sequence of speaker embeddings from short segments and those are averaged to obtain an utterance level speaker representation. In this work we propose the use of an attention mechanism to obtain a discriminative speaker embedding given non fixed length speech utterances. Our system is based on a Convolutional Neural Network (CNN) that encodes short-term speaker features from the spectrogram and a self multi-head attention model that maps these representations into a long-term speaker embedding. The attention model that we propose produces multiple alignments from different subsegments of the CNN encoded states over the sequence. Hence this mechanism works as a pooling layer which decides the most discriminative features over the sequence to obtain an utterance level representation. We have tested this approach for the verification task for the VoxCeleb1 dataset. The results show that self multi-head attention outperforms both temporal and statistical pooling methods with a 18\textbackslash\% of relative EER. Obtained results show a 58\textbackslash\% relative improvement in EER compared to i-vector+PLDA.},
  archivePrefix = {arXiv},
  eprint = {1906.09890},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/india et al_2019_self multi-head attention for speaker recognition.pdf;/home/trung/Zotero/storage/8ESCVN6Y/1906.html},
  journal = {arXiv:1906.09890 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Computer Science - Sound,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ioannou00_DecisionForestsConvolutional,
  title = {Decision {{Forests}}, {{Convolutional Networks}} and the {{Models}} in-{{Between}}},
  author = {Ioannou, Yani and Robertson, Duncan and Zikic, Darko and Kontschieder, Peter and Shotton, Jamie and Brown, Matthew and Criminisi, Antonio},
  pages = {9},
  abstract = {This paper investigates the connections between two state of the art classifiers: decision forests (DFs, including decision jungles) and convolutional neural networks (CNNs). Decision forests are computationally efficient thanks to their conditional computation property (computation is confined to only a small region of the tree, the nodes along a single branch). CNNs achieve state of the art accuracy, thanks to their representation learning capabilities.},
  annotation = {ZSCC: 0000042},
  file = {/home/trung/GoogleDrive/Zotero/ioannou et al_decision forests, convolutional networks and the models in-between.pdf},
  language = {en}
}

@article{ioannou00_StructuralPriorsDeep,
  title = {Structural {{Priors}} in {{Deep Neural Networks}}},
  author = {Ioannou, Yani Andrew},
  pages = {219},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/ioannou_structural priors in deep neural networks.pdf},
  language = {en}
}

@incollection{ioffe06_ProbabilisticLinearDiscriminant,
  title = {Probabilistic {{Linear Discriminant Analysis}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2006},
  author = {Ioffe, Sergey},
  editor = {Leonardis, Ale{\v s} and Bischof, Horst and Pinz, Axel},
  year = {2006},
  volume = {3954},
  pages = {531--542},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11744085_41},
  abstract = {Linear dimensionality reduction methods, such as LDA, are often used in object recognition for feature extraction, but do not address the problem of how to use these features for recognition. In this paper, we propose Probabilistic LDA, a generative probability model with which we can both extract the features and combine them for recognition. The latent variables of PLDA represent both the class of the object and the view of the object within a class. By making examples of the same class share the class variable, we show how to train PLDA and use it for recognition on previously unseen classes. The usual LDA features are derived as a result of training PLDA, but in addition have a probability model attached to them, which automatically gives more weight to the more discriminative features. With PLDA, we can build a model of a previously unseen class from a single example, and can combine multiple examples for a better representation of the class. We show applications to classification, hypothesis testing, class inference, and clustering, on classes not observed during training.},
  file = {/home/trung/GoogleDrive/Zotero/ioffe_2006_probabilistic linear discriminant analysis.pdf},
  isbn = {978-3-540-33838-3 978-3-540-33839-0},
  language = {en}
}

@article{ioffe15_BatchNormalizationAccelerating,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archivePrefix = {arXiv},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ioffe et al_2015_batch normalization.pdf},
  journal = {arXiv:1502.03167 [cs]},
  primaryClass = {cs}
}

@article{ioffe17_BatchRenormalizationReducing,
  title = {Batch {{Renormalization}}: {{Towards Reducing Minibatch Dependence}} in {{Batch}}-{{Normalized Models}}},
  shorttitle = {Batch {{Renormalization}}},
  author = {Ioffe, Sergey},
  year = {2017},
  month = feb,
  abstract = {Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.},
  archivePrefix = {arXiv},
  eprint = {1702.03275},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ioffe_2017_batch renormalization.pdf;/home/trung/Zotero/storage/QHBVN9X4/1702.html},
  journal = {arXiv:1702.03275 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{ishaque12_Rhodiolaroseaphysical,
  title = {Rhodiola Rosea for Physical and Mental Fatigue: A Systematic Review},
  shorttitle = {Rhodiola Rosea for Physical and Mental Fatigue},
  author = {Ishaque, Sana and Shamseer, Larissa and Bukutu, Cecilia and Vohra, Sunita},
  year = {2012},
  month = may,
  volume = {12},
  pages = {70},
  issn = {1472-6882},
  doi = {10.1186/1472-6882-12-70},
  abstract = {Background Rhodiola rosea (R. rosea) is grown at high altitudes and northern latitudes. Due to its purported adaptogenic properties, it has been studied for its performance-enhancing capabilities in healthy populations and its therapeutic properties in a number of clinical populations. To systematically review evidence of efficacy and safety of R. rosea for physical and mental fatigue. Methods Six electronic databases were searched to identify randomized controlled trials (RCTs) and controlled clinical trials (CCTs), evaluating efficacy and safety of R. rosea for physical and mental fatigue. Two reviewers independently screened the identified literature, extracted data and assessed risk of bias for included studies. Results Of 206 articles identified in the search, 11 met inclusion criteria for this review. Ten were described as RCTs and one as a CCT. Two of six trials examining physical fatigue in healthy populations report R. rosea to be effective as did three of five RCTs evaluating R. rosea for mental fatigue. All of the included studies exhibit either a high risk of bias or have reporting flaws that hinder assessment of their true validity (unclear risk of bias). Conclusion Research regarding R. rosea efficacy is contradictory. While some evidence suggests that the herb may be helpful for enhancing physical performance and alleviating mental fatigue, methodological flaws limit accurate assessment of efficacy. A rigorously-designed well reported RCT that minimizes bias is needed to determine true efficacy of R. rosea for fatigue.},
  file = {/home/trung/GoogleDrive/Zotero/ishaque et al_2012_rhodiola rosea for physical and mental fatigue.pdf},
  journal = {BMC Complementary and Alternative Medicine},
  pmcid = {PMC3541197},
  pmid = {22643043}
}

@article{ishiguro20_WeisfeilerLehmanEmbeddingMolecular,
  title = {Weisfeiler-{{Lehman Embedding}} for {{Molecular Graph Neural Networks}}},
  author = {Ishiguro, Katsuhiko and Oono, Kenta and Hayashi, Kohei},
  year = {2020},
  month = aug,
  abstract = {A graph neural network (GNN) is a good choice for predicting the chemical properties of molecules. Compared with other deep networks, however, the current performance of a GNN is limited owing to the ``curse of depth.'' Inspired by longestablished feature engineering in the field of chemistry, we expanded an atom representation using Weisfeiler-Lehman (WL) embedding, which is designed to capture local atomic patterns dominating the chemical properties of a molecule. In terms of representability, we show WL embedding can replace the first two layers of ReLU GNN \textemdash{} a normal embedding and a hidden GNN layer \textemdash{} with a smaller weight norm. We then demonstrate that WL embedding consistently improves the empirical performance over multiple GNN architectures and several molecular graph datasets.},
  archivePrefix = {arXiv},
  eprint = {2006.06909},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ishiguro et al_2020_weisfeiler-lehman embedding for molecular graph neural networks.pdf},
  journal = {arXiv:2006.06909 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{islam20_HowMuchPosition,
  title = {How {{Much Position Information Do Convolutional Neural Networks Encode}}?},
  author = {Islam, Md Amirul and Jia, Sen and Bruce, Neil D. B.},
  year = {2020},
  month = jan,
  abstract = {In contrast to fully connected networks, Convolutional Neural Networks (CNNs) achieve efficiency by learning weights associated with local filters with a finite spatial extent. An implication of this is that a filter may know what it is looking at, but not where it is positioned in the image. Information concerning absolute position is inherently useful, and it is reasonable to assume that deep CNNs may implicitly learn to encode this information if there is a means to do so. In this paper, we test this hypothesis revealing the surprising degree of absolute position information that is encoded in commonly used neural networks. A comprehensive set of experiments show the validity of this hypothesis and shed light on how and where this information is represented while offering clues to where positional information is derived from in deep CNNs.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.08248},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/islam et al_2020_how much position information do convolutional neural networks encode.pdf;/home/trung/Zotero/storage/HSYZ7XM8/2001.html},
  journal = {arXiv:2001.08248 [cs]},
  primaryClass = {cs}
}

@article{isola00_GenerativeModel,
  title = {Generative {{Model}} 2},
  author = {Isola, Phillip},
  pages = {126},
  file = {/home/trung/GoogleDrive/Zotero/isola_generative model 2.pdf},
  language = {en}
}

@article{itkina19_EvidentialDisambiguationLatent,
  title = {Evidential {{Disambiguation}} of {{Latent Multimodality}} in {{Conditional Variational Autoencoders}}},
  author = {Itkina, Masha and Ivanovic, Boris and Senanayake, Ransalu and Kochenderfer, Mykel J and Pavone, Marco},
  year = {2019},
  pages = {12},
  file = {/home/trung/GoogleDrive/Zotero/itkina et al_evidential disambiguation of latent multimodality in conditional variational autoencoders.pdf},
  language = {en}
}

@article{izmailov19_AveragingWeightsLeads,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = feb,
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and Shake-Shake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/izmailov et al_2019_averaging weights leads to wider optima and better generalization.pdf;/home/trung/Zotero/storage/IQMPPTMA/1803.html},
  journal = {arXiv:1803.05407 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{jacobsen19_ExcessiveInvarianceCauses,
  title = {Excessive {{Invariance Causes Adversarial Vulnerability}}},
  author = {Jacobsen, J{\"o}rn-Henrik and Behrmann, Jens and Zemel, Richard and Bethge, Matthias},
  year = {2019},
  month = jun,
  abstract = {Despite their impressive performance, deep neural networks exhibit striking failures on out-of-distribution inputs. One core idea of adversarial example research is to reveal neural network errors under such distribution shifts. We decompose these errors into two complementary sources: sensitivity and invariance. We show deep networks are not only too sensitive to task-irrelevant changes of their input, as is well-known from -adversarial examples, but are also too invariant to a wide range of task-relevant changes, thus making vast regions in input space vulnerable to adversarial attacks. We show such excessive invariance occurs across various tasks and architecture types. On MNIST and ImageNet one can manipulate the class-specific content of almost any image without changing the hidden activations. We identify an insufficiency of the standard cross-entropy loss as a reason for these failures. Further, we extend this objective based on an informationtheoretic analysis so it encourages the model to consider all task-dependent features in its decision. This provides the first approach tailored explicitly to overcome excessive invariance and resulting vulnerabilities.},
  archivePrefix = {arXiv},
  eprint = {1811.00401},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jacobsen et al_2019_excessive invariance causes adversarial vulnerability.pdf},
  journal = {arXiv:1811.00401 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jadon20_OverviewDeepLearning,
  title = {An {{Overview}} of {{Deep Learning Architectures}} in {{Few}}-{{Shot Learning Domain}}},
  author = {Jadon, Shruti},
  year = {2020},
  doi = {10.13140/RG.2.2.31573.24803/1},
  abstract = {Since 2012, Deep learning has revolutionized Artificial Intelligence and has achieved state-of-the-art outcomes in different domains, ranging from Image Classification to Speech Generation. Though it has many potentials, our current architectures come with the pre-requisite of large amounts of data. Few-Shot Learning(also known as few-shot learning) is a subfield of machine learning that aims to create such models that can learn the desired objective with less data, similar to how humans learn. In this paper, we have reviewed some of the well-known deep learning-based approaches towards few-shot learning. We have discussed the recent achievements, challenges, and possibilities of improvement of few-shot learning based deep learning architectures. Our aim for this paper is threefold: (i) Give a brief introduction to deep learning architectures for few-shot learning with pointers to core references. (ii) Indicate how deep learning has been applied to the low-data regime, from data preparation to model training. and, (iii) Provide a starting point for people interested in experimenting and perhaps contributing to the field of few-shot learning by pointing out some useful resources and open-source code. Our code is available at Github:https: //github.com/shruti-jadon/Hands-on-One-Shot-Learning Index Terms\textemdash Deep Learning, Neural Networks, Computer Vision, Architectures, Few-Shot Learning, Siamese Networks, Matching Networks, Optimization, Meta Networks, Model Agnostic Meta Learning, LSTM Meta learner, Memory Augmented Neural Networks.},
  archivePrefix = {arXiv},
  eprint = {2008.06365},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jadon_2020_an overview of deep learning architectures in few-shot learning domain.pdf},
  journal = {arXiv:2008.06365 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{jain19_DiscreteResidualFlow,
  title = {Discrete {{Residual Flow}} for {{Probabilistic Pedestrian Behavior Prediction}}},
  author = {Jain, Ajay and Casas, Sergio and Liao, Renjie and Xiong, Yuwen and Feng, Song and Segal, Sean and Urtasun, Raquel},
  year = {2019},
  month = oct,
  abstract = {Self-driving vehicles plan around both static and dynamic objects, applying predictive models of behavior to estimate future locations of the objects in the environment. However, future behavior is inherently uncertain, and models of motion that produce deterministic outputs are limited to short timescales. Particularly difficult is the prediction of human behavior. In this work, we propose the discrete residual flow network (DRF-Net), a convolutional neural network for human motion prediction that captures the uncertainty inherent in long-range motion forecasting. In particular, our learned network effectively captures multimodal posteriors over future human motion by predicting and updating a discretized distribution over spatial locations. We compare our model against several strong competitors and show that our model outperforms all baselines.},
  archivePrefix = {arXiv},
  eprint = {1910.08041},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jain et al_2019_discrete residual flow for probabilistic pedestrian behavior prediction.pdf;/home/trung/GoogleDrive/Zotero/jain et al_2019_discrete residual flow for probabilistic pedestrian behavior prediction2.pdf;/home/trung/GoogleDrive/Zotero/jain et al_2019_discrete residual flow for probabilistic pedestrian behavior prediction3.pdf;/home/trung/Zotero/storage/DG9AEJ8T/1910.html;/home/trung/Zotero/storage/X7VEJAT7/1910.html},
  journal = {arXiv:1910.08041 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,discrete,flow,probabilistic,residual,variational},
  primaryClass = {cs}
}

@article{jaiswal18_CapsuleGANGenerativeAdversarial,
  title = {{{CapsuleGAN}}: {{Generative Adversarial Capsule Network}}},
  shorttitle = {{{CapsuleGAN}}},
  author = {Jaiswal, Ayush and AbdAlmageed, Wael and Wu, Yue and Natarajan, Premkumar},
  year = {2018},
  month = feb,
  abstract = {We present Generative Adversarial Capsule Network (CapsuleGAN), a framework that uses capsule networks (CapsNets) instead of the standard convolutional neural networks (CNNs) as discriminators within the generative adversarial network (GAN) setting, while modeling image data. We provide guidelines for designing CapsNet discriminators and the updated GAN objective function, which incorporates the CapsNet margin loss, for training CapsuleGAN models. We show that CapsuleGAN outperforms convolutional-GAN at modeling image data distribution on MNIST and CIFAR-10 datasets, evaluated on the generative adversarial metric and at semi-supervised image classification.},
  archivePrefix = {arXiv},
  eprint = {1802.06167},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jaiswal et al_2018_capsulegan.pdf;/home/trung/Zotero/storage/CUFS4SYY/1802.html},
  journal = {arXiv:1802.06167 [cs, stat]},
  keywords = {capsule,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{JAL2019,
  title = {Semi-Supervised Learning Using Differentiable Reasoning},
  author = {{van Krieken}, Emile and Acar, Erman and {van Harmelen}, Frank},
  year = {2019},
  volume = {6},
  pages = {633--653},
  file = {/home/trung/GoogleDrive/Zotero/van krieken et al_2019_semi-supervised learning using differentiable reasoning.pdf},
  journal = {IFCoLog Journal of Logic and its Applications},
  keywords = {Machine Learning},
  number = {4},
  urlpaper = {http://www.cs.vu.nl/ frankh/postscript/JAL2019.pdf}
}

@article{jang16_CategoricalReparameterizationGumbelSoftmax,
  title = {Categorical {{Reparameterization}} with {{Gumbel}}-{{Softmax}}},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2016},
  month = nov,
  abstract = {Categorical variables are a natural choice for representing discrete structure in the world. However, stochastic neural networks rarely use categorical latent variables due to the inability to backpropagate through samples. In this work, we present an efficient gradient estimator that replaces the non-differentiable sample from a categorical distribution with a differentiable sample from a novel Gumbel-Softmax distribution. This distribution has the essential property that it can be smoothly annealed into a categorical distribution. We show that our Gumbel-Softmax estimator outperforms state-of-the-art gradient estimators on structured output prediction and unsupervised generative modeling tasks with categorical latent variables, and enables large speedups on semi-supervised classification.},
  archivePrefix = {arXiv},
  eprint = {1611.01144},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jang et al_2016_categorical reparameterization with gumbel-softmax.pdf;/home/trung/Zotero/storage/EYRCC9Y5/1611.html},
  journal = {arXiv:1611.01144 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{janzing00_Featurerelevancequantification,
  title = {Feature Relevance Quantification in Explainable {{AI}}: {{A}} Causal Problem},
  author = {Janzing, Dominik and Minorics, Lenon and Blobaum, Patrick},
  pages = {9},
  abstract = {We discuss promising recent contributions on quantifying feature relevance using Shapley values, where we observed some confusion on which probability distribution is the right one for dropped features. We argue that the confusion is based on not carefully distinguishing between observational and interventional conditional probabilities and try a clarification based on Pearl's seminal work on causality. We conclude that unconditional rather than conditional expectations provide the right notion of dropping features in contradiction to the theoretical justification of the software package SHAP. Parts of SHAP are unaffected because unconditional expectations (which we argue to be conceptually right) are used as approximation for the conditional ones, which encouraged others to `improve' SHAP in a way that we believe to be flawed.},
  file = {/home/trung/GoogleDrive/Zotero/janzing et al_feature relevance quantiﬁcation in explainable ai.pdf},
  keywords = {_tablet,causal,favorite},
  language = {en}
}

@article{jaques20_NewtonianVAEProportionalControl,
  title = {{{NewtonianVAE}}: {{Proportional Control}} and {{Goal Identification}} from {{Pixels}} via {{Physical Latent Spaces}}},
  shorttitle = {{{NewtonianVAE}}},
  author = {Jaques, Miguel and Burke, Michael and Hospedales, Timothy},
  year = {2020},
  month = jun,
  abstract = {Learning low-dimensional latent state space dynamics models has been a powerful paradigm for enabling vision-based planning and learning for control. We introduce a latent dynamics learning framework that is uniquely designed to induce proportional controlability in the latent space, thus enabling the use of much simpler controllers than prior work. We show that our learned dynamics model enables proportional control from pixels, dramatically simplifies and accelerates behavioural cloning of vision-based controllers, and provides interpretable goal discovery when applied to imitation learning of switching controllers from demonstration.},
  archivePrefix = {arXiv},
  eprint = {2006.01959},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jaques et al_2020_newtonianvae.pdf},
  journal = {arXiv:2006.01959 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{jawahar20_AutomaticDetectionMachine,
  title = {Automatic {{Detection}} of {{Machine Generated Text}}: {{A Critical Survey}}},
  author = {Jawahar, Ganesh and {Abdul-Mageed}, Muhammad and Lakshmanan, Laks V S},
  year = {2020},
  pages = {14},
  abstract = {Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.},
  file = {/home/trung/GoogleDrive/Zotero/jawahar et al_automatic detection of machine generated text.pdf},
  language = {en}
}

@article{jayasundara19_TreeCapsTreeStructuredCapsule,
  title = {{{TreeCaps}}: {{Tree}}-{{Structured Capsule Networks}} for {{Program Source Code Processing}}},
  shorttitle = {{{TreeCaps}}},
  author = {Jayasundara, Vinoj and Bui, Nghi Duy Quoc and Jiang, Lingxiao and Lo, David},
  year = {2019},
  month = oct,
  abstract = {Program comprehension is a fundamental task in software development and maintenance processes. Software developers often need to understand a large amount of existing code before they can develop new features or fix bugs in existing programs. Being able to process programming language code automatically and provide summaries of code functionality accurately can significantly help developers to reduce time spent in code navigation and understanding, and thus increase productivity. Different from natural language articles, source code in programming languages often follows rigid syntactical structures and there can exist dependencies among code elements that are located far away from each other through complex control flows and data flows. Existing studies on tree-based convolutional neural networks (TBCNN) and gated graph neural networks (GGNN) are not able to capture essential semantic dependencies among code elements accurately. In this paper, we propose novel tree-based capsule networks (TreeCaps) and relevant techniques for processing program code in an automated way that encodes code syntactical structures and captures code dependencies more accurately. Based on evaluation on programs written in different programming languages, we show that our TreeCaps-based approach can outperform other approaches in classifying the functionalities of many programs.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.12306},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jayasundara et al_2019_treecaps.pdf;/home/trung/Zotero/storage/C7D338RM/1910.html},
  journal = {arXiv:1910.12306 [cs, stat]},
  keywords = {capsule,Computer Science - Machine Learning,Computer Science - Software Engineering,Statistics - Machine Learning,tree},
  primaryClass = {cs, stat}
}

@book{jaynes03_Probabilitytheorylogic,
  title = {Probability Theory: {{The}} Logic of Science},
  author = {Jaynes, E. T.},
  editor = {Bretthorst, G. LarryEditor},
  year = {2003},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511790423},
  file = {/home/trung/GoogleDrive/Zotero/jaynes et al_2003_probability theory.pdf},
  keywords = {_tablet,favorite}
}

@article{jazbec20_FactorizedGaussianProcess,
  title = {Factorized {{Gaussian Process Variational Autoencoders}}},
  author = {Jazbec, Metod and Pearce, Michael and Fortuin, Vincent},
  year = {2020},
  month = nov,
  abstract = {Variational autoencoders often assume isotropic Gaussian priors and mean-field posteriors, hence do not exploit structure in scenarios where we may expect similarity or consistency across latent variables. Gaussian process variational autoencoders alleviate this problem through the use of a latent Gaussian process, but lead to a cubic inference time complexity. We propose a more scalable extension of these models by leveraging the independence of the auxiliary features, which is present in many datasets. Our model factorizes the latent kernel across these features in different dimensions, leading to a significant speed-up (in theory and practice), while empirically performing comparably to existing non-scalable approaches. Moreover, our approach allows for additional modeling of global latent information and for more general extrapolation to unseen input combinations.},
  archivePrefix = {arXiv},
  eprint = {2011.07255},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jazbec et al_2020_factorized gaussian process variational autoencoders.pdf},
  journal = {arXiv:2011.07255 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{jelodar18_LatentDirichletAllocation,
  title = {Latent {{Dirichlet Allocation}} ({{LDA}}) and {{Topic}} Modeling: Models, Applications, a Survey},
  shorttitle = {Latent {{Dirichlet Allocation}} ({{LDA}}) and {{Topic}} Modeling},
  author = {Jelodar, Hamed and Wang, Yongli and Yuan, Chi and Feng, Xia and Jiang, Xiahui and Li, Yanchao and Zhao, Liang},
  year = {2018},
  month = dec,
  abstract = {Topic modeling is one of the most powerful techniques in text mining for data mining, latent data discovery, and finding relationships among data, text documents. Researchers have published many articles in the field of topic modeling and applied in various fields such as software engineering, political science, medical and linguistic science, etc. There are various methods for topic modeling, which Latent Dirichlet allocation (LDA) is one of the most popular methods in this field. Researchers have proposed various models based on the LDA in topic modeling. According to previous work, this paper can be very useful and valuable for introducing LDA approaches in topic modeling. In this paper, we investigated scholarly articles highly (between 2003 to 2016) related to Topic Modeling based on LDA to discover the research development, current trends and intellectual structure of topic modeling. Also, we summarize challenges and introduce famous tools and datasets in topic modeling based on LDA.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1711.04305},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jelodar et al_2018_latent dirichlet allocation (lda) and topic modeling.pdf;/home/trung/Zotero/storage/9QT8UEXI/1711.html},
  journal = {arXiv:1711.04305 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{jeon20_MutualInformationdrivenSubjectinvariant,
  title = {Mutual {{Information}}-Driven {{Subject}}-Invariant and {{Class}}-Relevant {{Deep Representation Learning}} in {{BCI}}},
  author = {Jeon, Eunjin and Ko, Wonjun and Yoon, Jee Seok and Suk, Heung-Il},
  year = {2020},
  month = aug,
  abstract = {In recent years, deep learning-based feature representation methods have shown a promising impact in electroencephalography (EEG)-based brain-computer interface (BCI). Nonetheless, owing to high intra- and inter-subject variabilities, many studies on decoding EEG were designed in a subject-specific manner by using calibration samples, with no concern of its practical use, hampered by time-consuming steps and a large data requirement. To this end, recent studies adopted a transfer learning strategy, especially domain adaptation techniques. Among those, to our knowledge, an adversarial learning has shown its potential in BCIs. In the meantime, it is known that adversarial learning-based domain adaptation methods are prone to negative transfer that disrupts learning generalized feature representations, applicable to diverse domains, e.g., subjects or sessions in BCIs. In this paper, we propose a novel framework that learns class-relevant and subject-invariant feature representations in an information-theoretic manner, without using adversarial learning. To be specific, we devise two operational components in a deep network that explicitly estimate mutual information between feature representations; (1) to decompose features in an intermediate layer into class-relevant and class-irrelevant ones, (2) to enrich class-discriminative feature representation. On two large EEG datasets, we validated the effectiveness of our proposed framework by comparing with several comparative methods in performance. Further, we conducted rigorous analyses by performing an ablation study in regard to the components in our network, explaining our model's decision on input EEG signals via layer-wise relevance propagation, and visualizing the distribution of learned features via t-SNE.},
  archivePrefix = {arXiv},
  eprint = {1910.07747},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jeon et al_2020_mutual information-driven subject-invariant and class-relevant deep representation learning in bci.pdf},
  journal = {arXiv:1910.07747 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@techreport{jeong20_PRIMEprobabilisticimputation,
  title = {{{PRIME}}: A Probabilistic Imputation Method to Reduce Dropout Effects in Single Cell {{RNA}} Sequencing},
  shorttitle = {{{PRIME}}},
  author = {Jeong, Hyundoo and Liu, Zhandong},
  year = {2020},
  month = jan,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.01.03.893867},
  abstract = {Single-cell RNA sequencing technology provides a novel means to analyze the transcriptomic profiles of individual cells. The technique is vulnerable, however, to a type of noise called dropout effects, which lead to zero-inflated distributions in the transcriptome profile and reduce the reliability of the results. Single-cell RNA sequencing data therefore need to be carefully processed before in-depth analysis. Here we describe a novel imputation method that reduces dropout effects in single-cell sequencing. We construct a cell correspondence network and adjust gene expression estimates based on transcriptome profiles for the local community of cells of the same type. We comprehensively evaluated this method, called PRIME (PRobabilistic IMputation to reduce dropout effects in Expression profiles of single cell sequencing), on six datasets and verified that it improves the quality of visualization and accuracy of clustering analysis and can discover gene expression patterns hidden by noise.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/jeong et al_2020_prime.pdf},
  language = {en},
  type = {Preprint}
}

@article{jesson18_AdversariallyLearnedMixture,
  title = {Adversarially {{Learned Mixture Model}}},
  author = {Jesson, Andrew and {Low-Kam}, C{\'e}cile and Soudan, Florian and Chapados, Nicolas},
  year = {2018},
  month = jul,
  abstract = {The Adversarially Learned Mixture Model (AMM) is a generative model for unsupervised or semi-supervised data clustering. The AMM is the first adversarially optimized method to model the conditional dependence between inferred continuous and categorical latent variables. Experiments on the MNIST and SVHN datasets show that the AMM allows for semantic separation of complex data when little or no labeled data is available. The AMM achieves a state-of-the-art unsupervised clustering error rate of 2.86\% on the MNIST dataset. A semi-supervised extension of the AMM yields competitive results on the SVHN dataset.},
  archivePrefix = {arXiv},
  eprint = {1807.05344},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jesson et al_2018_adversarially learned mixture model.pdf;/home/trung/Zotero/storage/H9NRULX7/1807.html},
  journal = {arXiv:1807.05344 [cs, stat]},
  keywords = {adversarial,Computer Science - Machine Learning,mixture,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{jha18_DisentanglingFactorsVariation,
  title = {Disentangling {{Factors}} of {{Variation}} with {{Cycle}}-{{Consistent Variational Auto}}-{{Encoders}}},
  author = {Jha, Ananya Harsh and Anand, Saket and Singh, Maneesh and Veeravasarapu, V. S. R.},
  year = {2018},
  month = apr,
  abstract = {Generative models that learn disentangled representations for different factors of variation in an image can be very useful for targeted data augmentation. By sampling from the disentangled latent subspace of interest, we can efficiently generate new data necessary for a particular task. Learning disentangled representations is a challenging problem, especially when certain factors of variation are difficult to label. In this paper, we introduce a novel architecture that disentangles the latent space into two complementary subspaces by using only weak supervision in form of pairwise similarity labels. Inspired by the recent success of cycle-consistent adversarial architectures, we use cycle-consistency in a variational auto-encoder framework. Our non-adversarial approach is in contrast with the recent works that combine adversarial training with auto-encoders to disentangle representations. We show compelling results of disentangled latent subspaces on three datasets and compare with recent works that leverage adversarial training.},
  annotation = {ZSCC: 0000012},
  archivePrefix = {arXiv},
  eprint = {1804.10469},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jha et al_2018_disentangling factors of variation with cycle-consistent variational auto-encoders.pdf},
  journal = {arXiv:1804.10469 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{ji19_StochasticVariationalInference,
  title = {Stochastic {{Variational Inference}} via {{Upper Bound}}},
  author = {Ji, Chunlin and Shen, Haige},
  year = {2019},
  month = dec,
  abstract = {Stochastic variational inference (SVI) plays a key role in Bayesian deep learning. Recently various divergences have been proposed to design the surrogate loss for variational inference. We present a simple upper bound of the evidence as the surrogate loss. This evidence upper bound (EUBO) equals to the log marginal likelihood plus the KL-divergence between the posterior and the proposal. We show that the proposed EUBO is tighter than previous upper bounds introduced by \$\textbackslash chi\$-divergence or \$\textbackslash alpha\$-divergence. To facilitate scalable inference, we present the numerical approximation of the gradient of the EUBO and apply the SGD algorithm to optimize the variational parameters iteratively. Simulation study with Bayesian logistic regression shows that the upper and lower bounds well sandwich the evidence and the proposed upper bound is favorably tight. For Bayesian neural network, the proposed EUBO-VI algorithm outperforms state-of-the-art results for various examples.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1912.00650},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ji et al_2019_stochastic variational inference via upper bound.pdf;/home/trung/Zotero/storage/ERSZXH39/1912.html},
  journal = {arXiv:1912.00650 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ji20_ComprehensiveSurveyDeep,
  title = {A {{Comprehensive Survey}} on {{Deep Music Generation}}: {{Multi}}-Level {{Representations}}, {{Algorithms}}, {{Evaluations}}, and {{Future Directions}}},
  shorttitle = {A {{Comprehensive Survey}} on {{Deep Music Generation}}},
  author = {Ji, Shulei and Luo, Jing and Yang, Xinyu},
  year = {2020},
  month = nov,
  abstract = {The utilization of deep learning techniques in generating various contents (such as image, text, etc.) has become a trend. Especially music, the topic of this paper, has attracted widespread attention of countless researchers.The whole process of producing music can be divided into three stages, corresponding to the three levels of music generation: score generation produces scores, performance generation adds performance characteristics to the scores, and audio generation converts scores with performance characteristics into audio by assigning timbre or generates music in audio format directly. Previous surveys have explored the network models employed in the field of automatic music generation. However, the development history, the model evolution, as well as the pros and cons of same music generation task have not been clearly illustrated. This paper attempts to provide an overview of various composition tasks under different music generation levels, covering most of the currently popular music generation tasks using deep learning. In addition, we summarize the datasets suitable for diverse tasks, discuss the music representations, the evaluation methods as well as the challenges under different levels, and finally point out several future directions.},
  archivePrefix = {arXiv},
  eprint = {2011.06801},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ji et al_2020_a comprehensive survey on deep music generation.pdf},
  journal = {arXiv:2011.06801 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{ji20_MIPRE,
  title = {{{MIP}}*={{RE}}},
  author = {Ji, Zhengfeng and Natarajan, Anand and Vidick, Thomas and Wright, John and Yuen, Henry},
  year = {2020},
  month = sep,
  abstract = {We show that the class MIP{${_\ast}$} of languages that can be decided by a classical verifier interacting with multiple all-powerful quantum provers sharing entanglement is equal to the class RE of recursively enumerable languages. Our proof builds upon the quantum low-degree test of (Natarajan and Vidick, FOCS 2018) and the classical low-individual degree test of (Ji, et al., 2020) by integrating recent developments from (Natarajan and Wright, FOCS 2019) and combining them with the recursive compression framework of (Fitzsimons et al., STOC 2019).},
  archivePrefix = {arXiv},
  eprint = {2001.04383},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ji et al_2020_mip=re.pdf},
  journal = {arXiv:2001.04383 [quant-ph]},
  keywords = {favorite},
  language = {en},
  primaryClass = {quant-ph}
}

@article{jia00_LearningSemanticImage,
  title = {Learning {{Semantic Image Representations}} at a {{Large Scale}}},
  author = {Jia, Yangqing},
  pages = {104},
  file = {/home/trung/GoogleDrive/Zotero/jia_learning semantic image representations at a large scale.pdf},
  language = {en}
}

@article{jia19_Domaininvariantrepresentationlearning,
  title = {Domain-Invariant Representation Learning Using an Unsupervised Domain Adversarial Adaptation Deep Neural Network},
  author = {Jia, Xibin and Jin, Ya and Su, Xing and Hu, Yongli},
  year = {2019},
  month = aug,
  volume = {355},
  pages = {209--220},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.04.033},
  abstract = {Domain adaptation is proposed to improve the recognition performance of the domain shift or the dataset bias. The domain shift is a very common problem, which is caused by diverse factors, such as data capturing angles, illumination, and image quality existing in the natural scene image understanding. Since the domain shift leads to the feature distribution discrepancy, some solutions have been proposed to alleviate the distribution discrepancy by mapping feature spaces between source and target domains, so as to ensure the transferable features can be learned by the deep networks during the endto-end training for the classification tasks. However, it is still a big challenge to address the domain shift when the distribution spaces are not clearly separated. Inspired by the adversarial idea, we propose a novel unified deep neural network architecture named the unsupervised domain adversarial adaptation deep neural network. It addresses the domain adaptation problem by learning domain-invariant features through mitigating the feature discriminative ability in the domain classification task alternatively by alleviating the feature distribution discrepancy in the main classification task. Therefore, in our proposed unified deep network, we integrate two main modules. One is an auxiliary task module for the domain classifier, which is trained to make sure the learned features are domain-invariant under the adversarial optimization strategy by minimizing the domain discriminative ability. The other is the module at task-specific layers to enhance the learning of the transferable features with the less distribution discrepancy by adding multiple maximum mean discrepancy constraints to map the features to reproducing kernel Hilbert spaces. The experimental results show that the features learned by our proposed unified deep neural network perform better than the features learned by previous crossdomain neural networks on classification tasks. Our proposed approach achieves the state-of-the-art performance on three cross-domain datasets: Office-31 (different capturing angles, illumination, and image quality), Office-Caltech (modified from Office-31) and a combined cross-domain digit dataset, including MNIST, USPS and SVHN (different style digits in each dataset).},
  file = {/home/trung/Zotero/storage/FM39M76S/Jia et al. - 2019 - Domain-invariant representation learning using an .pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{jia19_TransferLearningSpeaker,
  title = {Transfer {{Learning}} from {{Speaker Verification}} to {{Multispeaker Text}}-{{To}}-{{Speech Synthesis}}},
  author = {Jia, Ye and Zhang, Yu and Weiss, Ron J. and Wang, Quan and Shen, Jonathan and Ren, Fei and Chen, Zhifeng and Nguyen, Patrick and Pang, Ruoming and Moreno, Ignacio Lopez and Wu, Yonghui},
  year = {2019},
  month = jan,
  abstract = {We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of many different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech from thousands of speakers without transcripts, to generate a fixed-dimensional embedding vector from seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2, which generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder that converts the mel spectrogram into a sequence of time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the new task, and is able to synthesize natural speech from speakers that were not seen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1806.04558},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jia et al_2019_transfer learning from speaker verification to multispeaker text-to-speech synthesis.pdf;/home/trung/Zotero/storage/DAFLQ89D/1806.html},
  journal = {arXiv:1806.04558 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,speaker embedding,tacotron,wavenet},
  primaryClass = {cs, eess}
}

@article{jia20_PhysicsGuidedMachineLearning,
  title = {Physics-{{Guided Machine Learning}} for {{Scientific Discovery}}: {{An Application}} in {{Simulating Lake Temperature Profiles}}},
  shorttitle = {Physics-{{Guided Machine Learning}} for {{Scientific Discovery}}},
  author = {Jia, Xiaowei and Willard, Jared and Karpatne, Anuj and Read, Jordan S. and Zwart, Jacob A. and Steinbach, Michael and Kumar, Vipin},
  year = {2020},
  month = jan,
  abstract = {Physics-based models of dynamical systems are often used to study engineering and environmental systems. Despite their extensive use, these models have several well-known limitations due to simplified representations of the physical processes being modeled or challenges in selecting appropriate parameters. While-state-of-the-art machine learning models can sometimes outperform physics-based models given ample amount of training data, they can produce results that are physically inconsistent. This paper proposes a physics-guided recurrent neural network model (PGRNN) that combines RNNs and physics-based models to leverage their complementary strengths and improves the modeling of physical processes. Specifically, we show that a PGRNN can improve prediction accuracy over that of physics-based models, while generating outputs consistent with physical laws. An important aspect of our PGRNN approach lies in its ability to incorporate the knowledge encoded in physics-based models. This allows training the PGRNN model using very few true observed data while also ensuring high prediction accuracy. Although we present and evaluate this methodology in the context of modeling the dynamics of temperature in lakes, it is applicable more widely to a range of scientific and engineering disciplines where physics-based (also known as mechanistic) models are used, e.g., climate science, materials science, computational chemistry, and biomedicine.},
  archivePrefix = {arXiv},
  eprint = {2001.11086},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/TT9M2HCS/Jia et al. - 2020 - Physics-Guided Machine Learning for Scientific Dis.pdf},
  journal = {arXiv:2001.11086 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{jiang00_FantasticGeneralizationMeasures,
  title = {Fantastic {{Generalization Measures}} and {{Where}} to {{Find Them}}},
  author = {Jiang, Yiding and Neyshabur, Behnam and Mobahi, Hossein and Krishnan, Dilip and Bengio, Samy},
  pages = {41},
  abstract = {Generalization of deep networks has been of great interest in recent years, resulting in a number of theoretically and empirically motivated complexity measures. However, most papers proposing such measures only study a small set of models, leaving open the question of whether the conclusion drawn from those experiments would remain valid in other settings. We present the first large scale study of generalization bounds and measures in deep networks. We train over two thousand convolutional networks with systematic changes in commonly used hyperparameters. Hoping to uncover potentially causal relationships between each measure and generalization, we analyze carefully controlled experiments and show surprising failures of some measures as well as promising measures for further research.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/jiang et al_fantastic generalization measures and where to find them.pdf},
  language = {en}
}

@article{jiang17_TrajectoryNetEmbeddedGPS,
  title = {{{TrajectoryNet}}: {{An Embedded GPS Trajectory Representation}} for {{Point}}-Based {{Classification Using Recurrent Neural Networks}}},
  shorttitle = {{{TrajectoryNet}}},
  author = {Jiang, Xiang and {de Souza}, Erico N. and Pesaranghader, Ahmad and Hu, Baifan and Silver, Daniel L. and Matwin, Stan},
  year = {2017},
  month = aug,
  abstract = {Understanding and discovering knowledge from GPS (Global Positioning System) traces of human activities is an essential topic in mobility-based urban computing. We propose TrajectoryNet-a neural network architecture for point-based trajectory classification to infer real world human transportation modes from GPS traces. To overcome the challenge of capturing the underlying latent factors in the low-dimensional and heterogeneous feature space imposed by GPS data, we develop a novel representation that embeds the original feature space into another space that can be understood as a form of basis expansion. We also enrich the feature space via segment-based information and use Maxout activations to improve the predictive power of Recurrent Neural Networks (RNNs). We achieve over 98\% classification accuracy when detecting four types of transportation modes, outperforming existing models without additional sensory data or location-based prior knowledge.},
  archivePrefix = {arXiv},
  eprint = {1705.02636},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jiang et al_2017_trajectorynet.pdf},
  journal = {arXiv:1705.02636 [cs]},
  primaryClass = {cs}
}

@article{jiang17_VariationalDeepEmbedding,
  title = {Variational {{Deep Embedding}}: {{An Unsupervised}} and {{Generative Approach}} to {{Clustering}}},
  shorttitle = {Variational {{Deep Embedding}}},
  author = {Jiang, Zhuxi and Zheng, Yin and Tan, Huachun and Tang, Bangsheng and Zhou, Hanning},
  year = {2017},
  month = jun,
  abstract = {Clustering is among the most fundamental tasks in computer vision and machine learning. In this paper, we propose Variational Deep Embedding (VaDE), a novel unsupervised generative clustering approach within the framework of Variational Auto-Encoder (VAE). Specifically, VaDE models the data generative procedure with a Gaussian Mixture Model (GMM) and a deep neural network (DNN): 1) the GMM picks a cluster; 2) from which a latent embedding is generated; 3) then the DNN decodes the latent embedding into observables. Inference in VaDE is done in a variational way: a different DNN is used to encode observables to latent embeddings, so that the evidence lower bound (ELBO) can be optimized using Stochastic Gradient Variational Bayes (SGVB) estimator and the reparameterization trick. Quantitative comparisons with strong baselines are included in this paper, and experimental results show that VaDE significantly outperforms the state-of-the-art clustering methods on 4 benchmarks from various modalities. Moreover, by VaDE's generative nature, we show its capability of generating highly realistic samples for any specified cluster, without using supervised information during training. Lastly, VaDE is a flexible and extensible framework for unsupervised generative clustering, more general mixture models than GMM can be easily plugged in.},
  archivePrefix = {arXiv},
  eprint = {1611.05148},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jiang et al_2017_variational deep embedding.pdf},
  journal = {arXiv:1611.05148 [cs]},
  primaryClass = {cs}
}

@article{jiang20_GenerativeNeurosymbolicMachines,
  title = {Generative {{Neurosymbolic Machines}}},
  author = {Jiang, Jindong and Ahn, Sungjin},
  year = {2020},
  month = oct,
  abstract = {Reconciling symbolic and distributed representations is a crucial challenge that can potentially resolve the limitations of current deep learning. Remarkable advances in this direction have been achieved recently via generative object-centric representation models. While learning a recognition model that infers object-centric symbolic representations like bounding boxes from raw images in an unsupervised way, no such model can provide another important ability of a generative model, i.e., generating (sampling) according to the structure of learned world density. In this paper, we propose Generative Neurosymbolic Machines, a generative model that combines the benefits of distributed and symbolic representations to support both structured representations of symbolic components and density-based generation. These two crucial properties are achieved by a two-layer latent hierarchy with the global distributed latent for flexible density modeling and the structured symbolic latent map. To increase the model flexibility in this hierarchical structure, we also propose the StructDRAW prior. In experiments, we show that the proposed model significantly outperforms the previous structured representation models as well as the state-of-the-art non-structured generative models in terms of both structure accuracy and image generation quality.},
  archivePrefix = {arXiv},
  eprint = {2010.12152},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jiang et al_2020_generative neurosymbolic machines.pdf},
  journal = {arXiv:2010.12152 [cs]},
  primaryClass = {cs}
}

@article{jiang20_GenerativeNeurosymbolicMachinesa,
  title = {Generative {{Neurosymbolic Machines}}},
  author = {Jiang, Jindong and Ahn, Sungjin},
  year = {2020},
  month = oct,
  abstract = {Reconciling symbolic and distributed representations is a crucial challenge that can potentially resolve the limitations of current deep learning. Remarkable advances in this direction have been achieved recently via generative object-centric representation models. While learning a recognition model that infers object-centric symbolic representations like bounding boxes from raw images in an unsupervised way, no such model can provide another important ability of a generative model, i.e., generating (sampling) according to the structure of learned world density. In this paper, we propose Generative Neurosymbolic Machines, a generative model that combines the benefits of distributed and symbolic representations to support both structured representations of symbolic components and density-based generation. These two crucial properties are achieved by a two-layer latent hierarchy with the global distributed latent for flexible density modeling and the structured symbolic latent map. To increase the model flexibility in this hierarchical structure, we also propose the StructDRAW prior. In experiments, we show that the proposed model significantly outperforms the previous structured representation models as well as the state-of-the-art non-structured generative models in terms of both structure accuracy and image generation quality.},
  archivePrefix = {arXiv},
  eprint = {2010.12152},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jiang et al_2020_generative neurosymbolic machines2.pdf},
  journal = {arXiv:2010.12152 [cs]},
  primaryClass = {cs}
}

@article{jiang20_SyntheticNoiseDeep,
  title = {Beyond {{Synthetic Noise}}: {{Deep Learning}} on {{Controlled Noisy Labels}}},
  shorttitle = {Beyond {{Synthetic Noise}}},
  author = {Jiang, Lu and Huang, Di and Liu, Mason and Yang, Weilong},
  year = {2020},
  month = aug,
  abstract = {Performing controlled experiments on noisy data is essential in understanding deep learning across noise levels. Due to the lack of suitable datasets, previous research has only examined deep learning on controlled synthetic label noise, and realworld label noise has never been studied in a controlled setting. This paper makes three contributions. First, we establish the first benchmark of controlled real-world label noise from the web. This new benchmark enables us to study the web label noise in a controlled setting for the first time. The second contribution is a simple but effective method to overcome both synthetic and real noisy labels. We show that our method achieves the best result on our dataset as well as on two public benchmarks (CIFAR and WebVision). Third, we conduct the largest study by far into understanding deep neural networks trained on noisy labels across different noise levels, noise types, network architectures, and training settings. The data and code are released at the following link http://www.lujiang.info/cnlw.},
  archivePrefix = {arXiv},
  eprint = {1911.09781},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jiang et al_2020_beyond synthetic noise.pdf},
  journal = {arXiv:1911.09781 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jiang21_kernelnonnegativematrix,
  title = {A Kernel Non-Negative Matrix Factorization Framework for Single Cell Clustering},
  author = {Jiang, Hao and Yi, Ming and Zhang, Shihua},
  year = {2021},
  month = feb,
  volume = {90},
  pages = {875--888},
  issn = {0307-904X},
  doi = {10.1016/j.apm.2020.08.065},
  abstract = {The emergence of single-cell RNA-sequencing is ideally placed to unravel cellular heterogeneity in biological systems, an extremely challenging problem in single cell RNA-sequencing studies. However, most current computational approaches lack the sensitivity to reliably detect nonlinear gene-gene relationships masked by dropout events. We proposed a kernel non-negative matrix factorization framework for detecting nonlinear relationships among genes, where the new kernel is developed using kernel tricks on cellular differentiability correlation. The newly constructed kernel not only provides a description on the gene-gene relationship, but also helps to build a new low-dimensional representation on the original data. Besides, we developed an efficient method for determining the optimal cluster number within each data set with the usage of Diffusion Maps. The proposed algorithm is further compared with representative algorithms: SC3 and several other state-of-the-art clustering methods, on several benchmark or real scRNA-Seq datasets using internal criteria (clustering number accuracy) and external criteria (Adjusted rand index and Normalized mutual information) to show effectiveness of our method.},
  journal = {Applied Mathematical Modelling},
  keywords = {Cellular heterogeneity;,Kernel non-negative matrix factorization,Single cell RNA-sequencing}
}

@article{jin19_JunctionTreeVariational,
  title = {Junction {{Tree Variational Autoencoder}} for {{Molecular Graph Generation}}},
  author = {Jin, Wengong and Barzilay, Regina and Jaakkola, Tommi},
  year = {2019},
  month = mar,
  abstract = {We seek to automate the design of molecules based on specific chemical properties. In computational terms, this task involves continuous embedding and generation of molecular graphs. Our primary contribution is the direct realization of molecular graphs, a task previously approached by generating linear SMILES strings instead of graphs. Our junction tree variational autoencoder generates molecular graphs in two phases, by first generating a tree-structured scaffold over chemical substructures, and then combining them into a molecule with a graph message passing network. This approach allows us to incrementally expand molecules while maintaining chemical validity at every step. We evaluate our model on multiple tasks ranging from molecular generation to optimization. Across these tasks, our model outperforms previous state-of-the-art baselines by a significant margin.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1802.04364},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/82YL5KNZ/Jin et al. - 2019 - Junction Tree Variational Autoencoder for Molecula.pdf},
  journal = {arXiv:1802.04364 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jin19_NonconvexOptimizationMachine,
  title = {On {{Nonconvex Optimization}} for {{Machine Learning}}: {{Gradients}}, {{Stochasticity}}, and {{Saddle Points}}},
  shorttitle = {On {{Nonconvex Optimization}} for {{Machine Learning}}},
  author = {Jin, Chi and Netrapalli, Praneeth and Ge, Rong and Kakade, Sham M. and Jordan, Michael I.},
  year = {2019},
  month = sep,
  abstract = {Gradient descent (GD) and stochastic gradient descent (SGD) are the workhorses of large-scale machine learning. While classical theory focused on analyzing the performance of these methods in convex optimization problems, the most notable successes in machine learning have involved nonconvex optimization, and a gap has arisen between theory and practice. Indeed, traditional analyses of GD and SGD show that both algorithms converge to stationary points efficiently. But these analyses do not take into account the possibility of converging to saddle points. More recent theory has shown that GD and SGD can avoid saddle points, but the dependence on dimension in these analyses is polynomial. For modern machine learning, where the dimension can be in the millions, such dependence would be catastrophic. We analyze perturbed versions of GD and SGD and show that they are truly efficient---their dimension dependence is only polylogarithmic. Indeed, these algorithms converge to second-order stationary points in essentially the same time as they take to converge to classical first-order stationary points.},
  archivePrefix = {arXiv},
  eprint = {1902.04811},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jin et al_2019_on nonconvex optimization for machine learning.pdf},
  journal = {arXiv:1902.04811 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{jin20_DiscriminativeGenerativeSelfSupervised,
  title = {Discriminative, {{Generative}} and {{Self}}-{{Supervised Approaches}} for {{Target}}-{{Agnostic Learning}}},
  author = {Jin, Yuan and Buntine, Wray and Petitjean, Francois and Webb, Geoffrey I.},
  year = {2020},
  month = nov,
  abstract = {Supervised learning, characterized by both discriminative and generative learning, seeks to predict the values of single (or sometimes multiple) predefined target attributes based on a predefined set of predictor attributes. For applications where the information available and predictions to be made may vary from instance to instance, we propose the task of target-agnostic learning where arbitrary disjoint sets of attributes can be used for each of predictors and targets for each to-be-predicted instance. For this task, we survey a wide range of techniques available for handling missing values, self-supervised training and pseudo-likelihood training, and adapt them to a suite of algorithms that are suitable for the task. We conduct extensive experiments on this suite of algorithms on a large collection of categorical, continuous and discretized datasets, and report their performance in terms of both classification and regression errors. We also report the training and prediction time of these algorithms when handling large-scale datasets. Both generative and self-supervised learning models are shown to perform well at the task, although their characteristics towards the different types of data are quite different. Nevertheless, our derived theorem for the pseudo-likelihood theory also shows that they are related for inferring a joint distribution model based on the pseudo-likelihood training.},
  archivePrefix = {arXiv},
  eprint = {2011.06428},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jin et al_2020_discriminative, generative and self-supervised approaches for target-agnostic learning.pdf},
  journal = {arXiv:2011.06428 [cs]},
  keywords = {representation},
  primaryClass = {cs}
}

@article{jing19_SGVAESequentialGraph,
  title = {{{SGVAE}}: {{Sequential Graph Variational Autoencoder}}},
  shorttitle = {{{SGVAE}}},
  author = {Jing, Bowen and Chi, Ethan A. and Tang, Jillian},
  year = {2019},
  month = dec,
  abstract = {Generative models of graphs are well-known, but many existing models are limited in scalability and expressivity. We present a novel sequential graphical variational autoencoder operating directly on graphical representations of data. In our model, the encoding and decoding of a graph as is framed as a sequential deconstruction and construction process, respectively, enabling the the learning of a latent space. Experiments on a cycle dataset show promise, but highlight the need for a relaxation of the distribution over node permutations.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.07800},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jing et al_2019_sgvae.pdf},
  journal = {arXiv:1912.07800 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jing20_ImplicitRankMinimizingAutoencoder,
  title = {Implicit {{Rank}}-{{Minimizing Autoencoder}}},
  author = {Jing, Li and Zbontar, Jure and LeCun, Yann},
  year = {2020},
  month = oct,
  abstract = {An important component of autoencoders is the method by which the information capacity of the latent representation is minimized or limited. In this work, the rank of the covariance matrix of the codes is implicitly minimized by relying on the fact that gradient descent learning in multi-layer linear networks leads to minimum-rank solutions. By inserting a number of extra linear layers between the encoder and the decoder, the system spontaneously learns representations with a low effective dimension. The model, dubbed Implicit Rank-Minimizing Autoencoder (IRMAE), is simple, deterministic, and learns compact latent spaces. We demonstrate the validity of the method on several image generation and representation learning tasks.},
  archivePrefix = {arXiv},
  eprint = {2010.00679},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jing et al_2020_implicit rank-minimizing autoencoder.pdf},
  journal = {arXiv:2010.00679 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{jipeng19_ShortTextTopic,
  title = {Short {{Text Topic Modeling Techniques}}, {{Applications}}, and {{Performance}}: {{A Survey}}},
  shorttitle = {Short {{Text Topic Modeling Techniques}}, {{Applications}}, and {{Performance}}},
  author = {Jipeng, Qiang and Zhenyu, Qian and Yun, Li and Yunhao, Yuan and Xindong, Wu},
  year = {2019},
  month = apr,
  abstract = {Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.},
  archivePrefix = {arXiv},
  eprint = {1904.07695},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jipeng et al_2019_short text topic modeling techniques, applications, and performance.pdf},
  journal = {arXiv:1904.07695 [cs]},
  primaryClass = {cs}
}

@article{johannemann19_SufficientRepresentationsCategorical,
  title = {Sufficient {{Representations}} for {{Categorical Variables}}},
  author = {Johannemann, Jonathan and Hadad, Vitor and Athey, Susan and Wager, Stefan},
  year = {2019},
  month = aug,
  abstract = {Many learning algorithms require categorical data to be transformed into real vectors before it can be used as input. Often, categorical variables are encoded as one-hot (or dummy) vectors. However, this mode of representation can be wasteful since it adds many low-signal regressors, especially when the number of unique categories is large. In this paper, we investigate simple alternative solutions for universally consistent estimators that rely on lower-dimensional real-valued representations of categorical variables that are "sufficient" in the sense that no predictive information is lost. We then compare preexisting and proposed methods on simulated and observational datasets.},
  archivePrefix = {arXiv},
  eprint = {1908.09874},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/johannemann et al_2019_sufficient representations for categorical variables.pdf;/home/trung/Zotero/storage/JUPISR6F/1908.html},
  journal = {arXiv:1908.09874 [cs, stat]},
  keywords = {Computer Science - Machine Learning,distribution,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{johnson16_PerceptualLossesRealTime,
  title = {Perceptual {{Losses}} for {{Real}}-{{Time Style Transfer}} and {{Super}}-{{Resolution}}},
  author = {Johnson, Justin and Alahi, Alexandre and {Fei-Fei}, Li},
  year = {2016},
  month = mar,
  abstract = {We consider image transformation problems, where an input image is transformed into an output image. Recent methods for such problems typically train feed-forward convolutional neural networks using a \textbackslash emph\{per-pixel\} loss between the output and ground-truth images. Parallel work has shown that high-quality images can be generated by defining and optimizing \textbackslash emph\{perceptual\} loss functions based on high-level features extracted from pretrained networks. We combine the benefits of both approaches, and propose the use of perceptual loss functions for training feed-forward networks for image transformation tasks. We show results on image style transfer, where a feed-forward network is trained to solve the optimization problem proposed by Gatys et al in real-time. Compared to the optimization-based method, our network gives similar qualitative results but is three orders of magnitude faster. We also experiment with single-image super-resolution, where replacing a per-pixel loss with a perceptual loss gives visually pleasing results.},
  archivePrefix = {arXiv},
  eprint = {1603.08155},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/johnson et al_2016_perceptual losses for real-time style transfer and super-resolution.pdf},
  journal = {arXiv:1603.08155 [cs]},
  primaryClass = {cs}
}

@inproceedings{jokinen16_VariationSpokenNorth,
  title = {Variation in {{Spoken North Sami Language}}},
  booktitle = {Interspeech 2016},
  author = {Jokinen, Kristiina and Trong, Trung Ngo and Hautam{\"a}ki, Ville},
  year = {2016},
  month = sep,
  pages = {3299--3303},
  doi = {10.21437/Interspeech.2016-1438},
  abstract = {The paper sets to investigate the amount of variation between the North Sami speakers living in two different majority language contexts: Finnish, spoken in Finland, and Norwegian Bokma\r{}l, spoken in Norway. We hypothesize that the majority language is a significant factor in recognizing variation of the North Sami language. Although North Sami is the biggest of the nine currently spoken Sami languages and it has become a lingua franca among the Sami speakers, there are clear differences in the pronunciation of the North Sami spoken in Finland and Norway, so that the difference can be used to recognize which majority language region the speaker comes from. Using a corpus of spoken North Sami collected in locations in Finland and Norway, we experimented in classifying the speech samples into categories based on the two majority languages. We used the i-vector methodology to model both intra- and between-dialect variations, and achieved the average recognition of about 17.31\% EER for classifying the Sami speech samples. The results support our hypothesis that the variation is due to the majority language, i.e. Finnish or Norwegian, spoken in the given context, rather than individual variation.},
  file = {/home/trung/GoogleDrive/Zotero/jokinen et al_2016_variation in spoken north sami language.pdf},
  language = {en}
}

@article{jolicoeur-martineau19_ConnectionsSupportVector,
  title = {Connections between {{Support Vector Machines}}, {{Wasserstein}} Distance and Gradient-Penalty {{GANs}}},
  author = {{Jolicoeur-Martineau}, Alexia and Mitliagkas, Ioannis},
  year = {2019},
  month = oct,
  abstract = {We generalize the concept of maximum-margin classifiers (MMCs) to arbitrary norms and non-linear functions. Support Vector Machines (SVMs) are a special case of MMC. We find that MMCs can be formulated as Integral Probability Metrics (IPMs) or classifiers with some form of gradient norm penalty. This implies a direct link to a class of Generative adversarial networks (GANs) which penalize a gradient norm. We show that the Discriminator in Wasserstein, Standard, Least-Squares, and Hinge GAN with Gradient Penalty is an MMC. We explain why maximizing a margin may be helpful in GANs. We hypothesize and confirm experimentally that \$L\^\textbackslash infty\$-norm penalties with Hinge loss produce better GANs than \$L\^2\$-norm penalties (based on common evaluation metrics). We derive the margins of Relativistic paired (Rp) and average (Ra) GANs.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.06922},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jolicoeur-martineau et al_2019_connections between support vector machines, wasserstein distance and gradient-penalty gans.pdf;/home/trung/Zotero/storage/3N8AQF9F/1910.html},
  journal = {arXiv:1910.06922 [cs, stat]},
  keywords = {Computer Science - Machine Learning,explain,gan,Statistics - Machine Learning,svm},
  primaryClass = {cs, stat}
}

@article{jonsson20_ConvergenceBehaviorDNNs,
  title = {Convergence {{Behavior}} of {{DNNs}} with {{Mutual}}-{{Information}}-{{Based Regularization}}},
  author = {J{\'o}nsson, Hlynur and Cherubini, Giovanni and Eleftheriou, Evangelos},
  year = {2020},
  volume = {22},
  issn = {1099-4300},
  doi = {10.3390/e22070727},
  abstract = {Information theory concepts are leveraged with the goal of better understanding and improving Deep Neural Networks (DNNs). The information plane of neural networks describes the behavior during training of the mutual information at various depths between input/output and hidden-layer variables. Previous analysis revealed that most of the training epochs are spent on compressing the input, in some networks where finiteness of the mutual information can be established. However, the estimation of mutual information is nontrivial for high-dimensional continuous random variables. Therefore, the computation of the mutual information for DNNs and its visualization on the information plane mostly focused on low-complexity fully connected networks. In fact, even the existence of the compression phase in complex DNNs has been questioned and viewed as an open problem. In this paper, we present the convergence of mutual information on the information plane for a high-dimensional VGG-16 Convolutional Neural Network (CNN) by resorting to Mutual Information Neural Estimation (MINE), thus confirming and extending the results obtained with low-dimensional fully connected networks. Furthermore, we demonstrate the benefits of regularizing a network, especially for a large number of training epochs, by adopting mutual information estimates as additional terms in the loss function characteristic of the network. Experimental results show that the regularization stabilizes the test accuracy and significantly reduces its variance.},
  file = {/home/trung/GoogleDrive/Zotero/jónsson et al_2020_convergence behavior of dnns with mutual-information-based regularization.pdf},
  journal = {Entropy},
  keywords = {deep neural networks,information,information bottleneck,regularization methods},
  number = {7}
}

@article{joo20_DeepLearningRequires,
  title = {Deep {{Learning Requires Explicit Regularization}} for {{Reliable Predictive Probability}}},
  author = {Joo, Taejong and Chung, Uijung},
  year = {2020},
  month = oct,
  abstract = {From the statistical learning perspective, complexity control via explicit regularization is a necessity for improving the generalization of over-parameterized models, which deters the memorization of intricate patterns existing only in the training data. However, the impressive generalization performance of over-parameterized neural networks with only implicit regularization challenges the importance of explicit regularization. Furthermore, explicit regularization does not prevent neural networks from memorizing unnatural patterns, such as random labels. In this work, we revisit the role and importance of explicit regularization methods for generalization of the predictive probability, not just the generalization of the 0-1 loss. Specifically, we analyze the possible cause of the poor predictive probability and identify that regularization of predictive confidence is required during training. We then empirically show that explicit regularization significantly improves the reliability of the predictive probability, which enables better predictive uncertainty representation and prevents the overconfidence problem. Our findings present a new direction to improve the predictive probability quality of deterministic neural networks, which can be an efficient and scalable alternative to Bayesian neural networks and ensemble methods.},
  archivePrefix = {arXiv},
  eprint = {2006.06399},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/joo et al_2020_deep learning requires explicit regularization for reliable predictive probability.pdf},
  journal = {arXiv:2006.06399 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{jordan00_ComputationalThinkingInferential,
  title = {Computational {{Thinking}} and {{Inferential Thinking}}},
  author = {Jordan, Michael},
  pages = {32},
  file = {/home/trung/GoogleDrive/Zotero/jordan_computational thinking and inferential thinking.pdf},
  language = {en}
}

@article{jordan12_HierarchicalModelsNested,
  title = {Hierarchical {{Models}}, {{Nested Models}} and {{Completely Random Measures}}},
  author = {Jordan, Michael},
  year = {2012},
  month = mar,
  file = {/home/trung/GoogleDrive/Zotero/jordan_2012_hierarchical models, nested models and completely random measures.pdf}
}

@incollection{jordan98_IntroductionVariationalMethods,
  title = {An {{Introduction}} to {{Variational Methods}} for {{Graphical Models}}},
  booktitle = {Learning in {{Graphical Models}}},
  author = {Jordan, Michael I. and Ghahramani, Zoubin and Jaakkola, Tommi S. and Saul, Lawrence K.},
  editor = {Jordan, Michael I.},
  year = {1998},
  pages = {105--161},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-011-5014-9_5},
  abstract = {This paper presents a tutorial introduction to the use of variational methods for inference and learning in graphical models (Bayesian networks and Markov random fields). We present a number of examples of graphical models, including the QMR-DT database, the sigmoid belief network, the Boltzmann machine, and several variants of hidden Markov models, in which it is infeasible to run exact inference algorithms. We then introduce variational methods, which exploit laws of large numbers to transform the original graphical model into a simplified graphical model in which inference is efficient. Inference in the simpified model provides bounds on probabilities of interest in the original model. We describe a general framework for generating variational transformations based on convex duality. Finally we return to the examples and demonstrate how variational algorithms can be formulated in each case.},
  file = {/home/trung/GoogleDrive/Zotero/jordan et al_1998_an introduction to variational methods for graphical models.pdf},
  isbn = {978-94-010-6104-9 978-94-011-5014-9},
  language = {en}
}

@misc{josephs00_InterpretableMachineLearning,
  title = {Interpretable {{Machine Learning Part}} 1},
  author = {Josephs, David},
  abstract = {Overview of Permutations, Partial Dependence, ALE, in English, math, R, and Python},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/3HNFI4AT/iml.html},
  howpublished = {https://josephsdavid.github.io/iml.html},
  journal = {josephs.david.github.io}
}

@article{jospin20_HandsonBayesianNeural,
  title = {Hands-on {{Bayesian Neural Networks}} -- a {{Tutorial}} for {{Deep Learning Users}}},
  author = {Jospin, Laurent Valentin and Buntine, Wray and Boussaid, Farid and Laga, Hamid and Bennamoun, Mohammed},
  year = {2020},
  month = jul,
  abstract = {Modern deep learning methods have equipped researchers and engineers with incredibly powerful tools to tackle problems that previously seemed impossible. However, since deep learning methods operate as black boxes, the uncertainty associated with their predictions is often challenging to quantify. Bayesian statistics offer a formalism to understand and quantify the uncertainty associated with deep neural networks predictions. This paper provides a tutorial for researchers and scientists who are using machine learning, especially deep learning, with an overview of the relevant literature and a complete toolset to design, implement, train, use and evaluate Bayesian neural networks. CCS Concepts: \textbullet{} Mathematics of computing \textrightarrow{} Probability and statistics; \textbullet{} Computing methodologies \textrightarrow{} Neural networks; Bayesian network models; Ensemble methods; Regularization.},
  archivePrefix = {arXiv},
  eprint = {2007.06823},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jospin et al_2020_hands-on bayesian neural networks -- a tutorial for deep learning users.pdf},
  journal = {arXiv:2007.06823 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jouppi17_InDatacenterPerformanceAnalysis,
  title = {In-{{Datacenter Performance Analysis}} of a {{Tensor Processing Unit}}},
  author = {Jouppi, Norman P. and Young, Cliff and Patil, Nishant and Patterson, David and Agrawal, Gaurav and Bajwa, Raminder and Bates, Sarah and Bhatia, Suresh and Boden, Nan and Borchers, Al and Boyle, Rick and Cantin, Pierre-luc and Chao, Clifford and Clark, Chris and Coriell, Jeremy and Daley, Mike and Dau, Matt and Dean, Jeffrey and Gelb, Ben and Ghaemmaghami, Tara Vazir and Gottipati, Rajendra and Gulland, William and Hagmann, Robert and Ho, C. Richard and Hogberg, Doug and Hu, John and Hundt, Robert and Hurt, Dan and Ibarz, Julian and Jaffey, Aaron and Jaworski, Alek and Kaplan, Alexander and Khaitan, Harshit and Koch, Andy and Kumar, Naveen and Lacy, Steve and Laudon, James and Law, James and Le, Diemthu and Leary, Chris and Liu, Zhuyuan and Lucke, Kyle and Lundin, Alan and MacKean, Gordon and Maggiore, Adriana and Mahony, Maire and Miller, Kieran and Nagarajan, Rahul and Narayanaswami, Ravi and Ni, Ray and Nix, Kathy and Norrie, Thomas and Omernick, Mark and Penukonda, Narayana and Phelps, Andy and Ross, Jonathan and Ross, Matt and Salek, Amir and Samadiani, Emad and Severn, Chris and Sizikov, Gregory and Snelham, Matthew and Souter, Jed and Steinberg, Dan and Swing, Andy and Tan, Mercedes and Thorson, Gregory and Tian, Bo and Toma, Horia and Tuttle, Erick and Vasudevan, Vijay and Walter, Richard and Wang, Walter and Wilcox, Eric and Yoon, Doe Hyun},
  year = {2017},
  month = apr,
  abstract = {Many architects believe that major improvements in cost-energy-performance must now come from domain-specific hardware. This paper evaluates a custom ASIC---called a Tensor Processing Unit (TPU)---deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile response-time requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs (caches, out-of-order execution, multithreading, multiprocessing, prefetching, ...) that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an Nvidia K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95\% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X - 30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X - 80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU.},
  archivePrefix = {arXiv},
  eprint = {1704.04760},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/jouppi et al_2017_in-datacenter performance analysis of a tensor processing unit.pdf},
  journal = {arXiv:1704.04760 [cs]},
  primaryClass = {cs}
}

@article{joy20_RethinkingSemiSupervisedLearning,
  title = {Rethinking {{Semi}}-{{Supervised Learning}} in {{VAEs}}},
  author = {Joy, Tom and Schmon, Sebastian M. and Torr, Philip H. S. and Siddharth, N. and Rainforth, Tom},
  year = {2020},
  month = jun,
  abstract = {We present an alternative approach to semi-supervision in variational autoencoders (VAEs) that incorporates labels through auxiliary variables rather than directly through the latent variables. Prior work has generally conflated the meaning of labels, i.e. the associated characteristics of interest, with the actual label values themselves\textemdash learning latent variables that directly correspond to the label values. We argue that to learn meaningful representations, semi-supervision should instead try to capture these richer characteristics and that the construction of latent variables as label values is not just unnecessary, but actively harmful. To this end, we develop a novel VAE model, the reparameterized VAE (REVAE), which ``reparameterizes'' supervision through auxiliary variables and a concomitant variational objective. Through judicious structuring of mappings between latent and auxiliary variables, we show that the REVAE can effectively learn meaningful representations of data. In particular, we demonstrate that the REVAE is able to match, and even improve on the classification accuracy of previous approaches, but more importantly, it also allows for more effective and more general interventions to be performed. We include a demo of REVAE at https://github.com/thwjoy/revae-demo.},
  archivePrefix = {arXiv},
  eprint = {2006.10102},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/joy et al_2020_rethinking semi-supervised learning in vaes.pdf},
  journal = {arXiv:2006.10102 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{jun00_DistributionAugmentationGenerative,
  title = {Distribution {{Augmentation}} for {{Generative Modeling}}},
  author = {Jun, Heewoo and Child, Rewon and Chen, Mark and Schulman, John and Ramesh, Aditya and Radford, Alec and Sutskever, Ilya},
  pages = {14},
  abstract = {We present distribution augmentation (DistAug), a simple and powerful method of regularizing generative models. Our approach applies augmentation functions to data and, importantly, conditions the generative model on the specific function used. Unlike typical data augmentation, DistAug allows usage of functions which modify the target density, enabling aggressive augmentations more commonly seen in supervised and self-supervised learning. We demonstrate this is a more effective regularizer than standard methods, and use it to train a 152M parameter autoregressive model on CIFAR-10 to 2.56 bits per dim (relative to the state-of-the-art 2.80). Samples from this model attain FID 12.75 and IS 8.40, outperforming the majority of GANs. We further demonstrate the technique is broadly applicable across model architectures and problem domains.},
  file = {/home/trung/GoogleDrive/Zotero/jun et al_distribution augmentation for generative modeling.pdf},
  keywords = {_tablet,favorite},
  language = {en}
}

@article{jung17_Comparisonefficiencyelastic,
  title = {Comparison of Efficiency of Elastic and Non-Elastic Taping on Induced Quadriceps Fatigue by Knee Extension Exercise},
  author = {Jung, Hyun-jun and Lee, Ju-young and Hwang, Jin-kyeong and Choi, Bo-ram},
  year = {2017},
  month = dec,
  volume = {29},
  pages = {2199--2200},
  issn = {0915-5287},
  doi = {10.1589/jpts.29.2199},
  abstract = {[Purpose] The purpose of this study is to examine the effects of elastic and non-elastic taping on induced quadriceps fatigue by knee extension exercise. [Subjects and Methods] Thirty healthy females were randomly assigned to an elastic tape group (ET, n=10), a non-elastic tape group (NET, n=10), or a no tape group (NT, n=10). Taping groups attached taping on quadriceps femoris. Three groups are conducted knee extension exercise for 10 times and 3 sets during 10 minutes. Knee extension peak torque measured before and after knee extension exercise. One-way analysis of variance was used to assess the statistical significance of knee extension peak torque. [Results] No significant difference in changes in maximum muscle strength per unit weight from before to after the muscle fatigue exercise was found in the ET, NET, or NT groups (12.1 {$\pm$} 4.2, 11.7 {$\pm$} 7.1, and 6.7 {$\pm$} 3.4 N/kg, respectively). [Conclusion] Taping facilitated muscle performance, but it also increased susceptibility to fatigue resulting from greater muscle activation.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/jung et al_2017_comparison of efficiency of elastic and non-elastic taping on induced quadriceps fatigue by knee extension exercise.pdf},
  journal = {Journal of Physical Therapy Science},
  number = {12},
  pmcid = {PMC5890230},
  pmid = {29643604}
}

@misc{jurgen00_Critique2018Turing,
  title = {Critique of 2018 {{Turing Award}} for {{Drs}}. {{Bengio}} \& {{Hinton}} \& {{LeCun}}},
  author = {J{\"u}rgen, Schmidhuber},
  howpublished = {http://people.idsia.ch/\textasciitilde juergen/critique-turing-award-bengio-hinton-lecun.html}
}

@misc{jurgen10_2010Breakthroughsupervised,
  title = {2010: {{Breakthrough}} of Supervised Deep Learning. {{No}} Unsupervised Pre-Training. {{The}} Rest Is History.},
  author = {J{\"u}rgen, Schmidhuber},
  year = {2010},
  howpublished = {http://people.idsia.ch/\textasciitilde juergen/2010-breakthrough-supervised-deep-learning.html}
}

@book{kadane11_PrinciplesUncertainty,
  title = {The {{Principles Of Uncertainty}}},
  author = {Kadane, B. Joseph},
  year = {2011},
  file = {/home/trung/GoogleDrive/Zotero/kadane_2011_the principles of uncertainty.pdf}
}

@incollection{kaiser16_CanActiveMemory,
  title = {Can {{Active Memory Replace Attention}}?},
  booktitle = {Advances in {{Neural Information Processing Systems}} 29},
  author = {Kaiser, {\L}ukasz and Bengio, Samy},
  editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
  year = {2016},
  pages = {3781--3789},
  publisher = {{Curran Associates, Inc.}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/kaiser et al_2016_can active memory replace attention.pdf;/home/trung/Zotero/storage/LLXWCZDL/6295-can-active-memory-replace-attention.html}
}

@article{kaiser17_OneModelLearn,
  title = {One {{Model To Learn Them All}}},
  author = {Kaiser, Lukasz and Gomez, Aidan N. and Shazeer, Noam and Vaswani, Ashish and Parmar, Niki and Jones, Llion and Uszkoreit, Jakob},
  year = {2017},
  month = jun,
  abstract = {Deep learning yields great results across many fields, from speech recognition, image classification, to translation. But for each problem, getting a deep model to work well involves research into the architecture and a long period of tuning. We present a single model that yields good results on a number of problems spanning multiple domains. In particular, this single model is trained concurrently on ImageNet, multiple translation tasks, image captioning (COCO dataset), a speech recognition corpus, and an English parsing task. Our model architecture incorporates building blocks from multiple domains. It contains convolutional layers, an attention mechanism, and sparsely-gated layers. Each of these computational blocks is crucial for a subset of the tasks we train on. Interestingly, even if a block is not crucial for a task, we observe that adding it never hurts performance and in most cases improves it on all tasks. We also show that tasks with less data benefit largely from joint training with other tasks, while performance on large tasks degrades only slightly if at all.},
  annotation = {ZSCC: 0000129},
  archivePrefix = {arXiv},
  eprint = {1706.05137},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kaiser et al_2017_one model to learn them all.pdf;/home/trung/Zotero/storage/DFGFFIZD/1706.html},
  journal = {arXiv:1706.05137 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kaiser18_DiscreteAutoencodersSequence,
  title = {Discrete {{Autoencoders}} for {{Sequence Models}}},
  author = {Kaiser, {\L}ukasz and Bengio, Samy},
  year = {2018},
  month = jan,
  abstract = {Recurrent models for sequences have been recently successful at many tasks, especially for language modeling and machine translation. Nevertheless, it remains challenging to extract good representations from these models. For instance, even though language has a clear hierarchical structure going from characters through words to sentences, it is not apparent in current language models. We propose to improve the representation in sequence models by augmenting current approaches with an autoencoder that is forced to compress the sequence through an intermediate discrete latent space. In order to propagate gradients though this discrete representation we introduce an improved semantic hashing technique. We show that this technique performs well on a newly proposed quantitative efficiency measure. We also analyze latent codes produced by the model showing how they correspond to words and phrases. Finally, we present an application of the autoencoder-augmented model to generating diverse translations.},
  annotation = {ZSCC: 0000013},
  archivePrefix = {arXiv},
  eprint = {1801.09797},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kaiser et al_2018_discrete autoencoders for sequence models.pdf;/home/trung/Zotero/storage/P3Q2XA7L/1801.html},
  journal = {arXiv:1801.09797 [cs, stat]},
  keywords = {autoencoder,Computer Science - Machine Learning,discrete,sequence,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kaiser18_FastDecodingSequence,
  title = {Fast {{Decoding}} in {{Sequence Models}} Using {{Discrete Latent Variables}}},
  author = {Kaiser, {\L}ukasz and Roy, Aurko and Vaswani, Ashish and Parmar, Niki and Bengio, Samy and Uszkoreit, Jakob and Shazeer, Noam},
  year = {2018},
  month = jun,
  abstract = {Autoregressive sequence models based on deep neural networks, such as RNNs, Wavenet and the Transformer attain state-of-the-art results on many tasks. However, they are difficult to parallelize and are thus slow at processing long sequences. RNNs lack parallelism both during training and decoding, while architectures like WaveNet and Transformer are much more parallelizable during training, yet still operate sequentially during decoding. Inspired by [arxiv:1711.00937], we present a method to extend sequence models using discrete latent variables that makes decoding much more parallelizable. We first auto-encode the target sequence into a shorter sequence of discrete latent variables, which at inference time is generated autoregressively, and finally decode the output sequence from this shorter latent sequence in parallel. To this end, we introduce a novel method for constructing a sequence of discrete latent variables and compare it with previously introduced methods. Finally, we evaluate our model end-to-end on the task of neural machine translation, where it is an order of magnitude faster at decoding than comparable autoregressive models. While lower in BLEU than purely autoregressive models, our model achieves higher scores than previously proposed non-autoregressive translation models.},
  annotation = {ZSCC: 0000013},
  archivePrefix = {arXiv},
  eprint = {1803.03382},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kaiser et al_2018_fast decoding in sequence models using discrete latent variables.pdf;/home/trung/Zotero/storage/92E4FLGD/1803.html},
  journal = {arXiv:1803.03382 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{kalibhat20_WinningLotteryTickets,
  title = {Winning {{Lottery Tickets}} in {{Deep Generative Models}}},
  author = {Kalibhat, Neha Mukund and Balaji, Yogesh and Feizi, Soheil},
  year = {2020},
  month = oct,
  abstract = {The lottery ticket hypothesis suggests that sparse, sub-networks of a given neural network, if initialized properly, can be trained to reach comparable or even better performance to that of the original network. Prior works in lottery tickets have primarily focused on the supervised learning setup, with several papers proposing effective ways of finding "winning tickets" in classification problems. In this paper, we confirm the existence of winning tickets in deep generative models such as GANs and VAEs. We show that the popular iterative magnitude pruning approach (with late rewinding) can be used with generative losses to find the winning tickets. This approach effectively yields tickets with sparsity up to 99\% for AutoEncoders, 93\% for VAEs and 89\% for GANs on CIFAR and Celeb-A datasets. We also demonstrate the transferability of winning tickets across different generative models (GANs and VAEs) sharing the same architecture, suggesting that winning tickets have inductive biases that could help train a wide range of deep generative models. Furthermore, we show the practical benefits of lottery tickets in generative models by detecting tickets at very early stages in training called "early-bird tickets". Through early-bird tickets, we can achieve up to 88\% reduction in floating-point operations (FLOPs) and 54\% reduction in training time, making it possible to train large-scale generative models over tight resource constraints. These results out-perform existing early pruning methods like SNIP (Lee, Ajanthan, and Torr 2019) and GraSP (Wang, Zhang, and Grosse 2020). Our findings shed light towards existence of proper network initializations that could improve convergence and stability of generative models.},
  archivePrefix = {arXiv},
  eprint = {2010.02350},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kalibhat et al_2020_winning lottery tickets in deep generative models.pdf},
  journal = {arXiv:2010.02350 [cs]},
  keywords = {_tablet},
  primaryClass = {cs}
}

@article{kalman12_Influencemethylsulfonylmethanemarkers,
  title = {Influence of Methylsulfonylmethane on Markers of Exercise Recovery and Performance in Healthy Men: A Pilot Study},
  shorttitle = {Influence of Methylsulfonylmethane on Markers of Exercise Recovery and Performance in Healthy Men},
  author = {Kalman, Douglas S and Feldman, Samantha and Scheinberg, Andrew R and Krieger, Diane R and Bloomer, Richard J},
  year = {2012},
  month = sep,
  volume = {9},
  pages = {46},
  issn = {1550-2783},
  doi = {10.1186/1550-2783-9-46},
  abstract = {Background Methylsulfonylmethane (MSM) has been reported to provide anti-inflammatory and antioxidant effects in both animal and man. Strenuous resistance exercise has the potential to induce both inflammation and oxidative stress. Using a pilot (proof of concept) study design, we determined the influence of MSM on markers of exercise recovery and performance in healthy men. Methods Eight, healthy men (27.1\,{$\pm$}\,6.9 yrs old) who were considered to be moderately exercise-trained (exercising {$<$}150 minutes per week) were randomly assigned to ingest MSM at either 1.5 grams per day or 3.0 grams per day for 30 days (28 days before and 2 days following exercise). Before and after the 28 day intervention period, subjects performed 18 sets of knee extension exercise in an attempt to induce muscle damage (and to be used partly as a measure of exercise performance). Sets 1\textendash 15 were performed at a predetermined weight for 10 repetitions each, while sets 16\textendash 18 were performed to muscular failure. Muscle soreness (using a 5-point Likert scale), fatigue (using the fatigue-inertia subset of the Profile of Mood States), blood antioxidant status (glutathione and Trolox Equivalent Antioxidant Capacity [TEAC]), and blood homocysteine were measured before and after exercise, pre and post intervention. Exercise performance (total work performed during sets 16\textendash 18 of knee extension testing) was also measured pre and post intervention. Results Muscle soreness increased following exercise and a trend was noted for a reduction in muscle soreness with 3.0 grams versus 1.5 grams of MSM (p\,=\,0.080), with a 1.0 point difference between dosages. Fatigue was slightly reduced with MSM (p\,=\,0.073 with 3.0 grams; p\,=\,0.087 for both dosages combined). TEAC increased significantly following exercise with 3.0 grams of MSM (p\,=\,0.035), while homocysteine decreased following exercise for both dosages combined (p\,=\,0.007). No significant effects were noted for glutathione or total work performed during knee extension testing (p\,{$>$}\,0.05). Conclusion MSM, especially when provided at 3.0 grams per day, may favorably influence selected markers of exercise recovery. More work is needed to extend these findings, in particular using a larger sample of subjects and the inclusion of additional markers of exercise recovery and performance.},
  annotation = {ZSCC: 0000030},
  file = {/home/trung/GoogleDrive/Zotero/kalman et al_2012_influence of methylsulfonylmethane on markers of exercise recovery and performance in healthy men.pdf},
  journal = {Journal of the International Society of Sports Nutrition},
  pmcid = {PMC3507661},
  pmid = {23013531}
}

@misc{kan20_enochkanawesomegansanddeepfakes,
  title = {Enochkan/Awesome-Gans-and-Deepfakes},
  author = {Kan, Enoch},
  year = {2020},
  month = nov,
  abstract = {A curated list of GAN \& Deepfake papers and repositories.}
}

@article{kapoor20_VariationalAutoRegressiveGaussian,
  title = {Variational {{Auto}}-{{Regressive Gaussian Processes}} for {{Continual Learning}}},
  author = {Kapoor, Sanyam and Karaletsos, Theofanis and Bui, Thang D.},
  year = {2020},
  month = jun,
  abstract = {This paper proposes Variational Auto-Regressive Gaussian Process (VAR-GP), a principled Bayesian updating mechanism to incorporate new data for sequential tasks in the context of continual learning. It relies on a novel auto-regressive characterization of the variational distribution and inference is made scalable using sparse inducing point approximations. Experiments on standard continual learning benchmarks demonstrate the ability of VAR-GPs to perform well at new tasks without compromising performance on old ones, yielding competitive results to state-of-the-art methods. In addition, we qualitatively show how VAR-GP improves the predictive entropy estimates as we train on new tasks. Further, we conduct a thorough ablation study to verify the effectiveness of inferential choices.},
  archivePrefix = {arXiv},
  eprint = {2006.05468},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kapoor et al_2020_variational auto-regressive gaussian processes for continual learning.pdf},
  journal = {arXiv:2006.05468 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{karaletsos00_ProbabilisticMetaRepresentationsNeural,
  title = {Probabilistic {{Meta}}-{{Representations Of Neural Networks}}},
  author = {Karaletsos, Theofanis and Dayan, Peter and Ghahramani, Zoubin},
  pages = {8},
  file = {/home/trung/GoogleDrive/Zotero/karaletsos et al_probabilistic meta-representations of neural networks.pdf},
  language = {en}
}

@misc{karami20_ahkaramiDeepLearninginProduction,
  title = {Ahkarami/{{Deep}}-{{Learning}}-in-{{Production}}},
  author = {Karami, Amir Hossein},
  year = {2020},
  month = dec,
  abstract = {In this repository, I will share some useful notes and references about deploying deep learning-based models in production.}
}

@article{karim20_DeepCOVIDExplainerExplainableCOVID19,
  title = {{{DeepCOVIDExplainer}}: {{Explainable COVID}}-19 {{Diagnosis Based}} on {{Chest X}}-Ray {{Images}}},
  shorttitle = {{{DeepCOVIDExplainer}}},
  author = {Karim, Md Rezaul and D{\"o}hmen, Till and {Rebholz-Schuhmann}, Dietrich and Decker, Stefan and Cochez, Michael and Beyan, Oya},
  year = {2020},
  month = jun,
  abstract = {Amid the coronavirus disease(COVID-19) pandemic, humanity experiences a rapid increase in infection numbers across the world. Challenge hospitals are faced with, in the fight against the virus, is the effective screening of incoming patients. One methodology is the assessment of chest radiography(CXR) images, which usually requires expert radiologist's knowledge. In this paper, we propose an explainable deep neural networks(DNN)-based method for automatic detection of COVID-19 symptoms from CXR images, which we call DeepCOVIDExplainer. We used 15,959 CXR images of 15,854 patients, covering normal, pneumonia, and COVID-19 cases. CXR images are first comprehensively preprocessed, before being augmented and classified with a neural ensemble method, followed by highlighting class-discriminating regions using gradient-guided class activation maps(Grad-CAM++) and layer-wise relevance propagation(LRP). Further, we provide human-interpretable explanations of the predictions. Evaluation results based on hold-out data show that our approach can identify COVID-19 confidently with a positive predictive value(PPV) of 91.6\%, 92.45\%, and 96.12\%; precision, recall, and F1 score of 94.6\%, 94.3\%, and 94.6\%, respectively for normal, pneumonia, and COVID-19 cases, respectively, making it comparable or improved results over recent approaches. We hope that our findings will be a useful contribution to the fight against COVID-19 and, in more general, towards an increasing acceptance and adoption of AI-assisted applications in the clinical practice.},
  archivePrefix = {arXiv},
  eprint = {2004.04582},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/karim et al_2020_deepcovidexplainer.pdf},
  journal = {arXiv:2004.04582 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{karimi20_surveyalgorithmicrecourse,
  title = {A Survey of Algorithmic Recourse: Definitions, Formulations, Solutions, and Prospects},
  shorttitle = {A Survey of Algorithmic Recourse},
  author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch{\"o}lkopf, Bernhard and Valera, Isabel},
  year = {2020},
  month = oct,
  abstract = {Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role for the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.},
  archivePrefix = {arXiv},
  eprint = {2010.04050},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/karimi et al_2020_a survey of algorithmic recourse.pdf},
  journal = {arXiv:2010.04050 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{karpen17_StatisticalMachineLearning,
  title = {Statistical {{Machine Learning}}},
  author = {Karpen, Bert and Classen, Tom},
  year = {2017},
  file = {/home/trung/GoogleDrive/Zotero/karpen et al_2017_statistical machine learning.pdf},
  howpublished = {http://www.snn.ru.nl/\textasciitilde bertk/sml/SML17\_slides.pdf}
}

@article{karras19_AnalyzingImprovingImage,
  title = {Analyzing and {{Improving}} the {{Image Quality}} of {{StyleGAN}}},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2019},
  month = dec,
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  archivePrefix = {arXiv},
  eprint = {1912.04958},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/karras et al_2019_analyzing and improving the image quality of stylegan.pdf;/home/trung/GoogleDrive/Zotero/karras et al_2019_analyzing and improving the image quality of stylegan2.pdf;/home/trung/Zotero/storage/QDC98QLN/1912.html},
  journal = {arXiv:1912.04958 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{karras19_AnalyzingImprovingImagea,
  title = {Analyzing and {{Improving}} the {{Image Quality}} of {{StyleGAN}}},
  author = {Karras, Tero and Laine, Samuli and Aittala, Miika and Hellsten, Janne and Lehtinen, Jaakko and Aila, Timo},
  year = {2019},
  month = dec,
  abstract = {The style-based GAN architecture (StyleGAN) yields state-of-the-art results in data-driven unconditional generative image modeling. We expose and analyze several of its characteristic artifacts, and propose changes in both model architecture and training methods to address them. In particular, we redesign generator normalization, revisit progressive growing, and regularize the generator to encourage good conditioning in the mapping from latent vectors to images. In addition to improving image quality, this path length regularizer yields the additional benefit that the generator becomes significantly easier to invert. This makes it possible to reliably detect if an image is generated by a particular network. We furthermore visualize how well the generator utilizes its output resolution, and identify a capacity problem, motivating us to train larger models for additional quality improvements. Overall, our improved model redefines the state of the art in unconditional image modeling, both in terms of existing distribution quality metrics as well as perceived image quality.},
  annotation = {ZSCC: 0000007},
  archivePrefix = {arXiv},
  eprint = {1912.04958},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/karras et al_2019_analyzing and improving the image quality of stylegan3.pdf;/home/trung/Zotero/storage/IYB9LJL3/1912.html},
  journal = {arXiv:1912.04958 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{kashima20_Singlecellsequencingtechniques,
  title = {Single-Cell Sequencing Techniques from Individual to Multiomics Analyses},
  author = {Kashima, Yukie and Sakamoto, Yoshitaka and Kaneko, Keiya and Seki, Masahide and Suzuki, Yutaka and Suzuki, Ayako},
  year = {2020},
  month = sep,
  issn = {2092-6413},
  doi = {10.1038/s12276-020-00499-2},
  abstract = {Here, we review single-cell sequencing techniques for individual and multiomics profiling in single cells. We mainly describe single-cell genomic, epigenomic, and transcriptomic methods, and examples of their applications. For the integration of multilayered data sets, such as the transcriptome data derived from single-cell RNA sequencing and chromatin accessibility data derived from single-cell ATAC-seq, there are several computational integration methods. We also describe single-cell experimental methods for the simultaneous measurement of two or more omics layers. We can achieve a detailed understanding of the basic molecular profiles and those associated with disease in each cell by utilizing a large number of single-cell sequencing techniques and the accumulated data sets.},
  file = {/home/trung/GoogleDrive/Zotero/kashima et al_2020_single-cell sequencing techniques from individual to multiomics analyses.pdf},
  journal = {Experimental \& Molecular Medicine}
}

@article{kato20_RateDistortionOptimizationGuided,
  title = {Rate-{{Distortion Optimization Guided Autoencoder}} for {{Isometric Embedding}} in {{Euclidean Latent Space}}},
  author = {Kato, Keizo and Zhou, Jing and Sasaki, Tomotake and Nakagawa, Akira},
  year = {2020},
  month = aug,
  abstract = {In the generative model approach of machine learning, it is essential to acquire an accurate probabilistic model and compress the dimension of data for easy treatment. However, in the conventional deep-autoencoder based generative model such as VAE, the probability of the real space cannot be obtained correctly from that of in the latent space, because the scaling between both spaces is not controlled. This has also been an obstacle to quantifying the impact of the variation of latent variables on data. In this paper, we propose Rate-Distortion Optimization guided autoencoder, in which the Jacobi matrix from real space to latent space has orthonormality. It is proved theoretically and experimentally that (i) the probability distribution of the latent space obtained by this model is proportional to the probability distribution of the real space because Jacobian between two spaces is constant; (ii) our model behaves as non-linear PCA, where energy of acquired latent space is concentrated on several principal components and the influence of each component can be evaluated quantitatively. Furthermore, to verify the usefulness on the practical application, we evaluate its performance in unsupervised anomaly detection and it outperforms current state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {1910.04329},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kato et al_2020_rate-distortion optimization guided autoencoder for isometric embedding in euclidean latent space.pdf},
  journal = {arXiv:1910.04329 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kawaguchi19_GeneralizationDeepLearning,
  title = {Generalization in {{Deep Learning}}},
  author = {Kawaguchi, Kenji and Kaelbling, Leslie Pack and Bengio, Yoshua},
  year = {2019},
  month = may,
  abstract = {This paper provides non-vacuous and numerically-tight generalization guarantees for deep learning, as well as theoretical insights into why and how deep learning can generalize well, despite its large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima, responding to an open question in the literature. We also propose new open problems and discuss the limitations of our results.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1710.05468},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kawaguchi et al_2019_generalization in deep learning.pdf;/home/trung/Zotero/storage/P6Q25QD2/1710.html},
  journal = {arXiv:1710.05468 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kaya19_DeepMetricLearning,
  title = {Deep {{Metric Learning}}: {{A Survey}}},
  shorttitle = {Deep {{Metric Learning}}},
  author = {{Kaya} and {Bilge}},
  year = {2019},
  month = aug,
  volume = {11},
  pages = {1066},
  issn = {2073-8994},
  doi = {10.3390/sym11091066},
  abstract = {Metric learning aims to measure the similarity among samples while using an optimal distance metric for learning tasks. Metric learning methods, which generally use a linear projection, are limited in solving real-world problems demonstrating non-linear characteristics. Kernel approaches are utilized in metric learning to address this problem. In recent years, deep metric learning, which provides a better solution for nonlinear data through activation functions, has attracted researchers' attention in many different areas. This article aims to reveal the importance of deep metric learning and the problems dealt with in this field in the light of recent studies. As far as the research conducted in this field are concerned, most existing studies that are inspired by Siamese and Triplet networks are commonly used to correlate among samples while using shared weights in deep metric learning. The success of these networks is based on their capacity to understand the similarity relationship among samples. Moreover, sampling strategy, appropriate distance metric, and the structure of the network are the challenging factors for researchers to improve the performance of the network model. This article is considered to be important, as it is the first comprehensive study in which these factors are systematically analyzed and evaluated as a whole and supported by comparing the quantitative results of the methods.},
  file = {/home/trung/GoogleDrive/Zotero/kaya et al_2019_deep metric learning.pdf},
  journal = {Symmetry},
  language = {en},
  number = {9}
}

@article{ke00_SparseAttentiveBacktracking,
  title = {Sparse {{Attentive Backtracking}}: {{Temporal Credit Assignment Through Reminding}}},
  author = {Ke, Nan Rosemary and Goyal, Anirudh and Bilaniuk, Olexa and Binas, Jonathan and Mozer, Michael C and Pal, Chris and Bengio, Yoshua},
  pages = {13},
  abstract = {Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly longterm dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.},
  annotation = {ZSCC: 0000014},
  file = {/home/trung/GoogleDrive/Zotero/ke et al_sparse attentive backtracking.pdf;/home/trung/GoogleDrive/Zotero/ke et al_sparse attentive backtracking2.pdf;/home/trung/Zotero/storage/4FZNCUH3/1809.html},
  keywords = {attention,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en}
}

@article{ke18_FocusedHierarchicalRNNs,
  title = {Focused {{Hierarchical RNNs}} for {{Conditional Sequence Processing}}},
  author = {Ke, Nan Rosemary and Zolna, Konrad and Sordoni, Alessandro and Lin, Zhouhan and Trischler, Adam and Bengio, Yoshua and Pineau, Joelle and Charlin, Laurent and Pal, Chris},
  year = {2018},
  month = jun,
  abstract = {Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.},
  annotation = {ZSCC: 0000012},
  archivePrefix = {arXiv},
  eprint = {1806.04342},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ke et al_2018_focused hierarchical rnns for conditional sequence processing.pdf;/home/trung/Zotero/storage/LP6WAHJW/1806.html},
  journal = {arXiv:1806.04342 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ke19_LearningNeuralCausal,
  title = {Learning {{Neural Causal Models}} from {{Unknown Interventions}}},
  author = {Ke, Nan Rosemary and Bilaniuk, Olexa and Goyal, Anirudh and Bauer, Stefan and Larochelle, Hugo and Pal, Chris and Bengio, Yoshua},
  year = {2019},
  month = oct,
  abstract = {Meta-learning over a set of distributions can be interpreted as learning different types of parameters corresponding to short-term vs long-term aspects of the mechanisms underlying the generation of data. These are respectively captured by quickly-changing parameters and slowly-changing meta-parameters. We present a new framework for meta-learning causal models where the relationship between each variable and its parents is modeled by a neural network, modulated by structural meta-parameters which capture the overall topology of a directed graphical model. Our approach avoids a discrete search over models in favour of a continuous optimization procedure. We study a setting where interventional distributions are induced as a result of a random intervention on a single unknown variable of an unknown ground truth causal model, and the observations arising after such an intervention constitute one meta-example. To disentangle the slow-changing aspects of each conditional from the fast-changing adaptations to each intervention, we parametrize the neural network into fast parameters and slow meta-parameters. We introduce a meta-learning objective that favours solutions robust to frequent but sparse interventional distribution change, and which generalize well to previously unseen interventions. Optimizing this objective is shown experimentally to recover the structure of the causal graph.},
  archivePrefix = {arXiv},
  eprint = {1910.01075},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Ke et al_2019_Learning Neural Causal Models from Unknown Interventions4.pdf;/home/trung/Zotero/storage/ZXYHEEZ3/1910.html},
  journal = {arXiv:1910.01075 [cs, stat]},
  keywords = {causal,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,intervention,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ke20_Amortizedlearningneural,
  title = {Amortized Learning of Neural Causal Representations},
  author = {Ke, Nan Rosemary and Wang, Jane X. and Mitrovic, Jovana and Szummer, Martin and Rezende, Danilo J.},
  year = {2020},
  month = aug,
  abstract = {Causal models can compactly and efficiently encode the data-generating process under all interventions and hence may generalize better under changes in distribution. These models are often represented as Bayesian networks and learning them scales poorly with the number of variables. Moreover, these approaches cannot leverage previously learned knowledge to help with learning new causal models. In order to tackle these challenges, we represent a novel algorithm called causal relational networks (CRN) for learning causal models using neural networks. The CRN represent causal models using continuous representations and hence could scale much better with the number of variables. These models also take in previously learned information to facilitate learning of new causal models. Finally, we propose a decoding-based metric to evaluate causal models with continuous representations. We test our method on synthetic data achieving high accuracy and quick adaptation to previously unseen causal models.},
  archivePrefix = {arXiv},
  eprint = {2008.09301},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ke et al_2020_amortized learning of neural causal representations.pdf},
  journal = {arXiv:2008.09301 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kearnes16_MolecularGraphConvolutions,
  title = {Molecular {{Graph Convolutions}}: {{Moving Beyond Fingerprints}}},
  shorttitle = {Molecular {{Graph Convolutions}}},
  author = {Kearnes, Steven and McCloskey, Kevin and Berndl, Marc and Pande, Vijay and Riley, Patrick},
  year = {2016},
  month = aug,
  volume = {30},
  pages = {595--608},
  issn = {0920-654X, 1573-4951},
  doi = {10.1007/s10822-016-9938-8},
  abstract = {Molecular "fingerprints" encoding structural information are the workhorse of cheminformatics and machine learning in drug discovery applications. However, fingerprint representations necessarily emphasize particular aspects of the molecular structure while ignoring others, rather than allowing the model to make data-driven decisions. We describe molecular "graph convolutions", a machine learning architecture for learning from undirected graphs, specifically small molecules. Graph convolutions use a simple encoding of the molecular graph---atoms, bonds, distances, etc.---which allows the model to take greater advantage of information in the graph structure. Although graph convolutions do not outperform all fingerprint-based methods, they (along with other graph-based methods) represent a new paradigm in ligand-based virtual screening with exciting opportunities for future improvement.},
  archivePrefix = {arXiv},
  eprint = {1603.00856},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kearnes et al_2016_molecular graph convolutions.pdf;/home/trung/Zotero/storage/7N7ZEPZ8/1603.html},
  journal = {Journal of Computer-Aided Molecular Design},
  keywords = {Computer Science - Machine Learning,graph,Statistics - Machine Learning},
  number = {8}
}

@misc{keitakurita18_OverviewNormalizationMethods,
  title = {An {{Overview}} of {{Normalization Methods}} in {{Deep Learning}}},
  author = {{keitakurita}, Author},
  year = {2018},
  month = nov,
  abstract = {Normalization in deep learning has always been a hot topic. Getting normalization right can be a crucial factor in getting your model to train effectively, but this isn't as easy as it sounds\ldots},
  file = {/home/trung/Zotero/storage/KGM7TGKR/an-overview-of-normalization-methods-in-deep-learning.html},
  journal = {Machine Learning Explained},
  keywords = {normalization},
  language = {en-US}
}

@article{keller18_FastWeightLong,
  title = {Fast {{Weight Long Short}}-{{Term Memory}}},
  author = {Keller, T. Anderson and Sridhar, Sharath Nittur and Wang, Xin},
  year = {2018},
  month = apr,
  abstract = {Associative memory using fast weights is a short-term memory mechanism that substantially improves the memory capacity and time scale of recurrent neural networks (RNNs). As recent studies introduced fast weights only to regular RNNs, it is unknown whether fast weight memory is beneficial to gated RNNs. In this work, we report a significant synergy between long short-term memory (LSTM) networks and fast weight associative memories. We show that this combination, in learning associative retrieval tasks, results in much faster training and lower test error, a performance boost most prominent at high memory task difficulties.},
  archivePrefix = {arXiv},
  eprint = {1804.06511},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/keller et al_2018_fast weight long short-term memory.pdf;/home/trung/Zotero/storage/F9BYA9LE/1804.html},
  journal = {arXiv:1804.06511 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,fast,lstm,recurrent},
  primaryClass = {cs}
}

@article{keller20_DeepArchetypalAnalysis,
  title = {Deep {{Archetypal Analysis}}},
  author = {Keller, Sebastian Mathias and Samarin, Maxim and Wieser, Mario and Roth, Volker},
  year = {2020},
  month = jan,
  abstract = {"Deep Archetypal Analysis" generates latent representations of high-dimensional datasets in terms of fractions of intuitively understandable basic entities called archetypes. The proposed method is an extension of linear "Archetypal Analysis" (AA), an unsupervised method to represent multivariate data points as sparse convex combinations of extremal elements of the dataset. Unlike the original formulation of AA, "Deep AA" can also handle side information and provides the ability for data-driven representation learning which reduces the dependence on expert knowledge. Our method is motivated by studies of evolutionary trade-offs in biology where archetypes are species highly adapted to a single task. Along these lines, we demonstrate that "Deep AA" also lends itself to the supervised exploration of chemical space, marking a distinct starting point for de novo molecular design. In the unsupervised setting we show how "Deep AA" is used on CelebA to identify archetypal faces. These can then be superimposed in order to generate new faces which inherit dominant traits of the archetypes they are based on.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1901.10799},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/keller et al_2020_deep archetypal analysis.pdf;/home/trung/Zotero/storage/JWDQBDVY/1901.html},
  journal = {arXiv:1901.10799 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{kendall18_MultiTaskLearningUsing,
  title = {Multi-{{Task Learning Using Uncertainty}} to {{Weigh Losses}} for {{Scene Geometry}} and {{Semantics}}},
  author = {Kendall, Alex and Gal, Yarin and Cipolla, Roberto},
  year = {2018},
  month = apr,
  abstract = {Numerous deep learning applications benefit from multi-task learning with multiple regression and classification objectives. In this paper we make the observation that the performance of such systems is strongly dependent on the relative weighting between each task's loss. Tuning these weights by hand is a difficult and expensive process, making multi-task learning prohibitive in practice. We propose a principled approach to multi-task deep learning which weighs multiple loss functions by considering the homoscedastic uncertainty of each task. This allows us to simultaneously learn various quantities with different units or scales in both classification and regression settings. We demonstrate our model learning per-pixel depth regression, semantic and instance segmentation from a monocular input image. Perhaps surprisingly, we show our model can learn multi-task weightings and outperform separate models trained individually on each task.},
  archivePrefix = {arXiv},
  eprint = {1705.07115},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kendall et al_2018_multi-task learning using uncertainty to weigh losses for scene geometry and semantics.pdf},
  journal = {arXiv:1705.07115 [cs]},
  keywords = {favorite},
  primaryClass = {cs}
}

@misc{keng18_HyperbolicGeometryPoincare,
  title = {Hyperbolic {{Geometry}} and {{Poincar\'e Embeddings}}},
  author = {Keng, Brian},
  year = {2018},
  month = jun,
  abstract = {An introduction to models of hyperbolic geometry and its application to Poincar\'e embeddings.},
  howpublished = {http://bjlkeng.github.io/posts/hyperbolic-geometry-and-poincare-embeddings/},
  journal = {Bounded Rationality},
  language = {en}
}

@misc{keng18_ManifoldsGentleIntroduction,
  title = {Manifolds: {{A Gentle Introduction}}},
  shorttitle = {Manifolds},
  author = {Keng, Brian},
  year = {2018},
  month = apr,
  abstract = {A quick introduction to manifolds.},
  howpublished = {http://bjlkeng.github.io/posts/manifolds/},
  journal = {Bounded Rationality},
  language = {en}
}

@misc{keng19_ImportanceSamplingEstimating,
  title = {Importance {{Sampling}} and {{Estimating Marginal Likelihood}} in {{Variational}}},
  author = {Keng, Brian},
  year = {2019},
  month = feb,
  abstract = {A short post describing how to use importance sampling to estimate marginal likelihood in variational autoencoders.},
  howpublished = {http://bjlkeng.github.io/posts/importance-sampling-and-estimating-marginal-likelihood-in-variational-autoencoders/},
  journal = {Bounded Rationality},
  language = {en}
}

@misc{keng19_NoteUsingLogLikelihood,
  title = {A {{Note}} on {{Using Log}}-{{Likelihood}} for {{Generative Models}}},
  author = {Keng, Brian},
  year = {2019},
  month = aug,
  abstract = {A short exploration on the theory behind using log-likelihood to train and measure generative models using image-like data.},
  howpublished = {http://bjlkeng.github.io/posts/a-note-on-using-log-likelihood-for-generative-models/},
  journal = {Bounded Rationality},
  language = {en}
}

@article{kerg20_Untanglingtradeoffsrecurrence,
  title = {Untangling Tradeoffs between Recurrence and Self-Attention in Neural Networks},
  author = {Kerg, Giancarlo and Kanuparthi, Bhargav and Goyal, Anirudh and Goyette, Kyle and Bengio, Yoshua and Lajoie, Guillaume},
  year = {2020},
  month = jun,
  abstract = {Attention and self-attention mechanisms, inspired by cognitive processes, are now central to state-of-the-art deep learning on sequential tasks. However, most recent progress hinges on heuristic approaches with limited understanding of attention's role in model optimization and computation, and rely on considerable memory and computational resources that scale poorly. In this work, we present a formal analysis of how self-attention affects gradient propagation in recurrent networks, and prove that it mitigates the problem of vanishing gradients when trying to capture long-term dependencies. Building on these results, we propose a relevancy screening mechanism, inspired by the cognitive process of memory consolidation, that allows for a scalable use of sparse self-attention with recurrence. While providing guarantees to avoid vanishing gradients, we use simple numerical experiments to demonstrate the tradeoffs in performance and computational resources by efficiently balancing attention and recurrence. Based on our results, we propose a concrete direction of research to improve scalability of attentive networks.},
  archivePrefix = {arXiv},
  eprint = {2006.09471},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kerg et al_2020_untangling tradeoffs between recurrence and self-attention in neural networks.pdf},
  journal = {arXiv:2006.09471 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{keskar17_LargeBatchTrainingDeep,
  title = {On {{Large}}-{{Batch Training}} for {{Deep Learning}}: {{Generalization Gap}} and {{Sharp Minima}}},
  shorttitle = {On {{Large}}-{{Batch Training}} for {{Deep Learning}}},
  author = {Keskar, Nitish Shirish and Mudigere, Dheevatsa and Nocedal, Jorge and Smelyanskiy, Mikhail and Tang, Ping Tak Peter},
  year = {2017},
  month = feb,
  abstract = {The stochastic gradient descent (SGD) method and its variants are algorithms of choice for many Deep Learning tasks. These methods operate in a small-batch regime wherein a fraction of the training data, say \$32\$-\$512\$ data points, is sampled to compute an approximation to the gradient. It has been observed in practice that when using a larger batch there is a degradation in the quality of the model, as measured by its ability to generalize. We investigate the cause for this generalization drop in the large-batch regime and present numerical evidence that supports the view that large-batch methods tend to converge to sharp minimizers of the training and testing functions - and as is well known, sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation. We discuss several strategies to attempt to help large-batch methods eliminate this generalization gap.},
  archivePrefix = {arXiv},
  eprint = {1609.04836},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/keskar et al_2017_on large-batch training for deep learning.pdf},
  journal = {arXiv:1609.04836 [cs, math]},
  keywords = {_tablet,favorite},
  primaryClass = {cs, math}
}

@article{kessler00_IndianBuffetNeural,
  title = {Indian {{Buffet Neural Networks}} for {{Continual Learning}}},
  author = {Kessler, Samuel and Nguyen, Vu and Zohren, Stefan and Roberts, Stephen},
  pages = {14},
  abstract = {We place an Indian Buffet Process (IBP) prior over the neural structure of a Bayesian Neural Network (BNN), thus allowing the complexity of the BNN to increase and decrease automatically. We apply this methodology to the problem of resource allocation in continual learning, where new tasks occur and the network requires extra resources. Our BNN exploits online variational inference with relaxations to the Bernoulli and Beta distributions (which constitute the IBP prior), so allowing the use of the reparameterisation trick to learn variational posteriors via gradient-based methods. As we automatically learn the number of weights in the BNN, overfitting and underfitting problems are largely overcome. We show empirically that the method offers competitive results compared to Variational Continual Learning (VCL) in some settings.},
  file = {/home/trung/GoogleDrive/Zotero/kessler et al_indian buffet neural networks for continual learning.pdf},
  language = {en}
}

@article{kessler20_HierarchicalIndianBuffet,
  title = {Hierarchical {{Indian Buffet Neural Networks}} for {{Bayesian Continual Learning}}},
  author = {Kessler, Samuel and Nguyen, Vu and Zohren, Stefan and Roberts, Stephen},
  year = {2020},
  month = feb,
  abstract = {We place an Indian Buffet process (IBP) prior over the structure of a Bayesian Neural Network (BNN), thus allowing the complexity of the BNN to increase and decrease automatically. We further extend this model such that the prior on the structure of each hidden layer is shared globally across all layers, using a Hierarchical-IBP (H-IBP). We apply this model to the problem of resource allocation in Continual Learning (CL) where new tasks occur and the network requires extra resources. Our model uses online variational inference with reparameterisation of the Bernoulli and Beta distributions which constitute the IBP and H-IBP priors. As we automatically learn the number of weights in each layer of the BNN, overfitting and underfitting problems are largely overcome. We show empirically that our approach offers a competitive edge over existing methods in CL.},
  archivePrefix = {arXiv},
  eprint = {1912.02290},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kessler et al_2020_hierarchical indian buffet neural networks for bayesian continual learning.pdf},
  journal = {arXiv:1912.02290 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{khanh00_EarlyReleaseTransmission,
  title = {Early {{Release}} - {{Transmission}} of {{Severe Acute Respiratory Syndrome Coronavirus}} 2 {{During Long Flight}} - {{Volume}} 26, {{Number}} 11\textemdash{{November}} 2020 - {{Emerging Infectious Diseases}} Journal - {{CDC}}},
  author = {Khanh, Nguyen Cong and Thai, Pham Quang and Quach, Ha-Linh and Thi, Ngoc-Anh Hoang and Dinh, Phung Cong and Duong, Tran Nhu and Mai, Le Thi Quynh and Nghia, Ngu Duy and Tu, Tran Anh and Quang, La Ngoc and Quang, Tran Dai and Nguyen, Trong-Tai and Vogt, Florian and Anh, Dang Duc},
  doi = {10.3201/eid2611.203299},
  abstract = {To assess the role of in-flight transmission of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), we investigated a cluster of cases among...},
  language = {en-us}
}

@article{khemakhem20_ICEBeeMIdentifiableConditionala,
  title = {{{ICE}}-{{BeeM}}: {{Identifiable Conditional Energy}}-{{Based Deep Models Based}} on {{Nonlinear ICA}}},
  shorttitle = {{{ICE}}-{{BeeM}}},
  author = {Khemakhem, Ilyes and Monti, Ricardo Pio and Kingma, Diederik P. and Hyv{\"a}rinen, Aapo},
  year = {2020},
  month = jul,
  abstract = {We consider the identifiability theory of probabilistic models and establish sufficient conditions under which the representations learned by a very broad family of conditional energy-based models are unique in function space, up to a simple transformation. In our model family, the energy function is the dot-product between two feature extractors, one for the dependent variable, and one for the conditioning variable. We show that under mild conditions, the features are unique up to scaling and permutation. Our results extend recent developments in nonlinear ICA, and in fact, they lead to an important generalization of ICA models. In particular, we show that our model can be used for the estimation of the components in the framework of Independently Modulated Component Analysis (IMCA), a new generalization of nonlinear ICA that relaxes the independence assumption. A thorough empirical study shows that representations learned by our model from real-world image datasets are identifiable, and improve performance in transfer learning and semi-supervised learning tasks.},
  archivePrefix = {arXiv},
  eprint = {2002.11537},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/khemakhem et al_2020_ice-beem.pdf},
  journal = {arXiv:2002.11537 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{khemakhem20_Variationalautoencodersnonlinear,
  title = {Variational Autoencoders and Nonlinear {{ICA}}: {{A}} Unifying Framework},
  author = {Khemakhem, Ilyes and Kingma, Diederik and Monti, Ricardo and Hyvarinen, Aapo},
  editor = {Chiappa, Silvia and Calandra, Roberto},
  year = {2020},
  month = aug,
  volume = {108},
  pages = {2207--2217},
  publisher = {{PMLR}},
  address = {{Online}},
  abstract = {The framework of variational autoencoders allows us to efficiently learn deep latent-variable models, such that the model's marginal distribution over observed variables fits the data. Often, we're interested in going a step further, and want to approximate the true joint distribution over observed and latent variables, including the true prior and posterior distributions over latent variables. This is known to be generally impossible due to unidentifiability of the model. We address this issue by showing that for a broad family of deep latent-variable models, identification of the true joint distribution over observed and latent variables is actually possible up to very simple transformations, thus achieving a principled and powerful form of disentanglement. Our result requires a factorized prior distribution over the latent variables that is conditioned on an additionally observed variable, such as a class label or almost any other observation. We build on recent developments in nonlinear ICA, which we extend to the case with noisy, undercomplete or discrete observations, integrated in a maximum likelihood framework. The result also trivially contains identifiable flow-based generative models as a special case.},
  file = {/home/trung/GoogleDrive/Zotero/khemakhem et al_2020_variational autoencoders and nonlinear ica.pdf},
  pdf = {http://proceedings.mlr.press/v108/khemakhem20a/khemakhem20a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{khoshaman19_GumBoltExtendingGumbel,
  title = {{{GumBolt}}: {{Extending Gumbel}} Trick to {{Boltzmann}} Priors},
  shorttitle = {{{GumBolt}}},
  author = {Khoshaman, Amir H. and Amin, Mohammad H.},
  year = {2019},
  month = mar,
  abstract = {Boltzmann machines (BMs) are appealing candidates for powerful priors in variational autoencoders (VAEs), as they are capable of capturing nontrivial and multi-modal distributions over discrete variables. However, non-differentiability of the discrete units prohibits using the reparameterization trick, essential for low-noise back propagation. The Gumbel trick resolves this problem in a consistent way by relaxing the variables and distributions, but it is incompatible with BM priors. Here, we propose the GumBolt, a model that extends the Gumbel trick to BM priors in VAEs. GumBolt is significantly simpler than the recently proposed methods with BM prior and outperforms them by a considerable margin. It achieves state-of-the-art performance on permutation invariant MNIST and OMNIGLOT datasets in the scope of models with only discrete latent variables. Moreover, the performance can be further improved by allowing multi-sampled (importance-weighted) estimation of log-likelihood in training, which was not possible with previous models.},
  archivePrefix = {arXiv},
  eprint = {1805.07349},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/khoshaman et al_2019_gumbolt.pdf},
  journal = {arXiv:1805.07349 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{khosla20_SupervisedContrastiveLearning,
  title = {Supervised {{Contrastive Learning}}},
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  year = {2020},
  month = apr,
  abstract = {Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1\%, setting a new state of the art number of 78.8\% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2004.11362},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/khosla et al_2020_supervised contrastive learning.pdf},
  journal = {arXiv:2004.11362 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kidger20_Heythatnot,
  title = {"{{Hey}}, That's Not an {{ODE}}": {{Faster ODE Adjoints}} with 12 {{Lines}} of {{Code}}},
  shorttitle = {"{{Hey}}, That's Not an {{ODE}}"},
  author = {Kidger, Patrick and Chen, Ricky T. Q. and Lyons, Terry},
  year = {2020},
  month = sep,
  abstract = {Neural differential equations may be trained by backpropagating gradients via the adjoint method, which is another differential equation typically solved using an adaptive-step-size numerical differential equation solver. A proposed step is accepted if its error, \textbackslash emph\{relative to some norm\}, is sufficiently small; else it is rejected, the step is shrunk, and the process is repeated. Here, we demonstrate that the particular structure of the adjoint equations makes the usual choices of norm (such as \$L\^2\$) unnecessarily stringent. By replacing it with a more appropriate (semi)norm, fewer steps are unnecessarily rejected and the backpropagation is made faster. This requires only minor code modifications. Experiments on a wide range of tasks---including time series, generative modeling, and physical control---demonstrate a median improvement of 40\% fewer function evaluations. On some problems we see as much as 62\% fewer function evaluations, so that the overall training time is roughly halved.},
  archivePrefix = {arXiv},
  eprint = {2009.09457},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kidger et al_2020_hey, that's not an ode.pdf},
  journal = {arXiv:2009.09457 [cs, math]},
  primaryClass = {cs, math}
}

@article{kilcher18_SemanticInterpolationImplicit,
  title = {Semantic {{Interpolation}} in {{Implicit Models}}},
  author = {Kilcher, Yannic and Lucchi, Aurelien and Hofmann, Thomas},
  year = {2018},
  month = feb,
  abstract = {In implicit models, one often interpolates between sampled points in latent space. As we show in this paper, care needs to be taken to match-up the distributional assumptions on code vectors with the geometry of the interpolating paths. Otherwise, typical assumptions about the quality and semantics of in-between points may not be justified. Based on our analysis we propose to modify the prior code distribution to put significantly more probability mass closer to the origin. As a result, linear interpolation paths are not only shortest paths, but they are also guaranteed to pass through high-density regions, irrespective of the dimensionality of the latent space. Experiments on standard benchmark image datasets demonstrate clear visual improvements in the quality of the generated samples and exhibit more meaningful interpolation paths.},
  archivePrefix = {arXiv},
  eprint = {1710.11381},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kilcher et al_2018_semantic interpolation in implicit models.pdf},
  journal = {arXiv:1710.11381 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kim00_InterpretableModelsProbabilistic,
  title = {Interpretable {{Models}} in {{Probabilistic Machine Learning}}},
  author = {Kim, Hyun Jik},
  pages = {202},
  file = {/home/trung/GoogleDrive/Zotero/kim_interpretable models in probabilistic machine learning.pdf},
  language = {en}
}

@article{kim17_StructuredAttentionNetworks,
  title = {Structured {{Attention Networks}}},
  author = {Kim, Yoon and Denton, Carl and Hoang, Luong and Rush, Alexander M.},
  year = {2017},
  month = feb,
  abstract = {Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention.},
  annotation = {ZSCC: 0000137},
  archivePrefix = {arXiv},
  eprint = {1702.00887},
  eprinttype = {arxiv},
  journal = {arXiv:1702.00887 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{kim18_BayesianModelAgnosticMetaLearning,
  title = {Bayesian {{Model}}-{{Agnostic Meta}}-{{Learning}}},
  author = {Kim, Taesup and Yoon, Jaesik and Dia, Ousmane and Kim, Sungwoong and Bengio, Yoshua and Ahn, Sungjin},
  year = {2018},
  month = nov,
  abstract = {Due to the inherent model uncertainty, learning to infer Bayesian posterior from a few-shot dataset is an important step towards robust meta-learning. In this paper, we propose a novel Bayesian model-agnostic meta-learning method. The proposed method combines efficient gradient-based meta-learning with nonparametric variational inference in a principled probabilistic framework. Unlike previous methods, during fast adaptation, the method is capable of learning complex uncertainty structure beyond a simple Gaussian approximation, and during meta-update, a novel Bayesian mechanism prevents meta-level overfitting. Remaining a gradientbased method, it is also the first Bayesian model-agnostic meta-learning method applicable to various tasks including reinforcement learning. Experiment results show the accuracy and robustness of the proposed method in sinusoidal regression, image classification, active learning, and reinforcement learning.},
  annotation = {ZSCC: 0000038},
  archivePrefix = {arXiv},
  eprint = {1806.03836},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/3V68SCAW/Kim et al. - 2018 - Bayesian Model-Agnostic Meta-Learning.pdf},
  journal = {arXiv:1806.03836 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{kim18_Disentanglingfactorising,
  title = {Disentangling by Factorising},
  author = {Kim, Hyunjik and Mnih, Andriy},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {2649--2658},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm Sweden}},
  abstract = {We define and address the problem of unsupervised learning of disentangled representations on data generated from independent factors of variation. We propose FactorVAE, a method that disentangles by encouraging the distribution of representations to be factorial and hence independent across the dimensions. We show that it improves upon beta-VAE by providing a better trade-off between disentanglement and reconstruction quality and being more robust to the number of training iterations. Moreover, we highlight the problems of a commonly used disentanglement metric and introduce a new metric that does not suffer from them.},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2018_disentangling by factorising.pdf},
  keywords = {_tablet,disentanglement,favorite},
  pdf = {http://proceedings.mlr.press/v80/kim18b/kim18b.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{kim18_SemiAmortizedVariationalAutoencoders,
  title = {Semi-{{Amortized Variational Autoencoders}}},
  author = {Kim, Yoon and Wiseman, Sam and Miller, Andrew C. and Sontag, David and Rush, Alexander M.},
  year = {2018},
  month = jul,
  abstract = {Amortized variational inference (AVI) replaces instance-specific local inference with a global inference network. While AVI has enabled efficient training of deep generative models such as variational autoencoders (VAE), recent empirical work suggests that inference networks can produce suboptimal variational parameters. We propose a hybrid approach, to use AVI to initialize the variational parameters and run stochastic variational inference (SVI) to refine them. Crucially, the local SVI procedure is itself differentiable, so the inference network and generative model can be trained end-to-end with gradient-based optimization. This semi-amortized approach enables the use of rich generative models without experiencing the posterior-collapse phenomenon common in training VAEs for problems like text generation. Experiments show this approach outperforms strong autoregressive and variational baselines on standard text and image datasets.},
  archivePrefix = {arXiv},
  eprint = {1802.02550},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2018_semi-amortized variational autoencoders.pdf},
  journal = {arXiv:1802.02550 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{kim18_TutorialDeepLatent,
  title = {A {{Tutorial}} on {{Deep Latent Variable Models}} of {{Natural Language}}},
  author = {Kim, Yoon and Wiseman, Sam and Rush, Alexander M.},
  year = {2018},
  month = dec,
  abstract = {There has been much recent, exciting work on combining the complementary strengths of latent variable models and deep learning. Latent variable modeling makes it easy to explicitly specify model constraints through conditional independence properties, while deep learning makes it possible to parameterize these conditional likelihoods with powerful function approximators. While these "deep latent variable" models provide a rich, flexible framework for modeling many real-world phenomena, difficulties exist: deep parameterizations of conditional likelihoods usually make posterior inference intractable, and latent variable objectives often complicate backpropagation by introducing points of non-differentiability. This tutorial explores these issues in depth through the lens of variational inference.},
  archivePrefix = {arXiv},
  eprint = {1812.06834},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2018_a tutorial on deep latent variable models of natural language.pdf},
  journal = {arXiv:1812.06834 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kim19_AttentiveNeuralProcesses,
  title = {Attentive {{Neural Processes}}},
  author = {Kim, Hyunjik and Mnih, Andriy and Schwarz, Jonathan and Garnelo, Marta and Eslami, Ali and Rosenbaum, Dan and Vinyals, Oriol and Teh, Yee Whye},
  year = {2019},
  month = jan,
  abstract = {Neural Processes (NPs) (Garnelo et al 2018a;b) approach regression by learning to map a context set of observed input-output pairs to a distribution over regression functions. Each function models the distribution of the output given an input, conditioned on the context. NPs have the benefit of fitting observed data efficiently with linear complexity in the number of context input-output pairs, and can learn a wide family of conditional distributions; they learn predictive distributions conditioned on context sets of arbitrary size. Nonetheless, we show that NPs suffer a fundamental drawback of underfitting, giving inaccurate predictions at the inputs of the observed data they condition on. We address this issue by incorporating attention into NPs, allowing each input location to attend to the relevant context points for the prediction. We show that this greatly improves the accuracy of predictions, results in noticeably faster training, and expands the range of functions that can be modelled.},
  annotation = {ZSCC: 0000023},
  archivePrefix = {arXiv},
  eprint = {1901.05761},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2019_attentive neural processes.pdf;/home/trung/Zotero/storage/HYP6TNAW/1901.html},
  journal = {arXiv:1901.05761 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,gaussian process,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{kim19_BayesFactorVAEHierarchicalBayesian,
  title = {Bayes-{{Factor}}-{{VAE}}: {{Hierarchical Bayesian Deep Auto}}-{{Encoder Models}} for {{Factor Disentanglement}}},
  shorttitle = {Bayes-{{Factor}}-{{VAE}}},
  author = {Kim, Minyoung and Wang, Yuting and Sahu, Pritish and Pavlovic, Vladimir},
  year = {2019},
  month = sep,
  abstract = {We propose a family of novel hierarchical Bayesian deep auto-encoder models capable of identifying disentangled factors of variability in data. While many recent attempts at factor disentanglement have focused on sophisticated learning objectives within the VAE framework, their choice of a standard normal as the latent factor prior is both suboptimal and detrimental to performance. Our key observation is that the disentangled latent variables responsible for major sources of variability, the relevant factors, can be more appropriately modeled using long-tail distributions. The typical Gaussian priors are, on the other hand, better suited for modeling of nuisance factors. Motivated by this, we extend the VAE to a hierarchical Bayesian model by introducing hyper-priors on the variances of Gaussian latent priors, mimicking an infinite mixture, while maintaining tractable learning and inference of the traditional VAEs. This analysis signifies the importance of partitioning and treating in a different manner the latent dimensions corresponding to relevant factors and nuisances. Our proposed models, dubbed Bayes-Factor-VAEs, are shown to outperform existing methods both quantitatively and qualitatively in terms of latent disentanglement across several challenging benchmark tasks.},
  archivePrefix = {arXiv},
  eprint = {1909.02820},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2019_bayes-factor-vae.pdf},
  journal = {arXiv:1909.02820 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{kim19_CrossAttentionEndtoEndASR,
  title = {Cross-{{Attention End}}-to-{{End ASR}} for {{Two}}-{{Party Conversations}}},
  author = {Kim, Suyoun and Dalmia, Siddharth and Metze, Florian},
  year = {2019},
  month = jul,
  abstract = {We present an end-to-end speech recognition model that learns interaction between two speakers based on the turn-changing information. Unlike conventional speech recognition models, our model exploits two speakers' history of conversational-context information that spans across multiple turns within an end-to-end framework. Specifically, we propose a speaker-specific cross-attention mechanism that can look at the output of the other speaker side as well as the one of the current speaker for better at recognizing long conversations. We evaluated the models on the Switchboard conversational speech corpus and show that our model outperforms standard end-to-end speech recognition models.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1907.10726},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Kim et al_2019_Cross-Attention End-to-End ASR for Two-Party Conversations2.pdf;/home/trung/GoogleDrive/Zotero/kim et al_2019_cross-attention end-to-end asr for two-party conversations.pdf;/home/trung/Zotero/storage/Q5XH6S6E/1907.html;/home/trung/Zotero/storage/WCIMVEJ7/1907.html},
  journal = {arXiv:1907.10726 [cs, eess]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,cross modal,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{kim19_RelevanceFactorVAE,
  title = {Relevance {{Factor VAE}}: {{Learning}} and {{Identifying Disentangled Factors}}},
  shorttitle = {Relevance {{Factor VAE}}},
  author = {Kim, Minyoung and Wang, Yuting and Sahu, Pritish and Pavlovic, Vladimir},
  year = {2019},
  month = feb,
  abstract = {We propose a novel VAE-based deep auto-encoder model that can learn disentangled latent representations in a fully unsupervised manner, endowed with the ability to identify all meaningful sources of variation and their cardinality. Our model, dubbed Relevance-Factor-VAE, leverages the total correlation (TC) in the latent space to achieve the disentanglement goal, but also addresses the key issue of existing approaches which cannot distinguish between meaningful and nuisance factors of latent variation, often the source of considerable degradation in disentanglement performance. We tackle this issue by introducing the so-called relevance indicator variables that can be automatically learned from data, together with the VAE parameters. Our model effectively focuses the TC loss onto the relevant factors only by tolerating large prior KL divergences, a desideratum justified by our semi-parametric theoretical analysis. Using a suite of disentanglement metrics, including a newly proposed one, as well as qualitative evidence, we demonstrate that our model outperforms existing methods across several challenging benchmark datasets.},
  archivePrefix = {arXiv},
  eprint = {1902.01568},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2019_relevance factor vae.pdf},
  journal = {arXiv:1902.01568 [cs, stat]},
  keywords = {_tablet,disentanglement},
  primaryClass = {cs, stat}
}

@article{kim19_VariationalTemporalAbstraction,
  title = {Variational {{Temporal Abstraction}}},
  author = {Kim, Taesup and Ahn, Sungjin and Bengio, Yoshua},
  year = {2019},
  month = oct,
  abstract = {We introduce a variational approach to learning and inference of temporally hierarchical structure and representation for sequential data. We propose the Variational Temporal Abstraction (VTA), a hierarchical recurrent state space model that can infer the latent temporal structure and thus perform the stochastic state transition hierarchically. We also propose to apply this model to implement the jumpy-imagination ability in imagination-augmented agent-learning in order to improve the efficiency of the imagination. In experiments, we demonstrate that our proposed method can model 2D and 3D visual sequence datasets with interpretable temporal structure discovery and that its application to jumpy imagination enables more efficient agent-learning in a 3D navigation task.},
  archivePrefix = {arXiv},
  eprint = {1910.00775},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Kim et al_2019_Variational Temporal Abstraction2.pdf;/home/trung/GoogleDrive/Zotero/kim et al_2019_variational temporal abstraction.pdf;/home/trung/Zotero/storage/2ZEGNVDI/1910.html;/home/trung/Zotero/storage/EMDZG2C4/1910.html},
  journal = {arXiv:1910.00775 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,sequential,Statistics - Machine Learning,temporal,variational},
  primaryClass = {cs, stat}
}

@phdthesis{kim20_DeepLatentVariable,
  title = {Deep {{Latent Variable Models}} of {{Natural Language}}},
  author = {Kim, Yoon},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/kim_2020_deep latent variable models of natural language.pdf}
}

@techreport{kim20_Demystifyingdropoutssingle,
  title = {Demystifying ``Drop-Outs'' in Single Cell {{UMI}} Data},
  author = {Kim, Tae and Zhou, Xiang and Chen, Mengjie},
  year = {2020},
  month = apr,
  institution = {{Genomics}},
  doi = {10.1101/2020.03.31.018911},
  abstract = {Analysis of scRNA-seq data has been challenging particularly because of excessive zeros observed in UMI counts. Prevalent opinions are that many of the detected zeros are ``drop-outs'' that occur during experiments and that those zeros should be accounted for through procedures such as normalization, variance stabilization, and imputation. Here, we extensively analyze publicly available UMI datasets and challenge the existing scRNA-seq workflows. Our results strongly suggest that resolving cell-type heterogeneity should be the foremost step of the scRNA-seq analysis pipeline because once cell-type heterogeneity is resolved, ``drop-outs'' disappear. Additionally, we show that the simplest parametric count model, Poisson, is sufficient to fully leverage the biological information contained in the UMI data, thus offering a more optimistic view of the data analysis. However, if the cell-type heterogeneity is not appropriately taken into account, pre-processing such as normalization or imputation becomes inappropriate and can introduce unwanted noise. Inspired by these analyses, we propose a zero inflation test that can select gene features contributing to cell-type heterogeneity. We integrate feature selection and clustering into iterative pre-processing in our novel, efficient, and straightforward framework for UMI analysis, HIPPO (Heterogeneity Inspired Pre-Processing tOol). HIPPO leads to downstream analysis with much better interpretability than alternatives in our comparative studies.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/KHQQ6E5M/Kim et al. - 2020 - Demystifying “drop-outs” in single cell UMI data.pdf},
  language = {en},
  type = {Preprint}
}

@article{kim20_GlowTTSGenerativeFlow,
  title = {Glow-{{TTS}}: {{A Generative Flow}} for {{Text}}-to-{{Speech}} via {{Monotonic Alignment Search}}},
  shorttitle = {Glow-{{TTS}}},
  author = {Kim, Jaehyeon and Kim, Sungwon and Kong, Jungil and Yoon, Sungroh},
  year = {2020},
  month = may,
  abstract = {Recently, text-to-speech (TTS) models such as FastSpeech and ParaNet have been proposed to generate mel-spectrograms from text in parallel. Despite the advantage, the parallel TTS models cannot be trained without guidance from autoregressive TTS models as their external aligners. In this work, we propose Glow-TTS, a flow-based generative model for parallel TTS that does not require any external aligner. By combining the properties of flows and dynamic programming, the proposed model searches for the most probable monotonic alignment between text and the latent representation of speech on its own. We demonstrate that enforcing hard monotonic alignments enables robust TTS, which generalizes to long utterances, and employing generative flows enables fast, diverse, and controllable speech synthesis. Glow-TTS obtains an order-of-magnitude speed-up over the autoregressive model, Tacotron 2, at synthesis with comparable speech quality. We further show that our model can be easily extended to a multi-speaker setting.},
  archivePrefix = {arXiv},
  eprint = {2005.11129},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2020_glow-tts.pdf},
  journal = {arXiv:2005.11129 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{kim20_SemisupervisedDisentanglementIndependent,
  title = {Semi-Supervised {{Disentanglement}} with {{Independent Vector Variational Autoencoders}}},
  author = {Kim, Bo-Kyeong and Park, Sungjin and Kim, Geonmin and Lee, Soo-Young},
  year = {2020},
  month = mar,
  abstract = {We aim to separate the generative factors of data into two latent vectors in a variational autoencoder. One vector captures class factors relevant to target classification tasks, while the other vector captures style factors relevant to the remaining information. To learn the discrete class features, we introduce supervision using a small amount of labeled data, which can simply yet effectively reduce the effort required for hyperparameter tuning performed in existing unsupervised methods. Furthermore, we introduce a learning objective to encourage statistical independence between the vectors. We show that (i) this vector independence term exists within the result obtained on decomposing the evidence lower bound with multiple latent vectors, and (ii) encouraging such independence along with reducing the total correlation within the vectors enhances disentanglement performance. Experiments conducted on several image datasets demonstrate that the disentanglement achieved via our method can improve classification performance and generation controllability.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2003.06581},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2020_semi-supervised disentanglement with independent vector variational autoencoders2.pdf},
  journal = {arXiv:2003.06581 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kim20_SemisupervisedDisentanglementIndependenta,
  title = {Semi-Supervised {{Disentanglement}} with {{Independent Vector Variational Autoencoders}}},
  author = {Kim, Bo-Kyeong and Park, Sungjin and Kim, Geonmin and Lee, Soo-Young},
  year = {2020},
  month = mar,
  abstract = {We aim to separate the generative factors of data into two latent vectors in a variational autoencoder. One vector captures class factors relevant to target classification tasks, while the other vector captures style factors relevant to the remaining information. To learn the discrete class features, we introduce supervision using a small amount of labeled data, which can simply yet effectively reduce the effort required for hyperparameter tuning performed in existing unsupervised methods. Furthermore, we introduce a learning objective to encourage statistical independence between the vectors. We show that (i) this vector independence term exists within the result obtained on decomposing the evidence lower bound with multiple latent vectors, and (ii) encouraging such independence along with reducing the total correlation within the vectors enhances disentanglement performance. Experiments conducted on several image datasets demonstrate that the disentanglement achieved via our method can improve classification performance and generation controllability.},
  archivePrefix = {arXiv},
  eprint = {2003.06581},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kim et al_2020_semi-supervised disentanglement with independent vector variational autoencoders.pdf},
  journal = {arXiv:2003.06581 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@misc{kim20_taki0112Tensorflow2Cookbook,
  title = {Taki0112/{{Tensorflow2}}-{{Cookbook}}},
  author = {Kim, Junho},
  year = {2020},
  month = apr,
  abstract = {Simple Tensorflow 2.x Cookbook for easy-to-use. Contribute to taki0112/Tensorflow2-Cookbook development by creating an account on GitHub.},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {MIT}
}

@techreport{kimmel20_Disentanglinglatentrepresentations,
  title = {Disentangling Latent Representations of Single Cell {{RNA}}-Seq Experiments},
  author = {Kimmel, Jacob C.},
  year = {2020},
  month = mar,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.03.04.972166},
  abstract = {Single cell RNA sequencing (scRNA-seq) enables transcriptional profiling at the resolution of individual cells. These experiments measure features at the level of transcripts, but biological processes of interest often involve the complex coordination of many individual transcripts. It can therefore be difficult to extract interpretable insights directly from transcript-level cell profiles. Latent representations which capture biological variation in a smaller number of dimensions are therefore useful in interpreting many experiments. Variational autoencoders (VAEs) have emerged as a tool for scRNA-seq denoising and data harmonization, but the correspondence between latent dimensions in these models and generative factors remains unexplored. Here, we explore training VAEs with modifications to the objective function (i.e. {$\beta$}-VAE) to encourage disentanglement and make latent representations of single cell RNA-seq data more interpretable. Using simulated data, we find that VAE latent dimensions correspond more directly to data generative factors when using these modified objective functions. Applied to experimental data of stimulated peripheral blood mononuclear cells, we find better correspondence of latent dimensions to experimental factors and cell identity programs, but impaired performance on cell type clustering.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/QDQ9459D/Kimmel - 2020 - Disentangling latent representations of single cel.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{kimmel20_Disentanglinglatentrepresentationsa,
  title = {Disentangling Latent Representations of Single Cell {{RNA}}-Seq Experiments},
  author = {Kimmel, Jacob C.},
  year = {2020},
  month = mar,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.03.04.972166},
  abstract = {Single cell RNA sequencing (scRNA-seq) enables transcriptional profiling at the resolution of individual cells. These experiments measure features at the level of transcripts, but biological processes of interest often involve the complex coordination of many individual transcripts. It can therefore be difficult to extract interpretable insights directly from transcript-level cell profiles. Latent representations which capture biological variation in a smaller number of dimensions are therefore useful in interpreting many experiments. Variational autoencoders (VAEs) have emerged as a tool for scRNA-seq denoising and data harmonization, but the correspondence between latent dimensions in these models and generative factors remains unexplored. Here, we explore training VAEs with modifications to the objective function (i.e. {$\beta$}-VAE) to encourage disentanglement and make latent representations of single cell RNA-seq data more interpretable. Using simulated data, we find that VAE latent dimensions correspond more directly to data generative factors when using these modified objective functions. Applied to experimental data of stimulated peripheral blood mononuclear cells, we find better correspondence of latent dimensions to experimental factors and cell identity programs, but impaired performance on cell type clustering.},
  file = {/home/trung/GoogleDrive/Zotero/kimmel_2020_disentangling latent representations of single cell rna-seq experiments.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{kimmel20_scNymSemisupervisedadversarial,
  title = {{{scNym}}: {{Semi}}-Supervised Adversarial Neural Networks for Single Cell Classification},
  shorttitle = {{{scNym}}},
  author = {Kimmel, Jacob C. and Kelley, David R.},
  year = {2020},
  month = jun,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.06.04.132324},
  abstract = {Abstract           Annotating cell identities is a common bottleneck in the analysis of single cell genomics experiments. Here, we present scNym, a semi-supervised, adversarial neural network that learns to transfer cell identity annotations from one experiment to another. scNym takes advantage of information in both labeled datasets and new, unlabeled datasets to learn rich representations of cell identity that enable effective annotation transfer. We show that scNym effectively transfers annotations across experiments despite biological and technical differences, achieving performance superior to existing methods. We also show that scNym models can synthesize information from multiple training and target datasets to improve performance. In addition to high performance, we show that scNym models are well-calibrated and interpretable with saliency methods.},
  file = {/home/trung/GoogleDrive/Zotero/kimmel et al_2020_scnym.pdf},
  language = {en},
  type = {Preprint}
}

@article{kinalis19_Deconvolutionautoencoderslearn,
  title = {Deconvolution of Autoencoders to Learn Biological Regulatory Modules from Single Cell {{mRNA}} Sequencing Data},
  author = {Kinalis, Savvas and Nielsen, Finn Cilius and Winther, Ole and Bagger, Frederik Otzen},
  year = {2019},
  month = dec,
  volume = {20},
  pages = {379},
  issn = {1471-2105},
  doi = {10.1186/s12859-019-2952-9},
  abstract = {Background: Unsupervised machine learning methods (deep learning) have shown their usefulness with noisy single cell mRNA-sequencing data (scRNA-seq), where the models generalize well, despite the zero-inflation of the data. A class of neural networks, namely autoencoders, has been useful for denoising of single cell data, imputation of missing values and dimensionality reduction. Results: Here, we present a striking feature with the potential to greatly increase the usability of autoencoders: With specialized training, the autoencoder is not only able to generalize over the data, but also to tease apart biologically meaningful modules, which we found encoded in the representation layer of the network. Our model can, from scRNA-seq data, delineate biological meaningful modules that govern a dataset, as well as give information as to which modules are active in each single cell. Importantly, most of these modules can be explained by known biological functions, as provided by the Hallmark gene sets. Conclusions: We discover that tailored training of an autoencoder makes it possible to deconvolute biological modules inherent in the data, without any assumptions. By comparisons with gene signatures of canonical pathways we see that the modules are directly interpretable. The scope of this discovery has important implications, as it makes it possible to outline the drivers behind a given effect of a cell. In comparison with other dimensionality reduction methods, or supervised models for classification, our approach has the benefit of both handling well the zero-inflated nature of scRNA-seq, and validating that the model captures relevant information, by establishing a link between input and decoded data. In perspective, our model in combination with clustering methods is able to provide information about which subtype a given single cell belongs to, as well as which biological functions determine that membership.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/kinalis et al_2019_deconvolution of autoencoders to learn biological regulatory modules from single cell mrna sequencing data.pdf},
  journal = {BMC Bioinformatics},
  language = {en},
  number = {1}
}

@article{kindermans17_reliabilitysaliencymethods,
  title = {The ({{Un}})Reliability of Saliency Methods},
  author = {Kindermans, Pieter-Jan and Hooker, Sara and Adebayo, Julius and Alber, Maximilian and Sch{\"u}tt, Kristof T. and D{\"a}hne, Sven and Erhan, Dumitru and Kim, Been},
  year = {2017},
  month = nov,
  abstract = {Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.},
  archivePrefix = {arXiv},
  eprint = {1711.00867},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kindermans et al_2017_the (un)reliability of saliency methods.pdf},
  journal = {arXiv:1711.00867 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{kingma14_SemiSupervisedLearningDeep,
  title = {Semi-{{Supervised Learning}} with {{Deep Generative Models}}},
  author = {Kingma, Diederik P. and Rezende, Danilo J. and Mohamed, Shakir and Welling, Max},
  year = {2014},
  month = jun,
  abstract = {The ever-increasing size of modern data sets combined with the difficulty of obtaining label information has made semi-supervised learning one of the problems of significant practical importance in modern data analysis. We revisit the approach to semi-supervised learning with generative models and develop new models that allow for effective generalisation from small labelled data sets to large unlabelled ones. Generative approaches have thus far been either inflexible, inefficient or non-scalable. We show that deep generative models and approximate Bayesian inference exploiting recent advances in variational methods can be used to provide significant improvements, making generative approaches highly competitive for semi-supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1406.5298},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kingma et al_2014_semi-supervised learning with deep generative models.pdf},
  journal = {arXiv:1406.5298 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kingma15_VariationalDropoutLocal,
  title = {Variational {{Dropout}} and the {{Local Reparameterization Trick}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Welling, Max},
  year = {2015},
  month = dec,
  abstract = {We investigate a local reparameterizaton technique for greatly reducing the variance of stochastic gradients for variational Bayesian inference (SGVB) of a posterior over model parameters, while retaining parallelizability. This local reparameterization translates uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such parameterizations can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence. Additionally, we explore a connection with dropout: Gaussian dropout objectives correspond to SGVB with local reparameterization, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose variational dropout, a generalization of Gaussian dropout where the dropout rates are learned, often leading to better models. The method is demonstrated through several experiments.},
  annotation = {ZSCC: 0000410},
  archivePrefix = {arXiv},
  eprint = {1506.02557},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kingma et al_2015_variational dropout and the local reparameterization trick.pdf;/home/trung/GoogleDrive/Zotero/kingma et al_2015_variational dropout and the local reparameterization trick2.pdf;/home/trung/Zotero/storage/2APFW48C/1506.html;/home/trung/Zotero/storage/ZH6MFCQH/1506.html},
  journal = {arXiv:1506.02557 [cs, stat]},
  keywords = {Computer Science - Machine Learning,dropout,reparameterization,Statistics - Computation,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{kingma17_ImprovingVariationalInference,
  title = {Improving {{Variational Inference}} with {{Inverse Autoregressive Flow}}},
  author = {Kingma, Diederik P. and Salimans, Tim and Jozefowicz, Rafal and Chen, Xi and Sutskever, Ilya and Welling, Max},
  year = {2017},
  month = jan,
  abstract = {The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.},
  archivePrefix = {arXiv},
  eprint = {1606.04934},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kingma et al_2017_improving variational inference with inverse autoregressive flow.pdf},
  journal = {arXiv:1606.04934 [cs, stat]},
  primaryClass = {cs, stat}
}

@phdthesis{kingma17_VariationalInferenceDeep,
  title = {Variational {{Inference}} and {{Deep Learning}}},
  author = {Kingma, Durk},
  year = {2017},
  file = {/home/trung/GoogleDrive/Zotero/kingma_2017_variational inference and deep learning.pdf},
  keywords = {_tablet,favorite}
}

@incollection{kingma18_GlowGenerativeFlow,
  title = {Glow: {{Generative Flow}} with {{Invertible}} 1x1 {{Convolutions}}},
  shorttitle = {Glow},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Kingma, Durk P and Dhariwal, Prafulla},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {10215--10224},
  publisher = {{Curran Associates, Inc.}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/kingma et al_2018_glow.pdf;/home/trung/Zotero/storage/QZ5WPFC2/8224-glow-generative-flow-with-invertible-1x1-convolutions.html}
}

@article{kingma19_IntroductionVariationalAutoencoders,
  title = {An {{Introduction}} to {{Variational Autoencoders}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2019},
  month = jun,
  abstract = {Variational autoencoders provide a principled framework for learning deep latent-variable models and corresponding inference models. In this work, we provide an introduction to variational autoencoders and some important extensions.},
  archivePrefix = {arXiv},
  eprint = {1906.02691},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kingma et al_2019_an introduction to variational autoencoders.pdf;/home/trung/Zotero/storage/5SE8FVQ6/1906.html},
  journal = {arXiv:1906.02691 [cs, stat]},
  keywords = {_tablet,autoencoder,Computer Science - Machine Learning,favorite,gaussian process,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{kini20_AnalyticStudyDouble,
  title = {Analytic {{Study}} of {{Double Descent}} in {{Binary Classification}}: {{The Impact}} of {{Loss}}},
  shorttitle = {Analytic {{Study}} of {{Double Descent}} in {{Binary Classification}}},
  author = {Kini, Ganesh and Thrampoulidis, Christos},
  year = {2020},
  month = jan,
  abstract = {Extensive empirical evidence reveals that, for a wide range of different learning methods and datasets, the risk curve exhibits a double-descent (DD) trend as a function of the model size. In our recent coauthored paper [DKT19], we studied binary linear classification models and showed that the test error of gradient descent (GD) with logistic loss undergoes a DD. In this paper, we complement these results by extending them to GD with square loss. We show that the DD phenomenon persists, but we also identify several differences compared to logistic loss. This emphasizes that crucial features of DD curves (such as their transition threshold and global minima) depend both on the training data and on the learning algorithm. We further study the dependence of DD curves on the size of the training set. Similar to our earlier work, our results are analytic: we plot the DD curves by first deriving sharp asymptotics for the test error under Gaussian features. Albeit simple, the models permit a principled study of DD features, the outcomes of which theoretically corroborate related empirical findings occurring in more complex learning tasks.},
  archivePrefix = {arXiv},
  eprint = {2001.11572},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kini et al_2020_analytic study of double descent in binary classification.pdf},
  journal = {arXiv:2001.11572 [cs, eess, math, stat]},
  language = {en},
  primaryClass = {cs, eess, math, stat}
}

@phdthesis{kipf20_Deeplearninggraphstructured,
  title = {Deep Learning with Graph-Structured Representations},
  author = {Kipf, Thomas},
  year = {2020},
  annotation = {ZSCC: 0000000  OCLC: 1151818405},
  file = {/home/trung/GoogleDrive/Zotero/kipf_2020_deep learning with graph-structured representations.pdf},
  isbn = {9789463758512},
  language = {en}
}

@article{kirsch20_UnpackingInformationBottlenecks,
  title = {Unpacking {{Information Bottlenecks}}: {{Unifying Information}}-{{Theoretic Objectives}} in {{Deep Learning}}},
  shorttitle = {Unpacking {{Information Bottlenecks}}},
  author = {Kirsch, Andreas and Lyle, Clare and Gal, Yarin},
  year = {2020},
  month = apr,
  abstract = {The information bottleneck (IB) principle offers both a mechanism to explain how deep neural networks train and generalize, as well as a regularized objective with which to train models. However, multiple competing objectives have been proposed based on this principle, and the information-theoretic quantities in these objectives are difficult to compute for large deep neural networks. This, in turn, limits their use as a training objective. In this work, we review these quantities, compare and unify previously proposed objectives and relate them to surrogate objectives more friendly to optimization. We find that these surrogate objectives allow us to apply the information bottleneck to modern neural network architectures. We demonstrate our insights on Permutation-MNIST, MNIST and CIFAR10.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2003.12537},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/RN5IU28T/Kirsch et al. - 2020 - Unpacking Information Bottlenecks Unifying Inform.pdf},
  journal = {arXiv:2003.12537 [cs, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kitada19_impactdietaryprotein,
  title = {The Impact of Dietary Protein Intake on Longevity and Metabolic Health},
  author = {Kitada, Munehiro and Ogura, Yoshio and Monno, Itaru and Koya, Daisuke},
  year = {2019},
  month = may,
  volume = {43},
  pages = {632--640},
  issn = {23523964},
  doi = {10.1016/j.ebiom.2019.04.005},
  abstract = {Lifespan and metabolic health are influenced by dietary nutrients. Recent studies show that a reduced protein intake or low-protein/high-carbohydrate diet plays a critical role in longevity/metabolic health. Additionally, specific amino acids (AAs), including methionine or branched-chain AAs (BCAAs), are associated with the regulation of lifespan/ageing and metabolism through multiple mechanisms. Therefore, methionine or BCAAs restriction may lead to the benefits on longevity/metabolic health. Moreover, epidemiological studies show that a high intake of animal protein, particularly red meat, which contains high levels of methionine and BCAAs, may be related to the promotion of age-related diseases. Therefore, a low animal protein diet, particularly a diet low in red meat, may provide health benefits. However, malnutrition, including sarcopenia/frailty due to inadequate protein intake, is harmful to longevity/metabolic health. Therefore, further study is necessary to elucidate the specific restriction levels of individual AAs that are most effective for longevity/metabolic health in humans.},
  file = {/home/trung/GoogleDrive/Zotero/kitada et al_2019_the impact of dietary protein intake on longevity and metabolic health.pdf},
  journal = {EBioMedicine},
  language = {en}
}

@article{klambauer17_SelfNormalizingNeuralNetworks,
  title = {Self-{{Normalizing Neural Networks}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  month = jun,
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are "scaled exponential linear units" (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance -- even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs and other machine learning methods such as random forests and support vector machines. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  archivePrefix = {arXiv},
  eprint = {1706.02515},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/klambauer et al_2017_self-normalizing neural networks.pdf;/home/trung/Zotero/storage/K6N2WPY5/1706.html},
  journal = {arXiv:1706.02515 [cs, stat]},
  keywords = {activation,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{klambauer17_SelfNormalizingNeuralNetworksa,
  title = {Self-{{Normalizing Neural Networks}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  month = sep,
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ``scaled exponential linear units'' (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance \textemdash{} even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  annotation = {ZSCC: 0000860},
  archivePrefix = {arXiv},
  eprint = {1706.02515},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/klambauer et al_2017_self-normalizing neural networks2.pdf},
  journal = {arXiv:1706.02515 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{klein20_emergenceinformativehigher,
  title = {The Emergence of Informative Higher Scales in Complex Networks},
  author = {Klein, Brennan and Hoel, Erik},
  year = {2020},
  month = jan,
  abstract = {The connectivity of a network conveys information about the dependencies between nodes. We show that this information can be analyzed by measuring the uncertainty (and certainty) contained in paths along nodes and links in a network. Specifically, we derive from first principles a measure known as effective information and describe its behavior in common network models. Networks with higher effective information contain more information within the dependencies between nodes. We show how subgraphs of nodes can be grouped into macro-nodes, reducing the size of a network while increasing its effective information, a phenomenon known as causal emergence. We find that causal emergence is common in simulated and real networks across biological, social, informational, and technological domains. Ultimately, these results show that the emergence of higher scales in networks can be directly assessed, and that these higher scales offer a way to create certainty out of uncertainty.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1907.03902},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/klein et al_2020_the emergence of informative higher scales in complex networks.pdf},
  journal = {arXiv:1907.03902 [physics]},
  keywords = {causal},
  language = {en},
  primaryClass = {physics}
}

@techreport{klimovskaia19_PoincareMapsAnalyzing,
  title = {Poincar\'e {{Maps}} for {{Analyzing Complex Hierarchies}} in {{Single}}-{{Cell Data}}},
  author = {Klimovskaia, Anna and {Lopez-Paz}, David and Bottou, L{\'e}on and Nickel, Maximilian},
  year = {2019},
  month = jul,
  institution = {{Bioinformatics}},
  doi = {10.1101/689547},
  abstract = {The need to understand cell developmental processes spawned a plethora of computational methods for discovering hierarchies from scRNAseq data. However, existing techniques are based on Euclidean geometry, a suboptimal choice for modeling complex cell trajectories with multiple branches. To overcome this fundamental representation issue we propose Poincar\'e maps, a method that harness the power of hyperbolic geometry into the realm of single-cell data analysis. Often understood as a continuous extension of trees, hyperbolic geometry enables the embedding of complex hierarchical data in only two dimensions while preserving the pairwise distances between points in the hierarchy. This enables direct exploratory analysis and the use of our embeddings in a wide variety of downstream data analysis tasks, such as visualization, clustering, lineage detection and pseudo-time inference. When compared to existing methods \textemdash unable to address all these important tasks using a single embedding\textemdash{} Poincar\'e maps produce state-of-the-art two-dimensional representations of cell trajectories on multiple scRNAseq datasets. More specifically, we demonstrate that Poincar\'e maps allow in a straightforward manner to formulate new hypotheses about biological processes unbeknown to prior methods.},
  file = {/home/trung/GoogleDrive/Zotero/klimovskaia et al_2019_poincaré maps for analyzing complex hierarchies in single-cell data.pdf},
  language = {en},
  type = {Preprint}
}

@article{klindt20_NonlinearDisentanglementNatural,
  title = {Towards {{Nonlinear Disentanglement}} in {{Natural Data}} with {{Temporal Sparse Coding}}},
  author = {Klindt, David and Schott, Lukas and Sharma, Yash and Ustyuzhaninov, Ivan and Brendel, Wieland and Bethge, Matthias and Paiton, Dylan},
  year = {2020},
  month = jul,
  abstract = {We construct an unsupervised learning model that achieves nonlinear disentanglement of underlying factors of variation in naturalistic videos. Previous work suggests that representations can be disentangled if all but a few factors in the environment stay constant at any point in time. As a result, algorithms proposed for this problem have only been tested on carefully constructed datasets with this exact property, leaving it unclear whether they will transfer to natural scenes. Here we provide evidence that objects in segmented natural movies undergo transitions that are typically small in magnitude with occasional large jumps, which is characteristic of a temporally sparse distribution. We leverage this finding and present SlowVAE, a model for unsupervised representation learning that uses a sparse prior on temporally adjacent observations to disentangle generative factors without any assumptions on the number of changing factors. We provide a proof of identifiability and show that the model reliably learns disentangled representations on several established benchmark datasets, often surpassing the current state-of-the-art. We additionally demonstrate transferability towards video datasets with natural dynamics, Natural Sprites and KITTI Masks, which we contribute as benchmarks for guiding disentanglement research towards more natural data domains.},
  archivePrefix = {arXiv},
  eprint = {2007.10930},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/klindt et al_2020_towards nonlinear disentanglement in natural data with temporal sparse coding.pdf},
  journal = {arXiv:2007.10930 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{klinger20_StudyCompositionalGeneralization,
  title = {A {{Study}} of {{Compositional Generalization}} in {{Neural Models}}},
  author = {Klinger, Tim and Adjodah, Dhaval and Marois, Vincent and Joseph, Josh and Riemer, Matthew and Pentland, Alex 'Sandy' and Campbell, Murray},
  year = {2020},
  month = jul,
  abstract = {Compositional and relational learning is a hallmark of human intelligence, but one which presents challenges for neural models. One difficulty in the development of such models is the lack of benchmarks with clear compositional and relational task structure on which to systematically evaluate them. In this paper, we introduce an environment called ConceptWorld, which enables the generation of images from compositional and relational concepts, defined using a logical domain specific language. We use it to generate images for a variety of compositional structures: 2x2 squares, pentominoes, sequences, scenes involving these objects, and other more complex concepts. We perform experiments to test the ability of standard neural architectures to generalize on relations with compositional arguments as the compositional depth of those arguments increases and under substitution. We compare standard neural networks such as MLP, CNN and ResNet, as well as state-of-the-art relational networks including WReN and PrediNet in a multi-class image classification setting. For simple problems, all models generalize well to close concepts but struggle with longer compositional chains. For more complex tests involving substitutivity, all models struggle, even with short chains. In highlighting these difficulties and providing an environment for further experimentation, we hope to encourage the development of models which are able to generalize effectively in compositional, relational domains.},
  archivePrefix = {arXiv},
  eprint = {2006.09437},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/klinger et al_2020_a study of compositional generalization in neural models.pdf},
  journal = {arXiv:2006.09437 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{klushyn19_IncreasingGeneralisationCapacity,
  title = {Increasing the {{Generalisation Capacity}} of {{Conditional VAEs}}},
  author = {Klushyn, Alexej and Chen, Nutan and Cseke, Botond and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2019},
  month = aug,
  abstract = {We address the problem of one-to-many mappings in supervised learning, where a single instance has many different solutions of possibly equal cost. The framework of conditional variational autoencoders describes a class of methods to tackle such structured-prediction tasks by means of latent variables. We propose to incentivise informative latent representations for increasing the generalisation capacity of conditional variational autoencoders. To this end, we modify the latent variable model by defining the likelihood as a function of the latent variable only and introduce an expressive multimodal prior to enable the model for capturing semantically meaningful features of the data. To validate our approach, we train our model on the Cornell Robot Grasping dataset, and modified versions of MNIST and Fashion-MNIST obtaining results that show a significantly higher generalisation capability.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1908.08750},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/klushyn et al_2019_increasing the generalisation capacity of conditional vaes.pdf;/home/trung/Zotero/storage/A64QBR8N/1908.html},
  journal = {arXiv:1908.08750 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{klushyn19_IncreasingGeneralisationCapacitya,
  title = {Increasing the {{Generalisation Capacity}} of {{Conditional VAEs}}},
  author = {Klushyn, Alexej and Chen, Nutan and Cseke, Botond and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2019},
  month = sep,
  abstract = {We address the problem of one-to-many mappings in supervised learning, where a single instance has many different solutions of possibly equal cost. The framework of conditional variational autoencoders describes a class of methods to tackle such structured-prediction tasks by means of latent variables. We propose to incentivise informative latent representations for increasing the generalisation capacity of conditional variational autoencoders. To this end, we modify the latent variable model by defining the likelihood as a function of the latent variable only and introduce an expressive multimodal prior to enable the model for capturing semantically meaningful features of the data. To validate our approach, we train our model on the Cornell Robot Grasping dataset, and modified versions of MNIST and Fashion-MNIST obtaining results that show a significantly higher generalisation capability.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1908.08750},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/klushyn et al_2019_increasing the generalisation capacity of conditional vaes2.pdf},
  journal = {arXiv:1908.08750 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{klushyn19_IncreasingGeneralisationCapacityb,
  title = {Increasing the {{Generalisation Capacity}} of {{Conditional VAEs}}},
  author = {Klushyn, Alexej and Chen, Nutan and Cseke, Botond and Bayer, Justin and {van der Smagt}, Patrick},
  year = {2019},
  month = sep,
  abstract = {We address the problem of one-to-many mappings in supervised learning, where a single instance has many different solutions of possibly equal cost. The framework of conditional variational autoencoders describes a class of methods to tackle such structured-prediction tasks by means of latent variables. We propose to incentivise informative latent representations for increasing the generalisation capacity of conditional variational autoencoders. To this end, we modify the latent variable model by defining the likelihood as a function of the latent variable only and introduce an expressive multimodal prior to enable the model for capturing semantically meaningful features of the data. To validate our approach, we train our model on the Cornell Robot Grasping dataset, and modified versions of MNIST and Fashion-MNIST obtaining results that show a significantly higher generalisation capability.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1908.08750},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/false},
  journal = {arXiv:1908.08750 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{klushyn19_LearningHierarchicalPriorsa,
  title = {Learning {{Hierarchical Priors}} in {{VAEs}}},
  author = {Klushyn, Alexej and Chen, Nutan and Kurle, Richard and Cseke, Botond and {van der Smagt}, Patrick},
  year = {2019},
  month = oct,
  abstract = {We propose to learn a hierarchical prior in the context of variational autoencoders to avoid the over-regularisation resulting from a standard normal prior distribution. To incentivise an informative latent representation of the data, we formulate the learning problem as a constrained optimisation problem by extending the Taming VAEs framework to two-level hierarchical models. We introduce a graph-based interpolation method, which shows that the topology of the learned latent representation corresponds to the topology of the data manifold---and present several examples, where desired properties of latent representation such as smoothness and simple explanatory factors are learned by the prior.},
  archivePrefix = {arXiv},
  eprint = {1905.04982},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/false},
  journal = {arXiv:1905.04982 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{knagg19_intuitiveguideGaussian,
  title = {An Intuitive Guide to {{Gaussian}} Processes},
  author = {Knagg, Oscar},
  year = {2019},
  month = jan,
  abstract = {A maths-free explanation of an underappreciated algorithm},
  file = {/home/trung/Zotero/storage/5KGXTU6N/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d.html},
  howpublished = {https://towardsdatascience.com/an-intuitive-guide-to-gaussian-processes-ec2f0b45c71d},
  journal = {Medium},
  language = {en}
}

@article{knoblauch19_GeneralizedVariationalInference,
  title = {Generalized {{Variational Inference}}: {{Three}} Arguments for Deriving New {{Posteriors}}},
  shorttitle = {Generalized {{Variational Inference}}},
  author = {Knoblauch, Jeremias and Jewson, Jack and Damoulas, Theodoros},
  year = {2019},
  month = dec,
  abstract = {We advocate an optimization-centric view on and introduce a novel generalization of Bayesian inference. Our inspiration is the representation of Bayes' rule as infinite-dimensional optimization problem (Csiszar, 1975; Donsker and Varadhan; 1975, Zellner; 1988). First, we use it to prove an optimality result of standard Variational Inference (VI): Under the proposed view, the standard Evidence Lower Bound (ELBO) maximizing VI posterior is preferable to alternative approximations of the Bayesian posterior. Next, we argue for generalizing standard Bayesian inference. The need for this arises in situations of severe misalignment between reality and three assumptions underlying standard Bayesian inference: (1) Well-specified priors, (2) well-specified likelihoods, (3) the availability of infinite computing power. Our generalization addresses these shortcomings with three arguments and is called the Rule of Three (RoT). We derive it axiomatically and recover existing posteriors as special cases, including the Bayesian posterior and its approximation by standard VI. In contrast, approximations based on alternative ELBO-like objectives violate the axioms. Finally, we study a special case of the RoT that we call Generalized Variational Inference (GVI). GVI posteriors are a large and tractable family of belief distributions specified by three arguments: A loss, a divergence and a variational family. GVI posteriors have appealing properties, including consistency and an interpretation as approximate ELBO. The last part of the paper explores some attractive applications of GVI in popular machine learning models, including robustness and more appropriate marginals. After deriving black box inference schemes for GVI posteriors, their predictive performance is investigated on Bayesian Neural Networks and Deep Gaussian Processes, where GVI can comprehensively improve upon existing methods.},
  archivePrefix = {arXiv},
  eprint = {1904.02063},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/knoblauch et al_2019_generalized variational inference.pdf},
  journal = {arXiv:1904.02063 [cs, stat]},
  keywords = {_tablet,favorite,representation},
  primaryClass = {cs, stat}
}

@article{knollmuller20_BayesianReasoningDeepLearned,
  title = {Bayesian {{Reasoning}} with {{Deep}}-{{Learned Knowledge}}},
  author = {Knollm{\"u}ller, Jakob and En{\ss}lin, Torsten},
  year = {2020},
  month = jan,
  abstract = {We access the internalized understanding of trained, deep neural networks to perform Bayesian reasoning on complex tasks. Independently trained networks are arranged to jointly answer questions outside their original scope, which are formulated in terms of a Bayesian inference problem. We solve this approximately with variational inference, which provides uncertainty on the outcomes. We demonstrate how following tasks can be approached this way: Combining independently trained networks to sample from a conditional generator, solving riddles involving multiple constraints simultaneously, and combine deep-learned knowledge with conventional noisy measurements in the context of high-resolution images of human faces.},
  archivePrefix = {arXiv},
  eprint = {2001.11031},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/AZ5JQWXB/Knollmüller and Enßlin - 2020 - Bayesian Reasoning with Deep-Learned Knowledge.pdf},
  journal = {arXiv:2001.11031 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kobyzev19_NormalizingFlowsIntroduction,
  title = {Normalizing {{Flows}}: {{Introduction}} and {{Ideas}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon and Brubaker, Marcus A.},
  year = {2019},
  month = aug,
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archivePrefix = {arXiv},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kobyzev et al_2019_normalizing flows.pdf;/home/trung/Zotero/storage/HIP3B8Z8/1908.html},
  journal = {arXiv:1908.09257 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,flow,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{kobyzev20_NormalizingFlowsIntroduction,
  title = {Normalizing {{Flows}}: {{An Introduction}} and {{Review}} of {{Current Methods}}},
  shorttitle = {Normalizing {{Flows}}},
  author = {Kobyzev, Ivan and Prince, Simon J. D. and Brubaker, Marcus A.},
  year = {2020},
  pages = {1--1},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2020.2992934},
  abstract = {Normalizing Flows are generative models which produce tractable distributions where both sampling and density evaluation can be efficient and exact. The goal of this survey article is to give a coherent and comprehensive review of the literature around the construction and use of Normalizing Flows for distribution learning. We aim to provide context and explanation of the models, review current state-of-the-art literature, and identify open questions and promising future directions.},
  archivePrefix = {arXiv},
  eprint = {1908.09257},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kobyzev et al_2020_normalizing flows.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en}
}

@article{kochurov00_PyMC4ExploitingCoroutines,
  title = {{{PyMC4}}: {{Exploiting Coroutines}} for {{Implementing}} a {{Probabilistic Programming Framework}}},
  author = {Kochurov, Max and Wiecki, Thomas and Carroll, Colin and Lao, Junpeng},
  pages = {4},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/PNU3DI9B/Kochurov et al. - PyMC4 Exploiting Coroutines for Implementing a Pr.pdf},
  language = {en}
}

@misc{koerth20_WhyItFreaking,
  title = {Why {{It}}'s {{So Freaking Hard To Make A Good COVID}}-19 {{Model}}},
  author = {Koerth, Maggie},
  year = {2020},
  month = mar,
  abstract = {Here we are, in the middle of a pandemic, staring out our living room windows like aquarium fish. The question on everybody's minds: How bad will this really ge\ldots},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/FIU9XRTU/why-its-so-freaking-hard-to-make-a-good-covid-19-model.html},
  journal = {FiveThirtyEight},
  language = {en-US}
}

@article{kohl19_ProbabilisticUNetSegmentation,
  title = {A {{Probabilistic U}}-{{Net}} for {{Segmentation}} of {{Ambiguous Images}}},
  author = {Kohl, Simon A. A. and {Romera-Paredes}, Bernardino and Meyer, Clemens and De Fauw, Jeffrey and Ledsam, Joseph R. and {Maier-Hein}, Klaus H. and Eslami, S. M. Ali and Rezende, Danilo Jimenez and Ronneberger, Olaf},
  year = {2019},
  month = jan,
  abstract = {Many real-world vision problems suffer from inherent ambiguities. In clinical applications for example, it might not be clear from a CT scan alone which particular region is cancer tissue. Therefore a group of graders typically produces a set of diverse but plausible segmentations. We consider the task of learning a distribution over segmentations given an input. To this end we propose a generative segmentation model based on a combination of a U-Net with a conditional variational autoencoder that is capable of efficiently producing an unlimited number of plausible hypotheses. We show on a lung abnormalities segmentation task and on a Cityscapes segmentation task that our model reproduces the possible segmentation variants as well as the frequencies with which they occur, doing so significantly better than published approaches. These models could have a high impact in real-world applications, such as being used as clinical decision-making algorithms accounting for multiple plausible semantic segmentation hypotheses to provide possible diagnoses and recommend further actions to resolve the present ambiguities.},
  archivePrefix = {arXiv},
  eprint = {1806.05034},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/D43IQV2Q/Kohl et al. - 2019 - A Probabilistic U-Net for Segmentation of Ambiguou.pdf},
  journal = {arXiv:1806.05034 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kolchinsky19_NonlinearInformationBottleneck,
  title = {Nonlinear {{Information Bottleneck}}},
  author = {Kolchinsky, Artemy and Tracey, Brendan D. and Wolpert, David H.},
  year = {2019},
  volume = {21},
  issn = {1099-4300},
  doi = {10.3390/e21121181},
  abstract = {Information bottleneck (IB) is a technique for extracting information in one random variable X that is relevant for predicting another random variable Y. IB works by encoding X in a compressed \&ldquo;bottleneck\&rdquo; random variable M from which Y can be accurately decoded. However, finding the optimal bottleneck variable involves a difficult optimization problem, which until recently has been considered for only two limited cases: discrete X and Y with small state spaces, and continuous X and Y with a Gaussian joint distribution (in which case optimal encoding and decoding maps are linear). We propose a method for performing IB on arbitrarily-distributed discrete and/or continuous X and Y, while allowing for nonlinear encoding and decoding maps. Our approach relies on a novel non-parametric upper bound for mutual information. We describe how to implement our method using neural networks. We then show that it achieves better performance than the recently-proposed \&ldquo;variational IB\&rdquo; method on several real-world datasets.},
  file = {/home/trung/GoogleDrive/Zotero/kolchinsky et al_2019_nonlinear information bottleneck.pdf},
  journal = {Entropy},
  keywords = {information,information bottleneck,mutual information,neural networks,representation learning},
  number = {12}
}

@book{koller09_Probabilisticgraphicalmodels,
  title = {Probabilistic Graphical Models: Principles and Techniques},
  shorttitle = {Probabilistic Graphical Models},
  author = {Koller, Daphne and Friedman, Nir},
  year = {2009},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA}},
  annotation = {ZSCC: 0006933},
  file = {/home/trung/GoogleDrive/Zotero/koller et al_2009_probabilistic graphical models.pdf},
  isbn = {978-0-262-01319-2},
  keywords = {favorite,Graphic methods,Graphical modeling (Statistics)},
  language = {en},
  lccn = {QA279.5 .K65 2009},
  series = {Adaptive Computation and Machine Learning}
}

@misc{kolter00_Uniformconvergencemay,
  title = {Uniform Convergence May Be Unable to Explain Generalization in Deep Learning},
  author = {Kolter, Zico},
  abstract = {Empirical and theoretical evidence demonstrating that uniform convergence based generalization bounds may be meaningless for overparameterized deep networks trained by stochastic gradient descent.},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/Zotero/storage/PSVSZJLT/2019-07-09-uniform-convergence.html},
  howpublished = {http://locuslab.github.io/2019-07-09-uniform-convergence/},
  language = {en}
}

@article{komatsuzaki19_ExtractiveSummaryDiscrete,
  title = {Extractive {{Summary}} as {{Discrete Latent Variables}}},
  author = {Komatsuzaki, Aran},
  year = {2019},
  month = jan,
  abstract = {In this paper, we compare various methods to compress a text using a neural model. We find that extracting tokens as latent variables significantly outperforms the state-of-the-art discrete latent variable models such as VQ-VAE. Furthermore, we compare various extractive compression schemes. There are two best-performing methods that perform equally. One method is to simply choose the tokens with the highest tf-idf scores. Another is to train a bidirectional language model similar to ELMo and choose the tokens with the highest loss. If we consider any subsequence of a text to be a text in a broader sense, we conclude that language is a strong compression code of itself. Our finding justifies the high quality of generation achieved with hierarchical method, as their latent variables are nothing but natural language summary. We also conclude that there is a hierarchy in language such that an entire text can be predicted much more easily based on a sequence of a small number of keywords, which can be easily found by classical methods as tf-idf. We speculate that this extraction process may be useful for unsupervised hierarchical text generation.},
  archivePrefix = {arXiv},
  eprint = {1811.05542},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/komatsuzaki_2019_extractive summary as discrete latent variables.pdf},
  journal = {arXiv:1811.05542 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kong20_DiffWaveVersatileDiffusion,
  title = {{{DiffWave}}: {{A Versatile Diffusion Model}} for {{Audio Synthesis}}},
  shorttitle = {{{DiffWave}}},
  author = {Kong, Zhifeng and Ping, Wei and Huang, Jiaji and Zhao, Kexin and Catanzaro, Bryan},
  year = {2020},
  month = sep,
  abstract = {In this work, we propose DiffWave, a versatile Diffusion probabilistic model for conditional and unconditional Waveform generation. The model is non-autoregressive, and converts the white noise signal into structured waveform through a Markov chain with a constant number of steps at synthesis. It is efficiently trained by optimizing a variant of variational bound on the data likelihood. DiffWave produces high-fidelity audios in Different Waveform generation tasks, including neural vocoding conditioned on mel spectrogram, class-conditional generation, and unconditional generation. We demonstrate that DiffWave matches a strong WaveNet vocoder in terms of speech quality\textasciitilde (MOS: 4.44 versus 4.43), while synthesizing orders of magnitude faster. In particular, it significantly outperforms autoregressive and GAN-based waveform models in the challenging unconditional generation task in terms of audio quality and sample diversity from various automatic and human evaluations.},
  archivePrefix = {arXiv},
  eprint = {2009.09761},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kong et al_2020_diffwave.pdf},
  journal = {arXiv:2009.09761 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{kong20_HiFiGANGenerativeAdversarial,
  title = {{{HiFi}}-{{GAN}}: {{Generative Adversarial Networks}} for {{Efficient}} and {{High Fidelity Speech Synthesis}}},
  shorttitle = {{{HiFi}}-{{GAN}}},
  author = {Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  year = {2020},
  month = oct,
  abstract = {Several recent studies on speech synthesis have employed generative adversarial networks (GANs) to produce raw waveforms. Although such methods improve the sampling efficiency and memory usage, their sample quality has not yet reached that of autoregressive and flow-based generative models. In this study, we propose HiFi-GAN, which achieves both efficient and high-fidelity speech synthesis. As speech audio consists of sinusoidal signals with various periods, we demonstrate that modeling periodic patterns of an audio is crucial for enhancing sample quality. A subjective human evaluation (mean opinion score, MOS) of a single speaker dataset indicates that our proposed method demonstrates similarity to human quality while generating 22.05 kHz high-fidelity audio 167.9 times faster than real-time on a single V100 GPU. We further show the generality of HiFi-GAN to the mel-spectrogram inversion of unseen speakers and end-to-end speech synthesis. Finally, a small footprint version of HiFi-GAN generates samples 13.4 times faster than real time on CPU with comparable quality to an autoregressive counterpart.},
  archivePrefix = {arXiv},
  eprint = {2010.05646},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kong et al_2020_hifi-gan.pdf},
  journal = {arXiv:2010.05646 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{konstantinos20_MLEngineeringBrief,
  title = {Towards {{ML Engineering}}: {{A Brief History Of TensorFlow Extended}} ({{TFX}})},
  shorttitle = {Towards {{ML Engineering}}},
  author = {Konstantinos and Katsiapis and Karmarkar, Abhijit and Altay, Ahmet and Zaks, Aleksandr and Polyzotis, Neoklis and Ramesh, Anusha and Mathes, Ben and Vasudevan, Gautam and Giannoumis, Irene and Wilkiewicz, Jarek and Simsa, Jiri and Hong, Justin and Trott, Mitch and Lutz, No{\'e} and Dournov, Pavel A. and Crowe, Robert and Sirajuddin, Sarah and Warkentin, Tris Brian and Li, Zhitao},
  year = {2020},
  month = sep,
  abstract = {Software Engineering, as a discipline, has matured over the past 5+ decades. The modern world heavily depends on it, so the increased maturity of Software Engineering was an eventuality. Practices like testing and reliable technologies help make Software Engineering reliable enough to build industries upon. Meanwhile, Machine Learning (ML) has also grown over the past 2+ decades. ML is used more and more for research, experimentation and production workloads. ML now commonly powers widely-used products integral to our lives. But ML Engineering, as a discipline, has not widely matured as much as its Software Engineering ancestor. Can we take what we have learned and help the nascent field of applied ML evolve into ML Engineering the way Programming evolved into Software Engineering [1]? In this article we will give a whirlwind tour of Sibyl [2] and TensorFlow Extended (TFX) [3], two successive end-to-end (E2E) ML platforms at Alphabet. We will share the lessons learned from over a decade of applied ML built on these platforms, explain both their similarities and their differences, and expand on the shifts (both mental and technical) that helped us on our journey. In addition, we will highlight some of the capabilities of TFX that help realize several aspects of ML Engineering. We argue that in order to unlock the gains ML can bring, organizations should advance the maturity of their ML teams by investing in robust ML infrastructure and promoting ML Engineering education. We also recommend that before focusing on cutting-edge ML modeling techniques, product leaders should invest more time in adopting interoperable ML platforms for their organizations. In closing, we will also share a glimpse into the future of TFX.},
  archivePrefix = {arXiv},
  eprint = {2010.02013},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/konstantinos et al_2020_towards ml engineering.pdf},
  journal = {arXiv:2010.02013 [cs]},
  primaryClass = {cs}
}

@article{kool00_StochasticBeamsWhere,
  title = {Stochastic {{Beams}} and {{Where}} to {{Find Them}}: {{The Gumbel}}-{{Top}}-k {{Trick}} for {{Sampling Sequences Without Replacement}}},
  author = {Kool, Wouter and {van Hoof}, Herke and Welling, Max},
  pages = {14},
  annotation = {ZSCC: 0000003},
  file = {/home/trung/GoogleDrive/Zotero/kool et al_stochastic beams and where to find them.pdf},
  keywords = {beam search,variational},
  language = {en}
}

@article{korshunova19_DiscriminativeTopicModeling,
  title = {Discriminative {{Topic Modeling}} with {{Logistic LDA}}},
  author = {Korshunova, Iryna and Xiong, Hanchen and Fedoryszak, Mateusz and Theis, Lucas},
  year = {2019},
  month = sep,
  abstract = {Despite many years of research into latent Dirichlet allocation (LDA), applying LDA to collections of non-categorical items is still challenging. Yet many problems with much richer data share a similar structure and could benefit from the vast literature on LDA. We propose logistic LDA, a novel discriminative variant of latent Dirichlet allocation which is easy to apply to arbitrary inputs. In particular, our model can easily be applied to groups of images, arbitrary text embeddings, and integrate well with deep neural networks. Although it is a discriminative model, we show that logistic LDA can learn from unlabeled data in an unsupervised manner by exploiting the group structure present in the data. In contrast to other recent topic models designed to handle arbitrary inputs, our model does not sacrifice the interpretability and principled motivation of LDA.},
  archivePrefix = {arXiv},
  eprint = {1909.01436},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/korshunova et al_2019_discriminative topic modeling with logistic lda.pdf;/home/trung/Zotero/storage/98B9BAK8/1909.html},
  journal = {arXiv:1909.01436 [cs, stat]},
  keywords = {Computer Science - Machine Learning,information,LDA,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{korsunsky19_Fastsensitiveaccurate,
  title = {Fast, Sensitive and Accurate Integration of Single-Cell Data with {{Harmony}}},
  author = {Korsunsky, Ilya and Millard, Nghia and Fan, Jean and Slowikowski, Kamil and Zhang, Fan and Wei, Kevin and Baglaenko, Yuriy and Brenner, Michael and Loh, Po-ru and Raychaudhuri, Soumya},
  year = {2019},
  month = nov,
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0619-0},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/korsunsky et al_2019_fast, sensitive and accurate integration of single-cell data with harmony.pdf},
  journal = {Nature Methods},
  language = {en}
}

@article{kosiorek19_StackedCapsuleAutoencoders,
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam R. and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E.},
  year = {2019},
  month = jun,
  abstract = {An object can be seen as a geometrically organized set of interrelated parts. A system that makes explicit use of these geometric relationships to recognize objects should be naturally robust to changes in viewpoint, because the intrinsic geometric relationships are viewpoint-invariant. We describe an unsupervised version of capsule networks, in which a neural encoder, which looks at all of the parts, is used to infer the presence and poses of object capsules. The encoder is trained by backpropagating through a decoder, which predicts the pose of each already discovered part using a mixture of pose predictions. The parts are discovered directly from an image, in a similar manner, by using a neural encoder, which infers parts and their affine transformations. The corresponding decoder models each image pixel as a mixture of predictions made by affine-transformed parts. We learn object- and their part-capsules on unlabeled data, and then cluster the vectors of presences of object capsules. When told the names of these clusters, we achieve state-of-the-art results for unsupervised classification on SVHN (55\%) and near state-of-the-art on MNIST (98.5\%).},
  archivePrefix = {arXiv},
  eprint = {1906.06818},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Kosiorek et al_2019_Stacked Capsule Autoencoders6.pdf},
  journal = {arXiv:1906.06818 [cs, stat]},
  keywords = {capsule,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,transfer learning},
  primaryClass = {cs, stat}
}

@article{kouw19_introductiondomainadaptation,
  title = {An Introduction to Domain Adaptation and Transfer Learning},
  author = {Kouw, Wouter M. and Loog, Marco},
  year = {2019},
  month = jan,
  abstract = {In machine learning, if the training data is an unbiased sample of an underlying distribution, then the learned classification function will make accurate predictions for new samples. However, if the training data is not an unbiased sample, then there will be differences between how the training data is distributed and how the test data is distributed. Standard classifiers cannot cope with changes in data distributions between training and test phases, and will not perform well. Domain adaptation and transfer learning are sub-fields within machine learning that are concerned with accounting for these types of changes. Here, we present an introduction to these fields, guided by the question: when and how can a classifier generalize from a source to a target domain? We will start with a brief introduction into risk minimization, and how transfer learning and domain adaptation expand upon this framework. Following that, we discuss three special cases of data set shift, namely prior, covariate and concept shift. For more complex domain shifts, there are a wide variety of approaches. These are categorized into: importance-weighting, subspace mapping, domain-invariant spaces, feature augmentation, minimax estimators and robust algorithms. A number of points will arise, which we will discuss in the last section. We conclude with the remark that many open questions will have to be addressed before transfer learners and domain-adaptive classifiers become practical.},
  archivePrefix = {arXiv},
  eprint = {1812.11806},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kouw et al_2019_an introduction to domain adaptation and transfer learning.pdf},
  journal = {arXiv:1812.11806 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{koyama20_OutofDistributionGeneralizationMaximal,
  title = {Out-of-{{Distribution Generalization}} with {{Maximal Invariant Predictor}}},
  author = {Koyama, Masanori and Yamaguchi, Shoichiro},
  year = {2020},
  month = aug,
  abstract = {Out-of-Distribution (OOD) generalization problem is a problem of seeking the predictor function whose performance in the worst environments is optimal. This paper makes two contributions to OOD problem. We first use the basic results of probability to prove Maximal Invariant Predictor(MIP) condition, a theoretical result that can be used to identify the OOD optimal solution. We then use our MIP to derive Inter-environmental Gradient Alignment (IGA) algorithm that can be used to help seek the OOD optimal predictor. Previous studies that have investigated the theoretical aspect of the OOD problem use strong structural assumptions such as causal DAG. However, in cases involving image datasets, for example, the identification of hidden structural relations is itself a difficult problem. Our theoretical results are different from those of many previous studies in that it can be applied to cases in which the underlying structure of dataset is difficult to analyze. We present an extensive comparison of previous theoretical approaches to the OOD problems based on the assumptions they make. We also present an extension of the Colored-MNIST that can more accurately represent the pathological OOD situation than the original version, and demonstrate the superiority of IGA over previous methods on both the original and the extended version of Colored-MNIST.},
  archivePrefix = {arXiv},
  eprint = {2008.01883},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/koyama et al_2020_out-of-distribution generalization with maximal invariant predictor.pdf},
  journal = {arXiv:2008.01883 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{krenn19_PredictingResearchTrends,
  title = {Predicting {{Research Trends}} with {{Semantic}} and {{Neural Networks}} with an Application in {{Quantum Physics}}},
  author = {Krenn, Mario and Zeilinger, Anton},
  year = {2019},
  month = jun,
  abstract = {The vast and growing number of publications in all disciplines of science cannot be comprehended by a single human researcher. As a consequence, researchers have to specialize in narrow sub-disciplines, which makes it challenging to uncover scientific connections beyond the own field of research. Thus access to structured knowledge from a large corpus of publications could help pushing the frontiers of science. Here we demonstrate a method to build a semantic network from published scientific literature, which we call SemNet. We use SemNet to predict future trends in research and to inspire new, personalized and surprising seeds of ideas in science. We apply it in the discipline of quantum physics, which has seen an unprecedented growth of activity in recent years. In SemNet, scientific knowledge is represented as an evolving network using the content of 750,000 scientific papers published since 1919. The nodes of the network correspond to physical concepts, and links between two nodes are drawn when two physical concepts are concurrently studied in research articles. We identify influential and prize-winning research topics from the past inside SemNet thus confirm that it stores useful semantic knowledge. We train a deep neural network using states of SemNet of the past, to predict future developments in quantum physics research, and confirm high quality predictions using historic data. With the neural network and theoretical network tools we are able to suggest new, personalized, out-of-the-box ideas, by identifying pairs of concepts which have unique and extremal semantic network properties. Finally, we consider possible future developments and implications of our findings.},
  archivePrefix = {arXiv},
  eprint = {1906.06843},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/krenn et al_2019_predicting research trends with semantic and neural networks with an application in quantum physics.pdf;/home/trung/Zotero/storage/E9CNF2DD/1906.html},
  journal = {arXiv:1906.06843 [physics, physics:quant-ph]},
  primaryClass = {physics, physics:quant-ph}
}

@article{krieken19_SemiSupervisedLearningUsing,
  title = {Semi-{{Supervised Learning Using Differentiable Reasoning}}},
  author = {Krieken, Emile Van},
  year = {2019},
  pages = {18},
  abstract = {We introduce Differentiable Reasoning (DR), a novel semi-supervised learning technique which uses relational background knowledge to benefit from unlabeled data. We apply it to the Semantic Image Interpretation (SII) task and show that background knowledge provides significant improvement. We find that there is a strong but interesting imbalance between the contributions of updates from Modus Ponens (MP) and its logical equivalent Modus Tollens (MT) to the learning process, suggesting that our approach is very sensitive to a phenomenon called the Raven Paradox [10]. We propose a solution to overcome this situation.},
  file = {/home/trung/GoogleDrive/Zotero/krieken_2019_semi-supervised learning using differentiable reasoning.pdf},
  keywords = {_tablet,favorite},
  language = {en}
}

@inproceedings{krishnan18_challengeslearninginference,
  title = {On the Challenges of Learning with Inference Networks on Sparse, High-Dimensional Data},
  author = {Krishnan, Rahul and Liang, Dawen and Hoffman, Matthew},
  editor = {Storkey, Amos and {Perez-Cruz}, Fernando},
  year = {2018},
  month = apr,
  volume = {84},
  pages = {143--151},
  publisher = {{PMLR}},
  address = {{Playa Blanca, Lanzarote, Canary Islands}},
  abstract = {We study parameter estimation in Nonlinear Factor Analysis (NFA) where the generative model is parameterized by a deep neural network. Recent work has focused on learning such models using inference (or recognition) networks; we identify a crucial problem when modeling large, sparse, high-dimensional datasets \textendash{} underfitting. We study the extent of underfitting, highlighting that its severity increases with the sparsity of the data. We propose methods to tackle it via iterative optimization inspired by stochastic variational inference (Hoffman et al., 2013) and improvements in the data representation used for inference. The proposed techniques drastically improve the ability of these powerful models to fit sparse data, achieving state-of-the-art results on a benchmark text-count dataset and excellent results on the task of top-N recommendation.},
  file = {/home/trung/GoogleDrive/Zotero/krishnan et al_2018_on the challenges of learning with inference networks on sparse, high-dimensional data.pdf},
  keywords = {favorite,vae_issues},
  pdf = {http://proceedings.mlr.press/v84/krishnan18a/krishnan18a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{krol20_VeganAthleteHeart,
  title = {A {{Vegan Athlete}}'s {{Heart}}\textemdash{{Is It Different}}? {{Morphology}} and {{Function}} in {{Echocardiography}}},
  shorttitle = {A {{Vegan Athlete}}'s {{Heart}}\textemdash{{Is It Different}}?},
  author = {Kr{\'o}l, Wojciech and Price, Szymon and {\'S}li{\.z}, Daniel and Parol, Damian and Konopka, Marcin and Mamcarz, Artur and We{\l}nicki, Marcin and Braksator, Wojciech},
  year = {2020},
  month = jul,
  volume = {10},
  issn = {2075-4418},
  doi = {10.3390/diagnostics10070477},
  abstract = {Plant-based diets are a growing trend, including among athletes. This study compares the differences in physical performance and heart morphology and function between vegan and omnivorous amateur runners. A study group and a matched control group were recruited comprising N = 30 participants each. Eight members of the study group were excluded, leaving N = 22 participants. Members of both groups were of similar age and trained with similar frequency and intensity. Vegans displayed a higher VO2max (54.08 vs. 50.10 mL/kg/min, p {$<$} 0.05), which correlated positively with carbohydrate intake ({$\rho$} = 0.52) and negatively with MUFA (monounsaturated fatty acids) intake ({$\rho$} = -0.43). The vegans presented a more eccentric form of remodelling with greater left ventricular end diastolic diameter (LVEDd, 2.93 vs. 2.81 cm/m2, p = 0.04) and a lower relative wall thickness (RWT, 0.39 vs. 0.42, p = 0.04) and left ventricular mass (LVM, 190 vs. 210 g, p = 0.01). The left ventricular mass index (LVMI) was similar (108 vs. 115 g/m2, p = NS). Longitudinal strain was higher in the vegan group (-20.5 vs. -19.6\%, p = 0.04), suggesting better systolic function. Higher E-wave velocities (87 vs. 78 cm/s, p = 0.001) and E/e{${'}$} ratios (6.32 vs. 5.6, p = 0.03) may suggest better diastolic function in the vegan group. The results demonstrate that following a plant-based diet does not impair amateur athletes' performance and influences both morphological and functional heart remodelling. The lower RWT and better LV systolic and diastolic function are most likely positive echocardiographic findings.},
  file = {/home/trung/GoogleDrive/Zotero/król et al_2020_a vegan athlete’s heart—is it different.pdf},
  journal = {Diagnostics},
  number = {7},
  pmcid = {PMC7400409},
  pmid = {32674452}
}

@article{krueger20_HiddenIncentivesAutoInduced,
  title = {Hidden {{Incentives}} for {{Auto}}-{{Induced Distributional Shift}}},
  author = {Krueger, David and Maharaj, Tegan and Leike, Jan},
  year = {2020},
  month = sep,
  abstract = {Decisions made by machine learning systems have increasing influence on the world, yet it is common for machine learning algorithms to assume that no such influence exists. An example is the use of the i.i.d. assumption in content recommendation. In fact, the (choice of) content displayed can change users' perceptions and preferences, or even drive them away, causing a shift in the distribution of users. We introduce the term auto-induced distributional shift (ADS) to describe the phenomenon of an algorithm causing a change in the distribution of its own inputs. Our goal is to ensure that machine learning systems do not leverage ADS to increase performance when doing so could be undesirable. We demonstrate that changes to the learning algorithm, such as the introduction of meta-learning, can cause hidden incentives for auto-induced distributional shift (HI-ADS) to be revealed. To address this issue, we introduce `unit tests' and a mitigation strategy for HI-ADS, as well as a toy environment for modelling real-world issues with HI-ADS in content recommendation, where we demonstrate that strong meta-learners achieve gains in performance via ADS. We show meta-learning and Q-learning both sometimes fail unit tests, but pass when using our mitigation strategy.},
  archivePrefix = {arXiv},
  eprint = {2009.09153},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/krueger et al_2020_hidden incentives for auto-induced distributional shift.pdf},
  journal = {arXiv:2009.09153 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{kruengkrai19_BetterExploitingLatent,
  title = {Better {{Exploiting Latent Variables}} in {{Text Modeling}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kruengkrai, Canasai},
  year = {2019},
  pages = {5527--5532},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1553},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/C5SAXUBZ/Kruengkrai - 2019 - Better Exploiting Latent Variables in Text Modelin.pdf},
  keywords = {disentanglement,favorite},
  language = {en}
}

@article{kryscinski19_EvaluatingFactualConsistency,
  title = {Evaluating the {{Factual Consistency}} of {{Abstractive Text Summarization}}},
  author = {Kry{\'s}ci{\'n}ski, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard},
  year = {2019},
  month = oct,
  abstract = {Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.12840},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kryściński et al_2019_evaluating the factual consistency of abstractive text summarization.pdf;/home/trung/Zotero/storage/4MNEK4YY/1910.html},
  journal = {arXiv:1910.12840 [cs]},
  keywords = {Computer Science - Computation and Language,explain},
  primaryClass = {cs}
}

@article{kuleshov00_DeepHybridModels,
  title = {Deep {{Hybrid Models}}: {{Bridging Discriminative}} and {{Generative Approaches}}},
  author = {Kuleshov, Volodymyr and Ermon, Stefano},
  pages = {10},
  abstract = {Most methods in machine learning are described as either discriminative or generative. The former often attain higher predictive accuracy, while the latter are more strongly regularized and can deal with missing data. Here, we propose a new framework to combine a broad class of discriminative and generative models, interpolating between the two extremes with a multiconditional likelihood objective. Unlike previous approaches, we couple the two components through shared latent variables, and train using recent advances in variational inference. Instantiating our framework with modern deep architectures gives rise to deep hybrid models, a highly flexible family that generalizes several existing models and is effective in the semi-supervised setting, where it results in improvements over the state of the art on the SVHN dataset.},
  file = {/home/trung/GoogleDrive/Zotero/kuleshov et al_deep hybrid models.pdf},
  language = {en}
}

@article{kulkarni15_DeepConvolutionalInverse,
  title = {Deep {{Convolutional Inverse Graphics Network}}},
  author = {Kulkarni, Tejas D. and Whitney, Will and Kohli, Pushmeet and Tenenbaum, Joshua B.},
  year = {2015},
  month = jun,
  abstract = {This paper presents the Deep Convolution Inverse Graphics Network (DC-IGN), a model that learns an interpretable representation of images. This representation is disentangled with respect to transformations such as out-of-plane rotations and lighting variations. The DC-IGN model is composed of multiple layers of convolution and de-convolution operators and is trained using the Stochastic Gradient Variational Bayes (SGVB) algorithm [11]. We propose a training procedure to encourage neurons in the graphics code layer to represent a specific transformation (e.g. pose or light). Given a single input image, our model can generate new images of the same object with variations in pose and lighting. We present qualitative and quantitative results of the model's efficacy at learning a 3D rendering engine.},
  annotation = {ZSCC: 0000577},
  archivePrefix = {arXiv},
  eprint = {1503.03167},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kulkarni et al_2015_deep convolutional inverse graphics network.pdf},
  journal = {arXiv:1503.03167 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{kumar00_SemisupervisedLearningGANs,
  title = {Semi-Supervised {{Learning}} with {{GANs}}: {{Manifold Invariance}} with {{Improved Inference}}},
  author = {Kumar, Abhishek and Sattigeri, Prasanna and Fletcher, Tom},
  pages = {11},
  abstract = {Semi-supervised learning methods using Generative adversarial networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.},
  file = {/home/trung/GoogleDrive/Zotero/kumar et al_semi-supervised learning with gans.pdf},
  language = {en}
}

@article{kumar18_VariationalInferenceDisentangled,
  title = {Variational {{Inference}} of {{Disentangled Latent Concepts}} from {{Unlabeled Observations}}},
  author = {Kumar, Abhishek and Sattigeri, Prasanna and Balakrishnan, Avinash},
  year = {2018},
  month = dec,
  abstract = {Disentangled representations, where the higher level data generative factors are reflected in disjoint latent dimensions, offer several benefits such as ease of deriving invariant representations, transferability to other tasks, interpretability, etc. We consider the problem of unsupervised learning of disentangled representations from large pool of unlabeled observations, and propose a variational inference based approach to infer disentangled latent factors. We introduce a regularizer on the expectation of the approximate posterior over observed data that encourages the disentanglement. We also propose a new disentanglement metric which is better aligned with the qualitative disentanglement observed in the decoder's output. We empirically observe significant improvement over existing methods in terms of both disentanglement and data likelihood (reconstruction quality).},
  archivePrefix = {arXiv},
  eprint = {1711.00848},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kumar et al_2018_variational inference of disentangled latent concepts from unlabeled observations.pdf},
  journal = {arXiv:1711.00848 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{kumar19_DeepAttentiveRanking,
  title = {Deep {{Attentive Ranking Networks}} for {{Learning}} to {{Order Sentences}}},
  author = {Kumar, Pawan and Brahma, Dhanajit and Karnick, Harish and Rai, Piyush},
  year = {2019},
  month = dec,
  abstract = {We present an attention-based ranking framework for learning to order sentences given a paragraph. Our framework is built on a bidirectional sentence encoder and a self-attention based transformer network to obtain an input order invariant representation of paragraphs. Moreover, it allows seamless training using a variety of ranking based loss functions, such as pointwise, pairwise, and listwise ranking. We apply our framework on two tasks: Sentence Ordering and Order Discrimination. Our framework outperforms various state-of-the-art methods on these tasks on a variety of evaluation metrics. We also show that it achieves better results when using pairwise and listwise ranking losses, rather than the pointwise ranking loss, which suggests that incorporating relative positions of two or more sentences in the loss function contributes to better learning.},
  archivePrefix = {arXiv},
  eprint = {2001.00056},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kumar et al_2019_deep attentive ranking networks for learning to order sentences.pdf;/home/trung/Zotero/storage/J87QISLN/2001.html},
  journal = {arXiv:2001.00056 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{kumar19_MelGANGenerativeAdversarial,
  title = {{{MelGAN}}: {{Generative Adversarial Networks}} for {{Conditional Waveform Synthesis}}},
  shorttitle = {{{MelGAN}}},
  author = {Kumar, Kundan and Kumar, Rithesh and {de Boissiere}, Thibault and Gestin, Lucas and Teoh, Wei Zhen and Sotelo, Jose and {de Brebisson}, Alexandre and Bengio, Yoshua and Courville, Aaron},
  year = {2019},
  month = oct,
  abstract = {Previous works \textbackslash citep\{donahue2018adversarial, engel2019gansynth\} have found that generating coherent raw audio waveforms with GANs is challenging. In this paper, we show that it is possible to train GANs reliably to generate high quality coherent waveforms by introducing a set of architectural changes and simple training techniques. Subjective evaluation metric (Mean Opinion Score, or MOS) shows the effectiveness of the proposed approach for high quality mel-spectrogram inversion. To establish the generality of the proposed techniques, we show qualitative results of our model in speech synthesis, music domain translation and unconditional music synthesis. We evaluate the various components of the model through ablation studies and suggest a set of guidelines to design general purpose discriminators and generators for conditional sequence synthesis tasks. Our model is non-autoregressive, fully convolutional, with significantly fewer parameters than competing models and generalizes to unseen speakers for mel-spectrogram inversion. Our pytorch implementation runs at more than 100x faster than realtime on GTX 1080Ti GPU and more than 2x faster than real-time on CPU, without any hardware specific optimization tricks. Blog post with samples and accompanying code coming soon.},
  archivePrefix = {arXiv},
  eprint = {1910.06711},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kumar et al_2019_melgan.pdf;/home/trung/Zotero/storage/VWBB4S2H/1910.html},
  journal = {arXiv:1910.06711 [cs, eess]},
  keywords = {adversarial,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,gan},
  primaryClass = {cs, eess}
}

@article{kumar20_ImplicitRegularizationbetaVAEs,
  title = {On {{Implicit Regularization}} in Beta-{{VAEs}}},
  author = {Kumar, Abhishek and Poole, Ben},
  year = {2020},
  month = feb,
  abstract = {While the impact of variational inference (VI) on posterior inference in a fixed generative model is well-characterized, its role in regularizing a learned generative model when used in variational autoencoders (VAEs) is poorly understood. We study the regularizing effects of variational distributions on learning in generative models from two perspectives. First, we analyze the role that the choice of variational family plays in imparting uniqueness to the learned model by restricting the set of optimal generative models. Second, we study the regularization effect of the variational family on the local geometry of the decoding model. This analysis uncovers the regularizer implicit in the {$\beta$}-VAE objective, and leads to an approximation consisting of a deterministic autoencoding objective plus analytic regularizers that depend on the Hessian or Jacobian of the decoding model, unifying VAEs with recent heuristics proposed for training regularized autoencoders. We empirically verify these findings, observing that the proposed deterministic objective exhibits similar behavior to the {$\beta$}-VAE in terms of objective value and sample quality.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {2002.00041},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kumar et al_2020_on implicit regularization in beta-vaes.pdf},
  journal = {arXiv:2002.00041 [cs, stat]},
  keywords = {_tablet,disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kumar20_RegularizedAutoencodersRelaxed,
  title = {Regularized {{Autoencoders}} via {{Relaxed Injective Probability Flow}}},
  author = {Kumar, Abhishek and Poole, Ben and Murphy, Kevin},
  year = {2020},
  month = feb,
  abstract = {Invertible flow-based generative models are an effective method for learning to generate samples, while allowing for tractable likelihood computation and inference. However, the invertibility requirement restricts models to have the same latent dimensionality as the inputs. This imposes significant architectural, memory, and computational costs, making them more challenging to scale than other classes of generative models such as Variational Autoencoders (VAEs). We propose a generative model based on probability flows that does away with the bijectivity requirement on the model and only assumes injectivity. This also provides another perspective on regularized autoencoders (RAEs), with our final objectives resembling RAEs with specific regularizers that are derived by lower bounding the probability flow objective. We empirically demonstrate the promise of the proposed model, improving over VAEs and AEs in terms of sample quality.},
  archivePrefix = {arXiv},
  eprint = {2002.08927},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kumar et al_2020_regularized autoencoders via relaxed injective probability flow.pdf},
  journal = {arXiv:2002.08927 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{kundu20_PredefinedSparsityLowComplexity,
  title = {Pre-Defined {{Sparsity}} for {{Low}}-{{Complexity Convolutional Neural Networks}}},
  author = {Kundu, Souvik and Nazemi, Mahdi and Pedram, Massoud and Chugg, Keith M. and Beerel, Peter A.},
  year = {2020},
  month = feb,
  abstract = {The high energy cost of processing deep convolutional neural networks impedes their ubiquitous deployment in energy-constrained platforms such as embedded systems and IoT devices. This work introduces convolutional layers with pre-defined sparse 2D kernels that have support sets that repeat periodically within and across filters. Due to the efficient storage of our periodic sparse kernels, the parameter savings can translate into considerable improvements in energy efficiency due to reduced DRAM accesses, thus promising significant improvements in the trade-off between energy consumption and accuracy for both training and inference. To evaluate this approach, we performed experiments with two widely accepted datasets, CIFAR-10 and Tiny ImageNet in sparse variants of the ResNet18 and VGG16 architectures. Compared to baseline models, our proposed sparse variants require up to {$\sim$}82\% fewer model parameters with 5.6\texttimes{} fewer FLOPs with negligible loss in accuracy for ResNet18 on CIFAR-10. For VGG16 trained on Tiny ImageNet, our approach requires 5.8\texttimes{} fewer FLOPs and up to {$\sim$}83.3\% fewer model parameters with a drop in top-5 (top-1) accuracy of only 1.2\% ({$\sim$}2.1\%). We also compared the performance of our proposed architectures with that of ShuffleNet and MobileNetV2. Using similar hyperparameters and FLOPs, our ResNet18 variants yield an average accuracy improvement of {$\sim$}2.8\%.},
  archivePrefix = {arXiv},
  eprint = {2001.10710},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/L92JK72D/Kundu et al. - 2020 - Pre-defined Sparsity for Low-Complexity Convolutio.pdf},
  journal = {arXiv:2001.10710 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{kurakin20_ReMixMatchSemisupervisedlearning,
  title = {{{ReMixMatch}}: {{Semi}}-Supervised Learning with Distribution Matching and Augmentation Anchoring},
  booktitle = {{{ICLR}}},
  author = {Kurakin, Alex and Raffel, Colin and Berthelot, David and Cubuk, Ekin Dogus and Zhang, Han and Sohn, Kihyuk and Carlini, Nicholas},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/kurakin et al_2020_remixmatch2.pdf}
}

@article{kuri-cervantes20_Comprehensivemappingimmune,
  title = {Comprehensive Mapping of Immune Perturbations Associated with Severe {{COVID}}-19},
  author = {{Kuri-Cervantes}, Leticia and Pampena, M. Betina and Meng, Wenzhao and Rosenfeld, Aaron M. and Ittner, Caroline A.G. and Weisman, Ariel R. and Agyekum, Roseline S. and Mathew, Divij and Baxter, Amy E. and Vella, Laura A. and Kuthuru, Oliva and Apostolidis, Sokratis A. and Bershaw, Luanne and Dougherty, Jeanette and Greenplate, Allison R. and Pattekar, Ajinkya and Kim, Justin and Han, Nicholas and Gouma, Sigrid and Weirick, Madison E. and Arevalo, Claudia P. and Bolton, Marcus J. and Goodwin, Eileen C. and Anderson, Elizabeth M. and Hensley, Scott E. and Jones, Tiffanie K. and Mangalmurti, Nilam S. and Luning Prak, Eline T. and Wherry, E. John and Meyer, Nuala J. and Betts, Michael R.},
  year = {2020},
  month = jul,
  volume = {5},
  pages = {eabd7114},
  issn = {2470-9468},
  doi = {10.1126/sciimmunol.abd7114},
  abstract = {Although critical illness has been associated with SARS-CoV-2-induced hyperinflammation, the immune correlates of severe COVID-19 remain unclear. Here, we comprehensively analyzed peripheral blood immune perturbations in 42 SARS-CoV-2 infected and recovered individuals. We identified extensive induction and activation of multiple immune lineages, including T cell activation, oligoclonal plasmablast expansion, and Fc and trafficking receptor modulation on innate lymphocytes and granulocytes, that distinguished severe COVID-19 cases from healthy donors or SARS-CoV-2-recovered or moderate severity patients. We found the neutrophil to lymphocyte ratio to be a prognostic biomarker of disease severity and organ failure. Our findings demonstrate broad innate and adaptive leukocyte perturbations that distinguish dysregulated host responses in severe SARS-CoV-2 infection and warrant therapeutic investigation.},
  file = {/home/trung/GoogleDrive/Zotero/kuri-cervantes et al_2020_comprehensive mapping of immune perturbations associated with severe covid-19.pdf},
  journal = {Science Immunology},
  language = {en},
  number = {49}
}

@article{kurle19_MultiSourceNeuralVariational,
  title = {Multi-{{Source Neural Variational Inference}}},
  author = {Kurle, Richard and G{\"u}nnemann, Stephan and {Van der Smagt}, Patrick},
  year = {2019},
  month = jul,
  volume = {33},
  pages = {4114--4121},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v33i01.33014114},
  abstract = {Learning from multiple sources of information is an important problem in machine-learning research. The key challenges are learning representations and formulating inference methods that take into account the complementarity and redundancy of various information sources. In this paper we formulate a variational autoencoder based multi-source learning framework in which each encoder is conditioned on a different information source. This allows us to relate the sources via the shared latent variables by computing divergence measures between individual source's posterior approximations. We explore a variety of options to learn these encoders and to integrate the beliefs they compute into a consistent posterior approximation. We visualise learned beliefs on a toy dataset and evaluate our methods for learning shared representations and structured output prediction, showing trade-offs of learning separate encoders for each information source. Furthermore, we demonstrate how conflict detection and redundancy can increase robustness of inference in a multi-source setting.},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/GoogleDrive/Zotero/kurle et al_2019_multi-source neural variational inference.pdf},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  keywords = {semi-supervised,variational},
  language = {en}
}

@article{kurmi19_AttendingDiscriminativeCertainty,
  title = {Attending to {{Discriminative Certainty}} for {{Domain Adaptation}}},
  author = {Kurmi, Vinod Kumar and Kumar, Shanu and Namboodiri, Vinay P.},
  year = {2019},
  month = jun,
  abstract = {In this paper, we aim to solve for unsupervised domain adaptation of classifiers where we have access to label information for the source domain while these are not available for a target domain. While various methods have been proposed for solving these including adversarial discriminator based methods, most approaches have focused on the entire image based domain adaptation. In an image, there would be regions that can be adapted better, for instance, the foreground object may be similar in nature. To obtain such regions, we propose methods that consider the probabilistic certainty estimate of various regions and specify focus on these during classification for adaptation. We observe that just by incorporating the probabilistic certainty of the discriminator while training the classifier, we are able to obtain state of the art results on various datasets as compared against all the recent methods. We provide a thorough empirical analysis of the method by providing ablation analysis, statistical significance test, and visualization of the attention maps and t-SNE embeddings. These evaluations convincingly demonstrate the effectiveness of the proposed approach.},
  annotation = {ZSCC: 0000003},
  archivePrefix = {arXiv},
  eprint = {1906.03502},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kurmi et al_2019_attending to discriminative certainty for domain adaptation.pdf;/home/trung/Zotero/storage/3VSW5JJP/1906.html},
  journal = {arXiv:1906.03502 [cs, stat]},
  keywords = {attention,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,domain adaptation,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{kuwajima20_Engineeringproblemsmachine,
  title = {Engineering Problems in Machine Learning Systems},
  author = {Kuwajima, Hiroshi and Yasuoka, Hirotoshi and Nakae, Toshihiro},
  year = {2020},
  month = may,
  volume = {109},
  pages = {1103--1126},
  issn = {1573-0565},
  doi = {10.1007/s10994-020-05872-w},
  abstract = {Fatal accidents are a major issue hindering the wide acceptance of safety-critical systems that employ machine learning and deep learning models, such as automated driving vehicles. In order to use machine learning in a safety-critical system, it is necessary to demonstrate the safety and security of the system through engineering processes. However, thus far, no such widely accepted engineering concepts or frameworks have been established for these systems. The key to using a machine learning model in a deductively engineered system is decomposing the data-driven training of machine learning models into requirement, design, and verification, particularly for machine learning models used in safety-critical systems. Simultaneously, open problems and relevant technical fields are not organized in a manner that enables researchers to select a theme and work on it. In this study, we identify, classify, and explore the open problems in engineering (safety-critical) machine learning systems\textemdash that is, in terms of requirement, design, and verification of machine learning models and systems\textemdash as well as discuss related works and research directions, using automated driving vehicles as an example. Our results show that machine learning models are characterized by a lack of requirements specification, lack of design specification, lack of interpretability, and lack of robustness. We also perform a gap analysis on a conventional system quality standard SQuaRE with the characteristics of machine learning models to study quality models for machine learning systems. We find that a lack of requirements specification and lack of robustness have the greatest impact on conventional quality models.},
  file = {/home/trung/GoogleDrive/Zotero/kuwajima et al_2020_engineering problems in machine learning systems.pdf},
  journal = {Machine Learning},
  number = {5}
}

@article{kuzina19_BooVAEscalableframework,
  title = {{{BooVAE}}: {{A}} Scalable Framework for Continual {{VAE}} Learning under Boosting Approach},
  shorttitle = {{{BooVAE}}},
  author = {Kuzina, Anna and Egorov, Evgenii and Burnaev, Evgeny},
  year = {2019},
  month = aug,
  abstract = {Variational Auto Encoders (VAE) are capable of generating realistic images, sounds and video sequences. From practitioners point of view, we are usually interested in solving problems where tasks are learned sequentially, in a way that avoids revisiting all previous data at each stage. We address this problem by introducing a conceptually simple and scalable end-to-end approach of incorporating past knowledge by learning prior directly from the data. We consider scalable boosting-like approximation for intractable theoretical optimal prior. We provide empirical studies on two commonly used benchmarks, namely MNIST and Fashion MNIST on disjoint sequential image generation tasks. For each dataset proposed method delivers the best results or comparable to SOTA, avoiding catastrophic forgetting in a fully automatic way.},
  archivePrefix = {arXiv},
  eprint = {1908.11853},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/kuzina et al_2019_boovae.pdf},
  journal = {arXiv:1908.11853 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{lab19_Shouldwezeroinflate,
  title = {Should We Zero-Inflate {{scVI}}?},
  author = {Lab, Yosef},
  year = {2019},
  abstract = {Yosef Lab Blog - Computational Biology/Computer Science @ UC Berkeley},
  file = {/home/trung/Zotero/storage/4QFBMYHC/ZeroInflation.html},
  howpublished = {https://yoseflab.github.io/2019/06/25/ZeroInflation/},
  journal = {Yosef Lab Blog}
}

@article{lachaux20_UnsupervisedTranslationProgramming,
  title = {Unsupervised {{Translation}} of {{Programming Languages}}},
  author = {Lachaux, Marie-Anne and Roziere, Baptiste and Chanussot, Lowik and Lample, Guillaume},
  year = {2020},
  month = sep,
  abstract = {A transcompiler, also known as source-to-source translator, is a system that converts source code from a high-level programming language (such as C++ or Python) to another. Transcompilers are primarily used for interoperability, and to port codebases written in an obsolete or deprecated language (e.g. COBOL, Python 2) to a modern one. They typically rely on handcrafted rewrite rules, applied to the source code abstract syntax tree. Unfortunately, the resulting translations often lack readability, fail to respect the target language conventions, and require manual modifications in order to work properly. The overall translation process is timeconsuming and requires expertise in both the source and target languages, making code-translation projects expensive. Although neural models significantly outperform their rule-based counterparts in the context of natural language translation, their applications to transcompilation have been limited due to the scarcity of parallel data in this domain. In this paper, we propose to leverage recent approaches in unsupervised machine translation to train a fully unsupervised neural transcompiler. We train our model on source code from open source GitHub projects, and show that it can translate functions between C++, Java, and Python with high accuracy. Our method relies exclusively on monolingual source code, requires no expertise in the source or target languages, and can easily be generalized to other programming languages. We also build and release a test set composed of 852 parallel functions, along with unit tests to check the correctness of translations. We show that our model outperforms rule-based commercial baselines by a significant margin.},
  archivePrefix = {arXiv},
  eprint = {2006.03511},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lachaux et al_2020_unsupervised translation of programming languages.pdf},
  journal = {arXiv:2006.03511 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{lahnemann20_Elevengrandchallenges,
  title = {Eleven Grand Challenges in Single-Cell Data Science},
  author = {L{\"a}hnemann, David and K{\"o}ster, Johannes and Szczurek, Ewa and McCarthy, Davis J. and Hicks, Stephanie C. and Robinson, Mark D. and Vallejos, Catalina A. and Campbell, Kieran R. and Beerenwinkel, Niko and Mahfouz, Ahmed and Pinello, Luca and Skums, Pavel and Stamatakis, Alexandros and Attolini, Camille Stephan-Otto and Aparicio, Samuel and Baaijens, Jasmijn and Balvert, Marleen and de Barbanson, Buys and Cappuccio, Antonio and Corleone, Giacomo and Dutilh, Bas E. and Florescu, Maria and Guryev, Victor and Holmer, Rens and Jahn, Katharina and Lobo, Thamar Jessurun and Keizer, Emma M. and Khatri, Indu and Kielbasa, Szymon M. and Korbel, Jan O. and Kozlov, Alexey M. and Kuo, Tzu-Hao and Lelieveldt, Boudewijn P.F. and Mandoiu, Ion I. and Marioni, John C. and Marschall, Tobias and M{\"o}lder, Felix and Niknejad, Amir and Raczkowski, Lukasz and Reinders, Marcel and de Ridder, Jeroen and Saliba, Antoine-Emmanuel and Somarakis, Antonios and Stegle, Oliver and Theis, Fabian J. and Yang, Huan and Zelikovsky, Alex and McHardy, Alice C. and Raphael, Benjamin J. and Shah, Sohrab P. and Sch{\"o}nhuth, Alexander},
  year = {2020},
  month = dec,
  volume = {21},
  pages = {31},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-1926-6},
  abstract = {The recent boom in microfluidics and combinatorial indexing strategies, combined with low sequencing costs, has empowered single-cell sequencing technology. Thousands\textemdash or even millions\textemdash of cells analyzed in a single experiment amount to a data revolution in single-cell biology and pose unique data science problems. Here, we outline eleven challenges that will be central to bringing this emerging field of single-cell data science forward. For each challenge, we highlight motivating research questions, review prior work, and formulate open problems. This compendium is for established researchers, newcomers, and students alike, highlighting interesting and rewarding problems for the coming years.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/lähnemann et al_2020_eleven grand challenges in single-cell data science.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{laine17_TemporalEnsemblingSemiSupervised,
  title = {Temporal {{Ensembling}} for {{Semi}}-{{Supervised Learning}}},
  author = {Laine, Samuli and Aila, Timo},
  year = {2017},
  month = mar,
  abstract = {In this paper, we present a simple and efficient method for training deep neural networks in a semi-supervised setting where only a small portion of training data is labeled. We introduce self-ensembling, where we form a consensus prediction of the unknown labels using the outputs of the network-in-training on different epochs, and most importantly, under different regularization and input augmentation conditions. This ensemble prediction can be expected to be a better predictor for the unknown labels than the output of the network at the most recent training epoch, and can thus be used as a target for training. Using our method, we set new records for two standard semi-supervised learning benchmarks, reducing the (non-augmented) classification error rate from 18.44\% to 7.05\% in SVHN with 500 labels and from 18.63\% to 16.55\% in CIFAR-10 with 4000 labels, and further to 5.12\% and 12.16\% by enabling the standard augmentations. We additionally obtain a clear improvement in CIFAR-100 classification accuracy by using random images from the Tiny Images dataset as unlabeled extra inputs during training. Finally, we demonstrate good tolerance to incorrect labels.},
  archivePrefix = {arXiv},
  eprint = {1610.02242},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/laine et al_2017_temporal ensembling for semi-supervised learning.pdf;/home/trung/Zotero/storage/QTJ3VIY3/1610.html},
  journal = {arXiv:1610.02242 [cs]},
  primaryClass = {cs}
}

@article{laine19_HighQualitySelfSupervisedDeep,
  title = {High-{{Quality Self}}-{{Supervised Deep Image Denoising}}},
  author = {Laine, Samuli and Karras, Tero and Lehtinen, Jaakko and Aila, Timo},
  year = {2019},
  month = jun,
  abstract = {We describe a novel method for training high-quality image denoising models based on unorganized collections of corrupted images. The training does not need access to clean reference images, or explicit pairs of corrupted images, and can thus be applied in situations where such data is unacceptably expensive or impossible to acquire. We build on a recent technique that removes the need for reference data by employing networks with a "blind spot" in the receptive field, and significantly improve two key aspects: image quality and training efficiency. Our result quality is on par with state-of-the-art neural network denoisers in the case of i.i.d. additive Gaussian noise, and not far behind with Poisson and impulse noise. We also successfully handle cases where parameters of the noise model are variable and/or unknown in both training and evaluation data.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1901.10277},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/laine et al_2019_high-quality self-supervised deep image denoising.pdf;/home/trung/Zotero/storage/3RTEL9DN/1901.html},
  journal = {arXiv:1901.10277 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{lake16_BuildingMachinesThat,
  title = {Building {{Machines That Learn}} and {{Think Like People}}},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2016},
  month = nov,
  abstract = {Recent progress in artificial intelligence (AI) has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn, and how they learn it. Specifically, we argue that these machines should (a) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (b) ground learning in intuitive theories of physics and psychology, to support and enrich the knowledge that is learned; and (c) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes towards these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1604.00289},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lake et al_2016_building machines that learn and think like people.pdf},
  journal = {arXiv:1604.00289 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lake18_EmergenceOrganizingStructure,
  title = {The {{Emergence}} of {{Organizing Structure}} in {{Conceptual Representation}}},
  author = {Lake, Brenden M. and Lawrence, Neil D. and Tenenbaum, Joshua B.},
  year = {2018},
  volume = {42},
  pages = {809--832},
  issn = {1551-6709},
  doi = {10.1111/cogs.12580},
  abstract = {Both scientists and children make important structural discoveries, yet their computational underpinnings are not well understood. Structure discovery has previously been formalized as probabilistic inference about the right structural form\textemdash where form could be a tree, ring, chain, grid, etc. (Kemp \& Tenenbaum, 2008). Although this approach can learn intuitive organizations, including a tree for animals and a ring for the color circle, it assumes a strong inductive bias that considers only these particular forms, and each form is explicitly provided as initial knowledge. Here we introduce a new computational model of how organizing structure can be discovered, utilizing a broad hypothesis space with a preference for sparse connectivity. Given that the inductive bias is more general, the model's initial knowledge shows little qualitative resemblance to some of the discoveries it supports. As a consequence, the model can also learn complex structures for domains that lack intuitive description, as well as predict human property induction judgments without explicit structural forms. By allowing form to emerge from sparsity, our approach clarifies how both the richness and flexibility of human conceptual organization can coexist.},
  file = {/home/trung/GoogleDrive/Zotero/lake et al_2018_the emergence of organizing structure in conceptual representation.pdf;/home/trung/Zotero/storage/2NL476YV/cogs.html},
  journal = {Cognitive Science},
  keywords = {_tablet,favorite,Sparsity,Structure discovery,Unsupervised learning},
  language = {en},
  number = {S3}
}

@article{lake20_Wordmeaningminds,
  title = {Word Meaning in Minds and Machines},
  author = {Lake, Brenden M. and Murphy, Gregory L.},
  year = {2020},
  month = aug,
  abstract = {Machines show an increasingly broad set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Many algorithms stem from past computational work in psychology, raising the question of whether they understand words as people do. In this paper, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are promising models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people use words in order to express. Word meanings must also be grounded in vision and action, and capable of flexible combinations, in ways that current systems are not. We pose concrete challenges for developing machines with a more human-like, conceptual basis for word meaning. We also discuss implications for cognitive science and NLP.},
  archivePrefix = {arXiv},
  eprint = {2008.01766},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lake et al_2020_word meaning in minds and machines.pdf},
  journal = {arXiv:2008.01766 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{lamb19_InterpolatedAdversarialTraining,
  title = {Interpolated {{Adversarial Training}}: {{Achieving Robust Neural Networks}} without {{Sacrificing Too Much Accuracy}}},
  shorttitle = {Interpolated {{Adversarial Training}}},
  author = {Lamb, Alex and Verma, Vikas and Kannala, Juho and Bengio, Yoshua},
  year = {2019},
  month = sep,
  abstract = {Adversarial robustness has become a central goal in deep learning, both in the theory and the practice. However, successful methods to improve the adversarial robustness (such as adversarial training) greatly hurt generalization performance on the unperturbed data. This could have a major impact on how the adversarial robustness affects real world systems (i.e. many may opt to forego robustness if it can improve accuracy on the unperturbed data). We propose Interpolated Adversarial Training, which employs recently proposed interpolation based training methods in the framework of adversarial training. On CIFAR-10, adversarial training increases the standard test error ( when there is no adversary) from 4.43\% to 12.32\%, whereas with our Interpolated adversarial training we retain the adversarial robustness while achieving a standard test error of only 6.45\%. With our technique, the relative increase in the standard error for the robust model is reduced from 178.1\% to just 45.5\%.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1906.06784},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/GGL9RCS5/Lamb et al. - 2019 - Interpolated Adversarial Training Achieving Robus.pdf},
  journal = {arXiv:1906.06784 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{lamb20_GraphNeuralNetworks,
  title = {Graph {{Neural Networks Meet Neural}}-{{Symbolic Computing}}: {{A Survey}} and {{Perspective}}},
  shorttitle = {Graph {{Neural Networks Meet Neural}}-{{Symbolic Computing}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Lamb, Lu{\'i}s C. and d'Avila Garcez, Artur and Gori, Marco and Prates, Marcelo O.R. and Avelar, Pedro H.C. and Vardi, Moshe Y.},
  year = {2020},
  month = jul,
  pages = {4877--4884},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/679},
  abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNNs) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as their relationship to current developments in neural-symbolic computing.},
  file = {/home/trung/GoogleDrive/Zotero/lamb et al_2020_graph neural networks meet neural-symbolic computing.pdf},
  isbn = {978-0-9992411-6-5},
  language = {en}
}

@article{lamb20_GraphNeuralNetworksa,
  title = {Graph {{Neural Networks Meet Neural}}-{{Symbolic Computing}}: {{A Survey}} and {{Perspective}}},
  shorttitle = {Graph {{Neural Networks Meet Neural}}-{{Symbolic Computing}}},
  author = {Lamb, Luis and Garcez, Artur and Gori, Marco and Prates, Marcelo and Avelar, Pedro and Vardi, Moshe},
  year = {2020},
  month = mar,
  abstract = {Neural-symbolic computing has now become the subject of interest of both academic and industry research laboratories. Graph Neural Networks (GNN) have been widely used in relational and symbolic domains, with widespread application of GNNs in combinatorial optimization, constraint satisfaction, relational reasoning and other scientific domains. The need for improved explainability, interpretability and trust of AI systems in general demands principled methodologies, as suggested by neural-symbolic computing. In this paper, we review the state-of-the-art on the use of GNNs as a model of neural-symbolic computing. This includes the application of GNNs in several domains as well as its relationship to current developments in neural-symbolic computing.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2003.00330},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/8MQ4PZ4E/Lamb et al. - 2020 - Graph Neural Networks Meet Neural-Symbolic Computi.pdf},
  journal = {arXiv:2003.00330 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{lample17_FaderNetworksManipulating,
  title = {Fader {{Networks}}: {{Manipulating Images}} by {{Sliding Attributes}}},
  shorttitle = {Fader {{Networks}}},
  author = {Lample, Guillaume and Zeghidour, Neil and Usunier, Nicolas and Bordes, Antoine and Denoyer, Ludovic and Ranzato, Marc'Aurelio},
  year = {2017},
  month = jun,
  abstract = {This paper introduces a new encoder-decoder architecture that is trained to reconstruct images by disentangling the salient information of the image and the values of attributes directly in the latent space. As a result, after training, our model can generate different realistic versions of an input image by varying the attribute values. By using continuous attribute values, we can choose how much a specific attribute is perceivable in the generated image. This property could allow for applications where users can modify an image using sliding knobs, like faders on a mixing console, to change the facial expression of a portrait, or to update the color of some objects. Compared to the state-of-the-art which mostly relies on training adversarial networks in pixel space by altering attribute values at train time, our approach results in much simpler training schemes and nicely scales to multiple attributes. We present evidence that our model can significantly change the perceived value of the attributes while preserving the naturalness of images.},
  archivePrefix = {arXiv},
  eprint = {1706.00409},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lample et al_2017_fader networks.pdf},
  journal = {arXiv:1706.00409 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,controllable,gan},
  language = {en},
  primaryClass = {cs}
}

@article{lample19_DeepLearningSymbolic,
  title = {Deep {{Learning}} for {{Symbolic Mathematics}}},
  author = {Lample, Guillaume and Charton, Fran{\c c}ois},
  year = {2019},
  month = dec,
  abstract = {Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.01412},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lample et al_2019_deep learning for symbolic mathematics.pdf},
  journal = {arXiv:1912.01412 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{lan19_ALBERTLiteBERT,
  title = {{{ALBERT}}: {{A Lite BERT}} for {{Self}}-Supervised {{Learning}} of {{Language Representations}}},
  shorttitle = {{{ALBERT}}},
  author = {Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  year = {2019},
  month = sep,
  abstract = {Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations, longer training times, and unexpected model degradation. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and SQuAD benchmarks while having fewer parameters compared to BERT-large.},
  archivePrefix = {arXiv},
  eprint = {1909.11942},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lan et al_2019_albert.pdf;/home/trung/Zotero/storage/DBPSET82/1909.html},
  journal = {arXiv:1909.11942 [cs]},
  keywords = {bert,Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{lan19_ProbabilisticRepresentationDeep,
  title = {A {{Probabilistic Representation}} of {{Deep Learning}}},
  author = {Lan, Xinjie and Barner, Kenneth E.},
  year = {2019},
  month = aug,
  abstract = {In this work, we introduce a novel probabilistic representation of deep learning, which provides an explicit explanation for the Deep Neural Networks (DNNs) in three aspects: (i) neurons define the energy of a Gibbs distribution; (ii) the hidden layers of DNNs formulate Gibbs distributions; and (iii) the whole architecture of DNNs can be interpreted as a Bayesian neural network. Based on the proposed probabilistic representation, we investigate two fundamental properties of deep learning: hierarchy and generalization. First, we explicitly formulate the hierarchy property from the Bayesian perspective, namely that some hidden layers formulate a prior distribution and the remaining layers formulate a likelihood distribution. Second, we demonstrate that DNNs have an explicit regularization by learning a prior distribution and the learning algorithm is one reason for decreasing the generalization ability of DNNs. Moreover, we clarify two empirical phenomena of DNNs that cannot be explained by traditional theories of generalization. Simulation results validate the proposed probabilistic representation and the insights into these properties of deep learning based on a synthetic dataset.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.09772},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lan et al_2019_a probabilistic representation of deep learning.pdf;/home/trung/Zotero/storage/5BV69Y93/1908.html},
  journal = {arXiv:1908.09772 [cs, stat]},
  keywords = {Computer Science - Machine Learning,generalization,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{lan20_Sequencingdropoutandbatcheffect,
  title = {Sequencing Dropout-and-Batch Effect Normalization for Single-Cell {{mRNA}} Profiles: A Survey and Comparative Analysis},
  author = {Lan, Tian and Hutvagner, Gyorgy and Lan, Qing and Liu, Tao and Li, Jinyan},
  year = {2020},
  month = oct,
  issn = {1477-4054},
  doi = {10.1093/bib/bbaa248},
  abstract = {Single-cell mRNA sequencing has been adopted as a powerful technique for understanding gene expression profiles at the single-cell level. However, challenges remain due to factors such as the inefficiency of mRNA molecular capture, technical noises and separate sequencing of cells in different batches. Normalization methods have been developed to ensure a relatively accurate analysis. This work presents a survey on 10 tools specifically designed for single-cell mRNA sequencing data preprocessing steps, among which 6 tools are used for dropout normalization and 4 tools are for batch effect correction. In this survey, we outline the main methodology for each of these tools, and we also compare these tools to evaluate their normalization performance on datasets which are simulated under the constraints of dropout inefficiency, batch effect or their combined effects. We found that Saver and Baynorm performed better than other methods in dropout normalization, in most cases. Beer and Batchelor performed better in the batch effect normalization, and the Saver\textendash Beer tool combination and the Baynorm\textendash Beer combination performed better in the mixed dropout-and-batch effect normalization. Over-normalization is a common issue occurred to these dropout normalization tools that is worth of future investigation. For the batch normalization tools, the capability of retaining heterogeneity between different groups of cells after normalization can be another direction for future improvement.},
  journal = {Briefings in Bioinformatics},
  number = {bbaa248}
}

@article{landauer00_HowMuchPeople,
  title = {How {{Much Do People Remember}}! {{Some Estimates}} of the {{Quantity}} of {{Learned Information}} in {{Long}}-Term {{Memory}}},
  author = {Landauer, K},
  pages = {17},
  file = {/home/trung/GoogleDrive/Zotero/landauer_how much do people remember.pdf},
  language = {en}
}

@article{landolfi19_ModelbasedApproachSampleefficient,
  title = {A {{Model}}-Based {{Approach}} for {{Sample}}-Efficient {{Multi}}-Task {{Reinforcement Learning}}},
  author = {Landolfi, Nicholas C. and Thomas, Garrett and Ma, Tengyu},
  year = {2019},
  month = nov,
  abstract = {The aim of multi-task reinforcement learning is two-fold: (1) efficiently learn by training against multiple tasks and (2) quickly adapt, using limited samples, to a variety of new tasks. In this work, the tasks correspond to reward functions for environments with the same (or similar) dynamical models. We propose to learn a dynamical model during the training process and use this model to perform sample-efficient adaptation to new tasks at test time. We use significantly fewer samples by performing policy optimization only in a ``virtual" environment whose transitions are given by our learned dynamical model. Our algorithm sequentially trains against several tasks. Upon encountering a new task, we first warm-up a policy on our learned dynamical model, which requires no new samples from the environment. We then adapt the dynamical model with samples from this policy in the real environment. We evaluate our approach on several continuous control benchmarks and demonstrate its efficacy over MAML, a state-of-the-art meta-learning algorithm, on these tasks.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1907.04964},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/landolfi et al_2019_a model-based approach for sample-efficient multi-task reinforcement learning.pdf},
  journal = {arXiv:1907.04964 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{langevin18_DeepGenerativeModel,
  title = {A {{Deep Generative Model}} for {{Semi}}-{{Supervised Classification}} with {{Noisy Labels}}},
  author = {Langevin, Maxime and Mehlman, Edouard and Regier, Jeffrey and Lopez, Romain and Jordan, Michael I. and Yosef, Nir},
  year = {2018},
  month = sep,
  abstract = {Class labels are often imperfectly observed, due to mistakes and to genuine ambiguity among classes. We propose a new semi-supervised deep generative model that explicitly models noisy labels, called the Mislabeled VAE (M-VAE). The M-VAE can perform better than existing deep generative models which do not account for label noise. Additionally, the derivation of M-VAE gives new theoretical insights into the popular M1+M2 semi-supervised model.},
  archivePrefix = {arXiv},
  eprint = {1809.05957},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/langevin et al_2018_a deep generative model for semi-supervised classification with noisy labels.pdf},
  journal = {arXiv:1809.05957 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{lao19_DualAdversarialInference,
  title = {Dual {{Adversarial Inference}} for {{Text}}-to-{{Image Synthesis}}},
  author = {Lao, Qicheng and Havaei, Mohammad and Pesaranghader, Ahmad and Dutil, Francis and Di Jorio, Lisa and Fevens, Thomas},
  year = {2019},
  month = aug,
  abstract = {Synthesizing images from a given text description involves engaging two types of information: the content, which includes information explicitly described in the text (e.g., color, composition, etc.), and the style, which is usually not well described in the text (e.g., location, quantity, size, etc.). However, in previous works, it is typically treated as a process of generating images only from the content, i.e., without considering learning meaningful style representations. In this paper, we aim to learn two variables that are disentangled in the latent space, representing content and style respectively. We achieve this by augmenting current text-to-image synthesis frameworks with a dual adversarial inference mechanism. Through extensive experiments, we show that our model learns, in an unsupervised manner, style representations corresponding to certain meaningful information present in the image that are not well described in the text. The new framework also improves the quality of synthesized images when evaluated on Oxford-102, CUB and COCO datasets.},
  archivePrefix = {arXiv},
  eprint = {1908.05324},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lao et al_2019_dual adversarial inference for text-to-image synthesis.pdf;/home/trung/Zotero/storage/LI8VHTFM/1908.html},
  journal = {arXiv:1908.05324 [cs]},
  keywords = {adversarial,Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{larranaga06_Machinelearningbioinformatics,
  title = {Machine Learning in Bioinformatics},
  author = {Larra{\~n}aga, Pedro and Calvo, Borja and Santana, Roberto and Bielza, Concha and Galdiano, Josu and Inza, I{\~n}aki and Lozano, Jos{\'e} A. and Arma{\~n}anzas, Rub{\'e}n and Santaf{\'e}, Guzm{\'a}n and P{\'e}rez, Aritz and Robles, Victor},
  year = {2006},
  month = mar,
  volume = {7},
  pages = {86--112},
  issn = {1467-5463},
  doi = {10.1093/bib/bbk007},
  abstract = {This article reviews machine learning methods for bioinformatics. It presents modelling methods, such as supervised classification, clustering and probabilistic graphical models for knowledge discovery, as well as deterministic and stochastic heuristics for optimization. Applications in genomics, proteomics, systems biology, evolution and text mining are also shown.},
  file = {/home/trung/GoogleDrive/Zotero/larrañaga et al_2006_machine learning in bioinformatics.pdf},
  journal = {Briefings in Bioinformatics},
  language = {en},
  number = {1}
}

@article{lawrence19_DataScienceDigital,
  title = {Data {{Science}} and {{Digital Systems}}: {{The 3Ds}} of {{Machine Learning Systems Design}}},
  shorttitle = {Data {{Science}} and {{Digital Systems}}},
  author = {Lawrence, Neil D.},
  year = {2019},
  month = mar,
  abstract = {Machine learning solutions, in particular those based on deep learning methods, form an underpinning of the current revolution in "artificial intelligence" that has dominated popular press headlines and is having a significant influence on the wider tech agenda. Here we give an overview of the 3Ds of ML systems design: Data, Design and Deployment. By considering the 3Ds we can move towards \textbackslash emph\{data first\} design.},
  archivePrefix = {arXiv},
  eprint = {1903.11241},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lawrence_2019_data science and digital systems.pdf;/home/trung/Zotero/storage/BZCKH6YD/1903.html},
  journal = {arXiv:1903.11241 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{le00_OptimizationMethodsDeep,
  title = {On {{Optimization Methods}} for {{Deep Learning}}},
  author = {Le, Quoc V and Ngiam, Jiquan and Coates, Adam and Lahiri, Abhik and Prochnow, Bobby and Ng, Andrew Y},
  pages = {8},
  abstract = {The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between LBFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69\% on the standard MNIST dataset. This is a state-of-theart result on MNIST among algorithms that do not use distortions or pretraining.},
  language = {en}
}

@incollection{le11_ICAReconstructionCost,
  title = {{{ICA}} with {{Reconstruction Cost}} for {{Efficient Overcomplete Feature Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 24},
  author = {Le, Quoc V. and Karpenko, Alexandre and Ngiam, Jiquan and Ng, Andrew Y.},
  editor = {{Shawe-Taylor}, J. and Zemel, R. S. and Bartlett, P. L. and Pereira, F. and Weinberger, K. Q.},
  year = {2011},
  pages = {1017--1025},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/le et al_2011_ica with reconstruction cost for efficient overcomplete feature learning.pdf}
}

@inproceedings{le11_Learninghierarchicalinvariant,
  title = {Learning Hierarchical Invariant Spatio-Temporal Features for Action Recognition with Independent Subspace Analysis},
  booktitle = {Proceedings of the 2011 {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Le, Q. V. and Zou, W. Y. and Yeung, S. Y. and Ng, A. Y.},
  year = {2011},
  pages = {3361--3368},
  publisher = {{IEEE Computer Society}},
  address = {{USA}},
  doi = {10.1109/CVPR.2011.5995496},
  abstract = {Previous work on action recognition has focused on adapting hand-designed local features, such as SIFT or HOG, from static images to the video domain. In this paper, we propose using unsupervised feature learning as a way to learn features directly from video data. More specifically, we present an extension of the Independent Subspace Analysis algorithm to learn invariant spatio-temporal features from unlabeled video data. We discovered that, despite its simplicity, this method performs surprisingly well when combined with deep learning techniques such as stacking and convolution to learn hierarchical representations. By replacing hand-designed features with our learned features, we achieve classification results superior to all previous published results on the Hollywood2, UCF, KTH and YouTube action recognition datasets. On the challenging Hollywood2 and YouTube action datasets we obtain 53.3\% and 75.8\% respectively, which are approximately 5\% better than the current best published results. Further benefits of this method, such as the ease of training and the efficiency of training and prediction, will also be discussed. You can download our code and learned spatio-temporal features here: http://ai.stanford.edu/ wzou/.},
  file = {/home/trung/GoogleDrive/Zotero/le et al_2011_learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis.pdf},
  isbn = {978-1-4577-0394-2},
  keywords = {action recognition,hand-designed local feature,hierarchical invariant spatio-temporal feature learning technique,hierarchical representation,HOG,independent subspace analysis algorithm,KTH,SIFT,static image,UCF,unsupervised feature learning,video data,video domain,YouTube action recognition dataset},
  series = {{{CVPR}} '11}
}

@inproceedings{le11_optimizationmethodsdeep,
  title = {On Optimization Methods for Deep Learning},
  booktitle = {Proceedings of the 28th International Conference on International Conference on Machine Learning},
  author = {Le, Quoc V. and Ngiam, Jiquan and Coates, Adam and Lahiri, Abhik and Prochnow, Bobby and Ng, Andrew Y.},
  year = {2011},
  pages = {265--272},
  publisher = {{Omnipress}},
  address = {{Madison, WI, USA}},
  abstract = {The predominant methodology in training deep learning advocates the use of stochastic gradient descent methods (SGDs). Despite its ease of implementation, SGDs are difficult to tune and parallelize. These problems make it challenging to develop, debug and scale up deep learning algorithms with SGDs. In this paper, we show that more sophisticated off-the-shelf optimization methods such as Limited memory BFGS (L-BFGS) and Conjugate gradient (CG) with line search can significantly simplify and speed up the process of pretraining deep algorithms. In our experiments, the difference between L-BFGS/CG and SGDs are more pronounced if we consider algorithmic extensions (e.g., sparsity regularization) and hardware extensions (e.g., GPUs or computer clusters). Our experiments with distributed optimization support the use of L-BFGS with locally connected networks and convolutional neural networks. Using L-BFGS, our convolutional network model achieves 0.69\% on the standard MNIST dataset. This is a state-of-the-art result on MNIST among algorithms that do not use distortions or pretraining.},
  file = {/home/trung/GoogleDrive/Zotero/le et al_2011_on optimization methods for deep learning.pdf},
  isbn = {978-1-4503-0619-5},
  series = {{{ICML}}'11}
}

@incollection{le18_VariationalMemoryEncoderDecoder,
  title = {Variational {{Memory Encoder}}-{{Decoder}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Le, Hung and Tran, Truyen and Nguyen, Thin and Venkatesh, Svetha},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {1508--1518},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/le et al_2018_variational memory encoder-decoder.pdf;/home/trung/Zotero/storage/WAHHFW44/7424-variational-memory-encoder-decoder.html},
  keywords = {memory,variational}
}

@article{le19_LearningRememberMore,
  title = {Learning to {{Remember More}} with {{Less Memorization}}},
  author = {Le, Hung and Tran, Truyen and Venkatesh, Svetha},
  year = {2019},
  month = mar,
  abstract = {Memory-augmented neural networks consisting of a neural controller and an external memory have shown potentials in long-term sequential learning. Current RAM-like memory models maintain memory accessing every timesteps, thus they do not effectively leverage the short-term memory held in the controller. We hypothesize that this scheme of writing is suboptimal in memory utilization and introduces redundant computation. To validate our hypothesis, we derive a theoretical bound on the amount of information stored in a RAM-like system and formulate an optimization problem that maximizes the bound. The proposed solution dubbed Uniform Writing is proved to be optimal under the assumption of equal timestep contributions. To relax this assumption, we introduce modifications to the original solution, resulting in a solution termed Cached Uniform Writing. This method aims to balance between maximizing memorization and forgetting via overwriting mechanisms. Through an extensive set of experiments, we empirically demonstrate the advantages of our solutions over other recurrent architectures, claiming the state-of-the-arts in various sequential modeling tasks.},
  archivePrefix = {arXiv},
  eprint = {1901.01347},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/le et al_2019_learning to remember more with less memorization.pdf},
  journal = {arXiv:1901.01347 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{le19_NeuralStoredprogramMemory,
  title = {Neural {{Stored}}-Program {{Memory}}},
  author = {Le, Hung and Tran, Truyen and Venkatesh, Svetha},
  year = {2019},
  month = may,
  abstract = {Neural networks powered with external memory simulate computer behaviors. These models, which use the memory to store data for a neural controller, can learn algorithms and other complex tasks. In this paper, we introduce a new memory to store weights for the controller, analogous to the stored-program memory in modern computer architectures. The proposed model, dubbed Neural Stored-program Memory, augments current memory-augmented neural networks, creating differentiable machines that can switch programs through time, adapt to variable contexts and thus fully resemble the Universal Turing Machine or Von Neumann Architecture. A wide range of experiments demonstrate that the resulting machines not only excel in classical algorithmic problems, but also have potential for compositional, continual, few-shot learning and question-answering tasks.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1906.08862},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/le et al_2019_neural stored-program memory.pdf;/home/trung/Zotero/storage/U7DBXUZ5/1906.html},
  journal = {arXiv:1906.08862 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,memory,Statistics - Machine Learning,turing machine},
  primaryClass = {cs, stat}
}

@article{le20_NeurocoderLearningGeneralPurpose,
  title = {Neurocoder: {{Learning General}}-{{Purpose Computation Using Stored Neural Programs}}},
  shorttitle = {Neurocoder},
  author = {Le, Hung and Venkatesh, Svetha},
  year = {2020},
  month = sep,
  abstract = {Artificial Neural Networks are uniquely adroit at machine learning by processing data through a network of artificial neurons. The inter-neuronal connection weights represent the learnt Neural Program that instructs the network on how to compute the data. However, without an external memory to store Neural Programs, they are restricted to only one, overwriting learnt programs when trained on new data. This is functionally equivalent to a special-purpose computer. Here we design Neurocoder, an entirely new class of general-purpose conditional computational machines in which the neural network "codes" itself in a data-responsive way by composing relevant programs from a set of shareable, modular programs. This can be considered analogous to building Lego structures from simple Lego bricks. Notably, our bricks change their shape through learning. External memory is used to create, store and retrieve modular programs. Like today's stored-program computers, Neurocoder can now access diverse programs to process different data. Unlike manually crafted computer programs, Neurocoder creates programs through training. Integrating Neurocoder into current neural architectures, we demonstrate new capacity to learn modular programs, handle severe pattern shifts and remember old programs as new ones are learnt, and show substantial performance improvement in solving object recognition, playing video games and continual learning tasks. Such integration with Neurocoder increases the computation capability of any current neural network and endows it with entirely new capacity to reuse simple programs to build complex ones. For the first time a Neural Program is treated as a datum in memory, paving the ways for modular, recursive and procedural neural programming.},
  archivePrefix = {arXiv},
  eprint = {2009.11443},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/le et al_2020_neurocoder.pdf},
  journal = {arXiv:2009.11443 [cs]},
  primaryClass = {cs}
}

@misc{learning00_MITDeepLearning,
  title = {{{MIT Deep Learning}} 6.{{S191}}},
  author = {Learning, MIT Deep},
  abstract = {MIT's official introductory course on deep learning methods and applications.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/QXINJ3V5/introtodeeplearning.com.html},
  howpublished = {http://introtodeeplearning.com},
  journal = {MIT Deep Learning 6.S191},
  language = {en}
}

@article{lecun00_TutorialEnergyBasedLearning,
  title = {A {{Tutorial}} on {{Energy}}-{{Based Learning}}},
  author = {LeCun, Yann and Chopra, Sumit and Hadsell, Raia and Ranzato, Marc'Aurelio and Huang, Fu Jie},
  pages = {59},
  abstract = {Energy-Based Models (EBMs) capture dependencies between variables by associating a scalar energy to each configuration of the variables. Inference consists in clamping the value of observed variables and finding configurations of the remaining variables that minimize the energy. Learning consists in finding an energy function in which observed configurations of the variables are given lower energies than unobserved ones. The EBM approach provides a common theoretical framework for many learning models, including traditional discriminative and generative approaches, as well as graph-transformer networks, conditional random fields, maximum margin Markov networks, and several manifold learning methods.},
  annotation = {ZSCC: 0000471},
  file = {/home/trung/GoogleDrive/Zotero/lecun et al_a tutorial on energy-based learning.pdf},
  language = {en}
}

@article{lecun15_Deeplearning,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  volume = {521},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  annotation = {ZSCC: 0022585},
  file = {/home/trung/Zotero/storage/LZT2VMCJ/LeCun et al. - 2015 - Deep learning.pdf},
  journal = {Nature},
  keywords = {favorite},
  language = {en},
  number = {7553}
}

@article{lee18_IndustrialArtificialIntelligence,
  title = {Industrial {{Artificial Intelligence}} for Industry 4.0-Based Manufacturing Systems},
  author = {Lee, Jay and Davari, Hossein and Singh, Jaskaran and Pandhare, Vibhor},
  year = {2018},
  month = oct,
  volume = {18},
  pages = {20--23},
  issn = {22138463},
  doi = {10.1016/j.mfglet.2018.09.002},
  abstract = {The recent White House report on Artificial Intelligence (AI) (Lee, 2016) highlights the significance of AI and the necessity of a clear roadmap and strategic investment in this area. As AI emerges from science fiction to become the frontier of world-changing technologies, there is an urgent need for systematic development and implementation of AI to see its real impact in the next generation of industrial systems, namely Industry 4.0. Within the 5C architecture previously proposed in Lee et al. (2015), this paper provides an insight into the current state of AI technologies and the eco-system required to harness the power of AI in industrial applications.},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2018_industrial artificial intelligence for industry 4.pdf},
  journal = {Manufacturing Letters},
  language = {en}
}

@article{lee19_I4USubmissionNIST,
  title = {{{I4U Submission}} to {{NIST SRE}} 2018: {{Leveraging}} from a {{Decade}} of {{Shared Experiences}}},
  shorttitle = {{{I4U Submission}} to {{NIST SRE}} 2018},
  author = {Lee, Kong Aik and Hautamaki, Ville and Kinnunen, Tomi and Yamamoto, Hitoshi and Okabe, Koji and Vestman, Ville and Huang, Jing and Ding, Guohong and Sun, Hanwu and Larcher, Anthony and Das, Rohan Kumar and Li, Haizhou and Rouvier, Mickael and Bousquet, Pierre-Michel and Rao, Wei and Wang, Qing and Zhang, Chunlei and Bahmaninezhad, Fahimeh and Delgado, Hector and Patino, Jose and Wang, Qiongqiong and Guo, Ling and Koshinaka, Takafumi and Zhang, Jiacen and Shinoda, Koichi and Trong, Trung Ngo and Sahidullah, Md and Lu, Fan and Tang, Yun and Tu, Ming and Teh, Kah Kuan and Tran, Huy Dat and George, Kuruvachan K. and Kukanov, Ivan and Desnous, Florent and Yang, Jichen and Yilmaz, Emre and Xu, Longting and Bonastre, Jean-Francois and Xu, Chenglin and Lim, Zhi Hao and Chng, Eng Siong and Ranjan, Shivesh and Hansen, John H. L. and Todisco, Massimiliano and Evans, Nicholas},
  year = {2019},
  month = apr,
  abstract = {The I4U consortium was established to facilitate a joint entry to NIST speaker recognition evaluations (SRE). The latest edition of such joint submission was in SRE 2018, in which the I4U submission was among the best-performing systems. SRE'18 also marks the 10-year anniversary of I4U consortium into NIST SRE series of evaluation. The primary objective of the current paper is to summarize the results and lessons learned based on the twelve sub-systems and their fusion submitted to SRE'18. It is also our intention to present a shared view on the advancements, progresses, and major paradigm shifts that we have witnessed as an SRE participant in the past decade from SRE'08 to SRE'18. In this regard, we have seen, among others, a paradigm shift from supervector representation to deep speaker embedding, and a switch of research challenge from channel compensation to domain adaptation.},
  archivePrefix = {arXiv},
  eprint = {1904.07386},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2019_i4u submission to nist sre 2018.pdf;/home/trung/Zotero/storage/9CZ85ZTL/1904.html},
  journal = {arXiv:1904.07386 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{lee19_MathematicalReasoningLatent,
  title = {Mathematical {{Reasoning}} in {{Latent Space}}},
  author = {Lee, Dennis and Szegedy, Christian and Rabe, Markus N. and Loos, Sarah M. and Bansal, Kshitij},
  year = {2019},
  month = sep,
  abstract = {We design and conduct a simple experiment to study whether neural networks can perform several steps of approximate reasoning in a fixed dimensional latent space. The set of rewrites (i.e. transformations) that can be successfully performed on a statement represents essential semantic features of the statement. We can compress this information by embedding the formula in a vector space, such that the vector associated with a statement can be used to predict whether a statement can be rewritten by other theorems. Predicting the embedding of a formula generated by some rewrite rule is naturally viewed as approximate reasoning in the latent space. In order to measure the effectiveness of this reasoning, we perform approximate deduction sequences in the latent space and use the resulting embedding to inform the semantic features of the corresponding formal statement (which is obtained by performing the corresponding rewrite sequence using real formulas). Our experiments show that graph neural networks can make non-trivial predictions about the rewrite-success of statements, even when they propagate predicted latent representations for several steps. Since our corpus of mathematical formulas includes a wide variety of mathematical disciplines, this experiment is a strong indicator for the feasibility of deduction in latent space in general.},
  archivePrefix = {arXiv},
  eprint = {1909.11851},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2019_mathematical reasoning in latent space.pdf},
  journal = {arXiv:1909.11851 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lee19_RobustRelationalCausal,
  title = {Towards {{Robust Relational Causal Discovery}}},
  author = {Lee, Sanghack and Honavar, Vasant},
  year = {2019},
  month = dec,
  abstract = {We consider the problem of learning causal relationships from relational data. Existing approaches rely on queries to a relational conditional independence (RCI) oracle to establish and orient causal relations in such a setting. In practice, queries to a RCI oracle have to be replaced by reliable tests for RCI against available data. Relational data present several unique challenges in testing for RCI. We study the conditions under which traditional iid-based conditional independence (CI) tests yield reliable answers to RCI queries against relational data. We show how to conduct CI tests against relational data to robustly recover the underlying relational causal structure. Results of our experiments demonstrate the effectiveness of our proposed approach.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.02390},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Lee_Honavar_2019_Towards Robust Relational Causal Discovery3.pdf;/home/trung/GoogleDrive/Zotero/2019/Lee_Honavar_2019_Towards Robust Relational Causal Discovery4.pdf;/home/trung/Zotero/storage/HYR8GFAS/1912.html;/home/trung/Zotero/storage/USEPDPP2/1912.html},
  journal = {arXiv:1912.02390 [cs, stat]},
  keywords = {causal,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,relational,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{lee20_CrossingYouStyle,
  title = {Crossing {{You}} in {{Style}}: {{Cross}}-Modal {{Style Transfer}} from {{Music}} to {{Visual Arts}}},
  shorttitle = {Crossing {{You}} in {{Style}}},
  author = {Lee, Cheng-Che and Lin, Wan-Yi and Shih, Yen-Ting and Kuo, Pei-Yi Patricia and Su, Li},
  year = {2020},
  month = sep,
  doi = {10.1145/3394171.3413624},
  abstract = {Music-to-visual style transfer is a challenging yet important cross-modal learning problem in the practice of creativity. Its major difference from the traditional image style transfer problem is that the style information is provided by music rather than images. Assuming that musical features can be properly mapped to visual contents through semantic links between the two domains, we solve the music-to-visual style transfer problem in two steps: music visualization and style transfer. The music visualization network utilizes an encoder-generator architecture with a conditional generative adversarial network to generate image-based music representations from music data. This network is integrated with an image style transfer method to accomplish the style transfer process. Experiments are conducted on WikiArt-IMSLP, a newly compiled dataset including Western music recordings and paintings listed by decades. By utilizing such a label to learn the semantic connection between paintings and music, we demonstrate that the proposed framework can generate diverse image style representations from a music piece, and these representations can unveil certain art forms of the same era. Subjective testing results also emphasize the role of the era label in improving the perceptual quality on the compatibility between music and visual content.},
  archivePrefix = {arXiv},
  eprint = {2009.08083},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2020_crossing you in style.pdf},
  journal = {arXiv:2009.08083 [cs]},
  primaryClass = {cs}
}

@article{lee20_DiscrepancyDensityEstimation,
  title = {On the {{Discrepancy}} between {{Density Estimation}} and {{Sequence Generation}}},
  author = {Lee, Jason and Tran, Dustin and Firat, Orhan and Cho, Kyunghyun},
  year = {2020},
  month = feb,
  abstract = {Many sequence-to-sequence generation tasks, including machine translation and text-to-speech, can be posed as estimating the density of the output y given the input x: p(y|x). Given this interpretation, it is natural to evaluate sequence-to-sequence models using conditional log-likelihood on a test set. However, the goal of sequence-to-sequence generation (or structured prediction) is to find the best output y\^ given an input x, and each task has its own downstream metric R that scores a model output by comparing against a set of references y*: R(y\^, y* | x). While we hope that a model that excels in density estimation also performs well on the downstream metric, the exact correlation has not been studied for sequence generation tasks. In this paper, by comparing several density estimators on five machine translation tasks, we find that the correlation between rankings of models based on log-likelihood and BLEU varies significantly depending on the range of the model families being compared. First, log-likelihood is highly correlated with BLEU when we consider models within the same family (e.g. autoregressive models, or latent variable models with the same parameterization of the prior). However, we observe no correlation between rankings of models across different families: (1) among non-autoregressive latent variable models, a flexible prior distribution is better at density estimation but gives worse generation quality than a simple prior, and (2) autoregressive models offer the best translation performance overall, while latent variable models with a normalizing flow prior give the highest held-out log-likelihood across all datasets. Therefore, we recommend using a simple prior for the latent variable non-autoregressive model when fast generation speed is desired.},
  archivePrefix = {arXiv},
  eprint = {2002.07233},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2020_on the discrepancy between density estimation and sequence generation.pdf},
  journal = {arXiv:2002.07233 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{lee20_EstimatingModelUncertainty,
  title = {Estimating {{Model Uncertainty}} of {{Neural Networks}} in {{Sparse Information Form}}},
  author = {Lee, Jongseok and Humt, Matthias and Feng, Jianxiang and Triebel, Rudolph},
  year = {2020},
  month = jun,
  abstract = {We present a sparse representation of model uncertainty for Deep Neural Networks (DNNs) where the parameter posterior is approximated with an inverse formulation of the Multivariate Normal Distribution (MND), also known as the information form. The key insight of our work is that the information matrix, i.e. the inverse of the covariance matrix tends to be sparse in its spectrum. Therefore, dimensionality reduction techniques such as low rank approximations (LRA) can be effectively exploited. To achieve this, we develop a novel sparsification algorithm and derive a cost-effective analytical sampler. As a result, we show that the information form can be scalably applied to represent model uncertainty in DNNs. Our exhaustive theoretical analysis and empirical evaluations on various benchmarks show the competitiveness of our approach over the current methods.},
  archivePrefix = {arXiv},
  eprint = {2006.11631},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2020_estimating model uncertainty of neural networks in sparse information form.pdf},
  journal = {arXiv:2006.11631 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{lee20_FiniteInfiniteNeural,
  title = {Finite {{Versus Infinite Neural Networks}}: An {{Empirical Study}}},
  shorttitle = {Finite {{Versus Infinite Neural Networks}}},
  author = {Lee, Jaehoon and Schoenholz, Samuel S. and Pennington, Jeffrey and Adlam, Ben and Xiao, Lechao and Novak, Roman and {Sohl-Dickstein}, Jascha},
  year = {2020},
  month = jul,
  abstract = {We perform a careful, thorough, and large scale empirical study of the correspondence between wide neural networks and kernel methods. By doing so, we resolve a variety of open questions related to the study of infinitely wide neural networks. Our experimental results include: kernel methods outperform fully-connected finite-width networks, but underperform convolutional finite width networks; neural network Gaussian process (NNGP) kernels frequently outperform neural tangent (NT) kernels; centered and ensembled finite networks have reduced posterior variance and behave more similarly to infinite networks; weight decay and the use of a large learning rate break the correspondence between finite and infinite networks; the NTK parameterization outperforms the standard parameterization for finite width networks; diagonal regularization of kernels acts similarly to early stopping; floating point precision limits kernel performance beyond a critical dataset size; regularized ZCA whitening improves accuracy; finite network performance depends non-monotonically on width in ways not captured by double descent phenomena; equivariance of CNNs is only beneficial for narrow networks far from the kernel regime. Our experiments additionally motivate an improved layer-wise scaling for weight decay which improves generalization in finite-width networks. Finally, we develop improved best practices for using NNGP and NT kernels for prediction, including a novel ensembling technique. Using these best practices we achieve state-of-the-art results on CIFAR-10 classification for kernels corresponding to each architecture class we consider.},
  archivePrefix = {arXiv},
  eprint = {2007.15801},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2020_finite versus infinite neural networks.pdf},
  journal = {arXiv:2007.15801 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lee20_HighFidelitySynthesisDisentangled,
  title = {High-{{Fidelity Synthesis}} with {{Disentangled Representation}}},
  author = {Lee, Wonkwang and Kim, Donggyun and Hong, Seunghoon and Lee, Honglak},
  year = {2020},
  month = jan,
  abstract = {Learning disentangled representation of data without supervision is an important step towards improving the interpretability of generative models. Despite recent advances in disentangled representation learning, existing approaches often suffer from the trade-off between representation learning and generation performance i.e. improving generation quality sacrifices disentanglement performance). We propose an Information-Distillation Generative Adversarial Network (ID-GAN), a simple yet generic framework that easily incorporates the existing state-of-the-art models for both disentanglement learning and high-fidelity synthesis. Our method learns disentangled representation using VAE-based models, and distills the learned representation with an additional nuisance variable to the separate GAN-based generator for high-fidelity synthesis. To ensure that both generative models are aligned to render the same generative factors, we further constrain the GAN generator to maximize the mutual information between the learned latent code and the output. Despite the simplicity, we show that the proposed method is highly effective, achieving comparable image generation quality to the state-of-the-art methods using the disentangled representation. We also show that the proposed decomposition leads to an efficient and stable model design, and we demonstrate photo-realistic high-resolution image synthesis results (1024x1024 pixels) for the first time using the disentangled representations.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.04296},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2020_high-fidelity synthesis with disentangled representation.pdf;/home/trung/Zotero/storage/SVL76R6U/2001.html},
  journal = {arXiv:2001.04296 [cs, eess]},
  keywords = {disentanglement},
  primaryClass = {cs, eess}
}

@article{lee20_NeuralDirichletProcess,
  title = {A {{Neural Dirichlet Process Mixture Model}} for {{Task}}-{{Free Continual Learning}}},
  author = {Lee, Soochan and Ha, Junsoo and Zhang, Dongsu and Kim, Gunhee},
  year = {2020},
  month = jan,
  abstract = {Despite the growing interest in continual learning, most of its contemporary works have been studied in a rather restricted setting where tasks are clearly distinguishable, and task boundaries are known during training. However, if our goal is to develop an algorithm that learns as humans do, this setting is far from realistic, and it is essential to develop a methodology that works in a task-free manner. Meanwhile, among several branches of continual learning, expansion-based methods have the advantage of eliminating catastrophic forgetting by allocating new resources to learn new data. In this work, we propose an expansion-based approach for task-free continual learning. Our model, named Continual Neural Dirichlet Process Mixture (CN-DPM), consists of a set of neural network experts that are in charge of a subset of the data. CN-DPM expands the number of experts in a principled way under the Bayesian nonparametric framework. With extensive experiments, we show that our model successfully performs task-free continual learning for both discriminative and generative tasks such as image classification and image generation.},
  archivePrefix = {arXiv},
  eprint = {2001.00689},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lee et al_2020_a neural dirichlet process mixture model for task-free continual learning.pdf},
  journal = {arXiv:2001.00689 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{leeb20_StructuralAutoencodersImprove,
  title = {Structural {{Autoencoders Improve Representations}} for {{Generation}} and {{Transfer}}},
  author = {Leeb, Felix and Annadani, Yashas and Bauer, Stefan and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = jun,
  abstract = {We study the problem of structuring a learned representation to significantly improve performance without supervision. Unlike most methods which focus on using side information like weak supervision or defining new regularization objectives, we focus on improving the learned representation by structuring the architecture of the model. We propose a self-attention based architecture to make the encoder explicitly associate parts of the representation with parts of the input observation. Meanwhile, our structural decoder architecture encourages a hierarchical structure in the latent space, akin to structural causal models, and learns a natural ordering of the latent mechanisms. We demonstrate how these models learn a representation which improves results in a variety of downstream tasks including generation, disentanglement, and transfer using several challenging and natural image datasets.},
  archivePrefix = {arXiv},
  eprint = {2006.07796},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/leeb et al_2020_structural autoencoders improve representations for generation and transfer.pdf},
  journal = {arXiv:2006.07796 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{leemis08_UnivariateDistributionRelationships,
  title = {Univariate {{Distribution Relationships}}},
  author = {Leemis, Lawrence M and McQueston, Jacquelyn T},
  year = {2008},
  month = feb,
  volume = {62},
  pages = {45--53},
  issn = {0003-1305, 1537-2731},
  doi = {10.1198/000313008X270448},
  file = {/home/trung/GoogleDrive/Zotero/leemis et al_2008_univariate distribution relationships.pdf},
  journal = {The American Statistician},
  language = {en},
  number = {1}
}

@article{legg07_UniversalIntelligenceDefinition,
  title = {Universal {{Intelligence}}: {{A Definition}} of {{Machine Intelligence}}},
  shorttitle = {Universal {{Intelligence}}},
  author = {Legg, Shane and Hutter, Marcus},
  year = {2007},
  month = dec,
  abstract = {A fundamental problem in artificial intelligence is that nobody really knows what intelligence is. The problem is especially acute when we need to consider artificial systems which are significantly different to humans. In this paper we approach this problem in the following way: We take a number of well known informal definitions of human intelligence that have been given by experts, and extract their essential features. These are then mathematically formalised to produce a general measure of intelligence for arbitrary machines. We believe that this equation formally captures the concept of machine intelligence in the broadest reasonable sense. We then show how this formal definition is related to the theory of universal optimal learning agents. Finally, we survey the many other tests and definitions of intelligence that have been proposed for machines.},
  annotation = {ZSCC: 0000370},
  archivePrefix = {arXiv},
  eprint = {0712.3329},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/legg et al_2007_universal intelligence.pdf;/home/trung/Zotero/storage/DK4VG39Z/0712.html},
  journal = {arXiv:0712.3329 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  primaryClass = {cs}
}

@article{legg11_ApproximationUniversalIntelligence,
  title = {An {{Approximation}} of the {{Universal Intelligence Measure}}},
  author = {Legg, Shane and Veness, Joel},
  year = {2011},
  month = sep,
  abstract = {The Universal Intelligence Measure is a recently proposed formal definition of intelligence. It is mathematically specified, extremely general, and captures the essence of many informal definitions of intelligence. It is based on Hutter's Universal Artificial Intelligence theory, an extension of Ray Solomonoff's pioneering work on universal induction. Since the Universal Intelligence Measure is only asymptotically computable, building a practical intelligence test from it is not straightforward. This paper studies the practical issues involved in developing a real-world UIM-based performance metric. Based on our investigation, we develop a prototype implementation which we use to evaluate a number of different artificial agents.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1109.5951},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/legg et al_2011_an approximation of the universal intelligence measure.pdf;/home/trung/Zotero/storage/ZZVIB9GR/1109.html},
  journal = {arXiv:1109.5951 [cs]},
  keywords = {Computer Science - Artificial Intelligence,universal intelligence measure},
  primaryClass = {cs}
}

@article{leglaive19_Semisupervisedmultichannelspeech,
  title = {Semi-Supervised Multichannel Speech Enhancement with Variational Autoencoders and Non-Negative Matrix Factorization},
  author = {Leglaive, Simon and Girin, Laurent and Horaud, Radu},
  year = {2019},
  month = may,
  pages = {101--105},
  doi = {10.1109/ICASSP.2019.8683704},
  abstract = {In this paper we address speaker-independent multichannel speech enhancement in unknown noisy environments. Our work is based on a well-established multichannel local Gaussian modeling framework. We propose to use a neural network for modeling the speech spectro-temporal content. The parameters of this supervised model are learned using the framework of variational autoencoders. The noisy recording environment is supposed to be unknown, so the noise spectro-temporal modeling remains unsupervised and is based on non-negative matrix factorization (NMF). We develop a Monte Carlo expectation-maximization algorithm and we experimentally show that the proposed approach outperforms its NMF-based counterpart, where speech is modeled using supervised NMF.},
  archivePrefix = {arXiv},
  eprint = {1811.06713},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/leglaive et al_2019_semi-supervised multichannel speech enhancement with variational autoencoders and non-negative matrix factorization.pdf},
  journal = {ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  language = {en}
}

@article{leiva00_MathematicsUnknown,
  title = {The {{Mathematics}} of the {{Unknown}}},
  author = {Leiva, R A Garc{\'i}a},
  pages = {242},
  file = {/home/trung/GoogleDrive/Zotero/leiva_the mathematics of the unknown.pdf},
  language = {en}
}

@article{lemberger20_PrimerDomainAdaptation,
  title = {A {{Primer}} on {{Domain Adaptation}}},
  author = {Lemberger, Pirmin and Panico, Ivan},
  year = {2020},
  month = feb,
  abstract = {Standard supervised machine learning assumes that the distribution of the source samples used to train an algorithm is the same as the one of the target samples on which it is supposed to make predictions. However, as any data scientist will confirm, this is hardly ever the case in practice. The set of statistical and numerical methods that deal with such situations is known as domain adaptation, a field with a long and rich history. The myriad of methods available and the unfortunate lack of a clear and universally accepted terminology can however make the topic rather daunting for the newcomer. Therefore, rather than aiming at completeness, which leads to exhibiting a tedious catalog of methods, this pedagogical review aims at a coherent presentation of four important special cases: (1) prior shift, a situation in which training samples were selected according to their labels without any knowledge of their actual distribution in the target, (2) covariate shift which deals with a situation where training examples were picked according to their features but with some selection bias, (3) concept shift where the dependence of the labels on the features defers between the source and the target, and last but not least (4) subspace mapping which deals with a situation where features in the target have been subjected to an unknown distortion with respect to the source features. In each case we first build an intuition, next we provide the appropriate mathematical framework and eventually we describe a practical application.},
  archivePrefix = {arXiv},
  eprint = {2001.09994},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lemberger et al_2020_a primer on domain adaptation.pdf},
  journal = {arXiv:2001.09994 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lengerich20_DropoutOverfittingInteraction,
  title = {On {{Dropout}}, {{Overfitting}}, and {{Interaction Effects}} in {{Deep Neural Networks}}},
  author = {Lengerich, Benjamin and Xing, Eric P. and Caruana, Rich},
  year = {2020},
  month = jul,
  abstract = {We examine Dropout through the perspective of interactions: learned effects that combine multiple input variables. Given N variables, there are O(N 2) possible pairwise interactions, O(N 3) possible 3-way interactions, etc. We show that Dropout implicitly sets a learning rate for interaction effects that decays exponentially with the size of the interaction, corresponding to a regularizer that balances against the hypothesis space which grows exponentially with number of variables in the interaction. This understanding of Dropout has implications for the optimal Dropout rate: higher Dropout rates should be used when we need stronger regularization against spurious high-order interactions. This perspective also issues caution against using Dropout to measure term saliency because Dropout regularizes against terms for high-order interactions. Finally, this view of Dropout as a regularizer of interaction effects provides insight into the varying effectiveness of Dropout for different architectures and data sets. We also compare Dropout to regularization via weight decay and early stopping and find that it is difficult to obtain the same regularization effect for high-order interactions with these methods.},
  archivePrefix = {arXiv},
  eprint = {2007.00823},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lengerich et al_2020_on dropout, overfitting, and interaction effects in deep neural networks.pdf},
  journal = {arXiv:2007.00823 [cs, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, stat}
}

@article{leon-mimila19_RelevanceMultiOmicsStudies,
  title = {Relevance of {{Multi}}-{{Omics Studies}} in {{Cardiovascular Diseases}}},
  author = {{Leon-Mimila}, Paola and Wang, Jessica and {Huertas-Vazquez}, Adriana},
  year = {2019},
  month = jul,
  volume = {6},
  pages = {91},
  issn = {2297-055X},
  doi = {10.3389/fcvm.2019.00091},
  annotation = {ZSCC: 0000007},
  file = {/home/trung/Zotero/storage/A4K23ASU/Leon-Mimila et al. - 2019 - Relevance of Multi-Omics Studies in Cardiovascular.pdf},
  journal = {Frontiers in Cardiovascular Medicine},
  language = {en}
}

@article{lesort18_StateRepresentationLearning,
  title = {State {{Representation Learning}} for {{Control}}: {{An Overview}}},
  shorttitle = {State {{Representation Learning}} for {{Control}}},
  author = {Lesort, Timoth{\'e}e and {D{\'i}az-Rodr{\'i}guez}, Natalia and Goudou, Jean-Fran{\c c}ois and Filliat, David},
  year = {2018},
  month = dec,
  volume = {108},
  pages = {379--392},
  issn = {08936080},
  doi = {10.1016/j.neunet.2018.07.006},
  abstract = {Representation learning algorithms are designed to learn abstract features that characterize data. State representation learning (SRL) focuses on a particular kind of representation learning where learned features are in low dimension, evolve through time, and are influenced by actions of an agent. The representation is learned to capture the variation in the environment generated by the agent's actions; this kind of representation is particularly suitable for robotics and control scenarios. In particular, the low dimension characteristic of the representation helps to overcome the curse of dimensionality, provides easier interpretation and utilization by humans and can help improve performance and speed in policy learning algorithms such as reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1802.04181},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lesort et al_2018_state representation learning for control.pdf},
  journal = {Neural Networks},
  language = {en}
}

@article{levitan20_OpinionInfectionThat,
  title = {Opinion | {{The Infection That}}'s {{Silently Killing Coronavirus Patients}}},
  author = {Levitan, Richard},
  year = {2020},
  month = apr,
  issn = {0362-4331},
  abstract = {This is what I learned during 10 days of treating Covid pneumonia at Bellevue Hospital.},
  annotation = {ZSCC: NoCitationData[s0]},
  chapter = {Opinion},
  journal = {The New York Times},
  language = {en-US}
}

@article{levitt20_HeadsTailsImpact,
  title = {Heads or {{Tails}}: {{The Impact}} of a {{Coin Toss}} on {{Major Life Decisions}} and {{Subsequent Happiness}}},
  author = {Levitt, Steven D},
  year = {2020},
  pages = {41},
  abstract = {Little is known about whether people make good choices when facing important decisions. This paper reports on a large-scale randomized field experiment in which research subjects having difficulty making a decision flipped a coin to help determine their choice. For important decisions (e.g. quitting a job or ending a relationship), those who make a change (regardless of the outcome of the coin toss) report being substantially happier two months and six months later. This correlation, however, need not reflect a causal impact. To assess causality, I use the outcome of a coin toss. Individuals who are told by the coin toss to make a change are much more likely to make a change and are happier six months later than those who were told by the coin to maintain the status quo. The results of this paper suggest that people may be excessively cautious when facing life-changing choices.},
  file = {/home/trung/GoogleDrive/Zotero/levitt_2020_heads or tails.pdf},
  language = {en}
}

@incollection{levy14_NeuralWordEmbedding,
  title = {Neural {{Word Embedding}} as {{Implicit Matrix Factorization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 27},
  author = {Levy, Omer and Goldberg, Yoav},
  editor = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
  year = {2014},
  pages = {2177--2185},
  publisher = {{Curran Associates, Inc.}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/levy et al_2014_neural word embedding as implicit matrix factorization.pdf;/home/trung/Zotero/storage/3TD364I2/5477-neural-word-embedding-as-implicit-matrix-factorization.html}
}

@article{lewis11_PRIDBproteinRNAinterface,
  title = {{{PRIDB}}: A Protein-{{RNA}} Interface Database},
  shorttitle = {{{PRIDB}}},
  author = {Lewis, B. A. and Walia, R. R. and Terribilini, M. and Ferguson, J. and Zheng, C. and Honavar, V. and Dobbs, D.},
  year = {2011},
  month = jan,
  volume = {39},
  pages = {D277-D282},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkq1108},
  abstract = {The Protein\textendash RNA Interface Database (PRIDB) is a comprehensive database of protein\textendash RNA interfaces extracted from complexes in the Protein Data Bank (PDB). It is designed to facilitate detailed analyses of individual protein\textendash RNA complexes and their interfaces, in addition to automated generation of user-defined data sets of protein\textendash RNA interfaces for statistical analyses and machine learning applications. For any chosen PDB complex or list of complexes, PRIDB rapidly displays interfacial amino acids and ribonucleotides within the primary sequences of the interacting protein and RNA chains. PRIDB also identifies ProSite motifs in protein chains and FR3D motifs in RNA chains and provides links to these external databases, as well as to structure files in the PDB. An integrated JMol applet is provided for visualization of interacting atoms and residues in the context of the 3D complex structures. The current version of PRIDB contains structural information regarding 926 protein\textendash RNA complexes available in the PDB (as of 10 October 2010). Atomic- and residue-level contact information for the entire data set can be downloaded in a simple machine-readable format. Also, several non-redundant benchmark data sets of protein\textendash RNA complexes are provided. The PRIDB database is freely available online at http://bindr .gdcb.iastate.edu/PRIDB.},
  file = {/home/trung/GoogleDrive/Zotero/lewis et al_2011_pridb.pdf},
  journal = {Nucleic Acids Research},
  language = {en},
  number = {Database}
}

@article{lewis12_Morganrothhypothesisrevisited,
  title = {The {{Morganroth}} Hypothesis Revisited: Endurance Exercise Elicits Eccentric Hypertrophy of the Heart},
  shorttitle = {The {{Morganroth}} Hypothesis Revisited},
  author = {Lewis, E J H and McKillop, A and Banks, L},
  year = {2012},
  month = jun,
  volume = {590},
  pages = {2833--2834},
  issn = {0022-3751},
  doi = {10.1113/jphysiol.2011.226217},
  file = {/home/trung/GoogleDrive/Zotero/lewis et al_2012_the morganroth hypothesis revisited.pdf},
  journal = {The Journal of Physiology},
  number = {Pt 12},
  pmcid = {PMC3448147},
  pmid = {22707591}
}

@article{li00_ApproximateInferenceNew,
  title = {Approximate {{Inference}}: {{New Visions}}},
  author = {Li, Yingzhen},
  pages = {222},
  file = {/home/trung/GoogleDrive/Zotero/li_approximate inference.pdf},
  keywords = {variational},
  language = {en}
}

@article{li00_estimatingepistemicuncertainty,
  title = {On Estimating Epistemic Uncertainty},
  author = {Li, Yingzhen},
  pages = {17},
  file = {/home/trung/GoogleDrive/Zotero/li_on estimating epistemic uncertainty.pdf},
  keywords = {representation},
  language = {en}
}

@article{li00_LiteratureReviewNeural,
  title = {A {{Literature Review}} of {{Neural Style Transfer}}},
  author = {Li, Haochen},
  pages = {9},
  abstract = {Neural Style Transfer is the problem of taking a content image and a style image as input, and outputting an image that has the content of the content image and the style of the style image. The key technique that makes neural style transfer possible is convolutional neural network(CNN). This paper will first survey major techniques of doing neural style transfer on images, and then briefly examine one way of extending neural style transfer to videos.},
  file = {/home/trung/GoogleDrive/Zotero/li_a literature review of neural style transfer.pdf},
  language = {en}
}

@inproceedings{li16_GenerativeTopicEmbedding,
  title = {Generative {{Topic Embedding}}: A {{Continuous Representation}} of {{Documents}}},
  shorttitle = {Generative {{Topic Embedding}}},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Li, Shaohua and Chua, Tat-Seng and Zhu, Jun and Miao, Chunyan},
  year = {2016},
  pages = {666--675},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-1063},
  abstract = {Word embedding maps words into a lowdimensional continuous embedding space by exploiting the local word collocation patterns in a small context window. On the other hand, topic modeling maps documents onto a low-dimensional topic space, by utilizing the global word collocation patterns in the same document. These two types of patterns are complementary. In this paper, we propose a generative topic embedding model to combine the two types of patterns. In our model, topics are represented by embedding vectors, and are shared across documents. The probability of each word is influenced by both its local context and its topic. A variational inference method yields the topic embeddings as well as the topic mixing proportions for each document. Jointly they represent the document in a low-dimensional continuous space. In two document classification tasks, our method performs better than eight existing methods, with fewer features. In addition, we illustrate with an example that our method can generate coherent topics even based on only one document.},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2016_generative topic embedding.pdf},
  language = {en}
}

@article{li16_TernaryWeightNetworks,
  title = {Ternary {{Weight Networks}}},
  author = {Li, Fengfu and Zhang, Bo and Liu, Bin},
  year = {2016},
  month = nov,
  abstract = {We introduce ternary weight networks (TWNs) - neural networks with weights constrained to +1, 0 and -1. The Euclidian distance between full (float or double) precision weights and the ternary weights along with a scaling factor is minimized. Besides, a threshold-based ternary function is optimized to get an approximated solution which can be fast and easily computed. TWNs have stronger expressive abilities than the recently proposed binary precision counterparts and are thus more effective than the latter. Meanwhile, TWNs achieve up to 16\$\textbackslash times\$ or 32\$\textbackslash times\$ model compression rate and need fewer multiplications compared with the full precision counterparts. Benchmarks on MNIST, CIFAR-10, and large scale ImageNet datasets show that the performance of TWNs is only slightly worse than the full precision counterparts but outperforms the analogous binary precision counterparts a lot.},
  annotation = {ZSCC: 0000348},
  archivePrefix = {arXiv},
  eprint = {1605.04711},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2016_ternary weight networks.pdf;/home/trung/Zotero/storage/BAC4FIZV/1605.html},
  journal = {arXiv:1605.04711 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{li17_GatedGraphSequence,
  title = {Gated {{Graph Sequence Neural Networks}}},
  author = {Li, Yujia and Tarlow, Daniel and Brockschmidt, Marc and Zemel, Richard},
  year = {2017},
  month = sep,
  abstract = {Graph-structured data appears frequently in domains including chemistry, natural language semantics, social networks, and knowledge bases. In this work, we study feature learning techniques for graph-structured inputs. Our starting point is previous work on Graph Neural Networks (Scarselli et al., 2009), which we modify to use gated recurrent units and modern optimization techniques and then extend to output sequences. The result is a flexible and broadly useful class of neural network models that has favorable inductive biases relative to purely sequence-based models (e.g., LSTMs) when the problem is graph-structured. We demonstrate the capabilities on some simple AI (bAbI) and graph algorithm learning tasks. We then show it achieves state-of-the-art performance on a problem from program verification, in which subgraphs need to be matched to abstract data structures.},
  archivePrefix = {arXiv},
  eprint = {1511.05493},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2017_gated graph sequence neural networks.pdf;/home/trung/GoogleDrive/Zotero/li et al_2017_gated graph sequence neural networks2.pdf;/home/trung/Zotero/storage/MAPGSTR6/1511.html;/home/trung/Zotero/storage/QSBPSCEE/1511.html},
  journal = {arXiv:1511.05493 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,graph,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{li18_DisentangledSequentialAutoencoder,
  title = {Disentangled {{Sequential Autoencoder}}},
  author = {Li, Yingzhen and Mandt, Stephan},
  year = {2018},
  month = jun,
  abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
  annotation = {ZSCC: 0000024},
  archivePrefix = {arXiv},
  eprint = {1803.02991},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2018_disentangled sequential autoencoder.pdf;/home/trung/Zotero/storage/NH8X6KNL/1803.html},
  journal = {arXiv:1803.02991 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{li18_DisentangledSequentialAutoencodera,
  title = {Disentangled {{Sequential Autoencoder}}},
  author = {Li, Yingzhen and Mandt, Stephan},
  year = {2018},
  month = jun,
  abstract = {We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio. Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content). This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features. In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping. For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics. Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.},
  annotation = {ZSCC: 0000026},
  archivePrefix = {arXiv},
  eprint = {1803.02991},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/5V697SA4/Li and Mandt - 2018 - Disentangled Sequential Autoencoder.pdf},
  journal = {arXiv:1803.02991 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{li18_DisentangledVariationalAutoEncoder,
  title = {Disentangled {{Variational Auto}}-{{Encoder}} for {{Semi}}-Supervised {{Learning}}},
  author = {Li, Yang and Pan, Quan and Wang, Suhang and Peng, Haiyun and Yang, Tao and Cambria, Erik},
  year = {2018},
  month = dec,
  abstract = {Semi-supervised learning is attracting increasing attention due to the fact that datasets of many domains lack enough labeled data. Variational Auto-Encoder (VAE), in particular, has demonstrated the benefits of semi-supervised learning. The majority of existing semi-supervised VAEs utilize a classifier to exploit label information, where the parameters of the classifier are introduced to the VAE. Given the limited labeled data, learning the parameters for the classifiers may not be an optimal solution for exploiting label information. Therefore, in this paper, we develop a novel approach for semi-supervised VAE without classifier. Specifically, we propose a new model called Semi-supervised Disentangled VAE (SDVAE), which encodes the input data into disentangled representation and non-interpretable representation, then the category information is directly utilized to regularize the disentangled representation via the equality constraint. To further enhance the feature learning ability of the proposed VAE, we incorporate reinforcement learning to relieve the lack of data. The dynamic framework is capable of dealing with both image and text data with its corresponding encoder and decoder networks. Extensive experiments on image and text datasets demonstrate the effectiveness of the proposed framework.},
  archivePrefix = {arXiv},
  eprint = {1709.05047},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2018_disentangled variational auto-encoder for semi-supervised learning.pdf},
  journal = {arXiv:1709.05047 [cs]},
  primaryClass = {cs}
}

@article{li18_LearningDeepGenerative,
  title = {Learning {{Deep Generative Models}} of {{Graphs}}},
  author = {Li, Yujia and Vinyals, Oriol and Dyer, Chris and Pascanu, Razvan and Battaglia, Peter},
  year = {2018},
  month = mar,
  abstract = {Graphs are fundamental data structures which concisely capture the relational structure in many important real-world domains, such as knowledge graphs, physical and social interactions, language, and chemistry. Here we introduce a powerful new approach for learning generative models over graphs, which can capture both their structure and attributes. Our approach uses graph neural networks to express probabilistic dependencies among a graph's nodes and edges, and can, in principle, learn distributions over any arbitrary graph. In a series of experiments our results show that once trained, our models can generate good quality samples of both synthetic graphs as well as real molecular graphs, both unconditionally and conditioned on data. Compared to baselines that do not use graph-structured representations, our models often perform far better. We also explore key challenges of learning generative models of graphs, such as how to handle symmetries and ordering of elements during the graph generation process, and offer possible solutions. Our work is the first and most general approach for learning generative models over arbitrary graphs, and opens new directions for moving away from restrictions of vector- and sequence-like knowledge representations, toward more expressive and flexible relational data structures.},
  annotation = {ZSCC: 0000138},
  archivePrefix = {arXiv},
  eprint = {1803.03324},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/Z7H9Z6IM/Li et al. - 2018 - Learning Deep Generative Models of Graphs.pdf},
  journal = {arXiv:1803.03324 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{li18_scImputeaccuraterobust,
  title = {{{scImpute}}: {{An}} Accurate and Robust Imputation Method {{scImpute}} for Single-Cell {{RNA}}-Seq Data},
  author = {Li, Wei Vivian and Li, Jingyi Jessica},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {997},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-03405-7},
  annotation = {ZSCC: 0000120},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2018_scimpute.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{li18_UnderstandingDisharmonyDropout,
  title = {Understanding the {{Disharmony}} between {{Dropout}} and {{Batch Normalization}} by {{Variance Shift}}},
  author = {Li, Xiang and Chen, Shuo and Hu, Xiaolin and Yang, Jian},
  year = {2018},
  month = jan,
  abstract = {This paper first answers the question "why do the two most powerful techniques Dropout and Batch Normalization (BN) often lead to a worse performance when they are combined together?" in both theoretical and statistical aspects. Theoretically, we find that Dropout would shift the variance of a specific neural unit when we transfer the state of that network from train to test. However, BN would maintain its statistical variance, which is accumulated from the entire learning procedure, in the test phase. The inconsistency of that variance (we name this scheme as "variance shift") causes the unstable numerical behavior in inference that leads to more erroneous predictions finally, when applying Dropout before BN. Thorough experiments on DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to the uncovered mechanism, we next explore several strategies that modifies Dropout and try to overcome the limitations of their combination by avoiding the variance shift risks.},
  archivePrefix = {arXiv},
  eprint = {1801.05134},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2018_understanding the disharmony between dropout and batch normalization by variance shift.pdf},
  journal = {arXiv:1801.05134 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{li19_ARMINMoreEfficient,
  title = {{{ARMIN}}: {{Towards}} a {{More Efficient}} and {{Light}}-Weight {{Recurrent Memory Network}}},
  shorttitle = {{{ARMIN}}},
  author = {Li, Zhangheng and Zhong, Jia-Xing and Huang, Jingjia and Zhang, Tao and Li, Thomas and Li, Ge},
  year = {2019},
  month = jun,
  abstract = {In recent years, memory-augmented neural networks(MANNs) have shown promising power to enhance the memory ability of neural networks for sequential processing tasks. However, previous MANNs suffer from complex memory addressing mechanism, making them relatively hard to train and causing computational overheads. Moreover, many of them reuse the classical RNN structure such as LSTM for memory processing, causing inefficient exploitations of memory information. In this paper, we introduce a novel MANN, the Auto-addressing and Recurrent Memory Integrating Network (ARMIN) to address these issues. The ARMIN only utilizes hidden state ht for automatic memory addressing, and uses a novel RNN cell for refined integration of memory information. Empirical results on a variety of experiments demonstrate that the ARMIN is more light-weight and efficient compared to existing memory networks. Moreover, we demonstrate that the ARMIN can achieve much lower computational overhead than vanilla LSTM while keeping similar performances. Codes are available on github.com/zoharli/armin.},
  archivePrefix = {arXiv},
  eprint = {1906.12087},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_armin.pdf;/home/trung/Zotero/storage/AHA37YVZ/1906.html},
  journal = {arXiv:1906.12087 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,memory,recurrent,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{li19_AttentiveNormalization,
  title = {Attentive {{Normalization}}},
  author = {Li, Xilai and Sun, Wei and Wu, Tianfu},
  year = {2019},
  month = aug,
  abstract = {Batch Normalization (BN) is a vital pillar in the development of deep learning with many recent variations such as Group Normalization (GN) and Switchable Normalization. Channel-wise feature attention methods such as the squeeze-and-excitation (SE) unit have also shown impressive performance improvement. BN and its variants take into account different ways of computing the mean and variance within a min-batch for feature normalization, followed by a learnable channel-wise affine transformation. SE explicitly learns how to adaptively recalibrate channel-wise feature responses. They have been studied separately, however. In this paper, we propose a novel and lightweight integration of feature normalization and feature channel-wise attention. We present Attentive Normalization (AN) as a simple and unified alternative. AN absorbs SE into the affine transformation of BN. AN learns a small number of scale and offset parameters per channel (i.e., different affine transformations). Their weighted sums (i.e., mixture) are used in the final affine transformation. The weights are instance-specific and learned in a way that channel-wise attention is considered, similar in spirit to the squeeze module in the SE unit. AN is complementary and applicable to existing variants of BN. In experiments, we test AN in the ImageNet-1K classification dataset and the MS-COCO object detection and instance segmentation dataset with significantly better performance obtained than the vanilla BN. Our AN also outperforms two state-of-the-art variants of BN, GN and SN. The source code will be released at \textbackslash url\{http://github.com/ivMCL/AttentiveNorm\}.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.01259},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_attentive normalization.pdf;/home/trung/Zotero/storage/VTA9FWCR/1908.html},
  journal = {arXiv:1908.01259 [cs]},
  keywords = {attetion,Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@techreport{li19_Deeplearningenables,
  title = {Deep Learning Enables Accurate Clustering and Batch Effect Removal in Single-Cell {{RNA}}-Seq Analysis: {{Supplementary}} Material},
  shorttitle = {Deep Learning Enables Accurate Clustering and Batch Effect Removal in Single-Cell {{RNA}}-Seq Analysis},
  author = {Li, Xiangjie and Lyu, Yafei and Park, Jihwan and Zhang, Jingxiao and Stambolian, Dwight and Susztak, Katalin and Hu, Gang and Li, Mingyao},
  year = {2019},
  month = jan,
  institution = {{Genomics}},
  doi = {10.1101/530378},
  abstract = {Single-cell RNA sequencing (scRNA-seq) can characterize cell types and states through unsupervised clustering, but the ever increasing number of cells imposes computational challenges. We present an unsupervised deep embedding algorithm for single-cell clustering (DESC) that iteratively learns cluster-specific gene expression signatures and cluster assignment. DESC significantly improves clustering accuracy across various datasets and is capable of removing complex batch effects while maintaining true biological variations.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_deep learning enables accurate clustering and batch effect removal in single-cell rna-seq analysis.pdf},
  language = {en},
  type = {Preprint}
}

@article{li19_ForestTreesGeneration,
  title = {A {{Forest}} from the {{Trees}}: {{Generation}} through {{Neighborhoods}}},
  shorttitle = {A {{Forest}} from the {{Trees}}},
  author = {Li, Yang and Gao, Tianxiang and Oliva, Junier B.},
  year = {2019},
  month = nov,
  abstract = {In this work, we propose to learn a generative model using both learned features (through a latent space) and memories (through neighbors). Although human learning makes seamless use of both learned perceptual features and instance recall, current generative learning paradigms only make use of one of these two components. Take, for instance, flow models, which learn a latent space of invertible features that follow a simple distribution. Conversely, kernel density techniques use instances to shift a simple distribution into an aggregate mixture model. Here we propose multiple methods to enhance the latent space of a flow model with neighborhood information. Not only does our proposed framework represent a more human-like approach by leveraging both learned features and memories, but it may also be viewed as a step forward in non-parametric methods. The efficacy of our model is shown empirically with standard image datasets. We observe compelling results and a significant improvement over baselines.},
  archivePrefix = {arXiv},
  eprint = {1902.01435},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_a forest from the trees.pdf},
  journal = {arXiv:1902.01435 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{li19_HierarchyGraphNeural,
  title = {A {{Hierarchy}} of {{Graph Neural Networks Based}} on {{Learnable Local Features}}},
  author = {Li, Michael Lingzhi and Dong, Meng and Zhou, Jiawei and Rush, Alexander M.},
  year = {2019},
  month = nov,
  abstract = {Graph neural networks (GNNs) are a powerful tool to learn representations on graphs by iteratively aggregating features from node neighbourhoods. Many variant models have been proposed, but there is limited understanding on both how to compare different architectures and how to construct GNNs systematically. Here, we propose a hierarchy of GNNs based on their aggregation regions. We derive theoretical results about the discriminative power and feature representation capabilities of each class. Then, we show how this framework can be utilized to systematically construct arbitrarily powerful GNNs. As an example, we construct a simple architecture that exceeds the expressiveness of the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theory on both synthetic and real-world benchmarks, and demonstrate our example's theoretical power translates to strong results on node classification, graph classification, and graph regression tasks.},
  archivePrefix = {arXiv},
  eprint = {1911.05256},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_a hierarchy of graph neural networks based on learnable local features.pdf},
  journal = {arXiv:1911.05256 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{li19_IdentifyingFlowsRecovering,
  title = {Identifying through {{Flows}} for {{Recovering Latent Representations}}},
  author = {Li, Shen and Hooi, Bryan and Lee, Gim Hee},
  year = {2019},
  month = sep,
  abstract = {Identifiability, or recovery of the true latent representations from which the observed data originates, is a fundamental goal of representation learning. However, most deep generative models do not address the question of identifiability, and cannot recover the true latent sources that generate the observations. Recent work proposed identifiable generative modelling using variational autoencoders (iVAE) with a theory of identifiability. However, due to the intractablity of KL divergence between variational approximate posterior and the true posterior, iVAE has to maximize the evidence lower bound of the marginal likelihood, leading to suboptimal solutions in both theory and practice. In contrast, we propose an identifiable framework for estimating latent representations using a flow-based model (iFlow). Our approach directly maximizes the marginal likelihood, allowing for theoretical guarantees on identifiability, without the need for variational approximations. We derive its learning objective in analytical form, making it possible to train iFlow in an end-to-end manner. Simulations on synthetic data validate the correctness and effectiveness of our proposed method and demonstrate its practical advantages over other existing methods.},
  archivePrefix = {arXiv},
  eprint = {1909.12555},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_identifying through flows for recovering latent representations.pdf;/home/trung/Zotero/storage/RPHPAPHA/1909.html},
  journal = {arXiv:1909.12555 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{li19_ImprovevariationalautoEncoder,
  title = {Improve Variational {{autoEncoder}} with Auxiliary Softmax Multiclassifier},
  author = {Li, Yao},
  year = {2019},
  month = nov,
  abstract = {As a general-purpose generative model architecture, VAE has been widely used in the field of image and natural language processing. VAE maps high dimensional sample data into continuous latent variables with unsupervised learning. Sampling in the latent variable space of the feature, VAE can construct new image or text data. As a general-purpose generation model, the vanilla VAE can not fit well with various data sets and neural networks with different structures. Because of the need to balance the accuracy of reconstruction and the convenience of latent variable sampling in the training process, VAE often has problems known as "posterior collapse". images reconstructed by VAE are also often blurred. In this paper, we analyze the main cause of these problem, which is the lack of mutual information between the sample variable and the latent feature variable during the training process. To maintain mutual information in model training, we propose to use the auxiliary softmax multi-classification network structure to improve the training effect of VAE, named VAE-AS. We use MNIST and Omniglot data sets to test the VAE-AS model. Based on the test results, It can be show that VAE-AS has obvious effects on the mutual information adjusting and solving the posterior collapse problem.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.06966},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li_2019_improve variational autoencoder with auxiliary softmax multiclassifier.pdf;/home/trung/Zotero/storage/DVLI6GJ7/1908.html},
  journal = {arXiv:1908.06966 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{li19_ImprovevariationalautoEncodera,
  title = {Improve Variational {{autoEncoder}} with Auxiliary Softmax Multiclassifier},
  author = {Li, Yao},
  year = {2019},
  month = nov,
  abstract = {As a general-purpose generative model architecture, VAE has been widely used in the field of image and natural language processing. VAE maps high dimensional sample data into continuous latent variables with unsupervised learning. Sampling in the latent variable space of the feature, VAE can construct new image or text data. As a general-purpose generation model, the vanilla VAE can not fit well with various data sets and networks with different structures. Because of the need to balance the accuracy of reconstruction and the convenience of latent variable sampling in the training process, VAE often has problems known as ``posterior collapse'', and images reconstructed by VAE are also often blurred. In this paper, we analyze the main cause of these problem, which is the lack of control over mutual information between the sample variable and the latent feature variable during the training process. To maintain mutual information in model training, we propose to use the auxiliary softmax multi-classification network structure to improve the training effect of VAE, named VAE-AS. We use MNIST and Omniglot data sets to test the VAE-AS model. Based on the test results, It can be show that VAE-AS has obvious effects on the mutual information adjusting and solving the posterior collapse problem.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.06966},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Li_2019_Improve variational autoEncoder with auxiliary softmax multiclassifier2.pdf},
  journal = {arXiv:1908.06966 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{li19_RelieveYourHeadache,
  title = {To {{Relieve Your Headache}} of {{Training}} an {{MRF}}, {{Take AdVIL}}},
  author = {Li, Chongxuan and Du, Chao and Xu, Kun and Welling, Max and Zhu, Jun and Zhang, Bo},
  year = {2019},
  month = jan,
  abstract = {We propose a black-box algorithm called \{\textbackslash it Adversarial Variational Inference and Learning\} (AdVIL) to perform inference and learning on a general Markov random field (MRF). AdVIL employs two variational distributions to approximately infer the latent variables and estimate the partition function of an MRF, respectively. The two variational distributions provide an estimate of the negative log-likelihood of the MRF as a minimax optimization problem, which is solved by stochastic gradient descent. AdVIL is proven convergent under certain conditions. On one hand, compared with contrastive divergence, AdVIL requires a minimal assumption about the model structure and can deal with a broader family of MRFs. On the other hand, compared with existing black-box methods, AdVIL provides a tighter estimate of the log partition function and achieves much better empirical results.},
  archivePrefix = {arXiv},
  eprint = {1901.08400},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_to relieve your headache of training an mrf, take advil.pdf;/home/trung/Zotero/storage/HWI33F2Z/1901.html},
  journal = {arXiv:1901.08400 [cs, stat]},
  keywords = {Computer Science - Machine Learning,mrf,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@techreport{li19_scOpenchromatinaccessibilityestimation,
  title = {{{scOpen}}: Chromatin-Accessibility Estimation of Single-Cell {{ATAC}} Data},
  shorttitle = {{{scOpen}}},
  author = {Li, Zhijian and Kuppe, Christoph and Cheng, Mingbo and Menzel, Sylvia and Zenke, Martin and Kramann, Rafael and Costa, Ivan G.},
  year = {2019},
  month = dec,
  institution = {{Bioinformatics}},
  doi = {10.1101/865931},
  abstract = {ABSTRACT           We propose scOpen, a computational method for quantifying the open chromatin status of regulatory regions from single cell ATAC-seq (scATAC-seq) experiments. scOpen is based on positive-unlabelled learning of matrices and estimates the probability that a region is open at a given cell by mitigating the sparsity of scATAC-seq matrices. We demonstrate that scOpen improves all down-stream analysis steps of scATAC-seq data as clustering, visualisation and chromatin conformation. Moreover, we show the power of scOpen and single cell-based footprinting analysis (scHINT) to dissect regulatory changes in the development of fibrosis in the kidney.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_scopen.pdf},
  language = {en},
  type = {Preprint}
}

@article{li19_SurprisinglyEffectiveFix,
  title = {A {{Surprisingly Effective Fix}} for {{Deep Latent Variable Modeling}} of {{Text}}},
  author = {Li, Bohan and He, Junxian and Neubig, Graham and {Berg-Kirkpatrick}, Taylor and Yang, Yiming},
  year = {2019},
  month = sep,
  abstract = {When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.},
  archivePrefix = {arXiv},
  eprint = {1909.00868},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_a surprisingly effective fix for deep latent variable modeling of text.pdf},
  journal = {arXiv:1909.00868 [cs, stat]},
  keywords = {_tablet,vae_issues},
  language = {en},
  primaryClass = {cs, stat}
}

@article{li19_tutorialDirichletprocess,
  title = {A Tutorial on {{Dirichlet}} Process Mixture Modeling},
  author = {Li, Yuelin and Schofield, Elizabeth and G{\"o}nen, Mithat},
  year = {2019},
  month = aug,
  volume = {91},
  pages = {128--144},
  issn = {00222496},
  doi = {10.1016/j.jmp.2019.04.004},
  abstract = {Bayesian nonparametric (BNP) models are becoming increasingly important in psychology, both as theoretical models of cognition and as analytic tools. However, existing tutorials tend to be at a level of abstraction largely impenetrable by non-technicians. This tutorial aims to help beginners understand key concepts by working through important but often omitted derivations carefully and explicitly, with a focus on linking the mathematics with a practical computation solution for a Dirichlet Process Mixture Model (DPMM)\textemdash one of the most widely used BNP methods. Abstract concepts are made explicit and concrete to non-technical readers by working through the theory that gives rise to them. A publicly accessible computer program written in the statistical language R is explained line-by-line to help readers understand the computation algorithm. The algorithm is also linked to a construction method called the Chinese Restaurant Process in an accessible tutorial in this journal (Gershman \& Blei, 2012). The overall goals are to help readers understand more fully the theory and application so that they may apply BNP methods in their own work and leverage the technical details in this tutorial to develop novel methods.},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2019_a tutorial on dirichlet process mixture modeling.pdf},
  journal = {Journal of Mathematical Psychology},
  language = {en}
}

@article{li20_AnalyzingCOVID19Online,
  title = {Analyzing {{COVID}}-19 on {{Online Social Media}}: {{Trends}}, {{Sentiments}} and {{Emotions}}},
  shorttitle = {Analyzing {{COVID}}-19 on {{Online Social Media}}},
  author = {Li, Xiaoya and Zhou, Mingxin and Wu, Jiawei and Yuan, Arianna and Wu, Fei and Li, Jiwei},
  year = {2020},
  month = jun,
  abstract = {At the time of writing, the ongoing pandemic of coronavirus disease (COVID-19) has caused severe impacts on society, economy and people's daily lives. People constantly express their opinions on various aspects of the pandemic on social media, making user-generated content an important source for understanding public emotions and concerns.},
  archivePrefix = {arXiv},
  eprint = {2005.14464},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_analyzing covid-19 on online social media.pdf},
  journal = {arXiv:2005.14464 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{li20_BayesianxvectorBayesian,
  title = {Bayesian X-Vector: {{Bayesian Neural Network}} Based x-Vector {{System}} for {{Speaker Verification}}},
  shorttitle = {Bayesian X-Vector},
  author = {Li, Xu and Zhong, Jinghua and Yu, Jianwei and Hu, Shoukang and Wu, Xixin and Liu, Xunying and Meng, Helen},
  year = {2020},
  month = apr,
  abstract = {Speaker verification systems usually suffer from the mismatch problem between training and evaluation data, such as speaker population mismatch, the channel and environment variations. In order to address this issue, it requires the system to have good generalization ability on unseen data. In this work, we incorporate Bayesian neural networks (BNNs) into the deep neural network (DNN) x-vector speaker verification system to improve the system's generalization ability. With the weight uncertainty modeling provided by BNNs, we expect the system could generalize better on the evaluation data and make verification decisions more accurately. Our experiment results indicate that the DNN x-vector system could benefit from BNNs especially when the mismatch problem is severe for evaluations using out-of-domain data. Specifically, results show that the system could benefit from BNNs by a relative EER decrease of 2.66\% and 2.32\% respectively for short- and long-utterance in-domain evaluations. Additionally, the fusion of DNN x-vector and Bayesian x-vector systems could achieve further improvement. Moreover, experiments conducted by out-of-domain evaluations, e.g. models trained on Voxceleb1 while evaluated on NIST SRE10 core test, suggest that BNNs could bring a larger relative EER decrease of around 4.69\%.},
  archivePrefix = {arXiv},
  eprint = {2004.04014},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_bayesian x-vector.pdf},
  journal = {arXiv:2004.04014 [cs, eess]},
  keywords = {_tablet},
  primaryClass = {cs, eess}
}

@article{li20_DividemixLearningNoisy,
  title = {Dividemix: {{Learning}} with {{Noisy Labels}} as {{Semi}}-{{Supervised Learning}}},
  author = {Li, Junnan and Socher, Richard and Hoi, Steven C H},
  year = {2020},
  pages = {14},
  abstract = {Deep neural networks are known to be annotation-hungry. Numerous efforts have been devoted to reducing the annotation cost when learning with deep networks. Two prominent directions include learning with noisy labels and semi-supervised learning by exploiting unlabeled data. In this work, we propose DivideMix, a novel framework for learning with noisy labels by leveraging semi-supervised learning techniques. In particular, DivideMix models the per-sample loss distribution with a mixture model to dynamically divide the training data into a labeled set with clean samples and an unlabeled set with noisy samples, and trains the model on both the labeled and unlabeled data in a semi-supervised manner. To avoid confirmation bias, we simultaneously train two diverged networks where each network uses the dataset division from the other network. During the semi-supervised training phase, we improve the MixMatch strategy by performing label co-refinement and label co-guessing on labeled and unlabeled samples, respectively. Experiments on multiple benchmark datasets demonstrate substantial improvements over state-of-the-art methods. Code is available at https://github.com/LiJunnan1992/DivideMix.},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_dividemix.pdf},
  language = {en}
}

@article{li20_LearningInvariantRepresentations,
  title = {Learning {{Invariant Representations}} and {{Risks}} for {{Semi}}-Supervised {{Domain Adaptation}}},
  author = {Li, Bo and Wang, Yezhen and Zhang, Shanghang and Li, Dongsheng and Darrell, Trevor and Keutzer, Kurt and Zhao, Han},
  year = {2020},
  month = oct,
  abstract = {The success of supervised learning hinges on the assumption that the training and test data come from the same underlying distribution, which is often not valid in practice due to potential distribution shift. In light of this, most existing methods for unsupervised domain adaptation focus on achieving domain-invariant representations and small source domain error. However, recent works have shown that this is not sufficient to guarantee good generalization on the target domain, and in fact, is provably detrimental under label distribution shift. Furthermore, in many real-world applications it is often feasible to obtain a small amount of labeled data from the target domain and use them to facilitate model training with source data. Inspired by the above observations, in this paper we propose the first method that aims to simultaneously learn invariant representations and risks under the setting of semi-supervised domain adaptation (Semi-DA). First, we provide a finite sample bound for both classification and regression problems under Semi-DA. The bound suggests a principled way to obtain target generalization, i.e. by aligning both the marginal and conditional distributions across domains in feature space. Motivated by this, we then introduce the LIRR algorithm for jointly \textbackslash textbf\{L\}earning \textbackslash textbf\{I\}nvariant \textbackslash textbf\{R\}epresentations and \textbackslash textbf\{R\}isks. Finally, extensive experiments are conducted on both classification and regression tasks, which demonstrates LIRR consistently achieves state-of-the-art performance and significant improvements compared with the methods that only learn invariant representations or invariant risks.},
  archivePrefix = {arXiv},
  eprint = {2010.04647},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_learning invariant representations and risks for semi-supervised domain adaptation.pdf},
  journal = {arXiv:2010.04647 [cs]},
  primaryClass = {cs}
}

@article{li20_OverfittingUnderfittingUnderstand,
  title = {Overfitting or {{Underfitting}}? {{Understand Robustness Drop}} in {{Adversarial Training}}},
  shorttitle = {Overfitting or {{Underfitting}}?},
  author = {Li, Zichao and Liu, Liyuan and Dong, Chengyu and Shang, Jingbo},
  year = {2020},
  month = oct,
  abstract = {Our goal is to understand why the robustness drops after conducting adversarial training for too long. Although this phenomenon is commonly explained as overfitting, our analysis suggest that its primary cause is perturbation underfitting. We observe that after training for too long, FGSM-generated perturbations deteriorate into random noise. Intuitively, since no parameter updates are made to strengthen the perturbation generator, once this process collapses, it could be trapped in such local optima. Also, sophisticating this process could mostly avoid the robustness drop, which supports that this phenomenon is caused by underfitting instead of overfitting. In the light of our analyses, we propose APART, an adaptive adversarial training framework, which parameterizes perturbation generation and progressively strengthens them. Shielding perturbations from underfitting unleashes the potential of our framework. In our experiments, APART provides comparable or even better robustness than PGD-10, with only about 1/4 of its computational cost.},
  archivePrefix = {arXiv},
  eprint = {2010.08034},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_overfitting or underfitting.pdf},
  journal = {arXiv:2010.08034 [cs]},
  primaryClass = {cs}
}

@article{li20_ROOTSGDSharpNonasymptotics,
  title = {{{ROOT}}-{{SGD}}: {{Sharp Nonasymptotics}} and {{Asymptotic Efficiency}} in a {{Single Algorithm}}},
  shorttitle = {{{ROOT}}-{{SGD}}},
  author = {Li, Chris Junchi and Mou, Wenlong and Wainwright, Martin J. and Jordan, Michael I.},
  year = {2020},
  month = aug,
  abstract = {The theory and practice of stochastic optimization has focused on stochastic gradient descent (SGD) in recent years, retaining the basic first-order stochastic nature of SGD while aiming to improve it via mechanisms such as averaging, momentum, and variance reduction. Improvement can be measured along various dimensions, however, and it has proved difficult to achieve improvements both in terms of nonasymptotic measures of convergence rate and asymptotic measures of distributional tightness. In this work, we consider first-order stochastic optimization from a general statistical point of view, motivating a specific form of recursive averaging of past stochastic gradients. The resulting algorithm, which we refer to as \textbackslash emph\{Recursive One-Over-T SGD\} (ROOT-SGD), matches the state-of-the-art convergence rate among online variance-reduced stochastic approximation methods. Moreover, under slightly stronger distributional assumptions, the rescaled last-iterate of ROOT-SGD converges to a zero-mean Gaussian distribution that achieves near-optimal covariance.},
  archivePrefix = {arXiv},
  eprint = {2008.12690},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_root-sgd.pdf},
  journal = {arXiv:2008.12690 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{li20_UnderstandingGeneralizationDeep,
  title = {Understanding {{Generalization}} in {{Deep Learning}} via {{Tensor Methods}}},
  author = {Li, Jingling and Sun, Yanchao and Su, Jiahao and Suzuki, Taiji and Huang, Furong},
  year = {2020},
  month = jan,
  abstract = {Deep neural networks generalize well on unseen data though the number of parameters often far exceeds the number of training examples. Recently proposed complexity measures have provided insights to understanding the generalizability in neural networks from perspectives of PAC-Bayes, robustness, overparametrization, compression and so on. In this work, we advance the understanding of the relations between the network's architecture and its generalizability from the compression perspective. Using tensor analysis, we propose a series of intuitive, data-dependent and easily-measurable properties that tightly characterize the compressibility and generalizability of neural networks; thus, in practice, our generalization bound outperforms the previous compression-based ones, especially for neural networks using tensors as their weight kernels (e.g. CNNs). Moreover, these intuitive measurements provide further insights into designing neural network architectures with properties favorable for better/guaranteed generalizability. Our experimental results demonstrate that through the proposed measurable properties, our generalization error bound matches the trend of the test error well. Our theoretical analysis further provides justifications for the empirical success and limitations of some widely-used tensor-based compression approaches. We also discover the improvements to the compressibility and robustness of current neural networks when incorporating tensor operations via our proposed layer-wise structure.},
  archivePrefix = {arXiv},
  eprint = {2001.05070},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/PM2Z6CUG/Li et al. - 2020 - Understanding Generalization in Deep Learning via .pdf},
  journal = {arXiv:2001.05070 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{li20_UnsupervisedClusteringGaussian,
  title = {Unsupervised {{Clustering}} through {{Gaussian Mixture Variational AutoEncoder}} with {{Non}}-{{Reparameterized Variational Inference}} and {{Std Annealing}}},
  booktitle = {2020 {{International Joint Conference}} on {{Neural Networks}} ({{IJCNN}})},
  author = {Li, Zhihan and Zhao, Youjian and Xu, Haowen and Chen, Wenxiao and Xu, Shangqing and Li, Yilin and Pei, Dan},
  year = {2020},
  month = jul,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Glasgow, United Kingdom}},
  doi = {10.1109/IJCNN48605.2020.9207493},
  abstract = {Clustering has long been an important research topic in machine learning, and is highly valuable in many application tasks. In recent years, many methods have achieved high clustering performance by applying deep generative models. In this paper, we point out that directly using q(z|y, x) instead of resorting to the mean-field approximation (as is adopted in previous works) in Gaussian Mixture Variational Auto-Encoder can benefit the unsupervised clustering task. We improve the performance of Gaussian Mixture VAE, by optimizing it with a Monte Carlo objective (including the q(z|y, x) term), with nonreparameterized Variational Inference for Monte Carlo Objectives (VIMCO) method. In addition, we propose std annealing to stabilize the training process and empirically show its effects on forming well-separated embeddings with different variational inference methods. Experimental results on five benchmark datasets show that our proposed algorithm NVISA outperforms several baseline algorithms as well as the previous clustering methods based on Gaussian Mixture VAE.},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_unsupervised clustering through gaussian mixture variational autoencoder with non-reparameterized variational inference and std annealing.pdf},
  isbn = {978-1-72816-926-2},
  language = {en}
}

@article{li20_VariationalDiffusionAutoencoders,
  title = {Variational {{Diffusion Autoencoders}} with {{Random Walk Sampling}}},
  author = {Li, Henry and Lindenbaum, Ofir and Cheng, Xiuyuan and Cloninger, Alexander},
  year = {2020},
  month = aug,
  abstract = {Variational autoencoders (VAEs) and generative adversarial networks (GANs) enjoy an intuitive connection to manifold learning: in training the decoder/generator is optimized to approximate a homeomorphism between the data distribution and the sampling space. This is a construction that strives to define the data manifold. A major obstacle to VAEs and GANs, however, is choosing a suitable prior that matches the data topology. Well-known consequences of poorly picked priors are posterior and mode collapse. To our knowledge, no existing method sidesteps this user choice. Conversely, \$\textbackslash textit\{diffusion maps\}\$ automatically infer the data topology and enjoy a rigorous connection to manifold learning, but do not scale easily or provide the inverse homeomorphism (i.e. decoder/generator). We propose a method that combines these approaches into a generative model that inherits the asymptotic guarantees of \$\textbackslash textit\{diffusion maps\}\$ while preserving the scalability of deep models. We prove approximation theoretic results for the dimension dependence of our proposed method. Finally, we demonstrate the effectiveness of our method with various real and synthetic datasets.},
  archivePrefix = {arXiv},
  eprint = {1905.12724},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/li et al_2020_variational diffusion autoencoders with random walk sampling.pdf},
  journal = {arXiv:1905.12724 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{liang00_ThinkLocallyAct,
  title = {Think {{Locally}}, {{Act Globally}}: {{Federated Learning}} with {{Local}} and {{Global Representations}}},
  author = {Liang, Paul Pu and Liu, Terrance and Ziyin, Liu and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  pages = {20},
  abstract = {Federated learning is an emerging research paradigm to train models on private data distributed over multiple devices. A key challenge involves keeping private all the data on each device and training a global model only by communicating parameters and updates. Overcoming this problem relies on the global model being sufficiently compact so that the parameters can be efficiently sent over communication channels such as wireless internet. Given the recent trend towards building deeper and larger neural networks, deploying such models in federated settings on real-world tasks is becoming increasingly difficult. To this end, we propose to augment federated learning with local representation learning on each device to learn useful and compact features from raw data. As a result, the global model can be smaller since it only operates on higher-level local representations. We show that our proposed method achieves superior or competitive results when compared to traditional federated approaches on a suite of publicly available real-world datasets spanning image recognition (MNIST, CIFAR) and multimodal learning (VQA). Our choice of local representation learning also reduces the number of parameters and updates that need to be communicated to and from the global model, thereby reducing the bottleneck in terms of communication cost. Finally, we show that our local models provide flexibility in dealing with online heterogeneous data and can be easily modified to learn fair representations that obfuscate protected attributes such as race, age, and gender, a feature crucial to preserving the privacy of on-device data.},
  file = {/home/trung/GoogleDrive/Zotero/liang et al_think locally, act globally.pdf},
  language = {en}
}

@misc{liang20_pliang279awesomemultimodalml,
  title = {Pliang279/Awesome-Multimodal-Ml},
  author = {Liang, Paul},
  year = {2020},
  month = sep,
  abstract = {Reading list for research topics in multimodal machine learning},
  copyright = {MIT License         ,                 MIT License}
}

@article{liao00_ObjectOrientedDeepLearning,
  title = {Object-{{Oriented Deep Learning}}},
  author = {Liao, Qianli and Poggio, Tomaso},
  pages = {14},
  abstract = {We investigate an unconventional direction of research that aims at converting neural networks, a class of distributed, connectionist, sub-symbolic models into a symbolic level with the ultimate goal of achieving AI interpretability and safety. To that end, we propose Object-Oriented Deep Learning, a novel computational paradigm of deep learning that adopts interpretable ``objects/symbols'' as a basic representational atom instead of N-dimensional tensors (as in traditional ``feature-oriented'' deep learning). For visual processing, each ``object/symbol'' can explicitly package common properties of visual objects like its position, pose, scale, probability of being an object, pointers to parts, etc., providing a full spectrum of interpretable visual knowledge throughout all layers. It achieves a form of ``symbolic disentanglement'', offering one solution to the important problem of disentangled representations and invariance. Basic computations of the network include predicting high-level objects and their properties from low-level objects and binding/aggregating relevant objects together. These computations operate at a more fundamental level than convolutions, capturing convolution as a special case while being significantly more general than it. All operations are executed in an input-driven fashion, thus sparsity and dynamic computation per sample are naturally supported, complementing recent popular ideas of dynamic networks and may enable new types of hardware accelerations. We experimentally show on CIFAR-10 that it can perform flexible visual processing, rivaling the performance of ConvNet, but without using any convolution. Furthermore, it can generalize to novel rotations of images that it was not trained for.},
  file = {/home/trung/GoogleDrive/Zotero/liao et al_object-oriented deep learning.pdf},
  language = {en}
}

@article{liao18_GraphPartitionNeural,
  title = {Graph {{Partition Neural Networks}} for {{Semi}}-{{Supervised Classification}}},
  author = {Liao, Renjie and Brockschmidt, Marc and Tarlow, Daniel and Gaunt, Alexander L. and Urtasun, Raquel and Zemel, Richard},
  year = {2018},
  month = mar,
  abstract = {We present graph partition neural networks (GPNN), an extension of graph neural networks (GNNs) able to handle extremely large graphs. GPNNs alternate between locally propagating information between nodes in small subgraphs and globally propagating information between the subgraphs. To efficiently partition graphs, we experiment with several partitioning algorithms and also propose a novel variant for fast processing of large scale graphs. We extensively test our model on a variety of semi-supervised node classification tasks. Experimental results indicate that GPNNs are either superior or comparable to state-of-the-art methods on a wide variety of datasets for graph-based semi-supervised classification. We also show that GPNNs can achieve similar performance as standard GNNs with fewer propagation steps.},
  archivePrefix = {arXiv},
  eprint = {1803.06272},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liao et al_2018_graph partition neural networks for semi-supervised classification.pdf},
  journal = {arXiv:1803.06272 [cs, stat]},
  keywords = {Computer Science - Machine Learning,graph,semi-supervised,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{liao19_EfficientGraphGeneration,
  title = {Efficient {{Graph Generation}} with {{Graph Recurrent Attention Networks}}},
  author = {Liao, Renjie and Li, Yujia and Song, Yang and Wang, Shenlong and Nash, Charlie and Hamilton, William L. and Duvenaud, David and Urtasun, Raquel and Zemel, Richard S.},
  year = {2019},
  month = oct,
  abstract = {We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. To the best of our knowledge, GRAN is the first deep graph generative model that can scale to this size. Our code is released at: https://github.com/lrjconan/GRAN.},
  archivePrefix = {arXiv},
  eprint = {1910.00760},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Liao et al_2019_Efficient Graph Generation with Graph Recurrent Attention Networks2.pdf;/home/trung/GoogleDrive/Zotero/liao et al_2019_efficient graph generation with graph recurrent attention networks.pdf;/home/trung/Zotero/storage/CA94QM8W/1910.html;/home/trung/Zotero/storage/GKPJBMMX/1910.html},
  journal = {arXiv:1910.00760 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,graph,recurrent,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{liao20_DEMIDiscriminativeEstimator,
  title = {{{DEMI}}: {{Discriminative Estimator}} of {{Mutual Information}}},
  shorttitle = {{{DEMI}}},
  author = {Liao, Ruizhi and Moyer, Daniel and Golland, Polina and Wells, William M.},
  year = {2020},
  month = nov,
  abstract = {Estimating mutual information between continuous random variables is often intractable and extremely challenging for high-dimensional data. Recent progress has leveraged neural networks to optimize variational lower bounds on mutual information. Although showing promise for this difficult problem, the variational methods have been theoretically and empirically proven to have serious statistical limitations: 1) many methods struggle to produce accurate estimates when the underlying mutual information is either low or high; 2) the resulting estimators may suffer from high variance. Our approach is based on training a classifier that provides the probability that a data sample pair is drawn from the joint distribution rather than from the product of its marginal distributions. Moreover, we establish a direct connection between mutual information and the average log odds estimate produced by the classifier on a test set, leading to a simple and accurate estimator of mutual information. We show theoretically that our method and other variational approaches are equivalent when they achieve their optimum, while our method sidesteps the variational bound. Empirical results demonstrate high accuracy of our approach and the advantages of our estimator in the context of representation learning. Our demo is available at https://github.com/RayRuizhiLiao/demi\_mi\_estimator.},
  archivePrefix = {arXiv},
  eprint = {2010.01766},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liao et al_2020_demi.pdf},
  journal = {arXiv:2010.01766 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{liao20_GraphAdversarialNetworks,
  title = {Graph {{Adversarial Networks}}: {{Protecting Information}} against {{Adversarial Attacks}}},
  shorttitle = {Graph {{Adversarial Networks}}},
  author = {Liao, Peiyuan and Zhao, Han and Xu, Keyulu and Jaakkola, Tommi and Gordon, Geoffrey and Jegelka, Stefanie and Salakhutdinov, Ruslan},
  year = {2020},
  month = oct,
  abstract = {We study the problem of protecting information when learning with graph-structured data. While the advent of Graph Neural Networks (GNNs) has greatly improved node and graph representational learning in many applications, the neighborhood aggregation paradigm exposes additional vulnerabilities to attackers seeking to extract node-level information about sensitive attributes. To counter this, we propose a minimax game between the desired GNN encoder and the worst-case attacker. The resulting adversarial training creates a strong defense against inference attacks, while only suffering a small loss in task performance. We analyze the effectiveness of our framework against a worst-case adversary, and characterize the trade-off between predictive accuracy and adversarial defense. Experiments across multiple datasets from recommender systems, knowledge graphs and quantum chemistry demonstrate that the proposed approach provides a robust defense across various graph structures and tasks, while producing competitive GNN encoders. Our code is available at https://github.com/liaopeiyuan/GAL.},
  archivePrefix = {arXiv},
  eprint = {2009.13504},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liao et al_2020_graph adversarial networks.pdf},
  journal = {arXiv:2009.13504 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{lievin00_HierarchicalDiscreteVariational,
  title = {Towards {{Hierarchical Discrete Variational Autoencoders}}},
  author = {Lievin, Valentin and Dittadi, Andrea and Maal{\o}e, Lars and Winther, Ole},
  pages = {16},
  abstract = {Variational Autoencoders (VAEs) have proven to be powerful latent variable models. However, the form of the approximate posterior can limit the expressiveness of the model. Categorical distributions are flexible and useful building blocks for example in neural memory layers. We introduce the Hierarchical Discrete Variational Autoencoder (HD-VAE): a hierarchy of variational memory layers. The Concrete/Gumbel-Softmax relaxation allows maximizing a surrogate of the Evidence Lower Bound by stochastic gradient ascent. We show that, when using a limited number of latent variables, HD-VAE outperforms the Gaussian baseline on modelling multiple binary image datasets. Training very deep HD-VAE remains a challenge due to the relaxation bias that is induced by the use of a surrogate objective. We introduce a formal definition and conduct a preliminary theoretical and empirical study of the bias.},
  file = {/home/trung/GoogleDrive/Zotero/lievin et al_towards hierarchical discrete variational autoencoders.pdf},
  language = {en}
}

@article{lifetimecommunity20_LifeTimeimprovingEuropean,
  title = {{{LifeTime}} and Improving {{European}} Healthcare through Cell-Based Interceptive Medicine},
  author = {{LifeTime Community} and Rajewsky, Nikolaus and Almouzni, Genevi{\`e}ve and Gorski, Stanislaw A. and Aerts, Stein and Amit, Ido and Bertero, Michela G. and Bock, Christoph and Bredenoord, Annelien L. and Cavalli, Giacomo and Chiocca, Susanna and Clevers, Hans and De Strooper, Bart and Eggert, Angelika and Ellenberg, Jan and Fern{\'a}ndez, Xos{\'e} M. and Figlerowicz, Marek and Gasser, Susan M. and Hubner, Norbert and Kjems, J{\o}rgen and Knoblich, J{\"u}rgen A. and Krabbe, Grietje and Lichter, Peter and Linnarsson, Sten and Marine, Jean-Christophe and Marioni, John and {Marti-Renom}, Marc A. and Netea, Mihai G. and Nickel, D{\"o}rthe and Nollmann, Marcelo and Novak, Halina R. and Parkinson, Helen and Piccolo, Stefano and Pinheiro, In{\^e}s and Pombo, Ana and Popp, Christian and Reik, Wolf and {Roman-Roman}, Sergio and Rosenstiel, Philip and Schultze, Joachim L. and Stegle, Oliver and Tanay, Amos and Testa, Giuseppe and Thanos, Dimitris and Theis, Fabian J. and {Torres-Padilla}, Maria-Elena and Valencia, Alfonso and Vallot, C{\'e}line and {van Oudenaarden}, Alexander and Vidal, Marie and Voet, Thierry},
  year = {2020},
  month = sep,
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-2715-9},
  abstract = {Abstract             LifeTime aims to track, understand and target human cells during the onset and progression of complex diseases and their response to therapy at single-cell resolution. This mission will be implemented through the development and integration of single-cell multi-omics and imaging, artificial intelligence and patient-derived experimental disease models during progression from health to disease. Analysis of such large molecular and clinical datasets will discover molecular mechanisms, create predictive computational models of disease progression, and reveal new drug targets and therapies. Timely detection and interception of disease embedded in an ethical and patient-centered vision will be achieved through interactions across academia, hospitals, patient-associations, health data management systems and industry. Applying this strategy to key medical challenges in cancer, neurological, infectious, chronic inflammatory and cardiovascular diseases at the single-cell level will usher in cell-based interceptive medicine in Europe over the next decade.},
  file = {/home/trung/GoogleDrive/Zotero/lifetime community et al_2020_lifetime and improving european healthcare through cell-based interceptive medicine.pdf},
  journal = {Nature},
  language = {en}
}

@article{lim18_Moleculargenerativemodel,
  title = {Molecular Generative Model Based on Conditional Variational Autoencoder for de Novo Molecular Design},
  author = {Lim, Jaechang and Ryu, Seongok and Kim, Jin Woo and Kim, Woo Youn},
  year = {2018},
  month = jun,
  abstract = {We propose a molecular generative model based on the conditional variational autoencoder for de novo molecular design. It is specialized to control multiple molecular properties simultaneously by imposing them on a latent space. As a proof of concept, we demonstrate that it can be used to generate drug-like molecules with five target properties. We were also able to adjust a single property without changing the others and to manipulate it beyond the range of the dataset.},
  annotation = {ZSCC: 0000037},
  archivePrefix = {arXiv},
  eprint = {1806.05805},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lim et al_2018_molecular generative model based on conditional variational autoencoder for de novo molecular design.pdf},
  journal = {arXiv:1806.05805 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lim20_ARDAEUnbiasedNeural,
  title = {{{AR}}-{{DAE}}: {{Towards Unbiased Neural Entropy Gradient Estimation}}},
  shorttitle = {{{AR}}-{{DAE}}},
  author = {Lim, Jae Hyun and Courville, Aaron and Pal, Christopher and Huang, Chin-Wei},
  year = {2020},
  month = jun,
  abstract = {Entropy is ubiquitous in machine learning, but it is in general intractable to compute the entropy of the distribution of an arbitrary continuous random variable. In this paper, we propose the amortized residual denoising autoencoder (AR-DAE) to approximate the gradient of the log density function, which can be used to estimate the gradient of entropy. Amortization allows us to significantly reduce the error of the gradient approximator by approaching asymptotic optimality of a regular DAE, in which case the estimation is in theory unbiased. We conduct theoretical and experimental analyses on the approximation error of the proposed method, as well as extensive studies on heuristics to ensure its robustness. Finally, using the proposed gradient approximator to estimate the gradient of entropy, we demonstrate state-of-the-art performance on density estimation with variational autoencoders and continuous control with soft actor-critic.},
  archivePrefix = {arXiv},
  eprint = {2006.05164},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lim et al_2020_ar-dae.pdf},
  journal = {arXiv:2006.05164 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{lin17_StructuredSelfattentiveSentence,
  title = {A {{Structured Self}}-Attentive {{Sentence Embedding}}},
  author = {Lin, Zhouhan and Feng, Minwei and dos Santos, Cicero Nogueira and Yu, Mo and Xiang, Bing and Zhou, Bowen and Bengio, Yoshua},
  year = {2017},
  month = mar,
  abstract = {This paper proposes a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks.},
  annotation = {ZSCC: 0000507},
  archivePrefix = {arXiv},
  eprint = {1703.03130},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lin et al_2017_a structured self-attentive sentence embedding.pdf;/home/trung/Zotero/storage/DQPP82LE/1703.html},
  journal = {arXiv:1703.03130 [cs]},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  primaryClass = {cs}
}

@article{lin19_CaptainGANNavigateEmbedding,
  title = {{{CaptainGAN}}: {{Navigate Through Embedding Space For Better Text Generation}}},
  shorttitle = {{{CaptainGAN}}},
  author = {Lin, Chun-Hsing and Chiang, Alvin and Liu, Chi-Liang and Lin, Chien-Fu and Chu, Po-Hsien and Wu, Siang-Ruei and Tsai, Yi-En and Huang, Chung-Yang (Ric)},
  year = {2019},
  month = sep,
  abstract = {Score-function-based text generation approaches such as REINFORCE, in general, suffer from high computational complexity and training instability problems. This is mainly due to the...},
  file = {/home/trung/GoogleDrive/Zotero/lin et al_2019_captaingan.pdf;/home/trung/Zotero/storage/7KJY3ES5/forum.html}
}

@article{lin20_InfoGANCRModelCentralitySelfsupervised,
  title = {{{InfoGAN}}-{{CR}} and {{ModelCentrality}}: {{Self}}-Supervised {{Model Training}} and {{Selection}} for {{Disentangling GANs}}},
  shorttitle = {{{InfoGAN}}-{{CR}} and {{ModelCentrality}}},
  author = {Lin, Zinan and Thekumparampil, Kiran Koshy and Fanti, Giulia and Oh, Sewoong},
  year = {2020},
  month = aug,
  abstract = {Disentangled generative models map a latent code vector to a target space, while enforcing that a subset of the learned latent codes are interpretable and associated with distinct properties of the target distribution. Recent advances have been dominated by Variational AutoEncoder (VAE)-based methods, while training disentangled generative adversarial networks (GANs) remains challenging. In this work, we show that the dominant challenges facing disentangled GANs can be mitigated through the use of self-supervision. We make two main contributions: first, we design a novel approach for training disentangled GANs with self-supervision. We propose contrastive regularizer, which is inspired by a natural notion of disentanglement: latent traversal. This achieves higher disentanglement scores than state-of-the-art VAE- and GAN-based approaches. Second, we propose an unsupervised model selection scheme called ModelCentrality, which uses generated synthetic samples to compute the medoid (multi-dimensional generalization of median) of a collection of models. The current common practice of hyper-parameter tuning requires using ground-truths samples, each labelled with known perfect disentangled latent codes. As real datasets are not equipped with such labels, we propose an unsupervised model selection scheme and show that it finds a model close to the best one, for both VAEs and GANs. Combining contrastive regularization with ModelCentrality, we improve upon the state-of-the-art disentanglement scores significantly, without accessing the supervised data.},
  archivePrefix = {arXiv},
  eprint = {1906.06034},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lin et al_2020_infogan-cr and modelcentrality.pdf},
  journal = {arXiv:1906.06034 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{lin20_InfoGANCRModelCentralitySelfsuperviseda,
  title = {{{InfoGAN}}-{{CR}} and {{ModelCentrality}}: {{Self}}-Supervised {{Model Training}} and {{Selection}} for {{Disentangling GANs}}},
  shorttitle = {{{InfoGAN}}-{{CR}} and {{ModelCentrality}}},
  author = {Lin, Zinan and Thekumparampil, Kiran Koshy and Fanti, Giulia and Oh, Sewoong},
  year = {2020},
  month = aug,
  abstract = {Disentangled generative models map a latent code vector to a target space, while enforcing that a subset of the learned latent codes are interpretable and associated with distinct properties of the target distribution. Recent advances have been dominated by Variational AutoEncoder (VAE)-based methods, while training disentangled generative adversarial networks (GANs) remains challenging. In this work, we show that the dominant challenges facing disentangled GANs can be mitigated through the use of self-supervision. We make two main contributions: first, we design a novel approach for training disentangled GANs with self-supervision. We propose contrastive regularizer, which is inspired by a natural notion of disentanglement: latent traversal. This achieves higher disentanglement scores than state-of-the-art VAE- and GAN-based approaches. Second, we propose an unsupervised model selection scheme called ModelCentrality, which uses generated synthetic samples to compute the medoid (multi-dimensional generalization of median) of a collection of models. The current common practice of hyper-parameter tuning requires using ground-truths samples, each labelled with known perfect disentangled latent codes. As real datasets are not equipped with such labels, we propose an unsupervised model selection scheme and show that it finds a model close to the best one, for both VAEs and GANs. Combining contrastive regularization with ModelCentrality, we improve upon the state-of-the-art disentanglement scores significantly, without accessing the supervised data.},
  archivePrefix = {arXiv},
  eprint = {1906.06034},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lin et al_2020_infogan-cr and modelcentrality2.pdf},
  journal = {arXiv:1906.06034 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{lin20_LaDDerLatentData,
  title = {{{LaDDer}}: {{Latent Data Distribution Modelling}} with a {{Generative Prior}}},
  shorttitle = {{{LaDDer}}},
  author = {Lin, Shuyu and Clark, Ronald},
  year = {2020},
  month = aug,
  abstract = {In this paper, we show that the performance of a learnt generative model is closely related to the model's ability to accurately represent the inferred \textbackslash textbf\{latent data distribution\}, i.e. its topology and structural properties. We propose LaDDer to achieve accurate modelling of the latent data distribution in a variational autoencoder framework and to facilitate better representation learning. The central idea of LaDDer is a meta-embedding concept, which uses multiple VAE models to learn an embedding of the embeddings, forming a ladder of encodings. We use a non-parametric mixture as the hyper prior for the innermost VAE and learn all the parameters in a unified variational framework. From extensive experiments, we show that our LaDDer model is able to accurately estimate complex latent distribution and results in improvement in the representation quality. We also propose a novel latent space interpolation method that utilises the derived data distribution.},
  archivePrefix = {arXiv},
  eprint = {2009.00088},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lin et al_2020_ladder.pdf},
  journal = {arXiv:2009.00088 [cs]},
  primaryClass = {cs}
}

@misc{linder-noren20_eriklindernorenMLFromScratch,
  title = {Eriklindernoren/{{ML}}-{{From}}-{{Scratch}}},
  author = {{Linder-Nor{\'e}n}, Erik},
  year = {2020},
  month = feb,
  abstract = {Machine Learning From Scratch. Bare bones NumPy implementations of machine learning models and algorithms with a focus on accessibility. Aims to cover everything from linear regression to deep lear...},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {MIT}
}

@article{lindsay00_simplecircuitmodel,
  title = {A Simple Circuit Model of Visual Cortex Explains Neural and Behavioral Aspects of Attention},
  author = {Lindsay, Grace W and Rubin, Daniel B and Miller, Kenneth D},
  pages = {49},
  abstract = {Selective visual attention modulates neural activity in the visual system and leads to enhanced performance on difficult visual tasks. Here, we use an existing circuit model of visual cortex, known as the stabilized supralinear network, to demonstrate that many neural correlates of attention can arise from simple circuit mechanisms. Using different variants of the model we replicate results from studies of both feature and spatial attention. In addition to firing rate changes, we also replicate findings regarding how attention impacts trial-to-trial variability. Finally, we expand this circuit model into an architecture that can perform visual tasks in order to show that these neural effects can enhance detection performance. This work advances our understanding of the physical underpinnings of attention.},
  file = {/home/trung/GoogleDrive/Zotero/lindsay et al_a simple circuit model of visual cortex explains neural and behavioral aspects of attention.pdf},
  language = {en}
}

@article{lindsay20_ConvolutionalNeuralNetworks,
  title = {Convolutional {{Neural Networks}} as a {{Model}} of the {{Visual System}}: {{Past}}, {{Present}}, and {{Future}}},
  shorttitle = {Convolutional {{Neural Networks}} as a {{Model}} of the {{Visual System}}},
  author = {Lindsay, Grace W.},
  year = {2020},
  month = jan,
  abstract = {Convolutional neural networks (CNNs) were inspired by early findings in the study of biological vision. They have since become successful tools in computer vision and state-of-the-art models of both neural activity and behavior on visual tasks. This review highlights what, in the context of CNNs, it means to be a good model in computational neuroscience and the various ways models can provide insight. Specifically, it covers the origins of CNNs and the methods by which we validate them as models of biological vision. It then goes on to elaborate on what we can learn about biological vision by understanding and experimenting on CNNs and discusses emerging opportunities for the use of CNNS in vision research beyond basic object recognition.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.07092},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lindsay_2020_convolutional neural networks as a model of the visual system.pdf;/home/trung/Zotero/storage/P4QNTJMF/2001.html},
  journal = {arXiv:2001.07092 [cs, q-bio]},
  primaryClass = {cs, q-bio}
}

@article{lippe20_CategoricalNormalizingFlows,
  title = {Categorical {{Normalizing Flows}} via {{Continuous Transformations}}},
  author = {Lippe, Phillip and Gavves, Efstratios},
  year = {2020},
  month = jun,
  abstract = {Despite their popularity, to date, the application of normalizing flows on categorical data stays limited. The current practice of using dequantization to map discrete data to a continuous space is inapplicable as categorical data has no intrinsic order. Instead, categorical data have complex and latent relations that must be inferred, like the synonymy between words. In this paper, we investigate Categorical Normalizing Flows, that is normalizing flows for categorical data. By casting the encoding of categorical data in continuous space as a variational inference problem, we jointly optimize the continuous representation and the model likelihood. To maintain unique decoding, we learn a partitioning of the latent space by factorizing the posterior. Meanwhile, the complex relations between the categorical variables are learned by the ensuing normalizing flow, thus maintaining a close-to exact likelihood estimate and making it possible to scale up to a large number of categories. Based on Categorical Normalizing Flows, we propose GraphCNF a permutation-invariant generative model on graphs, outperforming both one-shot and autoregressive flow-based state-of-the-art on molecule generation.},
  archivePrefix = {arXiv},
  eprint = {2006.09790},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lippe et al_2020_categorical normalizing flows via continuous transformations.pdf},
  journal = {arXiv:2006.09790 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lipton15_CriticalReviewRecurrent,
  title = {A {{Critical Review}} of {{Recurrent Neural Networks}} for {{Sequence Learning}}},
  author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
  year = {2015},
  month = oct,
  abstract = {Countless learning tasks require dealing with sequential data. Image captioning, speech synthesis, and music generation all require that a model produce outputs that are sequences. In other domains, such as time series prediction, video analysis, and musical information retrieval, a model must learn from inputs that are sequences. Interactive tasks, such as translating natural language, engaging in dialogue, and controlling a robot, often demand both capabilities. Recurrent neural networks (RNNs) are connectionist models that capture the dynamics of sequences via cycles in the network of nodes. Unlike standard feedforward neural networks, recurrent networks retain a state that can represent information from an arbitrarily long context window. Although recurrent neural networks have traditionally been difficult to train, and often contain millions of parameters, recent advances in network architectures, optimization techniques, and parallel computation have enabled successful large-scale learning with them. In recent years, systems based on long short-term memory (LSTM) and bidirectional (BRNN) architectures have demonstrated ground-breaking performance on tasks as varied as image captioning, language translation, and handwriting recognition. In this survey, we review and synthesize the research that over the past three decades first yielded and then made practical these powerful learning models. When appropriate, we reconcile conflicting notation and nomenclature. Our goal is to provide a self-contained explication of the state of the art together with a historical perspective and references to primary research.},
  annotation = {ZSCC: 0000858},
  archivePrefix = {arXiv},
  eprint = {1506.00019},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lipton et al_2015_a critical review of recurrent neural networks for sequence learning.pdf;/home/trung/Zotero/storage/5RET8XC6/1506.html},
  journal = {arXiv:1506.00019 [cs]},
  primaryClass = {cs}
}

@article{lipton18_TroublingTrendsMachine,
  title = {Troubling {{Trends}} in {{Machine Learning Scholarship}}},
  author = {Lipton, Zachary C. and Steinhardt, Jacob},
  year = {2018},
  month = jul,
  abstract = {Collectively, machine learning (ML) researchers are engaged in the creation and dissemination of knowledge about data-driven algorithms. In a given paper, researchers might aspire to any subset of the following goals, among others: to theoretically characterize what is learnable, to obtain understanding through empirically rigorous experiments, or to build a working system that has high predictive accuracy. While determining which knowledge warrants inquiry may be subjective, once the topic is fixed, papers are most valuable to the community when they act in service of the reader, creating foundational knowledge and communicating as clearly as possible. Recent progress in machine learning comes despite frequent departures from these ideals. In this paper, we focus on the following four patterns that appear to us to be trending in ML scholarship: (i) failure to distinguish between explanation and speculation; (ii) failure to identify the sources of empirical gains, e.g., emphasizing unnecessary modifications to neural architectures when gains actually stem from hyper-parameter tuning; (iii) mathiness: the use of mathematics that obfuscates or impresses rather than clarifies, e.g., by confusing technical and non-technical concepts; and (iv) misuse of language, e.g., by choosing terms of art with colloquial connotations or by overloading established technical terms. While the causes behind these patterns are uncertain, possibilities include the rapid expansion of the community, the consequent thinness of the reviewer pool, and the often-misaligned incentives between scholarship and short-term measures of success (e.g., bibliometrics, attention, and entrepreneurial opportunity). While each pattern offers a corresponding remedy (don't do it), we also discuss some speculative suggestions for how the community might combat these trends.},
  archivePrefix = {arXiv},
  eprint = {1807.03341},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lipton et al_2018_troubling trends in machine learning scholarship.pdf},
  journal = {arXiv:1807.03341 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{litany20_RepresentationLearningLatent,
  title = {Representation {{Learning Through Latent Canonicalizations}}},
  author = {Litany, Or and Morcos, Ari and Sridhar, Srinath and Guibas, Leonidas and Hoffman, Judy},
  year = {2020},
  month = feb,
  abstract = {We seek to learn a representation on a large annotated data source that generalizes to a target domain using limited new supervision. Many prior approaches to this problem have focused on learning ``disentangled'' representations so that as individual factors vary in a new domain, only a portion of the representation need be updated.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2002.11829},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/litany et al_2020_representation learning through latent canonicalizations.pdf},
  journal = {arXiv:2002.11829 [cs, stat]},
  keywords = {disentanglement,favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{little19_Causalbootstrapping,
  title = {Causal Bootstrapping},
  author = {Little, Max A. and Badawy, Reham},
  year = {2019},
  month = oct,
  abstract = {To draw scientifically meaningful conclusions and build reliable models of quantitative phenomena, cause and effect must be taken into consideration (either implicitly or explicitly). This is particularly challenging when the measurements are not from controlled experimental (interventional) settings, since cause and effect can be obscured by spurious, indirect influences. Modern predictive techniques from machine learning are capable of capturing high-dimensional, nonlinear relationships between variables while relying on few parametric or probabilistic model assumptions. However, since these techniques are associational, applied to observational data they are prone to picking up spurious influences from non-experimental (observational) data, making their predictions unreliable. Techniques from causal inference, such as probabilistic causal diagrams and do-calculus, provide powerful (nonparametric) tools for drawing causal inferences from such observational data. However, these techniques are often incompatible with modern, nonparametric machine learning algorithms since they typically require explicit probabilistic models. Here, we develop causal bootstrapping for augmenting classical nonparametric bootstrap resampling with information on the causal relationship between variables. This makes it possible to resample observational data such that, if it is possible to identify an interventional relationship from that data, new data representing that relationship can be simulated from the original observational data. In this way, we can use modern machine learning algorithms unaltered to make statistically powerful, yet causally-robust, predictions. We develop several causal bootstrapping algorithms for drawing interventional inferences from observational data, for classification and regression problems, and demonstrate, using synthetic and real-world examples, the value of this approach.},
  archivePrefix = {arXiv},
  eprint = {1910.09648},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/little et al_2019_causal bootstrapping.pdf},
  journal = {arXiv:1910.09648 [cs, math, stat]},
  keywords = {causal},
  primaryClass = {cs, math, stat}
}

@inproceedings{liu-etal-2019-multi-task,
  title = {Multi-Task Deep Neural Networks for Natural Language Understanding},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Liu, Xiaodong and He, Pengcheng and Chen, Weizhu and Gao, Jianfeng},
  year = {2019},
  month = jul,
  pages = {4487--4496},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1441},
  abstract = {In this paper, we present a Multi-Task Deep Neural Network (MT-DNN) for learning representations across multiple natural language understanding (NLU) tasks. MT-DNN not only leverages large amounts of cross-task data, but also benefits from a regularization effect that leads to more general representations to help adapt to new tasks and domains. MT-DNN extends the model proposed in Liu et al. (2015) by incorporating a pre-trained bidirectional transformer language model, known as BERT (Devlin et al., 2018). MT-DNN obtains new state-of-the-art results on ten NLU tasks, including SNLI, SciTail, and eight out of nine GLUE tasks, pushing the GLUE benchmark to 82.7\% (2.2\% absolute improvement) as of February 25, 2019 on the latest GLUE test set. We also demonstrate using the SNLI and SciTail datasets that the representations learned by MT-DNN allow domain adaptation with substantially fewer in-domain labels than the pre-trained BERT representations. Our code and pre-trained models will be made publicly available.},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_multi-task deep neural networks for natural language understanding.pdf},
  keywords = {favorite}
}

@article{liu00_ConstrainedGraphVariational,
  title = {Constrained {{Graph Variational Autoencoders}} for {{Molecule Design}}},
  author = {Liu, Qi and Allamanis, Miltiadis and Brockschmidt, Marc and Gaunt, Alexander},
  pages = {10},
  abstract = {Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on applications in chemistry, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graphstructured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.},
  annotation = {ZSCC: 0000066},
  file = {/home/trung/Zotero/storage/4IV8MIX2/Liu et al. - Constrained Graph Variational Autoencoders for Mol.pdf},
  language = {en}
}

@article{liu00_TransferableAdversarialTraining,
  title = {Transferable {{Adversarial Training}}:{{A General Approach}} to {{Adapting Deep Classifiers}}},
  author = {Liu, Hong and Long, Mingsheng and Wang, Jianmin and Jordan, Michael I},
  pages = {10},
  abstract = {Domain adaptation enables knowledge transfer from a labeled source domain to an unlabeled target domain. A mainstream approach is adversarial feature adaptation, which learns domain-invariant representations through aligning the feature distributions of both domains. However, a theoretical prerequisite of domain adaptation is the adaptability measured by the expected risk of an ideal joint hypothesis over the source and target domains. In this respect, adversarial feature adaptation may potentially deteriorate the adaptability, since it distorts the original feature distributions when suppressing domain-specific variations. To this end, we propose Transferable Adversarial Training (TAT) to enable the adaptation of deep classifiers. The approach generates transferable examples to fill in the gap between the source and target domains, and adversarially trains the deep classifiers to make consistent predictions over the transferable examples. Without learning domaininvariant representations at the expense of distorting the feature distributions, the adaptability in the theoretical learning bound is algorithmically guaranteed. A series of experiments validate that our approach advances the state of the arts on a variety of domain adaptation tasks in vision and NLP, including object recognition, learning from synthetic to real data, and sentiment classification.},
  annotation = {ZSCC: 0000007},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_transferable adversarial training.pdf},
  language = {en}
}

@article{liu00_ZeroInflatedExponentialFamily,
  title = {Zero-{{Inflated Exponential Family Embeddings}}},
  author = {Liu, Li-Ping and Blei, David M},
  pages = {9},
  abstract = {Word embeddings are a widely-used tool to analyze language, and exponential family embeddings (Rudolph et al., 2016) generalize the technique to other types of data. One challenge to fitting embedding methods is sparse data, such as a document/term matrix that contains many zeros. To address this issue, practitioners typically downweight or subsample the zeros, thus focusing learning on the non-zero entries. In this paper, we develop zero-inflated embeddings, a new embedding method that is designed to learn from sparse observations. In a zero-inflated embedding (ZIE), a zero in the data can come from an interaction to other data (i.e., an embedding) or from a separate process by which many observations are equal to zero (i.e. a probability mass at zero). Fitting a ZIE naturally downweights the zeros and dampens their influence on the model. Across many types of data\textemdash language, movie ratings, shopping histories, and bird watching logs\textemdash we found that zero-inflated embeddings provide improved predictive performance over standard approaches and find better vector representation of items.},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_zero-inflated exponential family embeddings.pdf},
  language = {en}
}

@article{liu04_ComparisoneffectsLcarnitine,
  title = {Comparison of the Effects of {{L}}-Carnitine and Acetyl-{{L}}-Carnitine on Carnitine Levels, Ambulatory Activity, and Oxidative Stress Biomarkers in the Brain of Old Rats},
  author = {Liu, Jiankang and Head, Elizabeth and Kuratsune, Hirohiko and Cotman, Carl W. and Ames, Bruce N.},
  year = {2004},
  month = nov,
  volume = {1033},
  pages = {117--131},
  issn = {0077-8923},
  doi = {10.1196/annals.1320.011},
  abstract = {L-carnitine and acetyl-L-carnitine (ALC) are both used to improve mitochondrial function. Although it has been argued that ALC is better than l-carnitine in absorption and activity, there has been no experiment to compare the two compounds at the same dose. In the present experiment, the effects of ALC and L-carnitine on the levels of free, acyl, and total L-carnitine in plasma and brain, rat ambulatory activity, and biomarkers of oxidative stress are investigated. Aged rats (23 months old) were given ALC or L-carnitine at 0.15\% in drinking water for 4 weeks. L-carnitine and ALC were similar in elevating carnitine levels in plasma and brain. Both increased ambulatory activity similarly. However, ALC decreased the lipid peroxidation (malondialdehyde, MDA) in the old rat brain, while L-carnitine did not. ALC decreased the extent of oxidized nucleotides (oxo8dG/oxo8G) immunostaining in the hippocampal CA1 and cortex, while L-carnitine did not. ALC decreased nitrotyrosine immunostaining in the hippocampal CA1 and white matter, while L-carnitine did not. In conclusion, ALC and L-carnitine were similar in increasing ambulatory activity in old rats and elevating carnitine levels in blood and brain. However, ALC was effective, unlike L-carnitine, in decreasing oxidative damage, including MDA, oxo8dG/oxo8G, and nitrotyrosine, in old rat brain. These data suggest that ALC may be a better dietary supplement than L-carnitine.},
  journal = {Annals of the New York Academy of Sciences},
  language = {eng},
  pmid = {15591009}
}

@article{liu18_UnifiedFeatureDisentangler,
  title = {A {{Unified Feature Disentangler}} for {{Multi}}-{{Domain Image Translation}} and {{Manipulation}}},
  author = {Liu, Alexander H. and Liu, Yen-Cheng and Yeh, Yu-Ying and Wang, Yu-Chiang Frank},
  year = {2018},
  month = oct,
  abstract = {We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.},
  annotation = {ZSCC: 0000033},
  archivePrefix = {arXiv},
  eprint = {1809.01361},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2018_a unified feature disentangler for multi-domain image translation and manipulation.pdf;/home/trung/Zotero/storage/9BNSW5QI/1809.html},
  journal = {arXiv:1809.01361 [cs]},
  primaryClass = {cs}
}

@article{liu19_AcceleratingFederatedLearning,
  title = {Accelerating {{Federated Learning}} via {{Momentum Gradient Descent}}},
  author = {Liu, Wei and Chen, Li and Chen, Yunfei and Zhang, Wenyi},
  year = {2019},
  month = oct,
  abstract = {Federated learning (FL) provides a communication-efficient approach to solve machine learning problems concerning distributed data, without sending raw data to a central server. However, existing works on FL only utilize first-order gradient descent (GD) and do not consider the preceding iterations to gradient update which can potentially accelerate convergence. In this paper, we consider momentum term which relates to the last iteration. The proposed momentum federated learning (MFL) uses momentum gradient descent (MGD) in the local update step of FL system. We establish global convergence properties of MFL and derive an upper bound on MFL convergence rate. Comparing the upper bounds on MFL and FL convergence rate, we provide conditions in which MFL accelerates the convergence. For different machine learning models, the convergence performance of MFL is evaluated based on experiments with MNIST dataset. Simulation results comfirm that MFL is globally convergent and further reveal significant convergence improvement over FL.},
  archivePrefix = {arXiv},
  eprint = {1910.03197},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_accelerating federated learning via momentum gradient descent.pdf;/home/trung/Zotero/storage/L2QY7N4I/1910.html},
  journal = {arXiv:1910.03197 [cs, stat]},
  keywords = {Computer Science - Machine Learning,gradient,I.2.6,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{liu19_AIfashiondesign,
  title = {Toward {{AI}} Fashion Design: {{An Attribute}}-{{GAN}} Model for Clothing Match},
  shorttitle = {Toward {{AI}} Fashion Design},
  author = {Liu, Linlin and Zhang, Haijun and Ji, Yuzhu and Jonathan Wu, Q.M.},
  year = {2019},
  month = may,
  volume = {341},
  pages = {156--167},
  issn = {09252312},
  doi = {10.1016/j.neucom.2019.03.011},
  abstract = {Dressing in clothes based on the matching rules of color, texture, shape, etc., can have a major impact on perception, including making people appear taller or thinner, as well as exhibiting personal style. Unlike the extant fashion mining literature, in which style is usually classified according to similarity, this paper investigates clothing match rules based on semantic attributes according to the generative adversarial network (GAN) model. Specifically, we propose an Attribute-GAN to generate clothing-match pairs automatically. The core of Attribute-GAN constitutes training a generator, supervised by an adversarial trained collocation discriminator and attribute discriminator. To implement the Attributed-GAN, we built a large-scale outfit dataset by ourselves and annotated clothing attributes manually. Extensive experimental results confirm the effectiveness of our proposed method in comparison to several state-of-the-art methods.},
  file = {/home/trung/Zotero/storage/XCUDPZMB/Liu et al. - 2019 - Toward AI fashion design An Attribute-GAN model f.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{liu19_ConstrainedGraphVariational,
  title = {Constrained {{Graph Variational Autoencoders}} for {{Molecule Design}}},
  author = {Liu, Qi and Allamanis, Miltiadis and Brockschmidt, Marc and Gaunt, Alexander L.},
  year = {2019},
  month = mar,
  abstract = {Graphs are ubiquitous data structures for representing interactions between entities. With an emphasis on the use of graphs to represent chemical molecules, we explore the task of learning to generate graphs that conform to a distribution observed in training data. We propose a variational autoencoder model in which both encoder and decoder are graph-structured. Our decoder assumes a sequential ordering of graph extension steps and we discuss and analyze design choices that mitigate the potential downsides of this linearization. Experiments compare our approach with a wide range of baselines on the molecule generation task and show that our method is more successful at matching the statistics of the original dataset on semantically important metrics. Furthermore, we show that by using appropriate shaping of the latent space, our model allows us to design molecules that are (locally) optimal in desired properties.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1805.09076},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_constrained graph variational autoencoders for molecule design.pdf;/home/trung/Zotero/storage/YWTIZ9F6/1805.html},
  journal = {arXiv:1805.09076 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{liu19_Deconvolutionsinglecellmultiomics,
  title = {Deconvolution of Single-Cell Multi-Omics Layers Reveals Regulatory Heterogeneity},
  author = {Liu, Longqi and Liu, Chuanyu and Quintero, Andr{\'e}s and Wu, Liang and Yuan, Yue and Wang, Mingyue and Cheng, Mengnan and Leng, Lizhi and Xu, Liqin and Dong, Guoyi and Li, Rui and Liu, Yang and Wei, Xiaoyu and Xu, Jiangshan and Chen, Xiaowei and Lu, Haorong and Chen, Dongsheng and Wang, Quanlei and Zhou, Qing and Lin, Xinxin and Li, Guibo and Liu, Shiping and Wang, Qi and Wang, Hongru and Fink, J. Lynn and Gao, Zhengliang and Liu, Xin and Hou, Yong and Zhu, Shida and Yang, Huanming and Ye, Yunming and Lin, Ge and Chen, Fang and Herrmann, Carl and Eils, Roland and Shang, Zhouchun and Xu, Xun},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {470},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08205-7},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_deconvolution of single-cell multi-omics layers reveals regulatory heterogeneity.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{liu19_hierarchicalBayesianmodel,
  title = {A Hierarchical {{Bayesian}} Model for Single-Cell Clustering Using {{RNA}}-Sequencing Data},
  author = {Liu, Yiyi and Warren, Joshua L. and Zhao, Hongyu},
  year = {2019},
  month = sep,
  volume = {13},
  pages = {1733--1752},
  issn = {1932-6157},
  doi = {10.1214/19-AOAS1250},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_a hierarchical bayesian model for single-cell clustering using rna-sequencing data.pdf},
  journal = {The Annals of Applied Statistics},
  language = {en},
  number = {3}
}

@article{liu19_LargeMarginSoftmax,
  title = {Large {{Margin Softmax Loss}} for {{Speaker Verification}}},
  author = {Liu, Yi and He, Liang and Liu, Jia},
  year = {2019},
  month = apr,
  abstract = {In neural network based speaker verification, speaker embedding is expected to be discriminative between speakers while the intra-speaker distance should remain small. A variety of loss functions have been proposed to achieve this goal. In this paper, we investigate the large margin softmax loss with different configurations in speaker verification. Ring loss and minimum hyperspherical energy criterion are introduced to further improve the performance. Results on VoxCeleb show that our best system outperforms the baseline approach by 15\textbackslash\% in EER, and by 13\textbackslash\%, 33\textbackslash\% in minDCF08 and minDCF10, respectively.},
  archivePrefix = {arXiv},
  eprint = {1904.03479},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_large margin softmax loss for speaker verification.pdf;/home/trung/Zotero/storage/FACZGVLD/1904.html},
  journal = {arXiv:1904.03479 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{liu19_NeuralVariationalCorrelated,
  title = {Neural {{Variational Correlated Topic Modeling}}},
  booktitle = {The {{World Wide Web Conference}} on   - {{WWW}} '19},
  author = {Liu, Luyang and Huang, Heyan and Gao, Yang and Zhang, Yongfeng and Wei, Xiaochi},
  year = {2019},
  pages = {1142--1152},
  publisher = {{ACM Press}},
  address = {{San Francisco, CA, USA}},
  doi = {10.1145/3308558.3313561},
  abstract = {With the rapid development of the Internet, millions of documents, such as news and web pages, are generated everyday. Mining the topics and knowledge on them has attracted a lot of interest on both academic and industrial areas. As one of the prevalent unsupervised data mining tools, topic models are usually explored as probabilistic generative models for large collections of texts. Traditional probabilistic topic models tend to find a closed form solution of model parameters and approach the intractable posteriors via approximation methods, which usually lead to the inaccurate inference of parameters and low efficiency when it comes to a quite large volume of data. Recently, an emerging trend of neural variational inference can overcome the above issues, which offers a scalable and powerful deep generative framework for modeling latent topics via neural networks. Interestingly, a common assumption for the most neural variational topic models is that topics are independent and irrelevant to each other. However, this assumption is unreasonable in many practical scenarios. In this paper, we propose a novel Centralized Transformation Flow to capture the correlations among topics by reshaping topic distributions. Furthermore, we present the Transformation Flow Lower Bound to improve the performance of the proposed model. Extensive experiments on two standard benchmark datasets have well-validated the effectiveness of the proposed approach.},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_neural variational correlated topic modeling.pdf},
  isbn = {978-1-4503-6674-8},
  language = {en}
}

@article{liu19_RoBERTaRobustlyOptimized,
  title = {{{RoBERTa}}: {{A Robustly Optimized BERT Pretraining Approach}}},
  shorttitle = {{{RoBERTa}}},
  author = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke and Stoyanov, Veselin},
  year = {2019},
  month = jul,
  abstract = {Language model pretraining has led to significant performance gains but careful comparison between different approaches is challenging. Training is computationally expensive, often done on private datasets of different sizes, and, as we will show, hyperparameter choices have significant impact on the final results. We present a replication study of BERT pretraining (Devlin et al., 2019) that carefully measures the impact of many key hyperparameters and training data size. We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our best model achieves state-of-the-art results on GLUE, RACE and SQuAD. These results highlight the importance of previously overlooked design choices, and raise questions about the source of recently reported improvements. We release our models and code.},
  annotation = {ZSCC: 0000030},
  archivePrefix = {arXiv},
  eprint = {1907.11692},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_roberta.pdf;/home/trung/Zotero/storage/CLD9MLHH/1907.html},
  journal = {arXiv:1907.11692 [cs]},
  keywords = {bert,Computer Science - Computation and Language,distill,pretraining,transfer},
  primaryClass = {cs}
}

@article{liu19_SteinVariationalGradient,
  title = {Stein {{Variational Gradient Descent}}: {{A General Purpose Bayesian Inference Algorithm}}},
  shorttitle = {Stein {{Variational Gradient Descent}}},
  author = {Liu, Qiang and Wang, Dilin},
  year = {2019},
  month = sep,
  abstract = {We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.},
  archivePrefix = {arXiv},
  eprint = {1608.04471},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2019_stein variational gradient descent.pdf;/home/trung/Zotero/storage/RX3JDGZ6/1608.html},
  journal = {arXiv:1608.04471 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{liu20_ComparativeReAssessmentFeature,
  title = {A {{Comparative Re}}-{{Assessment}} of {{Feature Extractors}} for {{Deep Speaker Embeddings}}},
  author = {Liu, Xuechen and Sahidullah, Md and Kinnunen, Tomi},
  year = {2020},
  month = jul,
  abstract = {Modern automatic speaker verification relies largely on deep neural networks (DNNs) trained on mel-frequency cepstral coefficient (MFCC) features. While there are alternative feature extraction methods based on phase, prosody and long-term temporal operations, they have not been extensively studied with DNN-based methods. We aim to fill this gap by providing extensive re-assessment of 14 feature extractors on VoxCeleb and SITW datasets. Our findings reveal that features equipped with techniques such as spectral centroids, group delay function, and integrated noise suppression provide promising alternatives to MFCCs for deep speaker embeddings extraction. Experimental results demonstrate up to 16.3\textbackslash\% (VoxCeleb) and 25.1\textbackslash\% (SITW) relative decrease in equal error rate (EER) to the baseline.},
  archivePrefix = {arXiv},
  eprint = {2007.15283},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2020_a comparative re-assessment of feature extractors for deep speaker embeddings.pdf},
  journal = {arXiv:2007.15283 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{liu20_MultilingualDenoisingPretraining,
  title = {Multilingual {{Denoising Pre}}-Training for {{Neural Machine Translation}}},
  author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
  year = {2020},
  month = jan,
  abstract = {This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART -- a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective. mBART is one of the first methods for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, while previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task-specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show it also enables new types of transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.08210},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2020_multilingual denoising pre-training for neural machine translation.pdf;/home/trung/Zotero/storage/CMCD68IU/2001.html},
  journal = {arXiv:2001.08210 [cs]},
  primaryClass = {cs}
}

@article{liu20_OOGANDisentanglingGAN,
  title = {{{OOGAN}}: {{Disentangling GAN}} with {{One}}-{{Hot Sampling}} and {{Orthogonal Regularization}}},
  shorttitle = {{{OOGAN}}},
  author = {Liu, Bingchen and Zhu, Yizhe and Fu, Zuohui and {de Melo}, Gerard and Elgammal, Ahmed},
  year = {2020},
  month = mar,
  abstract = {Exploring the potential of GANs for unsupervised disentanglement learning, this paper proposes a novel GAN-based disentanglement framework with One-Hot Sampling and Orthogonal Regularization (OOGAN). While previous works mostly attempt to tackle disentanglement learning through VAE and seek to implicitly minimize the Total Correlation (TC) objective with various sorts of approximation methods, we show that GANs have a natural advantage in disentangling with an alternating latent variable (noise) sampling method that is straightforward and robust. Furthermore, we provide a brand-new perspective on designing the structure of the generator and discriminator, demonstrating that a minor structural change and an orthogonal regularization on model weights entails an improved disentanglement. Instead of experimenting on simple toy datasets, we conduct experiments on higher-resolution images and show that OOGAN greatly pushes the boundary of unsupervised disentanglement.},
  archivePrefix = {arXiv},
  eprint = {1905.10836},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2020_oogan.pdf},
  journal = {arXiv:1905.10836 [cs]},
  primaryClass = {cs}
}

@article{liu20_SimplePrincipledUncertainty,
  title = {Simple and {{Principled Uncertainty Estimation}} with {{Deterministic Deep Learning}} via {{Distance Awareness}}},
  author = {Liu, Jeremiah Zhe and Lin, Zi and Padhy, Shreyas and Tran, Dustin and {Bedrax-Weiss}, Tania and Lakshminarayanan, Balaji},
  year = {2020},
  month = jun,
  abstract = {Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.},
  archivePrefix = {arXiv},
  eprint = {2006.10108},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2020_simple and principled uncertainty estimation with deterministic deep learning via distance awareness.pdf},
  journal = {arXiv:2006.10108 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{liu20_VeryDeepTransformers,
  title = {Very {{Deep Transformers}} for {{Neural Machine Translation}}},
  author = {Liu, Xiaodong and Duh, Kevin and Liu, Liyuan and Gao, Jianfeng},
  year = {2020},
  month = aug,
  abstract = {We explore the application of very deep Transformer models for Neural Machine Translation (NMT). Using a simple yet effective initialization technique that stabilizes training, we show that it is feasible to build standard Transformerbased models with up to 60 encoder layers and 12 decoder layers. These deep models outperform their baseline 6-layer counterparts by as much as 2.5 BLEU, and achieve new state-ofthe-art benchmark results on WMT14 EnglishFrench (43.8 BLEU) and WMT14 EnglishGerman (30.1 BLEU). The code and trained models will be publicly available at: https: //github.com/namisan/exdeep-nmt.},
  archivePrefix = {arXiv},
  eprint = {2008.07772},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/liu et al_2020_very deep transformers for neural machine translation.pdf},
  journal = {arXiv:2008.07772 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{livne19_HighMutualInformation,
  title = {High {{Mutual Information}} in {{Representation Learning}} with {{Symmetric Variational Inference}}},
  author = {Livne, Micha and Swersky, Kevin and Fleet, David J.},
  year = {2019},
  month = oct,
  abstract = {We introduce the Mutual Information Machine (MIM), a novel formulation of representation learning, using a joint distribution over the observations and latent state in an encoder/decoder framework. Our key principles are symmetry and mutual information, where symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, and mutual information, to encourage the learning of useful representations for downstream tasks. Our starting point is the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, plus a mutual information encouraging regularizer. We show that this can be bounded by a tractable cross entropy loss function between the true model and a parameterized approximation, and relate this to the maximum likelihood framework. We also relate MIM to variational autoencoders (VAEs) and demonstrate that MIM is capable of learning symmetric factorizations, with high mutual information that avoids posterior collapse.},
  archivePrefix = {arXiv},
  eprint = {1910.04153},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/livne et al_2019_high mutual information in representation learning with symmetric variational inference.pdf},
  journal = {arXiv:1910.04153 [cs, math, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{livne19_HighMutualInformationa,
  title = {High {{Mutual Information}} in {{Representation Learning}} with {{Symmetric Variational Inference}}},
  author = {Livne, Micha and Swersky, Kevin and Fleet, David J.},
  year = {2019},
  month = oct,
  abstract = {We introduce the Mutual Information Machine (MIM), a novel formulation of representation learning, using a joint distribution over the observations and latent state in an encoder/decoder framework. Our key principles are symmetry and mutual information, where symmetry encourages the encoder and decoder to learn different factorizations of the same underlying distribution, and mutual information, to encourage the learning of useful representations for downstream tasks. Our starting point is the symmetric Jensen-Shannon divergence between the encoding and decoding joint distributions, plus a mutual information encouraging regularizer. We show that this can be bounded by a tractable cross entropy loss function between the true model and a parameterized approximation, and relate this to the maximum likelihood framework. We also relate MIM to variational autoencoders (VAEs) and demonstrate that MIM is capable of learning symmetric factorizations, with high mutual information that avoids posterior collapse.},
  archivePrefix = {arXiv},
  eprint = {1910.04153},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/false},
  journal = {arXiv:1910.04153 [cs, math, stat]},
  keywords = {information},
  primaryClass = {cs, math, stat}
}

@article{livne20_MIMMutualInformation,
  title = {{{MIM}}: {{Mutual Information Machine}}},
  shorttitle = {{{MIM}}},
  author = {Livne, Micha and Swersky, Kevin and Fleet, David J.},
  year = {2020},
  month = feb,
  abstract = {We introduce the Mutual Information Machine (MIM), a probabilistic auto-encoder for learning joint distributions over observations and latent variables. MIM reflects three design principles: 1) low divergence, to encourage the encoder and decoder to learn consistent factorizations of the same underlying distribution; 2) high mutual information, to encourage an informative relation between data and latent variables; and 3) low marginal entropy, or compression, which tends to encourage clustered latent representations. We show that a combination of the Jensen-Shannon divergence and the joint entropy of the encoding and decoding distributions satisfies these criteria, and admits a tractable cross-entropy bound that can be optimized directly with Monte Carlo and stochastic gradient descent. We contrast MIM learning with maximum likelihood and VAEs. Experiments show that MIM learns representations with high mutual information, consistent encoding and decoding distributions, effective latent clustering, and data log likelihood comparable to VAE, while avoiding posterior collapse.},
  archivePrefix = {arXiv},
  eprint = {1910.03175},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/livne et al_2020_mim.pdf},
  journal = {arXiv:1910.03175 [cs, math, stat]},
  keywords = {information},
  primaryClass = {cs, math, stat}
}

@article{loaiza-ganem17_MaximumEntropyFlow,
  title = {Maximum {{Entropy Flow Networks}}},
  author = {{Loaiza-Ganem}, Gabriel and Gao, Yuanjun and Cunningham, John P.},
  year = {2017},
  month = apr,
  abstract = {Maximum entropy modeling is a flexible and popular framework for formulating statistical models given partial knowledge. In this paper, rather than the traditional method of optimizing over the continuous density directly, we learn a smooth and invertible transformation that maps a simple distribution to the desired maximum entropy distribution. Doing so is nontrivial in that the objective being maximized (entropy) is a function of the density itself. By exploiting recent developments in normalizing flow networks, we cast the maximum entropy problem into a finite-dimensional constrained optimization, and solve the problem by combining stochastic optimization with the augmented Lagrangian method. Simulation results demonstrate the effectiveness of our method, and applications to finance and computer vision show the flexibility and accuracy of using maximum entropy flow networks.},
  archivePrefix = {arXiv},
  eprint = {1701.03504},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/loaiza-ganem et al_2017_maximum entropy flow networks.pdf},
  journal = {arXiv:1701.03504 [stat]},
  primaryClass = {stat}
}

@incollection{loaiza-ganem19_continuousBernoullifixing,
  title = {The Continuous {{Bernoulli}}: Fixing a Pervasive Error in Variational Autoencoders},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {{Loaiza-Ganem}, Gabriel and Cunningham, John P},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {13287--13297},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/loaiza-ganem et al_2019_the continuous bernoulli.pdf}
}

@incollection{loaiza-ganem19_Deeprandomsplines,
  title = {Deep Random Splines for Point Process Intensity Estimation of Neural Population Data},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {{Loaiza-Ganem}, Gabriel and Perkins, Sean and Schroeder, Karen and Churchland, Mark and Cunningham, John P},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {13346--13356},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/loaiza-ganem et al_2019_deep random splines for point process intensity estimation of neural population data.pdf}
}

@inproceedings{locatello19_ChallengingCommonAssumptions,
  title = {Challenging {{Common Assumptions}} in the {{Unsupervised Learning}} of {{Disentangled Representations}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Locatello, Francesco and Bauer, Stefan and Lu{\v c}i{\'c}, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier Frederic},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/locatello et al_2019_challenging common assumptions in the unsupervised learning of disentangled representations.pdf},
  keywords = {disentanglement}
}

@article{locatello19_DisentanglingFactorsVariation,
  title = {Disentangling {{Factors}} of {{Variation Using Few Labels}}},
  author = {Locatello, Francesco and Tschannen, Michael and Bauer, Stefan and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2019},
  month = may,
  abstract = {Learning disentangled representations is considered a cornerstone problem in representation learning. Recently, Locatello et al. (2019) demonstrated that unsupervised disentanglement learning without inductive biases is theoretically impossible and that existing inductive biases and unsupervised methods do not allow to consistently learn disentangled representations. However, in many practical settings, one might have access to a very limited amount of supervision, for example through manual labeling of training examples. In this paper, we investigate the impact of such supervision on state-of-the-art disentanglement methods and perform a large scale study, training over 29000 models under well-defined and reproducible experimental conditions. We first observe that a very limited number of labeled examples (0.01--0.5\% of the data set) is sufficient to perform model selection on state-of-the-art unsupervised models. Yet, if one has access to labels for supervised model selection, this raises the natural question of whether they should also be incorporated into the training process. As a case-study, we test the benefit of introducing (very limited) supervision into existing state-of-the-art unsupervised disentanglement methods exploiting both the values of the labels and the ordinal information that can be deduced from them. Overall, we empirically validate that with very little and potentially imprecise supervision it is possible to reliably learn disentangled representations.},
  archivePrefix = {arXiv},
  eprint = {1905.01258},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/locatello et al_2019_disentangling factors of variation using few labels.pdf;/home/trung/Zotero/storage/QSXBAMFW/1905.html},
  journal = {arXiv:1905.01258 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{locatello19_FairnessDisentangledRepresentations,
  title = {On the {{Fairness}} of {{Disentangled Representations}}},
  author = {Locatello, Francesco and Abbati, Gabriele and Rainforth, Tom and Bauer, Stefan and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2019},
  month = may,
  abstract = {Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations. We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable. We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent. Analyzing more than 12 600 trained representations of state-of-the-art disentangled models, we observe that various disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.},
  archivePrefix = {arXiv},
  eprint = {1905.13662},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/locatello et al_2019_on the fairness of disentangled representations.pdf},
  journal = {arXiv:1905.13662 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{locatello20_CommentaryUnsupervisedLearning,
  title = {A {{Commentary}} on the {{Unsupervised Learning}} of {{Disentangled Representations}}},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2020},
  month = jul,
  abstract = {The goal of the unsupervised learning of disentangled representations is to separate the independent explanatory factors of variation in the data without access to supervision. In this paper, we summarize the results of (Locatello et al. 2019b) and focus on their implications for practitioners. We discuss the theoretical result showing that the unsupervised learning of disentangled representations is fundamentally impossible without inductive biases and the practical challenges it entails. Finally, we comment on our experimental findings, highlighting the limitations of state-of-the-art approaches and directions for future research.},
  archivePrefix = {arXiv},
  eprint = {2007.14184},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/locatello et al_2020_a commentary on the unsupervised learning of disentangled representations.pdf},
  journal = {arXiv:2007.14184 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{locatello20_SoberLookUnsupervised,
  title = {A {{Sober Look}} at the {{Unsupervised Learning}} of {{Disentangled Representations}} and Their {{Evaluation}}},
  author = {Locatello, Francesco and Bauer, Stefan and Lucic, Mario and R{\"a}tsch, Gunnar and Gelly, Sylvain and Sch{\"o}lkopf, Bernhard and Bachem, Olivier},
  year = {2020},
  volume = {21},
  pages = {62},
  file = {/home/trung/GoogleDrive/Zotero/locatello et al_2020_a sober look at the unsupervised learning of disentangled representations and their evaluation.pdf},
  journal = {Journal of Machine Learning Research},
  keywords = {_tablet,disentanglement,favorite},
  language = {en}
}

@article{locatello20_WeaklySupervisedDisentanglementCompromises,
  title = {Weakly-{{Supervised Disentanglement Without Compromises}}},
  author = {Locatello, Francesco and Poole, Ben and R{\"a}tsch, Gunnar and Sch{\"o}lkopf, Bernhard and Bachem, Olivier and Tschannen, Michael},
  year = {2020},
  month = jun,
  abstract = {Intelligent agents should be able to learn useful representations by observing changes in their environment. We model such observations as pairs of non-i.i.d. images sharing at least one of the underlying factors of variation. First, we theoretically show that only knowing how many factors have changed, but not which ones, is sufficient to learn disentangled representations. Second, we provide practical algorithms that learn disentangled representations from pairs of images without requiring annotation of groups, individual factors, or the number of factors that have changed. Third, we perform a large-scale empirical study and show that such pairs of observations are sufficient to reliably learn disentangled representations on several benchmark data sets. Finally, we evaluate our learned representations and find that they are simultaneously useful on a diverse suite of tasks, including generalization under covariate shifts, fairness, and abstract reasoning. Overall, our results demonstrate that weak supervision enables learning of useful disentangled representations in realistic scenarios.},
  archivePrefix = {arXiv},
  eprint = {2002.02886},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/locatello et al_2020_weakly-supervised disentanglement without compromises.pdf},
  journal = {arXiv:2002.02886 [cs, stat]},
  keywords = {_tablet,disentanglement,favorite},
  primaryClass = {cs, stat}
}

@article{loh00_TopoSketchDrawingLatent,
  title = {{{TopoSketch}}: {{Drawing}} in {{Latent Space}}},
  author = {Loh, Ian and White, Tom},
  pages = {4},
  abstract = {We review a sketch based interface for prototyping animations from the latent space of generative neural networks. The capability of our tool to support visual exploration and communication is demonstrated within the context of facial images, though our framework is domain independent. The motion of sketched gestures is utilised as a means for generating animations, and serves as a reusable visual encapsulation of an animation's semantic operation.},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/GoogleDrive/Zotero/loh et al_toposketch.pdf},
  language = {en}
}

@book{long01_Industrialropeaccess,
  title = {Industrial Rope Access: Investigation into Items of Personal Protective Equipment},
  shorttitle = {Industrial Rope Access},
  author = {Long, Adam and Lyon, Malcolm and Lyon, Graham and {Great Britain} and {Health and Safety Executive} and {Lyon Equipment Limited}},
  year = {2001},
  publisher = {{HSE Books}},
  address = {{Sudbury}},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: 166468782},
  file = {/home/trung/GoogleDrive/Zotero/long et al_2001_industrial rope access.pdf},
  isbn = {978-0-7176-2091-3},
  language = {en}
}

@article{long19_PreventingPosteriorCollapse,
  title = {Preventing {{Posterior Collapse}} in {{Sequence VAEs}} with {{Pooling}}},
  author = {Long, Teng and Cao, Yanshuai and Cheung, Jackie Chi Kit},
  year = {2019},
  month = nov,
  abstract = {Variational Autoencoders (VAEs) hold great potential for modelling text, as they could in theory separate high-level semantic and syntactic properties from local regularities of natural language. Practically, however, VAEs with autoregressive decoders often suffer from posterior collapse, a phenomenon where the model learns to ignore the latent variables, causing the sequence VAE to degenerate into a language model. Previous works attempt to solve this problem with complex architectural changes or costly optimization schemes. In this paper, we argue that posterior collapse is caused in part by the encoder network failing to capture the input variabilities. We verify this hypothesis empirically and propose a straightforward fix using pooling. This simple technique effectively prevents posterior collapse, allowing the model to achieve significantly better data log-likelihood than standard sequence VAEs. Compared to the previous SOTA on preventing posterior collapse, we are able to achieve comparable performances while being significantly faster.},
  archivePrefix = {arXiv},
  eprint = {1911.03976},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/long et al_2019_preventing posterior collapse in sequence vaes with pooling.pdf},
  journal = {arXiv:1911.03976 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@article{lopez00_UnderstandingPerceptionCovid19,
  title = {Understanding the {{Perception}} of {{Covid}}-19 {{Policies}} by {{Mining}} a {{Multilanguage Twitter Dataset}}},
  author = {Lopez, Christian E and Vasu, Malolan and Gallemore, Caleb},
  pages = {4},
  abstract = {The objective of this work is to explore popular discourse about the COVID-19 pandemic and policies implemented to manage it. Using Natural Language Processing, Text Mining, and Network Analysis to analyze corpus of tweets that relate to the COVID-19 pandemic, we identify common responses to the pandemic and how these responses differ across time. Moreover, insights as to how information and misinformation were transmitted via Twitter, starting at the early stages of this pandemic, are presented. Finally, this work introduces a dataset of tweets collected from all over the world, in multiple languages, dating back to January 22nd, when the total cases of reported COVID19 were below 600 worldwide. The insights presented in this work could help inform decision makers in the face of future pandemics, and the dataset introduced can be used to acquire valuable knowledge to help mitigate the COVID-19 pandemic.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/lopez et al_understanding the perception of covid-19 policies by mining a multilanguage twitter dataset.pdf},
  language = {en}
}

@article{lopez18_InformationConstraintsAutoEncoding,
  title = {Information {{Constraints}} on {{Auto}}-{{Encoding Variational Bayes}}},
  author = {Lopez, Romain and Regier, Jeffrey and Jordan, Michael I. and Yosef, Nir},
  year = {2018},
  month = may,
  abstract = {Parameterizing the approximate posterior of a generative model with neural networks has become a common theme in recent machine learning research. While providing appealing flexibility, this approach makes it difficult to impose or assess structural constraints such as conditional independence. We propose a framework for learning representations that relies on auto-encoding variational Bayes, in which the search space is constrained via kernel-based measures of independence. In particular, our method employs the d-variable Hilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between the latent representations and arbitrary nuisance factors. We show how this method can be applied to a range of problems, including problems that involve learning invariant and conditionally independent representations. We also present a full-fledged application to singlecell RNA sequencing (scRNA-seq). In this setting the biological signal is mixed in complex ways with sequencing errors and sampling effects. We show that our method outperforms the state-of-the-art approach in this domain.},
  archivePrefix = {arXiv},
  eprint = {1805.08672},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lopez et al_2018_information constraints on auto-encoding variational bayes.pdf},
  journal = {arXiv:1805.08672 [cs, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,information,Quantitative Biology - Genomics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{lopez20_Enhancingscientificdiscoveries,
  title = {Enhancing Scientific Discoveries in Molecular Biology with Deep Generative Models},
  author = {Lopez, Romain and Gayoso, Adam and Yosef, Nir},
  year = {2020},
  month = sep,
  volume = {16},
  pages = {e9198},
  publisher = {{John Wiley \& Sons, Ltd}},
  issn = {1744-4292},
  doi = {10.15252/msb.20199198},
  abstract = {Abstract Generative models provide a well-established statistical framework for evaluating uncertainty and deriving conclusions from large data sets especially in the presence of noise, sparsity, and bias. Initially developed for computer vision and natural language processing, these models have been shown to effectively summarize the complexity that underlies many types of data and enable a range of applications including supervised learning tasks, such as assigning labels to images; unsupervised learning tasks, such as dimensionality reduction; and out-of-sample generation, such as de novo image synthesis. With this early success, the power of generative models is now being increasingly leveraged in molecular biology, with applications ranging from designing new molecules with properties of interest to identifying deleterious mutations in our genomes and to dissecting transcriptional variability between single cells. In this review, we provide a brief overview of the technical notions behind generative models and their implementation with deep learning techniques. We then describe several different ways in which these models can be utilized in practice, using several recent applications in molecular biology as examples.},
  file = {/home/trung/GoogleDrive/Zotero/lopez et al_2020_enhancing scientific discoveries in molecular biology with deep generative models.pdf},
  journal = {Molecular Systems Biology},
  keywords = {deep generative models,molecular biology,neural networks},
  number = {9}
}

@article{lopresti17_SalviaSageReview,
  title = {Salvia ({{Sage}}): {{A Review}} of Its {{Potential Cognitive}}-{{Enhancing}} and {{Protective Effects}}},
  shorttitle = {Salvia ({{Sage}})},
  author = {Lopresti, Adrian L.},
  year = {2017},
  month = mar,
  volume = {17},
  pages = {53--64},
  issn = {1174-5886},
  doi = {10.1007/s40268-016-0157-5},
  abstract = {Genus Salvia, commonly known as sage, is the largest genus in the Lamiaceae family. It comprises many species traditionally used as brain-enhancing tonics. In vitro and animal studies have confirmed that several Salvia species contain a large array of active compounds that may enhance cognitive activity and protect against neurodegenerative disease. In this review, the active constituents in plants belonging to the genus Salvia are summarised, and their influence on pharmacodynamics pertinent to cognitive activity are detailed. In particular, the effects of plants belonging to the genus Salvia and their constituents on cognitive skills including memory, attention and learning are detailed. Their potential effects in dementia, including Alzheimer's disease, are also examined. Completed human trials are summarised, and factors influencing the potency of Salvia plants are covered. Finally, directions for future research are proposed to enhance our understanding of the potential health benefits of Salvia plants.},
  file = {/home/trung/GoogleDrive/Zotero/lopresti_2017_salvia (sage).pdf},
  journal = {Drugs in R\&D},
  number = {1},
  pmcid = {PMC5318325},
  pmid = {27888449}
}

@article{lorch20_SpatiotemporalEpidemicModel,
  title = {A {{Spatiotemporal Epidemic Model}} to {{Quantify}} the {{Effects}} of {{Contact Tracing}}, {{Testing}}, and {{Containment}}},
  author = {Lorch, Lars and Trouleau, William and Tsirtsis, Stratis and Szanto, Aron and Sch{\"o}lkopf, Bernhard and {Gomez-Rodriguez}, Manuel},
  year = {2020},
  month = apr,
  abstract = {We introduce a novel modeling framework for studying epidemics that is specifically designed to make use of fine-grained spatiotemporal data. Motivated by the current COVID-19 outbreak and the availability of data from contact or location tracing technologies, our model uses marked temporal point processes to represent individual mobility patterns and the course of the disease for each individual in a population. We design an efficient sampling algorithm for our model that can be used to predict the spread of infectious diseases such as COVID-19 under different testing and tracing strategies, social distancing measures, and business restrictions, given location or contact histories of individuals. Building on this algorithm, we use Bayesian optimization to estimate the risk of exposure of each individual at the sites they visit, the percentage of symptomatic individuals, and the difference in transmission rate between asymptomatic and symptomatic individuals from historical longitudinal testing data. Experiments using measured COVID-19 data and mobility patterns from T\textbackslash "\{u\}bingen, a town in the southwest of Germany, demonstrate that our model can be used to quantify the effects of tracing, testing, and containment strategies at an unprecedented spatiotemporal resolution. To facilitate research and informed policy-making, particularly in the context of the current COVID-19 outbreak, we are releasing an open-source implementation of our framework at https://github.com/covid19-model.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2004.07641},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/QBXDKLC6/Lorch et al. - 2020 - A Spatiotemporal Epidemic Model to Quantify the Ef.pdf},
  journal = {arXiv:2004.07641 [physics, q-bio, stat]},
  language = {en},
  primaryClass = {physics, q-bio, stat}
}

@article{lotfollahi19_Conditionaloutofsamplegeneration,
  title = {Conditional Out-of-Sample Generation for Unpaired Data Using {{trVAE}}},
  author = {Lotfollahi, Mohammad and Naghipourfar, Mohsen and Theis, Fabian J. and Wolf, F. Alexander},
  year = {2019},
  month = oct,
  abstract = {While generative models have shown great success in generating high-dimensional samples conditional on low-dimensional descriptors (learning e.g. stroke thickness in MNIST, hair color in CelebA, or speaker identity in Wavenet), their generation out-of-sample poses fundamental problems. The conditional variational autoencoder (CVAE) as a simple conditional generative model does not explicitly relate conditions during training and, hence, has no incentive of learning a compact joint distribution across conditions. We overcome this limitation by matching their distributions using maximum mean discrepancy (MMD) in the decoder layer that follows the bottleneck. This introduces a strong regularization both for reconstructing samples within the same condition and for transforming samples across conditions, resulting in much improved generalization. We refer to the architecture as \textbackslash emph\{transformer\} VAE (trVAE). Benchmarking trVAE on high-dimensional image and tabular data, we demonstrate higher robustness and higher accuracy than existing approaches. In particular, we show qualitatively improved predictions for cellular perturbation response to treatment and disease based on high-dimensional single-cell gene expression data, by tackling previously problematic minority classes and multiple conditions. For generic tasks, we improve Pearson correlations of high-dimensional estimated means and variances with their ground truths from 0.89 to 0.97 and 0.75 to 0.87, respectively.},
  archivePrefix = {arXiv},
  eprint = {1910.01791},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lotfollahi et al_2019_conditional out-of-sample generation for unpaired data using trvae.pdf;/home/trung/Zotero/storage/ZH52Q695/1910.html},
  journal = {arXiv:1910.01791 [cs, eess, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Quantitative Biology - Cell Behavior,Quantitative Biology - Genomics,Statistics - Machine Learning,variational},
  primaryClass = {cs, eess, q-bio, stat}
}

@article{louizos18_LearningSparseNeural,
  title = {Learning {{Sparse Neural Networks}} through {{L0 Regularization}}},
  author = {Louizos, Christos and Welling, Max and Kingma, Diederik P},
  year = {2018},
  pages = {13},
  abstract = {We propose a practical method for L0 norm regularization for neural networks: pruning the network during training by encouraging weights to become exactly zero. Such regularization is interesting since (1) it can greatly speed up training and inference, and (2) it can improve generalization. AIC and BIC, well-known model selection criteria, are special cases of L0 regularization. However, since the L0 norm of weights is non-differentiable, we cannot incorporate it directly as a regularization term in the objective function. We propose a solution through the inclusion of a collection of non-negative stochastic gates, which collectively determine which weights to set to zero. We show that, somewhat surprisingly, for certain distributions over the gates, the expected L0 regularized objective is differentiable with respect to the distribution parameters. We further propose the hard concrete distribution for the gates, which is obtained by ``stretching'' a binary concrete distribution and then transforming its samples with a hard-sigmoid. The parameters of the distribution over the gates can then be jointly optimized with the original network parameters. As a result our method allows for straightforward and efficient learning of model structures with stochastic gradient descent and allows for conditional computation in a principled way. We perform various experiments to demonstrate the effectiveness of the resulting approach and regularizer.},
  annotation = {ZSCC: 0000003},
  file = {/home/trung/GoogleDrive/Zotero/louizos et al_2018_learning sparse neural networks through l0 regularization.pdf},
  language = {en}
}

@article{loukas20_Howhardgraph,
  title = {How Hard Is Graph Isomorphism for Graph Neural Networks?},
  author = {Loukas, Andreas},
  year = {2020},
  month = may,
  abstract = {A hallmark of graph neural networks is their ability to distinguish the isomorphism class of their inputs. This study derives the first hardness results for graph isomorphism in the message-passing model (MPNN). MPNN encompasses the majority of graph neural networks used today and is universal in the limit when nodes are given unique features. The analysis relies on the introduced measure of communication capacity. Capacity measures how much information the nodes of a network can exchange during the forward pass and depends on the depth, message-size, global state, and width of the architecture. It is shown that the capacity of MPNN needs to grow linearly with the number of nodes so that a network can distinguish trees and quadratically for general connected graphs. Crucially, the derived bounds are applicable not only to worst-case instances but over a portion of all inputs. An empirical study involving 12 tasks of varying difficulty and 420 networks reveals strong alignment between actual performance and theoretical predictions.},
  archivePrefix = {arXiv},
  eprint = {2005.06649},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/loukas_2020_how hard is graph isomorphism for graph neural networks.pdf},
  journal = {arXiv:2005.06649 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{loukas20_Whatgraphneural,
  title = {What Graph Neural Networks Cannot Learn: Depth vs Width},
  shorttitle = {What Graph Neural Networks Cannot Learn},
  author = {Loukas, Andreas},
  year = {2020},
  month = jan,
  abstract = {This paper studies the expressive power of graph neural networks falling within the message-passing framework (GNNmp). Two results are presented. First, GNNmp are shown to be Turing universal under sufficient conditions on their depth, width, node attributes, and layer expressiveness. Second, it is discovered that GNNmp can lose a significant portion of their power when their depth and width is restricted. The proposed impossibility statements stem from a new technique that enables the repurposing of seminal results from distributed computing and leads to lower bounds for an array of decision, optimization, and estimation problems involving graphs. Strikingly, several of these problems are deemed impossible unless the product of a GNNmp's depth and width exceeds a polynomial of the graph size; this dependence remains significant even for tasks that appear simple or when considering approximation.},
  archivePrefix = {arXiv},
  eprint = {1907.03199},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/loukas_2020_what graph neural networks cannot learn.pdf},
  journal = {arXiv:1907.03199 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{love20_mikeloveawesomemultiomics,
  title = {Mikelove/Awesome-Multi-Omics},
  author = {Love, Mike},
  year = {2020},
  month = apr,
  abstract = {List of software packages for multi-omics analysis},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {MIT}
}

@article{lu00_StructuredVariationallyAutoencoded,
  title = {Structured {{Variationally Auto}}-Encoded {{Optimization}}},
  author = {Lu, Xiaoyu and Gonz{\'a}lez, Javier and Dai, Zhenwen and Lawrence, Neil D},
  pages = {9},
  abstract = {We tackle the problem of optimizing a blackbox objective function defined over a highlystructured input space. This problem is ubiquitous in machine learning. Inferring the structure of a neural network or the Automatic Statistician (AS), where the kernel combination for a Gaussian process is optimized, are two of many possible examples. We use the AS as a case study to describe our approach, that can be easily generalized to other domains. We propose an Structure Generating Variational Auto-encoder (SG-VAE) to embed the original space of kernel combinations into some low-dimensional continuous manifold where Bayesian optimization (BO) ideas are used. This is possible when structural knowledge of the problem is available, which can be given via a simulator or any other form of generating potentially good solutions. The right exploration-exploitation balance is imposed by propagating into the search the uncertainty of the latent space of the SG-VAE, that is computed using variational inference. The key aspect of our approach is that the SG-VAE can be used to bias the search towards relevant regions, making it suitable for transfer learning tasks. Several experiments in various application domains are used to illustrate the utility and generality of the approach described in this work.},
  file = {/home/trung/GoogleDrive/Zotero/lu et al_structured variationally auto-encoded optimization.pdf},
  language = {en}
}

@incollection{lu13_SemisupervisedLatentDirichlet,
  title = {Semi-Supervised {{Latent Dirichlet Allocation}} for {{Multi}}-Label {{Text Classification}}},
  booktitle = {Recent {{Trends}} in {{Applied Artificial Intelligence}}},
  author = {Lu, Youwei and Okada, Shogo and Nitta, Katsumi},
  editor = {Hutchison, David and Kanade, Takeo and Kittler, Josef and Kleinberg, Jon M. and Mattern, Friedemann and Mitchell, John C. and Naor, Moni and Nierstrasz, Oscar and Pandu Rangan, C. and Steffen, Bernhard and Sudan, Madhu and Terzopoulos, Demetri and Tygar, Doug and Vardi, Moshe Y. and Weikum, Gerhard and Ali, Moonis and Bosse, Tibor and Hindriks, Koen V. and Hoogendoorn, Mark and Jonker, Catholijn M. and Treur, Jan},
  year = {2013},
  volume = {7906},
  pages = {351--360},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-38577-3_36},
  abstract = {This paper proposes a semi-supervised latent Dirichlet allocation (ssLDA) method, which differs from the existing supervised topic models for multi-label classification in mainly two aspects. Firstly both labeled and unlabeled learning data are used in ssLDA to train a model, which is very important for reducing the cost by manually labeling, especially when obtaining a fully labeled dataset is difficult. Secondly ssLDA provides a more flexible training scheme that allows two ways of labeling assignment while existing topic model-based methods usually focus on either of them: (1) a documentlevel assignment of labels to a document; (2) imposing word-level correspondences between words and labels within a document. Our experiment results indicate that ssLDA gains an advantage over other methods in implementation flexibility and can outperform others in terms of multi-label classification performance.},
  file = {/home/trung/GoogleDrive/Zotero/lu et al_2013_semi-supervised latent dirichlet allocation for multi-label text classification.pdf},
  isbn = {978-3-642-38576-6 978-3-642-38577-3},
  language = {en}
}

@article{lu19_InterpretabledeepGaussian,
  title = {Interpretable Deep {{Gaussian}} Processes},
  author = {Lu, Chi-Ken and Yang, Scott Cheng-Hsin and Hao, Xiaoran and Shafto, Patrick},
  year = {2019},
  month = may,
  abstract = {We propose interpretable deep Gaussian Processes (GPs) that combine the expressiveness of deep Neural Networks (NNs) with quantified uncertainty of deep GPs. Our approach is based on approximating deep GP as a GP, which allows explicit, analytic forms for compositions of a wide variety of kernels. Consequently, our approach admits interpretation as both NNs with specified activation functions and as a variational approximation to deep GPs. We provide general recipes for deriving the effective kernels for deep GPs of two, three, or infinitely many layers, composed of homogeneous or heterogeneous kernels. Results illustrate the expressiveness of our effective kernels through samples from the prior and inference on simulated data and demonstrate advantages of interpretability by analysis of analytic forms, drawing relations and equivalences across kernels, and a priori identification of non-pathological regimes of hyperparameter space.},
  archivePrefix = {arXiv},
  eprint = {1905.10963},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lu et al_2019_interpretable deep gaussian processes.pdf;/home/trung/Zotero/storage/48VDZEY3/1905.html},
  journal = {arXiv:1905.10963 [cond-mat, stat]},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,Statistics - Machine Learning,variational},
  primaryClass = {cond-mat, stat}
}

@article{luan17_DeepPhotoStyle,
  title = {Deep {{Photo Style Transfer}}},
  author = {Luan, Fujun and Paris, Sylvain and Shechtman, Eli and Bala, Kavita},
  year = {2017},
  month = apr,
  abstract = {This paper introduces a deep-learning approach to photographic style transfer that handles a large variety of image content while faithfully transferring the reference style. Our approach builds upon the recent work on painterly transfer that separates style from the content of an image by considering different layers of a neural network. However, as is, this approach is not suitable for photorealistic style transfer. Even when both the input and reference images are photographs, the output still exhibits distortions reminiscent of a painting. Our contribution is to constrain the transformation from the input to the output to be locally affine in colorspace, and to express this constraint as a custom fully differentiable energy term. We show that this approach successfully suppresses distortion and yields satisfying photorealistic style transfers in a broad variety of scenarios, including transfer of the time of day, weather, season, and artistic edits.},
  archivePrefix = {arXiv},
  eprint = {1703.07511},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/luan et al_2017_deep photo style transfer.pdf},
  journal = {arXiv:1703.07511 [cs]},
  primaryClass = {cs}
}

@article{luan20_TrainingMattersUnlocking,
  title = {Training {{Matters}}: {{Unlocking Potentials}} of {{Deeper Graph Convolutional Neural Networks}}},
  shorttitle = {Training {{Matters}}},
  author = {Luan, Sitao and Zhao, Mingde and Chang, Xiao-Wen and Precup, Doina},
  year = {2020},
  month = aug,
  abstract = {The performance limit of Graph Convolutional Networks (GCNs) and the fact that we cannot stack more of them to increase the performance, which we usually do for other deep learning paradigms, are pervasively thought to be caused by the limitations of the GCN layers, including insufficient expressive power, etc.. However, if so, for a fixed architecture, it would be unlikely to lower the training difficulty and to improve performance by changing only the training procedure, which we show in this paper not only possible but possible in several ways. This paper first identify the training difficulty of GCNs from the perspective of graph signal energy loss. More specifically, we find that the loss of energy in the backward pass during training nullifies the learning of the layers closer to the input. Then, we propose several methodologies to mitigate the training problem by slightly modifying the GCN operator, from the energy perspective. After empirical validation, we confirm that these changes of operator lead to significant decrease in the training difficulties and notable performance boost, without changing the composition of parameters. With these, we conclude that the root cause of the problem is more likely the training difficulty than the others.},
  archivePrefix = {arXiv},
  eprint = {2008.08838},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/luan et al_2020_training matters.pdf},
  journal = {arXiv:2008.08838 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lucas17_AuxiliaryGuidedAutoregressive,
  title = {Auxiliary {{Guided Autoregressive Variational Autoencoders}}},
  author = {Lucas, Thomas and Verbeek, Jakob},
  year = {2017},
  month = nov,
  abstract = {Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models that encode global image structure into latent variables while autoregressively modeling low level detail. Previous approaches to such hybrid models restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our contribution is a training procedure relying on an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. Our approach can leverage arbitrarily powerful autoregressive decoders, achieves state-of-the art quantitative performance among models with latent variables, and generates qualitatively convincing samples.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1711.11479},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lucas et al_2017_auxiliary guided autoregressive variational autoencoders.pdf;/home/trung/Zotero/storage/DKQKM57W/1711.html},
  journal = {arXiv:1711.11479 [cs]},
  primaryClass = {cs}
}

@article{lucas19_AuxiliaryGuidedAutoregressive,
  title = {Auxiliary {{Guided Autoregressive Variational Autoencoders}}},
  author = {Lucas, Thomas and Verbeek, Jakob},
  year = {2019},
  month = apr,
  abstract = {Generative modeling of high-dimensional data is a key problem in machine learning. Successful approaches include latent variable models and autoregressive models. The complementary strengths of these approaches, to model global and local image statistics respectively, suggest hybrid models that encode global image structure into latent variables while autoregressively modeling low level detail. Previous approaches to such hybrid models restrict the capacity of the autoregressive decoder to prevent degenerate models that ignore the latent variables and only rely on autoregressive modeling. Our contribution is a training procedure relying on an auxiliary loss function that controls which information is captured by the latent variables and what is left to the autoregressive decoder. Our approach can leverage arbitrarily powerful autoregressive decoders, achieves state-of-the art quantitative performance among models with latent variables, and generates qualitatively convincing samples.},
  archivePrefix = {arXiv},
  eprint = {1711.11479},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lucas et al_2019_auxiliary guided autoregressive variational autoencoders.pdf},
  journal = {arXiv:1711.11479 [cs]},
  primaryClass = {cs}
}

@article{lucas19_DonBlameELBO,
  title = {Don't {{Blame}} the {{ELBO}}! {{A Linear VAE Perspective}} on {{Posterior Collapse}}},
  author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  year = {2019},
  month = nov,
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational posterior distribution closely matches the prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We explain how posterior collapse may occur in pPCA due to local maxima in the log marginal likelihood. Unexpectedly, we prove that the ELBO objective for the linear VAE does not introduce additional spurious local maxima relative to log marginal likelihood. We show further that training a linear VAE with exact variational inference recovers an identifiable global maximum corresponding to the principal component directions. Empirically, we find that our linear analysis is predictive even for high-capacity, non-linear VAEs and helps explain the relationship between the observation noise, local maxima, and posterior collapse in deep Gaussian VAEs.},
  archivePrefix = {arXiv},
  eprint = {1911.02469},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lucas et al_2019_don't blame the elbo.pdf},
  journal = {arXiv:1911.02469 [cs, stat]},
  keywords = {vae_issues},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lucas19_UnderstandingPosteriorCollapse,
  title = {Understanding {{Posterior Collapse}} in {{Generative Latent Variable Models}}},
  author = {Lucas, James and Tucker, George and Grosse, Roger and Norouzi, Mohammad},
  year = {2019},
  pages = {16},
  abstract = {Posterior collapse in Variational Autoencoders (VAEs) arises when the variational distribution closely matches the uninformative prior for a subset of latent variables. This paper presents a simple and intuitive explanation for posterior collapse through the analysis of linear VAEs and their direct correspondence with Probabilistic PCA (pPCA). We identify how local maxima can emerge from the marginal log-likelihood of pPCA, which yields similar local maxima for the evidence lower bound (ELBO). We show that training a linear VAE with variational inference recovers a uniquely identifiable global maximum corresponding to the principal component directions. We provide empirical evidence that the presence of local maxima causes posterior collapse in non-linear VAEs. Our findings help to explain a wide range of heuristic approaches in the literature that attempt to diminish the effect of the KL term in the ELBO to alleviate posterior collapse.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/GoogleDrive/Zotero/lucas et al_2019_understanding posterior collapse in generative latent variable models.pdf},
  keywords = {vae_issues},
  language = {en}
}

@article{lucas20_AdaptiveDensityEstimation,
  title = {Adaptive {{Density Estimation}} for {{Generative Models}}},
  author = {Lucas, Thomas and Shmelkov, Konstantin and Alahari, Karteek and Schmid, Cordelia and Verbeek, Jakob},
  year = {2020},
  month = jan,
  abstract = {Unsupervised learning of generative models has seen tremendous progress over recent years, in particular due to generative adversarial networks (GANs), variational autoencoders, and flow-based models. GANs have dramatically improved sample quality, but suffer from two drawbacks: (i) they mode-drop, i.e., do not cover the full support of the train data, and (ii) they do not allow for likelihood evaluations on held-out data. In contrast, likelihood-based training encourages models to cover the full support of the train data, but yields poorer samples. These mutual shortcomings can in principle be addressed by training generative latent variable models in a hybrid adversarial-likelihood manner. However, we show that commonly made parametric assumptions create a conflict between them, making successful hybrid models non trivial. As a solution, we propose to use deep invertible transformations in the latent variable decoder. This approach allows for likelihood computations in image space, is more efficient than fully invertible models, and can take full advantage of adversarial training. We show that our model significantly improves over existing hybrid models: offering GAN-like samples, IS and FID scores that are competitive with fully adversarial models, and improved likelihood scores.},
  archivePrefix = {arXiv},
  eprint = {1901.01091},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lucas et al_2020_adaptive density estimation for generative models.pdf},
  journal = {arXiv:1901.01091 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{lucic19_HighFidelityImageGeneration,
  title = {High-{{Fidelity Image Generation With Fewer Labels}}},
  author = {Lucic, Mario and Tschannen, Michael and Ritter, Marvin and Zhai, Xiaohua and Bachem, Olivier and Gelly, Sylvain},
  year = {2019},
  month = mar,
  abstract = {Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, highdimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10\% of the labels and outperform it using 20\% of the labels.},
  archivePrefix = {arXiv},
  eprint = {1903.02271},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lucic et al_2019_high-fidelity image generation with fewer labels.pdf},
  journal = {arXiv:1903.02271 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lucic19_HighFidelityImageGenerationa,
  title = {High-{{Fidelity Image Generation With Fewer Labels}}},
  author = {Lucic, Mario and Tschannen, Michael and Ritter, Marvin and Zhai, Xiaohua and Bachem, Olivier and Gelly, Sylvain},
  year = {2019},
  month = may,
  abstract = {Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, highdimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10\% of the labels and outperform it using 20\% of the labels.},
  annotation = {ZSCC: 0000024},
  archivePrefix = {arXiv},
  eprint = {1903.02271},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lucic et al_2019_high-fidelity image generation with fewer labels2.pdf},
  journal = {arXiv:1903.02271 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{lucic19_HighFidelityImageGenerationb,
  title = {High-{{Fidelity Image Generation With Fewer Labels}}},
  author = {Lucic, Mario and Tschannen, Michael and Ritter, Marvin and Zhai, Xiaohua and Bachem, Olivier and Gelly, Sylvain},
  year = {2019},
  month = may,
  abstract = {Deep generative models are becoming a cornerstone of modern machine learning. Recent work on conditional generative adversarial networks has shown that learning complex, highdimensional distributions over natural images is within reach. While the latest models are able to generate high-fidelity, diverse natural images at high resolution, they rely on a vast quantity of labeled data. In this work we demonstrate how one can benefit from recent work on self- and semi-supervised learning to outperform the state of the art on both unsupervised ImageNet synthesis, as well as in the conditional setting. In particular, the proposed approach is able to match the sample quality (as measured by FID) of the current state-of-the-art conditional model BigGAN on ImageNet using only 10\% of the labels and outperform it using 20\% of the labels.},
  archivePrefix = {arXiv},
  eprint = {1903.02271},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/4WLYHCKH/Lucic et al. - 2019 - High-Fidelity Image Generation With Fewer Labels.pdf},
  journal = {arXiv:1903.02271 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{luecken19_Currentbestpractices,
  title = {Current Best Practices in Single-cell {{RNA}}-seq Analysis: A Tutorial},
  shorttitle = {Current Best Practices in Single-cell {{RNA}}-seq Analysis},
  author = {Luecken, Malte D and Theis, Fabian J},
  year = {2019},
  month = jun,
  volume = {15},
  issn = {1744-4292, 1744-4292},
  doi = {10.15252/msb.20188746},
  abstract = {Single-cell RNA-seq has enabled gene expression to be studied at an unprecedented resolution. The promise of this technology is attracting a growing user base for single-cell analysis methods. As more analysis tools are becoming available, it is becoming increasingly difficult to navigate this landscape and produce an up-todate workflow to analyse one's data. Here, we detail the steps of a typical single-cell RNA-seq analysis, including pre-processing (quality control, normalization, data correction, feature selection, and dimensionality reduction) and cell- and gene-level downstream analysis. We formulate current best-practice recommendations for these steps based on independent comparison studies. We have integrated these best-practice recommendations into a workflow, which we apply to a public dataset to further illustrate how these steps work in practice. Our documented case study can be found at https://www.github.com/theislab/single-cell-tutorial. This review will serve as a workflow tutorial for new entrants into the field, and help established users update their analysis pipelines.},
  annotation = {ZSCC: 0000055},
  file = {/home/trung/Zotero/storage/GFRXUFS5/Luecken and Theis - 2019 - Current best practices in single‐cell RNA‐seq anal.pdf},
  journal = {Molecular Systems Biology},
  language = {en},
  number = {6}
}

@article{lugosch19_UsingSpeechSynthesis,
  title = {Using {{Speech Synthesis}} to {{Train End}}-to-{{End Spoken Language Understanding Models}}},
  author = {Lugosch, Loren and Meyer, Brett and Nowrouzezahrai, Derek and Ravanelli, Mirco},
  year = {2019},
  month = oct,
  abstract = {End-to-end models are an attractive new approach to spoken language understanding (SLU) in which the meaning of an utterance is inferred directly from the raw audio without employing the standard pipeline composed of a separately trained speech recognizer and natural language understanding module. The downside of end-to-end SLU is that in-domain speech data must be recorded to train the model. In this paper, we propose a strategy for overcoming this requirement in which speech synthesis is used to generate a large synthetic training dataset from several artificial speakers. Experiments on two open-source SLU datasets confirm the effectiveness of our approach, both as a sole source of training data and as a form of data augmentation.},
  archivePrefix = {arXiv},
  eprint = {1910.09463},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/lugosch et al_2019_using speech synthesis to train end-to-end spoken language understanding models.pdf;/home/trung/Zotero/storage/VXAMR7LX/1910.html},
  journal = {arXiv:1910.09463 [eess]},
  keywords = {Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {eess}
}

@techreport{lukassen19_Genesetinference,
  title = {Gene Set Inference from Single-Cell Sequencing Data Using a Hybrid of Matrix Factorization and Variational Autoencoders},
  author = {Lukassen, Soeren and Ten, Foo Wei and Eils, Roland and Conrad, Christian},
  year = {2019},
  month = aug,
  institution = {{Bioinformatics}},
  doi = {10.1101/740415},
  abstract = {Recent advances in single-cell RNA sequencing (scRNA-Seq) have driven the simultaneous measurement of the expression of 1,000s of genes in 1,000s of single cells. These growing data sets allow us to model gene sets in biological networks at an unprecedented level of detail, in spite of heterogenous cell populations. Here, we propose an unsupervised deep neural network model that is a hybrid of matrix factorization and conditional variational autoencoders (CVA), which utilizes weights as matrix factorizations to obtain gene sets, while class-specific inputs to the latent variable space facilitate a plausible identification of cell types. This artificial neural network model seamlessly integrates functional gene set inference, experimental batch effect correction, and static gene identification, which we conceptually prove here for three single-cell RNA-Seq datasets and suggest for future single-cell-gene analytics.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/K3RJS3II/Lukassen et al. - 2019 - Gene set inference from single-cell sequencing dat.pdf},
  language = {en},
  type = {Preprint}
}

@article{luo19_Bayesiandeeplearning,
  title = {Bayesian Deep Learning with Hierarchical Prior: {{Predictions}} from Limited and Noisy Data},
  shorttitle = {Bayesian Deep Learning with Hierarchical Prior},
  author = {Luo, Xihaier and Kareem, Ahsan},
  year = {2019},
  month = jul,
  abstract = {Datasets in engineering applications are often limited and contaminated, mainly due to unavoidable measurement noise and signal distortion. Thus, using conventional data-driven approaches to build a reliable discriminative model, and further applying this identified surrogate to uncertainty analysis remains to be very challenging. A deep learning approach is presented to provide predictions based on limited and noisy data. To address noise perturbation, the Bayesian learning method that naturally facilitates an automatic updating mechanism is considered to quantify and propagate model uncertainties into predictive quantities. Specifically, hierarchical Bayesian modeling (HBM) is first adopted to describe model uncertainties, which allows the prior assumption to be less subjective, while also makes the proposed surrogate more robust. Next, the Bayesian inference is seamlessly integrated into the DL framework, which in turn supports probabilistic programming by yielding a probability distribution of the quantities of interest rather than their point estimates. Variational inference (VI) is implemented for the posterior distribution analysis where the intractable marginalization of the likelihood function over parameter space is framed in an optimization format, and stochastic gradient descent method is applied to solve this optimization problem. Finally, Monte Carlo simulation is used to obtain an unbiased estimator in the predictive phase of Bayesian inference, where the proposed Bayesian deep learning (BDL) scheme is able to offer confidence bounds for the output estimation by analyzing propagated uncertainties. The effectiveness of Bayesian shrinkage is demonstrated in improving predictive performance using contaminated data, and various examples are provided to illustrate concepts, methodologies, and algorithms of this proposed BDL modeling technique.},
  archivePrefix = {arXiv},
  eprint = {1907.04240},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/luo et al_2019_bayesian deep learning with hierarchical prior.pdf;/home/trung/Zotero/storage/U3ZWVG3T/1907.html},
  journal = {arXiv:1907.04240 [physics, stat]},
  keywords = {Computer Science - Machine Learning,Electrical Engineering and Systems Science - Signal Processing,Physics - Computational Physics,Statistics - Machine Learning,variational},
  primaryClass = {physics, stat}
}

@article{luo19_LearningDisentangledRepresentations,
  title = {Learning {{Disentangled Representations}} of {{Timbre}} and {{Pitch}} for {{Musical Instrument Sounds Using Gaussian Mixture Variational Autoencoders}}},
  author = {Luo, Yin-Jyun and Agres, Kat and Herremans, Dorien},
  year = {2019},
  month = jun,
  abstract = {In this paper, we learn disentangled representations of timbre and pitch for musical instrument sounds. We adapt a framework based on variational autoencoders with Gaussian mixture latent distributions. Specifically, we use two separate encoders to learn distinct latent spaces for timbre and pitch, which form Gaussian mixture components representing instrument identity and pitch, respectively. For reconstruction, latent variables of timbre and pitch are sampled from corresponding mixture components, and are concatenated as the input to a decoder. We show the model efficacy by latent space visualization, and a quantitative analysis indicates the discriminability of these spaces, even with a limited number of instrument labels for training. The model allows for controllable synthesis of selected instrument sounds by sampling from the latent spaces. To evaluate this, we trained instrument and pitch classifiers using original labeled data. These classifiers achieve high accuracy when tested on our synthesized sounds, which verifies the model performance of controllable realistic timbre and pitch synthesis. Our model also enables timbre transfer between multiple instruments, with a single autoencoder architecture, which is evaluated by measuring the shift in posterior of instrument classification. Our in depth evaluation confirms the model ability to successfully disentangle timbre and pitch.},
  archivePrefix = {arXiv},
  eprint = {1906.08152},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/luo et al_2019_learning disentangled representations of timbre and pitch for musical instrument sounds using gaussian mixture variational autoencoders.pdf},
  journal = {arXiv:1906.08152 [cs, eess, stat]},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{luong15_EffectiveApproachesAttentionbased,
  title = {Effective {{Approaches}} to {{Attention}}-Based {{Neural Machine Translation}}},
  author = {Luong, Minh-Thang and Pham, Hieu and Manning, Christopher D.},
  year = {2015},
  month = aug,
  abstract = {An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches over the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems which already incorporate known techniques such as dropout. Our ensemble model using different attention architectures has established a new state-of-the-art result in the WMT'15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.},
  annotation = {ZSCC: 0002502},
  archivePrefix = {arXiv},
  eprint = {1508.04025},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/luong et al_2015_effective approaches to attention-based neural machine translation.pdf;/home/trung/Zotero/storage/P5GFUDZ3/1508.html},
  journal = {arXiv:1508.04025 [cs]},
  keywords = {attention,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{luu15_Prospectiveevaluationassociation,
  title = {Prospective Evaluation of the Association of Nut/Peanut Consumption with Total and Cause-Specific Mortality},
  author = {Luu, Hung N. and Blot, William J. and Xiang, Yong-Bing and Cai, Hui and Hargreaves, Margaret K. and Li, Honglan and Yang, Gong and Signorello, Lisa and Gao, Yu-Tang and Zheng, Wei and Shu, Xiao-Ou},
  year = {2015},
  month = may,
  volume = {175},
  pages = {755--766},
  issn = {2168-6114},
  doi = {10.1001/jamainternmed.2014.8347},
  abstract = {IMPORTANCE: High intake of nuts has been linked to a reduced risk of mortality. Previous studies, however, were primarily conducted among people of European descent, particularly those of high socioeconomic status. OBJECTIVE: To examine the association of nut consumption with total and cause-specific mortality in Americans of African and European descent who were predominantly of low socioeconomic status (SES) and in Chinese individuals in Shanghai, China. DESIGN, SETTING, AND PARTICIPANTS: Three large cohorts were evaluated in the study. One included 71 764 US residents of African and European descent, primarily of low SES, who were participants in the Southern Community Cohort Study (SCCS) in the southeastern United States (March 2002 to September 2009), and the other 2 cohorts included 134 265 participants in the Shanghai Women's Health Study (SWHS) (December 1996 to May 2000) and the Shanghai Men's Health Study (SMHS) (January 2002 to September 2006) in Shanghai, China. Self-reported nut consumption in the SCCS (approximately 50\% were peanuts) and peanut-only consumption in the SMHS/SWHS were assessed using validated food frequency questionnaires. MAIN OUTCOMES AND MEASURES: Deaths were ascertained through linkage with the National Death Index and Social Security Administration mortality files in the SCCS and annual linkage with the Shanghai Vital Statistics Registry and by biennial home visits in the SWHS/SMHS. Cox proportional hazards regression models were used to calculate hazard ratios (HRs) and 95\% CIs. RESULTS: With a median follow-up of 5.4 years in the SCCS, 6.5 years in the SMHS, and 12.2 years in the SWHS, 14,440 deaths were identified. More than half of the women in the SCCS were ever smokers compared with only 2.8\% in the SWHS. The ever-smoking rate for men was 77.1\% in the SCCS and 69.6\% in the SMHS. Nut intake was inversely associated with risk of total mortality in all 3 cohorts (all P{$<$}.001 for trend), with adjusted HRs associated with the highest vs lowest quintiles of intake being 0.79 (95\% CI, 0.73-0.86) and 0.83 (95\% CI, 0.77-0.88), respectively, for the US and Shanghai cohorts. This inverse association was predominantly driven by cardiovascular disease mortality (P{$<$}.05 for trend in the US cohort; P{$<$}.001 for trend in the Shanghai cohorts). When specific types of cardiovascular disease were examined, a significant inverse association was consistently seen for ischemic heart disease in all ethnic groups (HR, 0.62; 95\% CI, 0.45-0.85 in blacks; HR, 0.60; 95\% CI, 0.39-0.92 in whites; and HR, 0.70; 95\% CI, 0.54-0.89 in Asians for the highest vs lowest quintile of nut intake). The associations for ischemic stroke (HR, 0.77; 95\% CI, 0.60-1.00 for the highest vs lowest quintile of nut intake) and hemorrhagic stroke (HR, 0.77; 95\% CI, 0.60-0.99 for the highest vs lowest quintile of nut intake) were significant only in Asians. The nut-mortality association was similar for men and women and for blacks, whites, and Asians and was not modified by the presence of metabolic conditions at study enrollment. CONCLUSIONS AND RELEVANCE: Nut consumption was associated with decreased overall and cardiovascular disease mortality across different ethnic groups and among individuals from low SES groups. Consumption of nuts, particularly peanuts given their general affordability, may be considered a cost-effective measure to improve cardiovascular health.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/luu et al_2015_prospective evaluation of the association of nut-peanut consumption with total and cause-specific mortality.pdf},
  journal = {JAMA internal medicine},
  language = {eng},
  number = {5},
  pmcid = {PMC4474488},
  pmid = {25730101}
}

@article{luu20_DropClassDropAdaptDropping,
  title = {{{DropClass}} and {{DropAdapt}}: {{Dropping}} Classes for Deep Speaker Representation Learning},
  shorttitle = {{{DropClass}} and {{DropAdapt}}},
  author = {Luu, Chau and Bell, Peter and Renals, Steve},
  year = {2020},
  month = feb,
  abstract = {Many recent works on deep speaker embeddings train their feature extraction networks on large classification tasks, distinguishing between all speakers in a training set. Empirically, this has been shown to produce speaker-discriminative embeddings, even for unseen speakers. However, it is not clear that this is the optimal means of training embeddings that generalize well. This work proposes two approaches to learning embeddings, based on the notion of dropping classes during training. We demonstrate that both approaches can yield performance gains in speaker verification tasks. The first proposed method, DropClass, works via periodically dropping a random subset of classes from the training data and the output layer throughout training, resulting in a feature extractor trained on many different classification tasks. Combined with an additive angular margin loss, this method can yield a 7.9\% relative improvement in equal error rate (EER) over a strong baseline on VoxCeleb. The second proposed method, DropAdapt, is a means of adapting a trained model to a set of enrolment speakers in an unsupervised manner. This is performed by fine-tuning a model on only those classes which produce high probability predictions when the enrolment speakers are used as input, again also dropping the relevant rows from the output layer. This method yields a large 13.2\% relative improvement in EER on VoxCeleb. The code for this paper has been made publicly available.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {2002.00453},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/BF6P5AQV/Luu et al. - 2020 - DropClass and DropAdapt Dropping classes for deep.pdf},
  journal = {arXiv:2002.00453 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{lyu19_Ultralargelibrarydocking,
  title = {Ultra-Large Library Docking for Discovering New Chemotypes},
  author = {Lyu, Jiankun and Wang, Sheng and Balius, Trent E. and Singh, Isha and Levit, Anat and Moroz, Yurii S. and O'Meara, Matthew J. and Che, Tao and Algaa, Enkhjargal and Tolmachova, Kateryna and Tolmachev, Andrey A. and Shoichet, Brian K. and Roth, Bryan L. and Irwin, John J.},
  year = {2019},
  month = feb,
  volume = {566},
  pages = {224--229},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-0917-9},
  abstract = {Despite intense interest in expanding chemical space, libraries of hundreds-of-millions to billions of diverse molecules have remained inaccessible. Here, we investigate structure-based docking of 170 million make-on-demand compounds from 130 well-characterized reactions. The resulting library is diverse, representing over 10.7 million scaffolds otherwise unavailable. The library was docked against AmpC {$\beta$}-lactamase and the D4 dopamine receptor. From the top-ranking molecules, 44 and 549 were synthesized and tested, respectively. This revealed an unprecedented phenolate inhibitor of AmpC, which was optimized to 77 nM, the most potent non-covalent AmpC inhibitor known. Crystal structures of this and other new AmpC inhibitors confirmed the docking predictions. Against D4, hit rates fell monotonically with docking score, and a hit-rate vs. score curve predicted 453,000 D4 ligands in the library. Of 81 new chemotypes discovered, 30 were submicromolar, including a 180 pM sub-type selective agonist.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/lyu et al_2019_ultra-large library docking for discovering new chemotypes.pdf},
  journal = {Nature},
  language = {en},
  number = {7743}
}

@article{ma00_LearningDisentangledRepresentations,
  title = {Learning {{Disentangled Representations}} for {{Recommendation}}},
  author = {Ma, Jianxin and Zhou, Chang and Cui, Peng and Yang, Hongxia and Zhu, Wenwu},
  pages = {12},
  abstract = {User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users' decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user's preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_learning disentangled representations for recommendation.pdf},
  language = {en}
}

@article{ma18_VariationalImplicitProcesses,
  title = {Variational {{Implicit Processes}}},
  author = {Ma, Chao and Li, Yingzhen and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel},
  year = {2018},
  month = jun,
  abstract = {We introduce the implicit processes (IPs), a stochastic process that places implicitly defined multivariate distributions over any finite collections of random variables. IPs are therefore highly flexible implicit priors over functions, with examples including data simulators, Bayesian neural networks and non-linear transformations of stochastic processes. A novel and efficient approximate inference algorithm for IPs, namely the variational implicit processes (VIPs), is derived using generalised wake-sleep updates. This method returns simple update equations and allows scalable hyper-parameter learning with stochastic optimization. Experiments show that VIPs return better uncertainty estimates and lower errors over existing inference methods for challenging models such as Bayesian neural networks, and Gaussian processes.},
  archivePrefix = {arXiv},
  eprint = {1806.02390},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_2018_variational implicit processes.pdf;/home/trung/Zotero/storage/8HAW966L/1806.html},
  journal = {arXiv:1806.02390 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{ma19_FlowSeqNonAutoregressiveConditional,
  title = {{{FlowSeq}}: {{Non}}-{{Autoregressive Conditional Sequence Generation}} with {{Generative Flow}}},
  shorttitle = {{{FlowSeq}}},
  author = {Ma, Xuezhe and Zhou, Chunting and Li, Xian and Neubig, Graham and Hovy, Eduard},
  year = {2019},
  month = oct,
  abstract = {Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.},
  archivePrefix = {arXiv},
  eprint = {1909.02480},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_2019_flowseq.pdf},
  journal = {arXiv:1909.02480 [cs]},
  primaryClass = {cs}
}

@article{ma19_MAEMutualPosteriorDivergence,
  title = {{{MAE}}: {{Mutual Posterior}}-{{Divergence Regularization}} for {{Variational Autoencoders}}},
  author = {Ma, Xuezhe and Zhou, Chunting and Hovy, Eduard},
  year = {2019},
  pages = {16},
  abstract = {Variational Autoencoder (VAE), a simple and effective deep generative model, has led to a number of impressive empirical successes and spawned many advanced variants and theoretical investigations. However, recent studies demonstrate that, when equipped with expressive generative distributions (aka. decoders), VAE suffers from learning uninformative latent representations with the observation called KL Varnishing, in which case VAE collapses into an unconditional generative model. In this work, we introduce mutual posterior-divergence regularization, a novel regularization that is able to control the geometry of the latent space to accomplish meaningful representation learning, while achieving comparable or superior capability of density estimation. Experiments on three image benchmark datasets demonstrate that, when equipped with powerful decoders, our model performs well both on density estimation and representation learning.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_2019_mae.pdf},
  keywords = {information},
  language = {en}
}

@article{ma20_AEGCNAutoencoderConstrainedGraph,
  title = {{{AEGCN}}: {{An Autoencoder}}-{{Constrained Graph Convolutional Network}}},
  shorttitle = {{{AEGCN}}},
  author = {Ma, Mingyuan and Na, Sen and Wang, Hongyu and Xu, Jin},
  year = {2020},
  month = jul,
  abstract = {We propose a novel neural network architecture, called autoencoder-constrained graph convolutional network, to solve node classification task on graph domains. As suggested by its name, the core of this model is a convolutional network operating directly on graphs, whose hidden layers are constrained by an autoencoder. Comparing with vanilla graph convolutional networks, the autoencoder step is added to reduce the information loss brought by Laplacian smoothing. We consider applying our model on both homogeneous graphs and heterogeneous graphs. For homogeneous graphs, the autoencoder approximates the adjacency matrix of the input graph by taking hidden layer representations as encoder and another one-layer graph convolutional network as decoder. For heterogeneous graphs, since there are multiple adjacency matrices corresponding to different types of edges, the autoencoder approximates the feature matrix of the input graph instead, and changes the encoder to a particularly designed multi-channel pre-processing network with two layers. In both cases, the error occurred in the autoencoder approximation goes to the penalty term in the loss function. In extensive experiments on citation networks and other heterogeneous graphs, we demonstrate that adding autoencoder constraints significantly improves the performance of graph convolutional networks. We also notice that such technique can be applied on graph attention network to improve the performance as well. This reveals the wide applicability of the proposed autoencoder technique.},
  archivePrefix = {arXiv},
  eprint = {2007.03424},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_2020_aegcn.pdf},
  journal = {arXiv:2007.03424 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ma20_DecouplingGlobalLocal,
  title = {Decoupling {{Global}} and {{Local Representations}} from/for {{Image Generation}}},
  author = {Ma, Xuezhe and Kong, Xiang and Zhang, Shanghang and Hovy, Eduard},
  year = {2020},
  month = apr,
  abstract = {In this work, we propose a new generative model that is capable of automatically decoupling global and local representations of images in an entirely unsupervised setting. The proposed model utilizes the variational auto-encoding framework to learn a (low-dimensional) vector of latent variables to capture the global information of an image, which is fed as a conditional input to a flow-based invertible decoder with architecture borrowed from style transfer literature. Experimental results on standard image benchmarks demonstrate the effectiveness of our model in terms of density estimation, image generation and unsupervised representation learning. Importantly, this work demonstrates that with only architectural inductive biases, a generative model with a plain log-likelihood objective is capable of learning decoupled representations, requiring no explicit supervision. The code for our model is available at https://github.com/XuezheMax/wolf.},
  archivePrefix = {arXiv},
  eprint = {2004.11820},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_2020_decoupling global and local representations from-for image generation.pdf},
  journal = {arXiv:2004.11820 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{ma20_IntegrativeMethodsPractical,
  title = {Integrative {{Methods}} and {{Practical Challenges}} for {{Single}}-{{Cell Multi}}-Omics},
  author = {Ma, Anjun and McDermaid, Adam and Xu, Jennifer and Chang, Yuzhou and Ma, Qin},
  year = {2020},
  month = sep,
  volume = {38},
  pages = {1007--1022},
  issn = {01677799},
  doi = {10.1016/j.tibtech.2020.02.013},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_2020_integrative methods and practical challenges for single-cell multi-omics.pdf},
  journal = {Trends in Biotechnology},
  language = {en},
  number = {9}
}

@article{ma20_VAEMDeepGenerative,
  title = {{{VAEM}}: A {{Deep Generative Model}} for {{Heterogeneous Mixed Type Data}}},
  shorttitle = {{{VAEM}}},
  author = {Ma, Chao and Tschiatschek, Sebastian and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Turner, Richard and Zhang, Cheng},
  year = {2020},
  month = jun,
  abstract = {Deep generative models often perform poorly in real-world applications due to the heterogeneity of natural data sets. Heterogeneity arises from data containing different types of features (categorical, ordinal, continuous, etc.) and features of the same type having different marginal distributions. We propose an extension of variational autoencoders (VAEs) called VAEM to handle such heterogeneous data. VAEM is a deep generative model that is trained in a two stage manner such that the first stage provides a more uniform representation of the data to the second stage, thereby sidestepping the problems caused by heterogeneous data. We provide extensions of VAEM to handle partially observed data, and demonstrate its performance in data generation, missing data prediction and sequential feature selection tasks. Our results show that VAEM broadens the range of real-world applications where deep generative models can be successfully deployed.},
  archivePrefix = {arXiv},
  eprint = {2006.11941},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ma et al_2020_vaem.pdf},
  journal = {arXiv:2006.11941 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{maaloe00_FeatureMapVariational,
  title = {Feature {{Map Variational Auto}}-{{Encoders}}},
  author = {Maal{\o}e, Lars and Winther, Ole},
  file = {/home/trung/GoogleDrive/Zotero/maaløe et al_feature map variational auto-encoders.pdf},
  howpublished = {https://openreview.net/pdf?id=Hy\_o3x-0b}
}

@article{maaloe00_ImprovingSemiSupervisedLearning,
  title = {Improving {{Semi}}-{{Supervised Learning}} with {{Auxiliary Deep Generative Models}}},
  author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  pages = {5},
  abstract = {Deep generative models based upon continuous variational distributions parameterized by deep networks give state-of-the-art performance. In this paper we propose a framework for extending the latent representation with extra auxiliary variables in order to make the variational distribution more expressive for semi-supervised learning. By utilizing the stochasticity of the auxiliary variable we demonstrate how to train discriminative classifiers resulting in state-of-the-art performance within semi-supervised learning exemplified by an 0.96\% error on MNIST using 100 labeled data points. Furthermore we observe empirically that using auxiliary variables increases convergence speed suggesting that less expressive variational distributions, not only lead to looser bounds but also slower model training.},
  annotation = {ZSCC: 0000021},
  file = {/home/trung/GoogleDrive/Zotero/maaløe et al_improving semi-supervised learning with auxiliary deep generative models.pdf},
  keywords = {auxiliary,generative,semi supervised,variational},
  language = {en}
}

@article{maaloe15_DeepBeliefNets,
  title = {Deep {{Belief Nets}} for {{Topic Modeling}}},
  author = {Maaloe, Lars and Arngren, Morten and Winther, Ole},
  year = {2015},
  month = jan,
  abstract = {Applying traditional collaborative filtering to digital publishing is challenging because user data is very sparse due to the high volume of documents relative to the number of users. Content based approaches, on the other hand, is attractive because textual content is often very informative. In this paper we describe large-scale content based collaborative filtering for digital publishing. To solve the digital publishing recommender problem we compare two approaches: latent Dirichlet allocation (LDA) and deep belief nets (DBN) that both find low-dimensional latent representations for documents. Efficient retrieval can be carried out in the latent representation. We work both on public benchmarks and digital media content provided by Issuu, an online publishing platform. This article also comes with a newly developed deep belief nets toolbox for topic modeling tailored towards performance evaluation of the DBN model and comparisons to the LDA model.},
  archivePrefix = {arXiv},
  eprint = {1501.04325},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/maaloe et al_2015_deep belief nets for topic modeling.pdf},
  journal = {arXiv:1501.04325 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{maaloe16_AuxiliaryDeepGenerative,
  title = {Auxiliary {{Deep Generative Models}}},
  author = {Maal{\o}e, Lars and S{\o}nderby, Casper Kaae and S{\o}nderby, S{\o}ren Kaae and Winther, Ole},
  year = {2016},
  month = jun,
  abstract = {Deep generative models parameterized by neural networks have recently achieved state-of-the-art performance in unsupervised and semi-supervised learning. We extend deep generative models with auxiliary variables which improves the variational approximation. The auxiliary variables leave the generative model unchanged but make the variational distribution more expressive. Inspired by the structure of the auxiliary variable we also propose a model with two stochastic layers and skip connections. Our findings suggest that more expressive and properly specified deep generative models converge faster with better results. We show state-of-the-art performance within semi-supervised learning on MNIST, SVHN and NORB datasets.},
  archivePrefix = {arXiv},
  eprint = {1602.05473},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/maaløe et al_2016_auxiliary deep generative models.pdf},
  journal = {arXiv:1602.05473 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{maaloe17_SemiSupervisedGenerationClusteraware,
  title = {Semi-{{Supervised Generation}} with {{Cluster}}-Aware {{Generative Models}}},
  author = {Maal{\o}e, Lars and Fraccaro, Marco and Winther, Ole},
  year = {2017},
  month = apr,
  abstract = {Deep generative models trained with large amounts of unlabelled data have proven to be powerful within the domain of unsupervised learning. Many real life data sets contain a small amount of labelled data points, that are typically disregarded when training generative models. We propose the Cluster-aware Generative Model, that uses unlabelled information to infer a latent representation that models the natural clustering of the data, and additional labelled data points to refine this clustering. The generative performances of the model significantly improve when labelled information is exploited, obtaining a log-likelihood of -79.38 nats on permutation invariant MNIST, while also achieving competitive semi-supervised classification accuracies. The model can also be trained fully unsupervised, and still improve the log-likelihood performance with respect to related methods.},
  archivePrefix = {arXiv},
  eprint = {1704.00637},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/maaløe et al_2017_semi-supervised generation with cluster-aware generative models.pdf},
  journal = {arXiv:1704.00637 [cs, stat]},
  keywords = {clustering,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,semi-supervised,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{maaloe19_BIVAVeryDeep,
  title = {{{BIVA}}: {{A Very Deep Hierarchy}} of {{Latent Variables}} for {{Generative Modeling}}},
  shorttitle = {{{BIVA}}},
  author = {Maal{\o}e, Lars and Fraccaro, Marco and Li{\'e}vin, Valentin and Winther, Ole},
  year = {2019},
  month = feb,
  abstract = {With the introduction of the variational autoencoder (VAE), probabilistic latent variable models have received renewed attention as powerful generative models. However, their performance in terms of test likelihood and quality of generated samples has been surpassed by autoregressive models without stochastic units. Furthermore, flow-based models have recently been shown to be an attractive alternative that scales well to high-dimensional data. In this paper we close the performance gap by constructing VAE models that can effectively utilize a deep hierarchy of stochastic variables and model complex covariance structures. We introduce the Bidirectional-Inference Variational Autoencoder (BIVA), characterized by a skip-connected generative model and an inference network formed by a bidirectional stochastic inference path. We show that BIVA reaches state-of-the-art test likelihoods, generates sharp and coherent natural images, and uses the hierarchy of latent variables to capture different aspects of the data distribution. We observe that BIVA, in contrast to recent results, can be used for anomaly detection. We attribute this to the hierarchy of latent variables which is able to extract high-level semantic features. Finally, we extend BIVA to semi-supervised classification tasks and show that it performs comparably to state-of-the-art results by generative adversarial networks.},
  archivePrefix = {arXiv},
  eprint = {1902.02102},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/maaløe et al_2019_biva.pdf;/home/trung/Zotero/storage/KM8739P9/1902.html},
  journal = {arXiv:1902.02102 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,hierarchical,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@book{maathuis18_Handbookgraphicalmodels,
  title = {Handbook of Graphical Models},
  author = {Maathuis, Marloes and Drton, Mathias and Lauritzen, Steffen and Wainwright, Martin},
  year = {2018},
  edition = {1st},
  publisher = {{CRC Press, Inc.}},
  address = {{USA}},
  abstract = {A graphical model is a statistical model that isrepresented bya graph. The factorization properties underlying graphical models facilitate tractable computation with multivariate distributions, making the models a valuable tool with a plethora of applications. Furthermore, directed graphical models allow intuitive causal interpretations and have become a cornerstone for causal inference. While there exist a number of excellent books on graphical models, the field has grown so much that individual authors can hardly cover its entire scope. Moreover, the field is interdisciplinary by nature. Through chapters by leading researchers from different areas, this handbook provides a broad and accessible overview of the state of the art. Key features: * Contributions by leading researchers from a range of disciplines * Structured in five parts, covering foundations, computational aspects, statistical inference, causal inference, and applications * Balanced coverage of concepts, theory, methods, examples, and applications * Chapters can be read mostly independently, while cross-references highlight connections The handbook is targeted at a wide audience, including graduate students, applied researchers, and experts in graphical models.},
  file = {/home/trung/GoogleDrive/Zotero/maathuis et al_2018_handbook of graphical models.pdf},
  isbn = {1-4987-8862-9}
}

@article{macaluso13_FatSupplementsIncrease,
  title = {Do {{Fat Supplements Increase Physical Performance}}?},
  author = {Macaluso, Filippo and Barone, Rosario and Catanese, Patrizia and Carini, Francesco and Rizzuto, Luigi and Farina, Felicia and Felice, Valentina Di},
  year = {2013},
  month = feb,
  volume = {5},
  pages = {509--524},
  issn = {2072-6643},
  doi = {10.3390/nu5020509},
  abstract = {Fish oil and conjugated linoleic acid (CLA) belong to a popular class of food supplements known as ``fat supplements'', which are claimed to reduce muscle glycogen breakdown, reduce body mass, as well as reduce muscle damage and inflammatory responses. Sport athletes consume fish oil and CLA mainly to increase lean body mass and reduce body fat. Recent evidence indicates that this kind of supplementation may have other side-effects and a new role has been identified in steroidogenensis. Preliminary findings demonstrate that fish oil and CLA may induce a physiological increase in testosterone synthesis. The aim of this review is to describe the effects of fish oil and CLA on physical performance (endurance and resistance exercise), and highlight the new results on the effects on testosterone biosynthesis. In view of these new data, we can hypothesize that fat supplements may improve the anabolic effect of exercise.},
  annotation = {ZSCC: 0000033},
  file = {/home/trung/GoogleDrive/Zotero/macaluso et al_2013_do fat supplements increase physical performance.pdf},
  journal = {Nutrients},
  number = {2},
  pmcid = {PMC3635209},
  pmid = {23434906}
}

@article{mackay92_PracticalBayesianFramework,
  title = {A {{Practical Bayesian Framework}} for {{Backpropagation Networks}}},
  author = {MacKay, David J. C.},
  year = {1992},
  month = may,
  volume = {4},
  pages = {448--472},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/neco.1992.4.3.448},
  abstract = {A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible (1) objective comparisons between solutions using alternative network architectures, (2) objective stopping rules for network pruning or growing procedures, (3) objective choice of magnitude and type of weight decay terms or additive regularizers (for penalizing large weights, etc.), (4) a measure of the effective number of well-determined parameters in a model, (5) quantified estimates of the error bars on network parameters and on network output, and (6) objective comparisons with alternative learning and interpolation models such as splines and radial basis functions. The Bayesian "evidence" automatically embodies "Occam's razor," penalizing overflexible and overcomplex models. The Bayesian approach helps detect poor underlying assumptions in learning models. For learning models well matched to a problem, a good correlation between generalization ability and the Bayesian evidence is obtained.},
  file = {/home/trung/GoogleDrive/Zotero/mackay_1992_a practical bayesian framework for backpropagation networks.pdf},
  journal = {Neural Computation},
  language = {en},
  number = {3}
}

@article{maddison17_ConcreteDistributionContinuous,
  title = {The {{Concrete Distribution}}: {{A Continuous Relaxation}} of {{Discrete Random Variables}}},
  shorttitle = {The {{Concrete Distribution}}},
  author = {Maddison, Chris J. and Mnih, Andriy and Teh, Yee Whye},
  year = {2017},
  month = mar,
  abstract = {The reparameterization trick enables optimizing large scale stochastic computation graphs via gradient descent. The essence of the trick is to refactor each stochastic node into a differentiable function of its parameters and a random variable with fixed distribution. After refactoring, the gradients of the loss propagated by the chain rule through the graph are low variance unbiased estimators of the gradients of the expected loss. While many continuous random variables have such reparameterizations, discrete random variables lack useful reparameterizations due to the discontinuous nature of discrete states. In this work we introduce Concrete random variables---continuous relaxations of discrete random variables. The Concrete distribution is a new family of distributions with closed form densities and a simple reparameterization. Whenever a discrete stochastic node of a computation graph can be refactored into a one-hot bit representation that is treated continuously, Concrete stochastic nodes can be used with automatic differentiation to produce low-variance biased gradients of objectives (including objectives that depend on the log-probability of latent stochastic nodes) on the corresponding discrete graph. We demonstrate the effectiveness of Concrete relaxations on density estimation and structured prediction tasks using neural networks.},
  archivePrefix = {arXiv},
  eprint = {1611.00712},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/maddison et al_2017_the concrete distribution.pdf},
  journal = {arXiv:1611.00712 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{maddox19_SimpleBaselineBayesian,
  title = {A {{Simple Baseline}} for {{Bayesian Uncertainty}} in {{Deep Learning}}},
  author = {Maddox, Wesley and Garipov, Timur and Izmailov, Pavel and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = dec,
  abstract = {We propose SWA-Gaussian (SWAG), a simple, scalable, and general purpose approach for uncertainty representation and calibration in deep learning. Stochastic Weight Averaging (SWA), which computes the first moment of stochastic gradient descent (SGD) iterates with a modified learning rate schedule, has recently been shown to improve generalization in deep learning. With SWAG, we fit a Gaussian using the SWA solution as the first moment and a low rank plus diagonal covariance also derived from the SGD iterates, forming an approximate posterior distribution over neural network weights; we then sample from this Gaussian distribution to perform Bayesian model averaging. We empirically find that SWAG approximates the shape of the true posterior, in accordance with results describing the stationary distribution of SGD iterates. Moreover, we demonstrate that SWAG performs well on a wide variety of tasks, including out of sample detection, calibration, and transfer learning, in comparison to many popular alternatives including MC dropout, KFAC Laplace, SGLD, and temperature scaling.},
  archivePrefix = {arXiv},
  eprint = {1902.02476},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/FBPWRW8R/Maddox et al. - 2019 - A Simple Baseline for Bayesian Uncertainty in Deep.pdf},
  journal = {arXiv:1902.02476 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{maeda14_Bayesianencouragesdropout,
  title = {A {{Bayesian}} Encourages Dropout},
  author = {Maeda, Shin-ichi},
  year = {2014},
  month = dec,
  abstract = {Dropout is one of the key techniques to prevent the learning from overfitting. It is explained that dropout works as a kind of modified L2 regularization. Here, we shed light on the dropout from Bayesian standpoint. Bayesian interpretation enables us to optimize the dropout rate, which is beneficial for learning of weight parameters and prediction after learning. The experiment result also encourages the optimization of the dropout.},
  annotation = {ZSCC: 0000025},
  archivePrefix = {arXiv},
  eprint = {1412.7003},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/maeda_2014_a bayesian encourages dropout.pdf;/home/trung/Zotero/storage/QRGC7FJK/1412.html},
  journal = {arXiv:1412.7003 [cs, stat]},
  keywords = {favorite},
  primaryClass = {cs, stat}
}

@article{magner20_PowerGraphConvolutional,
  title = {The {{Power}} of {{Graph Convolutional Networks}} to {{Distinguish Random Graph Models}}: {{Short Version}}},
  shorttitle = {The {{Power}} of {{Graph Convolutional Networks}} to {{Distinguish Random Graph Models}}},
  author = {Magner, Abram and Baranwal, Mayank and Hero III, Alfred O.},
  year = {2020},
  month = feb,
  abstract = {Graph convolutional networks (GCNs) are a widely used method for graph representation learning. We investigate the power of GCNs, as a function of their number of layers, to distinguish between different random graph models on the basis of the embeddings of their sample graphs. In particular, the graph models that we consider arise from graphons, which are the most general possible parameterizations of infinite exchangeable graph models and which are the central objects of study in the theory of dense graph limits. We exhibit an infinite class of graphons that are well-separated in terms of cut distance and are indistinguishable by a GCN with nonlinear activation functions coming from a certain broad class if its depth is at least logarithmic in the size of the sample graph. These results theoretically match empirical observations of several prior works. Finally, we show a converse result that for pairs of graphons satisfying a degree profile separation property, a very simple GCN architecture suffices for distinguishability. To prove our results, we exploit a connection to random walks on graphs.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2002.05678},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/QYHTFBH9/Magner et al. - 2020 - The Power of Graph Convolutional Networks to Disti.pdf},
  journal = {arXiv:2002.05678 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{mahmood00_MaskedGraphModeling,
  title = {Masked {{Graph Modeling}} for {{Molecule Generation}}},
  author = {Mahmood, Omar and Mansimov, Elman and Bonneau, Richard and Cho, Kyunghyun},
  pages = {36},
  file = {/home/trung/GoogleDrive/Zotero/mahmood et al_masked graph modeling for molecule generation.pdf},
  language = {en}
}

@article{mahmud18_ApplicationsDeepLearning,
  title = {Applications of {{Deep Learning}} and {{Reinforcement Learning}} to {{Biological Data}}},
  author = {Mahmud, Mufti and Kaiser, Mohammed Shamim and Hussain, Amir and Vassanelli, Stefano},
  year = {2018},
  month = jun,
  volume = {29},
  pages = {2063--2079},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2018.2790388},
  abstract = {Rapid advances in hardware-based technologies during the past decades have opened up new possibilities for life scientists to gather multimodal data in various application domains, such as omics, bioimaging, medical imaging, and (brain/body)-machine interfaces. These have generated novel opportunities for development of dedicated data-intensive machine learning techniques. In particular, recent research in deep learning (DL), reinforcement learning (RL), and their combination (deep RL) promise to revolutionize the future of artificial intelligence. The growth in computational power accompanied by faster and increased data storage, and declining computing costs have already allowed scientists in various fields to apply these techniques on data sets that were previously intractable owing to their size and complexity. This paper provides a comprehensive survey on the application of DL, RL, and deep RL techniques in mining biological data. In addition, we compare the performances of DL techniques when applied to different data sets across various application domains. Finally, we outline open issues in this challenging research area and discuss future development perspectives.},
  annotation = {ZSCC: 0000116},
  file = {/home/trung/GoogleDrive/Zotero/mahmud et al_2018_applications of deep learning and reinforcement learning to biological data.pdf;/home/trung/Zotero/storage/4BVPTQGM/8277160.html},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {artificial intelligence,Bioimaging,biological data mining,Biology,biology computing,Biomedical imaging,brain–machine interfaces,computational power,Computer architecture,convolutional neural network (CNN),data mining,Data models,data-intensive machine learning techniques,deep autoencoder (DA),deep belief network (DBN),deep learning,deep learning performance,deep RL techniques,DL techniques,Feature extraction,hardware-based technologies,learning (artificial intelligence),Machine learning,medical imaging,omics,recurrent neural network (RNN),Recurrent neural networks,reinforcement learning,semi-supervised},
  number = {6}
}

@article{makhlouf20_SurveyCausalbasedMachine,
  title = {Survey on {{Causal}}-Based {{Machine Learning Fairness Notions}}},
  author = {Makhlouf, Karima and Zhioua, Sami and Palamidessi, Catuscia},
  year = {2020},
  month = oct,
  abstract = {Addressing the problem of fairness is crucial to safely use machine learning algorithms to support decisions with a critical impact on people's lives such as job hiring, child maltreatment, disease diagnosis, loan granting, etc. Several notions of fairness have been defined and examined in the past decade, such as, statistical parity and equalized odds. The most recent fairness notions, however, are causal-based and reflect the now widely accepted idea that using causality is necessary to appropriately address the problem of fairness. This paper examines an exhaustive list of causal-based fairness notions, in particular their applicability in real-world scenarios. As the majority of causal-based fairness notions are defined in terms of non-observable quantities (e.g. interventions and counterfactuals), their applicability depends heavily on the identifiability of those quantities from observational data. In this paper, we compile the most relevant identifiability criteria for the problem of fairness from the extensive literature on identifiability theory. These criteria are then used to decide about the applicability of causal-based fairness notions in concrete discrimination scenarios.},
  archivePrefix = {arXiv},
  eprint = {2010.09553},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/makhlouf et al_2020_survey on causal-based machine learning fairness notions.pdf},
  journal = {arXiv:2010.09553 [cs]},
  primaryClass = {cs}
}

@article{makhzani15_AdversarialAutoencoders,
  title = {Adversarial {{Autoencoders}}},
  author = {Makhzani, Alireza and Shlens, Jonathon and Jaitly, Navdeep and Goodfellow, Ian and Frey, Brendan},
  year = {2015},
  month = nov,
  abstract = {In this paper, we propose the ``adversarial autoencoder'' (AAE), which is a probabilistic autoencoder that uses the recently proposed generative adversarial networks (GAN) to perform variational inference by matching the aggregated posterior of the hidden code vector of the autoencoder with an arbitrary prior distribution. Matching the aggregated posterior to the prior ensures that generating from any part of prior space results in meaningful samples. As a result, the decoder of the adversarial autoencoder learns a deep generative model that maps the imposed prior to the data distribution. We show how the adversarial autoencoder can be used in applications such as semi-supervised classification, disentangling style and content of images, unsupervised clustering, dimensionality reduction and data visualization. We performed experiments on MNIST, Street View House Numbers and Toronto Face datasets and show that adversarial autoencoders achieve competitive results in generative modeling and semi-supervised classification tasks.},
  archivePrefix = {arXiv},
  eprint = {1511.05644},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/makhzani et al_2015_adversarial autoencoders.pdf;/home/trung/Zotero/storage/VR9ZIME8/1511.html},
  journal = {arXiv:1511.05644 [cs]},
  keywords = {Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@phdthesis{makhzani18_UnsupervisedRepresentationLearning,
  title = {Unsupervised {{Representation Learning}} with {{Autoencoders}}},
  author = {Makhzani, Alireza},
  year = {2018},
  file = {/home/trung/GoogleDrive/Zotero/makhzani_2018_unsupervised representation learning with autoencoders.pdf}
}

@article{makhzani19_ImplicitAutoencoders,
  title = {Implicit {{Autoencoders}}},
  author = {Makhzani, Alireza},
  year = {2019},
  month = feb,
  abstract = {In this paper, we describe the "implicit autoencoder" (IAE), a generative autoencoder in which both the generative path and the recognition path are parametrized by implicit distributions. We use two generative adversarial networks to define the reconstruction and the regularization cost functions of the implicit autoencoder, and derive the learning rules based on maximum-likelihood learning. Using implicit distributions allows us to learn more expressive posterior and conditional likelihood distributions for the autoencoder. Learning an expressive conditional likelihood distribution enables the latent code to only capture the abstract and high-level information of the data, while the remaining low-level information is captured by the implicit conditional likelihood distribution. We show the applications of implicit autoencoders in disentangling content and style information, clustering, semi-supervised classification, learning expressive variational distributions, and multimodal image-to-image translation from unpaired data.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1805.09804},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/makhzani_2019_implicit autoencoders.pdf},
  journal = {arXiv:1805.09804 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{malisiewicz00_MakingDeepNetworks,
  title = {Making {{Deep Networks Probabilistic}} via {{Test}}-Time {{Dropout}}},
  author = {Malisiewicz, Tomasz},
  abstract = {A Blog about Deep Learning, Computer Vision, and the algorithms that are shaping the future of Artificial Intelligence.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/Q8RDLALZ/making-deep-networks-probabilistic-via.html},
  language = {en}
}

@article{malte19_Evolutiontransferlearning,
  title = {Evolution of Transfer Learning in Natural Language Processing},
  author = {Malte, Aditya and Ratadiya, Pratik},
  year = {2019},
  month = oct,
  abstract = {In this paper, we present a study of the recent advancements which have helped bring Transfer Learning to NLP through the use of semi-supervised training. We discuss cutting-edge methods and architectures such as BERT, GPT, ELMo, ULMFit among others. Classically, tasks in natural language processing have been performed through rule-based and statistical methodologies. However, owing to the vast nature of natural languages these methods do not generalise well and failed to learn the nuances of language. Thus machine learning algorithms such as Naive Bayes and decision trees coupled with traditional models such as Bag-of-Words and N-grams were used to usurp this problem. Eventually, with the advent of advanced recurrent neural network architectures such as the LSTM, we were able to achieve state-of-the-art performance in several natural language processing tasks such as text classification and machine translation. We talk about how Transfer Learning has brought about the well-known ImageNet moment for NLP. Several advanced architectures such as the Transformer and its variants have allowed practitioners to leverage knowledge gained from unrelated task to drastically fasten convergence and provide better performance on the target task. This survey represents an effort at providing a succinct yet complete understanding of the recent advances in natural language processing using deep learning in with a special focus on detailing transfer learning and its potential advantages.},
  archivePrefix = {arXiv},
  eprint = {1910.07370},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/malte et al_2019_evolution of transfer learning in natural language processing.pdf;/home/trung/GoogleDrive/Zotero/malte et al_2019_evolution of transfer learning in natural language processing2.pdf;/home/trung/Zotero/storage/6F3828V8/1910.html},
  journal = {arXiv:1910.07370 [cs]},
  keywords = {attention,Computer Science - Computation and Language,recurrent,transfer,transformer},
  primaryClass = {cs}
}

@book{maltsev14_Systemsanalysishuman,
  title = {Systems Analysis of Human Multigene Disorders},
  editor = {Maltsev, Natalia and Rzhetski{\u \i}, A. IU and Gilliam, T. Conrad},
  year = {2014},
  publisher = {{Springer}},
  address = {{New York}},
  abstract = {"Understanding the genetic architecture underlying complex multigene disorders is one of the major goals of human genetics in the upcoming decades. Advances in whole genome sequencing and the success of high throughput functional genomics allow supplementing conventional reductionist biology with systems-level approaches to human heredity and health as systems of interacting genetic, epigenetic, and environmental factors. This integrative approach holds the promise of unveiling yet unexplored levels of molecular organization and biological complexity. It may also hold the key to deciphering the multigene patterns of disease inheritance"--},
  annotation = {ZSCC: NoCitationData[s0]  OCLC: ocn869776617},
  file = {/home/trung/Zotero/storage/65TZZJ3L/Maltsev et al. - 2014 - Systems analysis of human multigene disorders.pdf},
  isbn = {978-1-4614-8777-7},
  keywords = {Biological systems,Data processing,Genetics; Medical,Mathematical models,Medical genetics,Models; Biological,statistics \& numerical data,System analysis,Systems Analysis},
  language = {en},
  lccn = {QH431 .S986 2014},
  number = {799},
  series = {Advances in Experimental Medicine and Biology}
}

@book{manaswi18_DeepLearningApplications,
  title = {Deep {{Learning}} with {{Applications Using Python}}},
  author = {Manaswi, Navin Kumar},
  year = {2018},
  publisher = {{Apress}},
  address = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-3516-4},
  annotation = {ZSCC: 0000013},
  file = {/home/trung/GoogleDrive/Zotero/manaswi_2018_deep learning with applications using python.pdf},
  isbn = {978-1-4842-3515-7 978-1-4842-3516-4},
  language = {en}
}

@article{manica19_ExplainableAnticancerCompound,
  title = {Towards {{Explainable Anticancer Compound Sensitivity Prediction}} via {{Multimodal Attention}}-Based {{Convolutional Encoders}}},
  author = {Manica, Matteo and Oskooei, Ali and Born, Jannis and Subramanian, Vigneshwari and {S{\'a}ez-Rodr{\'i}guez}, Julio and Mart{\'i}nez, Mar{\'i}a Rodr{\'i}guez},
  year = {2019},
  month = apr,
  abstract = {In line with recent advances in neural drug design and sensitivity prediction, we propose a novel architecture for interpretable prediction of anticancer compound sensitivity using a multimodal attention-based convolutional encoder. Our model is based on the three key pillars of drug sensitivity: compounds' structure in the form of a SMILES sequence, gene expression profiles of tumors and prior knowledge on intracellular interactions from protein-protein interaction networks. We demonstrate that our multiscale convolutional attentionbased (MCA) encoder significantly outperforms a baseline model trained on Morgan fingerprints, a selection of encoders based on SMILES as well as previously reported state of the art for multimodal drug sensitivity prediction (R2 = 0.86 and RMSE = 0.89). Moreover, the explainability of our approach is demonstrated by a thorough analysis of the attention weights. We show that the attended genes significantly enrich apoptotic processes and that the drug attention is strongly correlated with a standard chemical structure similarity index. Finally, we report a case study of two receptor tyrosine kinase (RTK) inhibitors acting on a leukemia cell line, showcasing the ability of the model to focus on informative genes and submolecular regions of the two compounds. The demonstrated generalizability and the interpretability of our model testify its potential for in-silico prediction of anticancer compound efficacy on unseen cancer cells, positioning it as a valid solution for the development of personalized therapies as well as for the evaluation of candidate compounds in de novo drug design.},
  archivePrefix = {arXiv},
  eprint = {1904.11223},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/manica et al_2019_towards explainable anticancer compound sensitivity prediction via multimodal attention-based convolutional encoders.pdf},
  journal = {arXiv:1904.11223 [cs, q-bio, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Quantitative Methods,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{mao00_SSHLDASemiSupervisedHierarchical,
  title = {{{SSHLDA}}: {{A Semi}}-{{Supervised Hierarchical Topic Model}}},
  author = {Mao, Xian-Ling and Ming, Zhao-Yan and Chua, Tat-Seng and Li, Si and Yan, Hongfei and Li, Xiaoming},
  pages = {10},
  abstract = {Supervised hierarchical topic modeling and unsupervised hierarchical topic modeling are usually used to obtain hierarchical topics, such as hLLDA and hLDA. Supervised hierarchical topic modeling makes heavy use of the information from observed hierarchical labels, but cannot explore new topics; while unsupervised hierarchical topic modeling is able to detect automatically new topics in the data space, but does not make use of any information from hierarchical labels. In this paper, we propose a semi-supervised hierarchical topic model which aims to explore new topics automatically in the data space while incorporating the information from observed hierarchical labels into the modeling process, called SemiSupervised Hierarchical Latent Dirichlet Allocation (SSHLDA). We also prove that hLDA and hLLDA are special cases of SSHLDA. We conduct experiments on Yahoo! Answers and ODP datasets, and assess the performance in terms of perplexity and clustering. The experimental results show that predictive ability of SSHLDA is better than that of baselines, and SSHLDA can also achieve significant improvement over baselines for clustering on the FScore measure.},
  file = {/home/trung/GoogleDrive/Zotero/mao et al_sshlda.pdf},
  language = {en}
}

@article{mao19_NeuroSymbolicConceptLearner,
  title = {The {{Neuro}}-{{Symbolic Concept Learner}}: {{Interpreting Scenes}}, {{Words}}, and {{Sentences From Natural Supervision}}},
  shorttitle = {The {{Neuro}}-{{Symbolic Concept Learner}}},
  author = {Mao, Jiayuan and Gan, Chuang and Kohli, Pushmeet and Tenenbaum, Joshua B. and Wu, Jiajun},
  year = {2019},
  month = apr,
  abstract = {We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval.},
  annotation = {ZSCC: 0000044},
  archivePrefix = {arXiv},
  eprint = {1904.12584},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/8JZSEAGW/Mao et al. - 2019 - The Neuro-Symbolic Concept Learner Interpreting S.pdf},
  journal = {arXiv:1904.12584 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{marcus18_DeepLearningCritical,
  title = {Deep {{Learning}}: {{A Critical Appraisal}}},
  shorttitle = {Deep {{Learning}}},
  author = {Marcus, Gary},
  year = {2018},
  month = jan,
  abstract = {Although deep learning has historical roots going back decades, neither the term "deep learning" nor the approach was popular just over five years ago, when the field was reignited by papers such as Krizhevsky, Sutskever and Hinton's now classic (2012) deep network model of Imagenet. What has the field discovered in the five subsequent years? Against a background of considerable progress in areas such as speech recognition, image recognition, and game playing, and considerable enthusiasm in the popular press, I present ten concerns for deep learning, and suggest that deep learning must be supplemented by other techniques if we are to reach artificial general intelligence.},
  archivePrefix = {arXiv},
  eprint = {1801.00631},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/marcus_2018_deep learning.pdf;/home/trung/Zotero/storage/WVF6HEZW/1801.html},
  journal = {arXiv:1801.00631 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.0,I.2.6,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{marcus18_InnatenessAlphaZeroArtificial,
  title = {Innateness, {{AlphaZero}}, and {{Artificial Intelligence}}},
  author = {Marcus, Gary},
  year = {2018},
  month = jan,
  abstract = {The concept of innateness is rarely discussed in the context of artificial intelligence. When it is discussed, or hinted at, it is often the context of trying to reduce the amount of innate machinery in a given system. In this paper, I consider as a test case a recent series of papers by Silver et al (Silver et al., 2017a) on AlphaGo and its successors that have been presented as an argument that a "even in the most challenging of domains: it is possible to train to superhuman level, without human examples or guidance", "starting tabula rasa." I argue that these claims are overstated, for multiple reasons. I close by arguing that artificial intelligence needs greater attention to innateness, and I point to some proposals about what that innateness might look like.},
  archivePrefix = {arXiv},
  eprint = {1801.05667},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/marcus_2018_innateness, alphazero, and artificial intelligence.pdf},
  journal = {arXiv:1801.05667 [cs]},
  primaryClass = {cs}
}

@article{marcus20_NextDecadeAI,
  title = {The {{Next Decade}} in {{AI}}: {{Four Steps Towards Robust Artificial Intelligence}}},
  shorttitle = {The {{Next Decade}} in {{AI}}},
  author = {Marcus, Gary},
  year = {2020},
  month = feb,
  abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
  archivePrefix = {arXiv},
  eprint = {2002.06177},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/marcus_2020_the next decade in ai.pdf},
  journal = {arXiv:2002.06177 [cs]},
  primaryClass = {cs}
}

@article{marino18_IterativeAmortizedInference,
  title = {Iterative {{Amortized Inference}}},
  author = {Marino, Joseph and Yue, Yisong and Mandt, Stephan},
  year = {2018},
  month = jul,
  abstract = {Inference models are a key component in scaling variational inference to deep latent variable models, most notably as encoder networks in variational auto-encoders (VAEs). By replacing conventional optimization-based inference with a learned model, inference is amortized over data examples and therefore more computationally efficient. However, standard inference models are restricted to direct mappings from data to approximate posterior estimates. The failure of these models to reach fully optimized approximate posterior estimates results in an amortization gap. We aim toward closing this gap by proposing iterative inference models, which learn to perform inference optimization through repeatedly encoding gradients. Our approach generalizes standard inference models in VAEs and provides insight into several empirical findings, including top-down inference techniques. We demonstrate the inference optimization capabilities of iterative inference models and show that they outperform standard inference models on several benchmark data sets of images and text.},
  archivePrefix = {arXiv},
  eprint = {1807.09356},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/marino et al_2018_iterative amortized inference.pdf},
  journal = {arXiv:1807.09356 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{marino20_Improvingsequentiallatent,
  title = {Improving Sequential Latent Variable Models with Autoregressive Flows},
  author = {Marino, Joseph and Chen, Lei and He, Jiawei and Mandt, Stephan},
  editor = {Zhang, Cheng and Ruiz, Francisco and Bui, Thang and Dieng, Adji Bousso and Liang, Dawen},
  year = {2020},
  month = dec,
  volume = {118},
  pages = {1--16},
  publisher = {{PMLR}},
  abstract = {We propose an approach for sequence modeling based on autoregressive normalizing ows. Each autoregressive transform, acting across time, serves as a moving reference frame for modeling higher-level dynamics. This technique provides a simple, general-purpose method for improving sequence modeling, with connections to existing and classical techniques. We demonstrate the proposed approach both with standalone models, as well as a part of larger sequential latent variable models. Results are presented on three benchmark video datasets, where ow-based dynamics improve log-likelihood performance over baseline models.},
  file = {/home/trung/GoogleDrive/Zotero/marino et al_2020_improving sequential latent variable models with autoregressive flows.pdf},
  pdf = {http://proceedings.mlr.press/v118/marino20a/marino20a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{martens20_NeuralDecompositionFunctional,
  title = {Neural {{Decomposition}}: {{Functional ANOVA}} with {{Variational Autoencoders}}},
  author = {M{\"a}rtens, Kaspar and Yau, Christopher},
  year = {2020},
  pages = {10},
  abstract = {Variational Autoencoders (VAEs) have become a popular approach for dimensionality reduction. However, despite their ability to identify latent low-dimensional structures embedded within high-dimensional data, these latent representations are typically hard to interpret on their own. Due to the black-box nature of VAEs, their utility for healthcare and genomics applications has been limited. In this paper, we focus on characterising the sources of variation in Conditional VAEs. Our goal is to provide a feature-level variance decomposition, i.e. to decompose variation in the data by separating out the marginal additive effects of latent variables z and fixed inputs c from their non-linear interactions. We propose to achieve this through what we call Neural Decomposition \textendash{} an adaptation of the well-known concept of functional ANOVA variance decomposition from classical statistics to deep learning models. We show how identifiability can be achieved by training models subject to constraints on the marginal properties of the decoder networks. We demonstrate the utility of our Neural Decomposition on a series of synthetic examples as well as high-dimensional genomics data.},
  file = {/home/trung/GoogleDrive/Zotero/märtens et al_2020_neural decomposition.pdf},
  keywords = {disentanglement},
  language = {en}
}

@book{martin18_BayesianAnalysisPython,
  title = {Bayesian {{Analysis}} with {{Python}} - {{Second Edition}}},
  author = {Martin, Osvaldo},
  year = {2018},
  annotation = {OCLC: 1097676964},
  file = {/home/trung/GoogleDrive/Zotero/martin_2018_bayesian analysis with python - second edition.pdf},
  isbn = {978-1-78934-165-2},
  language = {en}
}

@article{martin18_ImplicitSelfRegularizationDeep,
  title = {Implicit {{Self}}-{{Regularization}} in {{Deep Neural Networks}}: {{Evidence}} from {{Random Matrix Theory}} and {{Implications}} for {{Learning}}},
  shorttitle = {Implicit {{Self}}-{{Regularization}} in {{Deep Neural Networks}}},
  author = {Martin, Charles H. and Mahoney, Michael W.},
  year = {2018},
  month = oct,
  abstract = {Random Matrix Theory (RMT) is applied to analyze the weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniatureAlexNet. Empirical and theoretical results clearly indicate that the DNN training process itself implicitly implements a form of Self-Regularization, implicitly sculpting a more regularized energy or penalty landscape. In particular, the empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization, such as Dropout or Weight Norm constraints. Building on relatively recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, and applying them to these empirical results, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. These phases can be observed during the training process as well as in the final learned DNNs. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a ``size scale'' separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems (such as classical models of actual neural activity). This results from correlations arising at all size scales, which for DNNs arises implicitly due to the training process itself. This implicit Self-Regularization can depend strongly on the many knobs of the training process. In particular, by exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size. This demonstrates that\textemdash all else being equal\textemdash DNN optimization with larger batch sizes leads to less-well implicitly-regularized models, and it provides an explanation for the generalization gap phenomena. Our results suggest that large, welltrained DNN architectures should exhibit Heavy-Tailed Self-Regularization, and we discuss the theoretical and practical implications of this.},
  annotation = {ZSCC: 0000019},
  archivePrefix = {arXiv},
  eprint = {1810.01075},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/martin et al_2018_implicit self-regularization in deep neural networks.pdf},
  journal = {arXiv:1810.01075 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{martin20_ComputingBayesBayesian,
  title = {Computing {{Bayes}}: {{Bayesian Computation}} from 1763 to the 21st {{Century}}},
  shorttitle = {Computing {{Bayes}}},
  author = {Martin, Gael M. and Frazier, David T. and Robert, Christian P.},
  year = {2020},
  month = apr,
  abstract = {The Bayesian statistical paradigm uses the language of probability to express uncertainty about the phenomena that generate observed data. Probability distributions thus characterize Bayesian inference, with the rules of probability used to transform prior probability distributions for all unknowns - models, parameters, latent variables - into posterior distributions, subsequent to the observation of data. Conducting Bayesian inference requires the evaluation of integrals in which these probability distributions appear. Bayesian computation is all about evaluating such integrals in the typical case where no analytical solution exists. This paper takes the reader on a chronological tour of Bayesian computation over the past two and a half centuries. Beginning with the one-dimensional integral first confronted by Bayes in 1763, through to recent problems in which the unknowns number in the millions, we place all computational problems into a common framework, and describe all computational methods using a common notation. The aim is to help new researchers in particular - and more genrally those interested in adopting a Bayesian approach to empirical work - \{make sense of the plethora of computational techniques that are now on offer; understand when and why different methods are useful; and see the links that do exist, between them all.\vphantom\}},
  archivePrefix = {arXiv},
  eprint = {2004.06425},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/martin et al_2020_computing bayes.pdf},
  journal = {arXiv:2004.06425 [stat]},
  keywords = {favorite},
  primaryClass = {stat}
}

@article{massague20_LearningDisentangledRepresentations,
  title = {Learning {{Disentangled Representations}} of {{Video}} with {{Missing Data}}},
  author = {Massague, Armand Comas and Zhang, Chi and Feric, Zlatan and Camps, Octavia and Yu, Rose},
  year = {2020},
  month = jun,
  abstract = {Missing data poses significant challenges while learning representations of video sequences. We present Disentangled Imputed Video autoEncoder (DIVE), a deep generative model that imputes and predicts future video frames in the presence of missing data. Specifically, DIVE introduces a missingness latent variable, disentangles the hidden video representations into static and dynamic appearance, pose, and missingness factors for each object. DIVE imputes each object's trajectory where the data is missing. On a moving MNIST dataset with various missing scenarios, DIVE outperforms the state of the art baselines by a substantial margin. We also present comparisons on a real-world MOTSChallenge pedestrian dataset, which demonstrates the practical value of our method in a more realistic setting.},
  archivePrefix = {arXiv},
  eprint = {2006.13391},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/massague et al_2020_learning disentangled representations of video with missing data.pdf},
  journal = {arXiv:2006.13391 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{massaia18_SingleCellGene,
  title = {Single {{Cell Gene Expression}} to {{Understand}} the {{Dynamic Architecture}} of the {{Heart}}},
  author = {Massaia, Andrea and Chaves, Patricia and Samari, Sara and Miragaia, Ricardo J{\'u}dice and Meyer, Kerstin and Teichmann, Sarah Amalia and Noseda, Michela},
  year = {2018},
  month = nov,
  volume = {5},
  issn = {2297-055X},
  doi = {10.3389/fcvm.2018.00167},
  abstract = {The recent development of single cell gene expression technologies, and especially single cell transcriptomics, have revolutionized the way biologists and clinicians investigate organs and organisms, allowing an unprecedented level of resolution to the description of cell demographics in both healthy and diseased states. Single cell transcriptomics provide information on prevalence, heterogeneity, and gene co-expression at the individual cell level. This enables a cell-centric outlook to define intracellular gene regulatory networks and to bridge toward the definition of intercellular pathways otherwise masked in bulk analysis. The technologies have developed at a fast pace producing a multitude of different approaches, with several alternatives to choose from at any step, including single cell isolation and capturing, lysis, RNA reverse transcription and cDNA amplification, library preparation, sequencing, and computational analyses. Here, we provide guidelines for the experimental design of single cell RNA sequencing experiments, exploring the current options for the crucial steps. Furthermore, we provide a complete overview of the typical data analysis workflow, from handling the raw sequencing data to making biological inferences. Significantly, advancements in single cell transcriptomics have already contributed to outstanding exploratory and functional studies of cardiac development and disease models, as summarized in this review. In conclusion, we discuss achievable outcomes of single cell transcriptomics' applications in addressing unanswered questions and influencing future cardiac clinical applications.},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/GoogleDrive/Zotero/massaia et al_2018_single cell gene expression to understand the dynamic architecture of the heart.pdf},
  journal = {Frontiers in Cardiovascular Medicine},
  pmcid = {PMC6258739},
  pmid = {30525044}
}

@article{mastakouri20_CausalanalysisCovid19,
  title = {Causal Analysis of {{Covid}}-19 Spread in {{Germany}}},
  author = {Mastakouri, Atalanti A. and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = aug,
  abstract = {In this work, we study the causal relations among German regions in terms of the spread of Covid-19 since the beginning of the pandemic, taking into account the restriction policies that were applied by the different federal states. We propose and prove a new theorem for a causal feature selection method for time series data, robust to latent confounders, which we subsequently apply on Covid-19 case numbers. We present findings about the spread of the virus in Germany and the causal impact of restriction measures, discussing the role of various policies in containing the spread. Since our results are based on rather limited target time series (only the numbers of reported cases), care should be exercised in interpreting them. However, it is encouraging that already such limited data seems to contain causal signals. This suggests that as more data becomes available, our causal approach may contribute towards meaningful causal analysis of political interventions on the development of Covid-19, and thus also towards the development of rational and data-driven methodologies for choosing interventions.},
  archivePrefix = {arXiv},
  eprint = {2007.11896},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mastakouri et al_2020_causal analysis of covid-19 spread in germany.pdf},
  journal = {arXiv:2007.11896 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{mateen20_Improvingqualitymachine,
  title = {Improving the Quality of Machine Learning in Health Applications and Clinical Research},
  author = {Mateen, Bilal A. and Liley, James and Denniston, Alastair K. and Holmes, Chris C. and Vollmer, Sebastian J.},
  year = {2020},
  month = oct,
  issn = {2522-5839},
  doi = {10.1038/s42256-020-00239-1},
  abstract = {For machine learning developers, the use of prediction tools in real-world clinical settings can be a distant goal. Recently published guidelines for reporting clinical research that involves machine learning will help connect clinical and computer science communities, and realize the full potential of machine learning tools.},
  file = {/home/trung/GoogleDrive/Zotero/mateen et al_2020_improving the quality of machine learning in health applications and clinical research.pdf},
  journal = {Nature Machine Intelligence}
}

@article{mathieu16_Disentanglingfactorsvariation,
  title = {Disentangling Factors of Variation in Deep Representations Using Adversarial Training},
  author = {Mathieu, Michael and Zhao, Junbo and Sprechmann, Pablo and Ramesh, Aditya and LeCun, Yann},
  year = {2016},
  month = nov,
  abstract = {We introduce a conditional generative model for learning to disentangle the hidden factors of variation within a set of labeled observations, and separate them into complementary codes. One code summarizes the specified factors of variation associated with the labels. The other summarizes the remaining unspecified variability. During training, the only available source of supervision comes from our ability to distinguish among different observations belonging to the same class. Examples of such observations include images of a set of labeled objects captured at different viewpoints, or recordings of set of speakers dictating multiple phrases. In both instances, the intra-class diversity is the source of the unspecified factors of variation: each object is observed at multiple viewpoints, and each speaker dictates multiple phrases. Learning to disentangle the specified factors from the unspecified ones becomes easier when strong supervision is possible. Suppose that during training, we have access to pairs of images, where each pair shows two different objects captured from the same viewpoint. This source of alignment allows us to solve our task using existing methods. However, labels for the unspecified factors are usually unavailable in realistic scenarios where data acquisition is not strictly controlled. We address the problem of disentanglement in this more general setting by combining deep convolutional autoencoders with a form of adversarial training. Both factors of variation are implicitly captured in the organization of the learned embedding space, and can be used for solving single-image analogies. Experimental results on synthetic and real datasets show that the proposed method is capable of generalizing to unseen classes and intra-class variabilities.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1611.03383},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mathieu et al_2016_disentangling factors of variation in deep representations using adversarial training.pdf;/home/trung/Zotero/storage/M82G5KKF/1611.html},
  journal = {arXiv:1611.03383 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@inproceedings{mathieu19_Continuoushierarchicalrepresentations,
  title = {Continuous Hierarchical Representations with Poincar\'e Variational Auto-Encoders},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Mathieu, Emile and Le Lan, Charline and Maddison, Chris J. and Tomioka, Ryota and Teh, Yee Whye},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  volume = {32},
  pages = {12565--12576},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/mathieu et al_2019_continuous hierarchical representations with poincaré variational auto-encoders.pdf},
  keywords = {_tablet,favorite}
}

@article{mathieu20_RiemannianContinuousNormalizing,
  title = {Riemannian {{Continuous Normalizing Flows}}},
  author = {Mathieu, Emile and Nickel, Maximilian},
  year = {2020},
  month = jun,
  abstract = {Normalizing flows have shown great promise for modelling flexible probability distributions in a computationally tractable way. However, whilst data is often naturally described on Riemannian manifolds such as spheres, torii, and hyperbolic spaces, most normalizing flows implicitly assume a flat geometry, making them either misspecified or ill-suited in these situations. To overcome this problem, we introduce Riemannian continuous normalizing flows, a model which admits the parametrization of flexible probability measures on smooth manifolds by defining flows as the solution to ordinary differential equations. We show that this approach can lead to substantial improvements on both synthetic and real-world data when compared to standard flows or previously introduced projected flows.},
  archivePrefix = {arXiv},
  eprint = {2006.10605},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mathieu et al_2020_riemannian continuous normalizing flows.pdf},
  journal = {arXiv:2006.10605 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{mathis20_PrimerMotionCapture,
  title = {A {{Primer}} on {{Motion Capture}} with {{Deep Learning}}: {{Principles}}, {{Pitfalls}} and {{Perspectives}}},
  shorttitle = {A {{Primer}} on {{Motion Capture}} with {{Deep Learning}}},
  author = {Mathis, Alexander and Schneider, Steffen and Lauer, Jessy and Mathis, Mackenzie W.},
  year = {2020},
  month = sep,
  abstract = {Extracting behavioral measurements non-invasively from video is stymied by the fact that it is a hard computational problem. Recent advances in deep learning have tremendously advanced predicting posture from videos directly, which quickly impacted neuroscience and biology more broadly. In this primer we review the budding field of motion capture with deep learning. In particular, we will discuss the principles of those novel algorithms, highlight their potential as well as pitfalls for experimentalists, and provide a glimpse into the future.},
  archivePrefix = {arXiv},
  eprint = {2009.00564},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mathis et al_2020_a primer on motion capture with deep learning.pdf},
  journal = {arXiv:2009.00564 [cs, q-bio]},
  primaryClass = {cs, q-bio}
}

@article{mattei18_LeveragingExactLikelihood,
  title = {Leveraging the {{Exact Likelihood}} of {{Deep Latent Variable Models}}},
  author = {Mattei, Pierre-Alexandre and Frellsen, Jes},
  year = {2018},
  month = jun,
  abstract = {Deep latent variable models (DLVMs) combine the approximation abilities of deep neural networks and the statistical foundations of generative models. Variational methods are commonly used for inference; however, the exact likelihood of these models has been largely overlooked. The purpose of this work is to study the general properties of this quantity and to show how they can be leveraged in practice. We focus on important inferential problems that rely on the likelihood: estimation and missing data imputation. First, we investigate maximum likelihood estimation for DLVMs: in particular, we show that most unconstrained models used for continuous data have an unbounded likelihood function. This problematic behaviour is demonstrated to be a source of mode collapse. We also show how to ensure the existence of maximum likelihood estimates, and draw useful connections with nonparametric mixture models. Finally, we describe an algorithm for missing data imputation using the exact conditional likelihood of a deep latent variable model. On several data sets, our algorithm consistently and significantly outperforms the usual imputation scheme used for DLVMs.},
  archivePrefix = {arXiv},
  eprint = {1802.04826},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mattei et al_2018_leveraging the exact likelihood of deep latent variable models.pdf},
  journal = {arXiv:1802.04826 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@article{matthews18_GaussianProcessBehaviour,
  title = {Gaussian {{Process Behaviour}} in {{Wide Deep Neural Networks}}},
  author = {Matthews, Alexander G. de G. and Rowland, Mark and Hron, Jiri and Turner, Richard E. and Ghahramani, Zoubin},
  year = {2018},
  month = apr,
  abstract = {Whilst deep neural networks have shown great empirical success, there is still much work to be done to understand their theoretical properties. In this paper, we study the relationship between random, wide, fully connected, feedforward networks with more than one hidden layer and Gaussian processes with a recursive kernel definition. We show that, under broad conditions, as we make the architecture increasingly wide, the implied random function converges in distribution to a Gaussian process, formalising and extending existing results by Neal (1996) to deep networks. To evaluate convergence rates empirically, we use maximum mean discrepancy. We then compare finite Bayesian deep networks from the literature to Gaussian processes in terms of the key predictive quantities of interest, finding that in some cases the agreement can be very close. We discuss the desirability of Gaussian process behaviour and review non-Gaussian alternative models from the literature.},
  archivePrefix = {arXiv},
  eprint = {1804.11271},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/matthews et al_2018_gaussian process behaviour in wide deep neural networks.pdf;/home/trung/Zotero/storage/EIQSL2SB/1804.html},
  journal = {arXiv:1804.11271 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{matusch20_EvaluatingAgentsRewards,
  title = {Evaluating {{Agents}} without {{Rewards}}},
  author = {Matusch, Brendon and Ba, Jimmy and Hafner, Danijar},
  year = {2020},
  month = dec,
  abstract = {Reinforcement learning has enabled agents to solve challenging tasks in unknown environments. However, manually crafting reward functions can be time consuming, expensive, and error prone to human error. Competing objectives have been proposed for agents to learn without external supervision, but it has been unclear how well they reflect task rewards or human behavior. To accelerate the development of intrinsic objectives, we retrospectively compute potential objectives on pre-collected datasets of agent behavior, rather than optimizing them online, and compare them by analyzing their correlations. We study input entropy, information gain, and empowerment across seven agents, three Atari games, and the 3D game Minecraft. We find that all three intrinsic objectives correlate more strongly with a human behavior similarity metric than with task reward. Moreover, input entropy and information gain correlate more strongly with human similarity than task reward does, suggesting the use of intrinsic objectives for designing agents that behave similarly to human players.},
  archivePrefix = {arXiv},
  eprint = {2012.11538},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/matusch et al_2020_evaluating agents without rewards.pdf},
  journal = {arXiv:2012.11538 [cs]},
  primaryClass = {cs}
}

@misc{maunz00_GreatHanoiRat,
  title = {The {{Great Hanoi Rat Massacre}} of 1902 {{Did Not Go}} as {{Planned}}},
  author = {Maunz, Shay},
  year = {12:56:00 -0400},
  abstract = {Instead of disappearing, the pesky rodents proliferated.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/MNVBLWJV/hanoi-rat-massacre-1902.html},
  howpublished = {http://www.atlasobscura.com/articles/hanoi-rat-massacre-1902},
  journal = {Atlas Obscura},
  language = {en}
}

@article{mavromatis20_GraphInfoClustLeveraging,
  title = {Graph {{InfoClust}}: {{Leveraging}} Cluster-Level Node Information for Unsupervised Graph Representation Learning},
  shorttitle = {Graph {{InfoClust}}},
  author = {Mavromatis, Costas and Karypis, George},
  year = {2020},
  month = sep,
  abstract = {Unsupervised (or self-supervised) graph representation learning is essential to facilitate various graph data mining tasks when external supervision is unavailable. The challenge is to encode the information about the graph structure and the attributes associated with the nodes and edges into a low dimensional space. Most existing unsupervised methods promote similar representations across nodes that are topologically close. Recently, it was shown that leveraging additional graph-level information, e.g., information that is shared among all nodes, encourages the representations to be mindful of the global properties of the graph, which greatly improves their quality. However, in most graphs, there is significantly more structure that can be captured, e.g., nodes tend to belong to (multiple) clusters that represent structurally similar nodes. Motivated by this observation, we propose a graph representation learning method called Graph InfoClust (GIC), that seeks to additionally capture cluster-level information content. These clusters are computed by a differentiable K-means method and are jointly optimized by maximizing the mutual information between nodes of the same clusters. This optimization leads the node representations to capture richer information and nodal interactions, which improves their quality. Experiments show that GIC outperforms state-of-art methods in various downstream tasks (node classification, link prediction, and node clustering) with a 0.9\% to 6.1\% gain over the best competing approach, on average.},
  archivePrefix = {arXiv},
  eprint = {2009.06946},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mavromatis et al_2020_graph infoclust.pdf},
  journal = {arXiv:2009.06946 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{mazarura20_GammaPoissonMixtureTopic,
  title = {A {{Gamma}}-{{Poisson Mixture Topic Model}} for {{Short Text}}},
  author = {Mazarura, Jocelyn and {de Waal}, Alta and {de Villiers}, Pieter},
  year = {2020},
  month = apr,
  abstract = {Most topic models are constructed under the assumption that documents follow a multinomial distribution. The Poisson distribution is an alternative distribution to describe the probability of count data. For topic modelling, the Poisson distribution describes the number of occurrences of a word in documents of fixed length. The Poisson distribution has been successfully applied in text classification, but its application to topic modelling is not well documented, specifically in the context of a generative probabilistic model. Furthermore, the few Poisson topic models in literature are admixture models, making the assumption that a document is generated from a mixture of topics. In this study, we focus on short text. Many studies have shown that the simpler assumption of a mixture model fits short text better. With mixture models, as opposed to admixture models, the generative assumption is that a document is generated from a single topic. One topic model, which makes this one-topic-per-document assumption, is the Dirichlet-multinomial mixture model. The main contributions of this work are a new Gamma-Poisson mixture model, as well as a collapsed Gibbs sampler for the model. The benefit of the collapsed Gibbs sampler derivation is that the model is able to automatically select the number of topics contained in the corpus. The results show that the Gamma-Poisson mixture model performs better than the Dirichlet-multinomial mixture model at selecting the number of topics in labelled corpora. Furthermore, the Gamma-Poisson mixture produces better topic coherence scores than the Dirichlet-multinomial mixture model, thus making it a viable option for the challenging task of topic modelling of short text.},
  archivePrefix = {arXiv},
  eprint = {2004.11464},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mazarura et al_2020_a gamma-poisson mixture topic model for short text.pdf},
  journal = {arXiv:2004.11464 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{mazyavkina20_ReinforcementLearningCombinatorial,
  title = {Reinforcement {{Learning}} for {{Combinatorial Optimization}}: {{A Survey}}},
  shorttitle = {Reinforcement {{Learning}} for {{Combinatorial Optimization}}},
  author = {Mazyavkina, Nina and Sviridov, Sergey and Ivanov, Sergei and Burnaev, Evgeny},
  year = {2020},
  month = aug,
  abstract = {Combinatorial optimization (CO) is the workhorse of numerous important applications in operations research, engineering, and other fields and, thus, has been attracting enormous attention from the research community recently. Some efficient approaches to common problems involve using hand-crafted heuristics to sequentially construct a solution. Therefore, it is intriguing to see how a CO problem can be reformulated as a sequential decision-making process, and whether these heuristics can be implicitly learned by a reinforcement learning (RL) agent. This survey explores the synergy between the CO and RL frameworks, which can become a promising direction for solving combinatorial problems.},
  archivePrefix = {arXiv},
  eprint = {2003.03600},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mazyavkina et al_2020_reinforcement learning for combinatorial optimization.pdf},
  journal = {arXiv:2003.03600 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@incollection{mcauliffe08_Supervisedtopicmodels,
  title = {Supervised Topic Models},
  booktitle = {Advances in Neural Information Processing Systems 20},
  author = {Mcauliffe, Jon D. and Blei, David M.},
  editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
  year = {2008},
  pages = {121--128},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/mcauliffe et al_2008_supervised topic models.pdf}
}

@inproceedings{mccarthy20_Addressingposteriorcollapse,
  title = {Addressing Posterior Collapse with Mutual Information for Improved Variational Neural Machine Translation},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {McCarthy, Arya D. and Li, Xian and Gu, Jiatao and Dong, Ning},
  year = {2020},
  month = jul,
  pages = {8512--8525},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.753},
  abstract = {This paper proposes a simple and effective approach to address the problem of posterior collapse in conditional variational autoencoders (CVAEs). It thus improves performance of machine translation models that use noisy or monolingual data, as well as in conventional settings. Extending Transformer and conditional VAEs, our proposed latent variable model measurably prevents posterior collapse by (1) using a modified evidence lower bound (ELBO) objective which promotes mutual information between the latent variable and the target, and (2) guiding the latent variable with an auxiliary bag-of-words prediction task. As a result, the proposed model yields improved translation quality compared to existing variational NMT models on WMT Ro{$\leftrightarrow$}En and De{$\leftrightarrow$}En. With latent variables being effectively utilized, our model demonstrates improved robustness over non-latent Transformer in handling uncertainty: exploiting noisy source-side monolingual data (up to +3.2 BLEU), and training with weakly aligned web-mined parallel data (up to +4.7 BLEU).},
  file = {/home/trung/GoogleDrive/Zotero/mccarthy et al_2020_addressing posterior collapse with mutual information for improved variational neural machine translation.pdf},
  keywords = {vae_issues}
}

@article{mcclure17_RepresentationUncertaintyDeep,
  title = {Representation of {{Uncertainty}} in {{Deep Neural Networks}} through {{Sampling}}},
  author = {McClure, Patrick and Kriegeskorte, Nikolaus},
  year = {2017},
  pages = {9},
  abstract = {As deep neural networks (DNNs) are applied to increasingly challenging problems, they will need to be able to represent their own uncertainty. Modeling uncertainty is one of the key features of Bayesian methods. Scalable Bayesian DNNs that use dropout-based variational distributions have recently been proposed. Here we evaluate the ability of Bayesian DNNs trained with Bernoulli or Gaussian distributions over units (dropout) or weights (dropconnect) to represent their own uncertainty at the time of inference through sampling. We tested how well Bayesian fully connected and convolutional DNNs represented their own uncertainty in classifying the MNIST handwritten digits. By adding different levels of Gaussian noise to the test images, we assessed how DNNs represented their uncertainty about regions of input space not covered by the training set. Bayesian DNNs estimated their own uncertainty more accurately than traditional DNNs with a softmax output. These results are important for building better deep learning systems and for investigating the hypothesis that biological neural networks use sampling to represent uncertainty.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/3PKAMJMB/McClure and Kriegeskorte - 2017 - REPRESENTATION OF UNCERTAINTY IN DEEP NEURAL NETWO.pdf},
  language = {en}
}

@inproceedings{mccoy19_RightWrongReasons,
  title = {Right for the {{Wrong Reasons}}: {{Diagnosing Syntactic Heuristics}} in {{Natural Language Inference}}},
  shorttitle = {Right for the {{Wrong Reasons}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {McCoy, Tom and Pavlick, Ellie and Linzen, Tal},
  year = {2019},
  month = jul,
  pages = {3428--3448},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1334},
  abstract = {A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.},
  annotation = {ZSCC: 0000035},
  file = {/home/trung/GoogleDrive/Zotero/mccoy et al_2019_right for the wrong reasons.pdf}
}

@article{mcdermott76_Artificialintelligencemeets,
  title = {Artificial Intelligence Meets Natural Stupidity},
  author = {McDermott, Drew},
  year = {1976},
  month = apr,
  pages = {4--9},
  issn = {01635719},
  doi = {10.1145/1045339.1045340},
  annotation = {ZSCC: 0000417},
  file = {/home/trung/GoogleDrive/Zotero/mcdermott_1976_artificial intelligence meets natural stupidity.pdf},
  journal = {ACM SIGART Bulletin},
  language = {en},
  number = {57}
}

@article{mcinnes18_UMAPUniformManifold,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2018},
  month = dec,
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  annotation = {ZSCC: 0000527},
  archivePrefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mcinnes et al_2018_umap.pdf;/home/trung/Zotero/storage/LMQ2C26E/1802.html},
  journal = {arXiv:1802.03426 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{mcinnes20_UMAPUniformManifold,
  title = {{{UMAP}}: {{Uniform Manifold Approximation}} and {{Projection}} for {{Dimension Reduction}}},
  shorttitle = {{{UMAP}}},
  author = {McInnes, Leland and Healy, John and Melville, James},
  year = {2020},
  month = sep,
  abstract = {UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.},
  archivePrefix = {arXiv},
  eprint = {1802.03426},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mcinnes et al_2020_umap.pdf},
  journal = {arXiv:1802.03426 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{md16_latestglucosaminechondroitin,
  title = {The Latest on Glucosamine/Chondroitin Supplements},
  author = {MD, Robert H. Shmerling},
  year = {2016},
  month = oct,
  abstract = {Millions of Americans take glucosamine, chondroitin, or both. While considered safe, there is no proof they protect joints or prevent joint pain.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/9VEFANIG/the-latest-on-glucosaminechondroitin-supplements-2016101710391.html},
  howpublished = {https://www.health.harvard.edu/blog/the-latest-on-glucosaminechondroitin-supplements-2016101710391},
  journal = {Harvard Health Blog},
  language = {en-US}
}

@article{mealer20_fledNewYork,
  title = {I Fled {{New York}} with My Wife, Kids and Dog \textendash{} Just as My Ancestors Fled the 1918 Pandemic},
  author = {Mealer, Bryan},
  year = {2020},
  month = apr,
  issn = {0261-3077},
  abstract = {The 1918 influenza pandemic killed my great-grandmother and her daughter. I thought this was merely tragic until I found myself leaving on the heels of another plague, a week before lockdown},
  annotation = {ZSCC: NoCitationData[s0]},
  chapter = {World news},
  file = {/home/trung/Zotero/storage/MPQCXJFX/coronavirus-1918-influenza-epidemic-fleeing-new-york.html},
  journal = {The Guardian},
  language = {en-GB}
}

@article{meghanathan00_9thInternationalConference,
  title = {9th {{International Conference}} on {{Computer Science}}, {{Engineering}} and {{Applications}} ({{CCSEA}} 2019)},
  author = {Meghanathan, Natarajan and Nagamalai, Dhinaharan},
  pages = {402},
  file = {/home/trung/GoogleDrive/Zotero/meghanathan et al_9th international conference on computer science, engineering and applications (ccsea 2019).pdf},
  language = {en}
}

@article{mehtonen19_Datadrivencharacterizationmolecular,
  title = {Data-Driven Characterization of Molecular Phenotypes across Heterogeneous Sample Collections},
  author = {Mehtonen, Juha and P{\"o}l{\"o}nen, Petri and H{\"a}yrynen, Sergei and Dufva, Olli and Lin, Jake and Liuksiala, Thomas and Granberg, Kirsi and Lohi, Olli and Hautam{\"a}ki, Ville and Nykter, Matti and Hein{\"a}niemi, Merja},
  year = {2019},
  month = jul,
  volume = {47},
  pages = {e76-e76},
  issn = {0305-1048, 1362-4962},
  doi = {10.1093/nar/gkz281},
  abstract = {Abstract             Existing large gene expression data repositories hold enormous potential to elucidate disease mechanisms, characterize changes in cellular pathways, and to stratify patients based on molecular profiles. To achieve this goal, integrative resources and tools are needed that allow comparison of results across datasets and data types. We propose an intuitive approach for data-driven stratifications of molecular profiles and benchmark our methodology using the dimensionality reduction algorithm t-distributed stochastic neighbor embedding (t-SNE) with multi-study and multi-platform data on hematological malignancies. Our approach enables assessing the contribution of biological versus technical variation to sample clustering, direct incorporation of additional datasets to the same low dimensional representation, comparison of molecular disease subtypes identified from separate t-SNE representations, and characterization of the obtained clusters based on pathway databases and additional data. In this manner, we performed an integrative analysis across multi-omics acute myeloid leukemia studies. Our approach indicated new molecular subtypes with differential survival and drug responsiveness among samples lacking fusion genes, including a novel myelodysplastic syndrome-like cluster and a cluster characterized with CEBPA mutations and differential activity of the S-adenosylmethionine-dependent DNA methylation pathway. In summary, integration across multiple studies can help to identify novel molecular disease subtypes and generate insight into disease biology.},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/Zotero/storage/ESW5IRFB/Mehtonen et al. - 2019 - Data-driven characterization of molecular phenotyp.pdf},
  journal = {Nucleic Acids Research},
  language = {en},
  number = {13}
}

@article{merity16_PointerSentinelMixture,
  title = {Pointer {{Sentinel Mixture Models}}},
  author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  year = {2016},
  month = sep,
  abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
  archivePrefix = {arXiv},
  eprint = {1609.07843},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/merity et al_2016_pointer sentinel mixture models.pdf;/home/trung/Zotero/storage/RWSEFWPY/1609.html},
  journal = {arXiv:1609.07843 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,mixture,pointer,sequential},
  primaryClass = {cs}
}

@article{merity19_SingleHeadedAttention,
  title = {Single {{Headed Attention RNN}}: {{Stop Thinking With Your Head}}},
  shorttitle = {Single {{Headed Attention RNN}}},
  author = {Merity, Stephen},
  year = {2019},
  month = nov,
  abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. We also achieve state-of-the-art on WikiText-103 - or do we? This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts and requires minimal computation. Take that Sesame Street.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.11423},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/merity_2019_single headed attention rnn.pdf;/home/trung/Zotero/storage/39X4D4MH/1911.html},
  journal = {arXiv:1911.11423 [cs]},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,lstm,rnn,transformer},
  primaryClass = {cs}
}

@article{mescheder18_AdversarialVariationalBayes,
  title = {Adversarial {{Variational Bayes}}: {{Unifying Variational Autoencoders}} and {{Generative Adversarial Networks}}},
  shorttitle = {Adversarial {{Variational Bayes}}},
  author = {Mescheder, Lars and Nowozin, Sebastian and Geiger, Andreas},
  year = {2018},
  month = jun,
  abstract = {Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihoodproblem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1701.04722},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mescheder et al_2018_adversarial variational bayes.pdf},
  journal = {arXiv:1701.04722 [cs]},
  language = {en},
  primaryClass = {cs}
}

@misc{mese19_IfYouLearn,
  title = {If {{You Learn}} to {{Write}}, {{You Can Change Your Life}}.},
  author = {Mese, Ali},
  year = {2019},
  month = oct,
  abstract = {The secret is in the line.},
  file = {/home/trung/Zotero/storage/UEXQT4D3/if-you-learn-to-write-you-can-change-your-life-d0df747e02c8.html},
  howpublished = {https://medium.com/swlh/if-you-learn-to-write-you-can-change-your-life-d0df747e02c8},
  journal = {Medium},
  language = {en}
}

@article{metz19_MetaLearningUpdateRules,
  title = {Meta-{{Learning Update Rules}} for {{Unsupervised Representation Learning}}},
  author = {Metz, Luke and Maheswaranathan, Niru and Cheung, Brian and {Sohl-Dickstein}, Jascha},
  year = {2019},
  month = feb,
  abstract = {A major goal of unsupervised learning is to discover data representations that are useful for subsequent tasks, without access to supervised labels during training. Typically, this involves minimizing a surrogate objective, such as the negative log likelihood of a generative model, with the hope that representations useful for subsequent tasks will arise as a side effect. In this work, we propose instead to directly target later desired tasks by meta-learning an unsupervised learning rule which leads to representations useful for those tasks. Specifically, we target semi-supervised classification performance, and we meta-learn an algorithm -- an unsupervised weight update rule -- that produces representations useful for this task. Additionally, we constrain our unsupervised update rule to a be a biologically-motivated, neuron-local function, which enables it to generalize to different neural network architectures, datasets, and data modalities. We show that the meta-learned update rule produces useful features and sometimes outperforms existing unsupervised learning techniques. We further show that the meta-learned unsupervised update rule generalizes to train networks with different widths, depths, and nonlinearities. It also generalizes to train on data with randomly permuted input dimensions and even generalizes from image datasets to a text task.},
  archivePrefix = {arXiv},
  eprint = {1804.00222},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/metz et al_2019_meta-learning update rules for unsupervised representation learning.pdf;/home/trung/Zotero/storage/6U78R2L8/1804.html},
  journal = {arXiv:1804.00222 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,meta learning,Statistics - Machine Learning,unsupervised},
  primaryClass = {cs, stat}
}

@article{metz20_Tasksstabilityarchitecture,
  title = {Tasks, Stability, Architecture, and Compute: {{Training}} More Effective Learned Optimizers, and Using Them to Train Themselves},
  shorttitle = {Tasks, Stability, Architecture, and Compute},
  author = {Metz, Luke and Maheswaranathan, Niru and Freeman, C. Daniel and Poole, Ben and {Sohl-Dickstein}, Jascha},
  year = {2020},
  month = sep,
  abstract = {Much as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.},
  archivePrefix = {arXiv},
  eprint = {2009.11243},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/metz et al_2020_tasks, stability, architecture, and compute.pdf},
  journal = {arXiv:2009.11243 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{mianjy00_ImplicitBiasDropout,
  title = {On the {{Implicit Bias}} of {{Dropout}}},
  author = {Mianjy, Poorya and Arora, Raman and Vidal, Rene},
  pages = {9},
  abstract = {Algorithmic approaches endow deep learning systems with implicit bias that helps them generalize even in over-parametrized settings. In this paper, we focus on understanding such a bias induced in learning through dropout, a popular technique to avoid overfitting in deep learning. For single hidden-layer linear neural networks, we show that dropout tends to make the norm of incoming/outgoing weight vectors of all the hidden nodes equal. In addition, we provide a complete characterization of the optimization landscape induced by dropout.},
  annotation = {ZSCC: 0000016},
  file = {/home/trung/GoogleDrive/Zotero/mianjy et al_on the implicit bias of dropout.pdf},
  language = {en}
}

@article{miao16_NeuralVariationalInference,
  title = {Neural {{Variational Inference}} for {{Text Processing}}},
  author = {Miao, Yishu and Yu, Lei and Blunsom, Phil},
  year = {2016},
  month = jun,
  abstract = {Recent advances in neural variational inference have spawned a renaissance in deep latent variable models. In this paper we introduce a generic variational inference framework for generative and conditional models of text. While traditional variational methods derive an analytic approximation for the intractable distributions over latent variables, here we construct an inference network conditioned on the discrete text input to provide the variational distribution. We validate this framework on two very different text modelling applications, generative document modelling and supervised question answering. Our neural variational document model combines a continuous stochastic document representation with a bag-of-words generative model and achieves the lowest reported perplexities on two standard test corpora. The neural answer selection model employs a stochastic representation layer within an attention mechanism to extract the semantics between a question and answer pair. On two question answering benchmarks this model exceeds all previous published benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1511.06038},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/miao et al_2016_neural variational inference for text processing.pdf},
  journal = {arXiv:1511.06038 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{miao18_DiscoveringDiscreteLatent,
  title = {Discovering {{Discrete Latent Topics}} with {{Neural Variational Inference}}},
  author = {Miao, Yishu and Grefenstette, Edward and Blunsom, Phil},
  year = {2018},
  month = may,
  abstract = {Topic models have been widely explored as probabilistic generative models of documents. Traditional inference methods have sought closedform derivations for updating the models, however as the expressiveness of these models grows, so does the difficulty of performing fast and accurate inference over their parameters. This paper presents alternative neural approaches to topic modelling by providing parameterisable distributions over topics which permit training by backpropagation in the framework of neural variational inference. In addition, with the help of a stick-breaking construction, we propose a recurrent network that is able to discover a notionally unbounded number of topics, analogous to Bayesian non-parametric topic models. Experimental results on the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the effectiveness and efficiency of these neural topic models.},
  archivePrefix = {arXiv},
  eprint = {1706.00359},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/miao et al_2018_discovering discrete latent topics with neural variational inference.pdf},
  journal = {arXiv:1706.00359 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{miao19_NewTimeFrequency,
  title = {A {{New Time}}\textendash{{Frequency Attention Tensor Network}} for {{Language Identification}}},
  author = {Miao, Xiaoxiao and McLoughlin, Ian and Yan, Yonghong},
  year = {2019},
  month = oct,
  issn = {0278-081X, 1531-5878},
  doi = {10.1007/s00034-019-01286-9},
  abstract = {In this paper, we aim to improve traditional DNN x-vector language identification performance by employing wide residual networks (WRN) as a powerful feature extractor which we combine with a novel frequency attention network. Compared with conventional time attention, our method learns discriminative weights for different frequency bands to generate weighted means and standard deviations for utterance-level classification. This mechanism enables the architecture to direct attention to important frequency bands rather than important time frames, as in traditional time attention methods. Furthermore, we then introduce a cross-layer frequency attention tensor network (CLF-ATN) which exploits information from different layers to recapture frame-level language characteristics that have been dropped by aggressive frequency pooling in lower layers. This effectively restores fine-grained discriminative language details. Finally, we explore the joint fusion of frame-level and frequency-band attention in a time\textendash frequency attention network. Experimental results show that firstly, WRN can significantly outperform a traditional DNN x-vector implementation; secondly, the proposed frequency attention method is more effective than time attention; and thirdly, frequency\textendash time score fusion can yield further improvement. Finally, extensive experiments on CLF-ATN demonstrate that it is able to improve discrimination by regaining dropped fine-grained frequency information, particularly for low-dimension frequency features.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/miao et al_2019_a new time–frequency attention tensor network for language identification.pdf},
  journal = {Circuits, Systems, and Signal Processing},
  language = {en}
}

@inproceedings{miao19_NewTimeFrequencyAttention,
  title = {A {{New Time}}-{{Frequency Attention Mechanism}} for {{TDNN}} and {{CNN}}-{{LSTM}}-{{TDNN}}, with {{Application}} to {{Language Identification}}},
  booktitle = {Interspeech 2019},
  author = {Miao, Xiaoxiao and McLoughlin, Ian and Yan, Yonghong},
  year = {2019},
  month = sep,
  pages = {4080--4084},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1256},
  abstract = {In this paper, we aim to improve traditional DNN x-vector language identification (LID) performance by employing Convolutional and Long Short Term Memory-Recurrent (CLSTM) Neural Networks, as they can strengthen feature extraction and capture longer temporal dependencies. We also propose a twodimensional attention mechanism. Compared with conventional one-dimensional time attention, our method introduces a frequency attention mechanism to give different weights to different frequency bands to generate weighted means and standard deviations. This mechanism can direct attention to either time or frequency information, and can be trained or fused singly or jointly. Experimental results show firstly that CLSTM can significantly outperform a traditional DNN x-vector implementation. Secondly, the proposed frequency attention method is more effective than time attention, particularly when the number of frequency bands matches the feature size. Furthermore, frequency-time score merging is the best, whereas frequencytime feature merge only shows improvements for small frequency dimension.},
  file = {/home/trung/GoogleDrive/Zotero/miao et al_2019_a new time-frequency attention mechanism for tdnn and cnn-lstm-tdnn, with application to language identification.pdf},
  language = {en}
}

@article{michalski19_EmpiricalStudyBatch,
  title = {An {{Empirical Study}} of {{Batch Normalization}} and {{Group Normalization}} in {{Conditional Computation}}},
  author = {Michalski, Vincent and Voleti, Vikram and Kahou, Samira Ebrahimi and Ortiz, Anthony and Vincent, Pascal and Pal, Chris and Precup, Doina},
  year = {2019},
  month = jul,
  abstract = {Batch normalization has been widely used to improve optimization in deep neural networks. While the uncertainty in batch statistics can act as a regularizer, using these dataset statistics specific to the training set impairs generalization in certain tasks. Recently, alternative methods for normalizing feature activations in neural networks have been proposed. Among them, group normalization has been shown to yield similar, in some domains even superior performance to batch normalization. All these methods utilize a learned affine transformation after the normalization operation to increase representational power. Methods used in conditional computation define the parameters of these transformations as learnable functions of conditioning information. In this work, we study whether and where the conditional formulation of group normalization can improve generalization compared to conditional batch normalization. We evaluate performances on the tasks of visual question answering, few-shot learning, and conditional image generation.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.00061},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/michalski et al_2019_an empirical study of batch normalization and group normalization in conditional computation.pdf},
  journal = {arXiv:1908.00061 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{mieth19_Usingtransferlearning,
  title = {Using Transfer Learning from Prior Reference Knowledge to Improve the Clustering of Single-Cell {{RNA}}-{{Seq}} Data},
  author = {Mieth, Bettina and Hockley, James R. F. and G{\"o}rnitz, Nico and Vidovic, Marina M.-C. and M{\"u}ller, Klaus-Robert and Gutteridge, Alex and Ziemek, Daniel},
  year = {2019},
  month = dec,
  volume = {9},
  pages = {20353},
  issn = {2045-2322},
  doi = {10.1038/s41598-019-56911-z},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/mieth et al_2019_using transfer learning from prior reference knowledge to improve the clustering of single-cell rna-seq data.pdf},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@book{mihail20_MachineLearningPrimer,
  title = {A {{Machine Learning Primer}}},
  author = {Mihail, Eric},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/mihail_2020_a machine learning primer.pdf}
}

@article{miladinovic19_DisentangledStateSpace,
  title = {Disentangled {{State Space Representations}}},
  author = {Miladinovi{\'c}, {\DJ}or{\dj}e and Gondal, Muhammad Waleed and Sch{\"o}lkopf, Bernhard and Buhmann, Joachim M. and Bauer, Stefan},
  year = {2019},
  month = jun,
  abstract = {Sequential data often originates from diverse domains across which statistical regularities and domain specifics exist. To specifically learn cross-domain sequence representations, we introduce disentangled state space models (DSSM) -- a class of SSM in which domain-invariant state dynamics is explicitly disentangled from domain-specific information governing that dynamics. We analyze how such separation can improve knowledge transfer to new domains, and enable robust prediction, sequence manipulation and domain characterization. We furthermore propose an unsupervised VAE-based training procedure to implement DSSM in form of Bayesian filters. In our experiments, we applied VAE-DSSM framework to achieve competitive performance in online ODE system identification and regression across experimental settings, and controlled generation and prediction of bouncing ball video sequences across varying gravitational influences.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1906.03255},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/miladinović et al_2019_disentangled state space representations.pdf;/home/trung/Zotero/storage/VDT25H42/1906.html},
  journal = {arXiv:1906.03255 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{milgrom19_HowArtificialIntelligence,
  title = {How {{Artificial Intelligence}} and {{Machine Learning Can Impact Market Design}}},
  author = {Milgrom, Paul and Tadelis, Steven},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/milgrom et al_2019_how artificial intelligence and machine learning can impact market design.pdf}
}

@misc{milgrom81_AxiomaticCharacterizationCommon,
  title = {An {{Axiomatic Characterization}} of {{Common Knowledge}}},
  author = {Milgrom, Paul},
  year = {1981},
  file = {/home/trung/GoogleDrive/Zotero/milgrom_1981_an axiomatic characterization of common knowledge.pdf},
  howpublished = {https://web.stanford.edu/\textasciitilde milgrom/publishedarticles/An\%20Axiomatic\%20Characterization\%20of\%20Common\%20Knowledge.pdf}
}

@misc{milgrom81_GoodNewsBad,
  title = {Good {{News}} and {{Bad News}}: Representation Theorems and Applications},
  author = {Milgrom, Paul},
  year = {1981},
  file = {/home/trung/GoogleDrive/Zotero/milgrom_1981_good news and bad news.pdf},
  howpublished = {https://web.stanford.edu/\textasciitilde milgrom/publishedarticles/Good\%20News\%20and\%20Bad\%20News.pdf}
}

@article{milgrom85_RelyingInformationInterested,
  title = {Relying on {{The Information}} of {{Interested Parties}}},
  author = {Milgrom, Paul and Roberts, John},
  year = {1985},
  file = {/home/trung/GoogleDrive/Zotero/1985/false}
}

@misc{milgrom93_SympathyEconomicValue,
  title = {Is {{Sympathy An Economic Value}}? {{Philosophy}}, {{Economics}}, and the {{Contigent Valuation Method}}},
  author = {Milgrom, Paul},
  year = {1993},
  file = {/home/trung/GoogleDrive/Zotero/milgrom_1993_is sympathy an economic value.pdf},
  howpublished = {https://web.stanford.edu/\textasciitilde milgrom/publishedarticles/Is\%20Sympathy\%20An\%20Economic\%20Value.pdf}
}

@article{miller55_MagicalNumberSeven,
  title = {The {{Magical Number Seven}}, {{Plus}} or {{Minus Two Some Limits}} on {{Our Capacity}} for {{Processing Information}}},
  author = {Miller, George},
  year = {1955},
  pages = {17},
  file = {/home/trung/GoogleDrive/Zotero/miller_1955_the magical number seven, plus or minus two some limits on our capacity for processing information.pdf},
  language = {en}
}

@article{minaee20_DeepLearningBased,
  title = {Deep {{Learning Based Text Classification}}: {{A Comprehensive Review}}},
  shorttitle = {Deep {{Learning Based Text Classification}}},
  author = {Minaee, Shervin and Kalchbrenner, Nal and Cambria, Erik and Nikzad, Narjes and Chenaghlu, Meysam and Gao, Jianfeng},
  year = {2020},
  month = apr,
  abstract = {Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this work, we provide a detailed review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.},
  archivePrefix = {arXiv},
  eprint = {2004.03705},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/minaee et al_2020_deep learning based text classification.pdf},
  journal = {arXiv:2004.03705 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{mingard20_SGDBayesiansampler,
  title = {Is {{SGD}} a {{Bayesian}} Sampler? {{Well}}, Almost},
  shorttitle = {Is {{SGD}} a {{Bayesian}} Sampler?},
  author = {Mingard, Chris and {Valle-P{\'e}rez}, Guillermo and Skalse, Joar and Louis, Ard A.},
  year = {2020},
  month = jun,
  abstract = {Overparameterised deep neural networks (DNNs) are highly expressive and so can, in principle, generate almost any function that fits a training dataset with zero error. The vast majority of these functions will perform poorly on unseen data, and yet in practice DNNs often generalise remarkably well. This success suggests that a trained DNN must have a strong inductive bias towards functions with low generalisation error. Here we empirically investigate this inductive bias by calculating, for a range of architectures and datasets, the probability PSGD(f |S) that an overparameterised DNN, trained with stochastic gradient descent (SGD) or one of its variants, converges on a function f consistent with a training set S. We also use Gaussian processes to estimate the Bayesian posterior probability PB(f |S) that the DNN expresses f upon random sampling of its parameters, conditioned on S.},
  archivePrefix = {arXiv},
  eprint = {2006.15191},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mingard et al_2020_is sgd a bayesian sampler.pdf},
  journal = {arXiv:2006.15191 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{mingote19_LanguageRecognitionUsing,
  title = {Language {{Recognition Using Triplet Neural Networks}}},
  booktitle = {Interspeech 2019},
  author = {Mingote, Victoria and Castan, Diego and McLaren, Mitchell and Nandwana, Mahesh Kumar and Ortega, Alfonso and Lleida, Eduardo and Miguel, Antonio},
  year = {2019},
  month = sep,
  pages = {4025--4029},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2437},
  abstract = {In this paper, we propose a novel neural network back-end approach based on triplets for the language recognition task, due to its success application in the related field of text-dependent speaker verification. A triplet is a training example constructed of three audio samples; two from the same class and one from a different class. In presenting two pairs of samples to the network, the triplet neural network learns to discriminate between samples from the same languages and pairs of different languages. Triplet-based training optimizes the Area Under the Curve (AUC) in contrast to other triplet loss functions proposed in the literature. The optimization of the AUC as cost function is appropriate for a detection task as it directly correlates with end-use performance of the system. Moreover, we show the importance of defining an appropriate method of triplet selection and how this impacts performance of the system. When benchmarked on the LRE09 database, the new triplet backend demonstrated superior performance compared to traditional back-ends used for language recognition. In addition, we performed an evaluation on the LRE15 and LRE17 databases to check the generalization power of the proposed systems.},
  file = {/home/trung/GoogleDrive/Zotero/mingote et al_2019_language recognition using triplet neural networks.pdf},
  language = {en}
}

@article{miroddi14_SystematicReviewClinical,
  title = {Systematic {{Review}} of {{Clinical Trials Assessing Pharmacological Properties}} of {{Salvia Species}} on {{Memory}}, {{Cognitive Impairment}} and {{Alzheimer}}'s {{Disease}}},
  author = {Miroddi, Marco and Navarra, Michele and Quattropani, Maria C. and Calapai, Fabrizio and Gangemi, Sebastiano and Calapai, Gioacchino},
  year = {2014},
  month = apr,
  volume = {20},
  pages = {485--495},
  issn = {1755-5930},
  doi = {10.1111/cns.12270},
  abstract = {Salvia officinalis  L. and Salvia lavandulaefolia  L. have a longstanding use as traditional herbal remedies that can enhance memory and improve cognitive functions. Pharmacological actions of S.~officinalis and S.~lavandulaefolia on healthy subjects and on patients suffering of cognitive decline have been investigated. Aim of this review was to summarize published clinical trials assessing effectiveness and safety of S.~officinalis and S.~lavandulaefolia in the enhancement of cognitive performance in healthy subjects and neurodegenerative illnesses. Furthermore, to purchase a more complete view on safety of S.~officinalis and S.~lavandulaefolia, we collected and discussed articles regarding toxicity and adverse reactions. Eight clinical studies investigating on acute effects of S.~officinalis on healthy subjects were included in the review. Six studies investigated on the effects of S.~officinalis and S.~lavandaeluaefolia on cognitive performance in healthy subjects. The two remaining were carried out to study the effects of sage on Azheimer's disease. Our review shows that S.~officinalis and S.~lavandulaefolia exert beneficial effects by enhancing cognitive performance both in healthy subjects and patients with dementia or cognitive impairment and is safe for this indication. Unfortunately, promising beneficial effects are debased by methodological issues, use of different herbal preparations (extracts, essential oil, use of raw material), lack of details on herbal products used. We believe that sage promising effects need further higher methodological standard clinical trials.},
  file = {/home/trung/GoogleDrive/Zotero/miroddi et al_2014_systematic review of clinical trials assessing pharmacological properties of salvia species on memory, cognitive impairment and alzheimer's disease.pdf},
  journal = {CNS Neuroscience \& Therapeutics},
  number = {6},
  pmcid = {PMC6493168},
  pmid = {24836739}
}

@article{mirsky20_CreationDetectionDeepfakes,
  title = {The {{Creation}} and {{Detection}} of {{Deepfakes}}: {{A Survey}}},
  shorttitle = {The {{Creation}} and {{Detection}} of {{Deepfakes}}},
  author = {Mirsky, Yisroel and Lee, Wenke},
  year = {2020},
  month = sep,
  abstract = {Generative deep learning algorithms have progressed to a point where it is difficult to tell the difference between what is real and what is fake. In 2018, it was discovered how easy it is to use this technology for unethical and malicious applications, such as the spread of misinformation, impersonation of political leaders, and the defamation of innocent individuals. Since then, these `deepfakes' have advanced significantly. In this paper, we explore the creation and detection of deepfakes and provide an in-depth view of how these architectures work. The purpose of this survey is to provide the reader with a deeper understanding of (1) how deepfakes are created and detected, (2) the current trends and advancements in this domain, (3) the shortcomings of the current defense solutions, and (4) the areas which require further research and attention.},
  archivePrefix = {arXiv},
  eprint = {2004.11138},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mirsky et al_2020_the creation and detection of deepfakes.pdf},
  journal = {arXiv:2004.11138 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{mishra18_SimpleNeuralAttentive,
  title = {A {{Simple Neural Attentive Meta}}-{{Learner}}},
  author = {Mishra, Nikhil and Rohaninejad, Mostafa and Chen, Xi and Abbeel, Pieter},
  year = {2018},
  month = feb,
  abstract = {Deep neural networks excel in regimes with large amounts of data, but tend to struggle when data is scarce or when they need to adapt quickly to changes in the task. In response, recent work in meta-learning proposes training a meta-learner on a distribution of similar tasks, in the hopes of generalization to novel but related tasks by learning a high-level strategy that captures the essence of the problem it is asked to solve. However, many recent meta-learning approaches are extensively hand-designed, either using architectures specialized to a particular application, or hard-coding algorithmic components that constrain how the meta-learner solves the task. We propose a class of simple and generic meta-learner architectures that use a novel combination of temporal convolutions and soft attention; the former to aggregate information from past experience and the latter to pinpoint specific pieces of information. In the most extensive set of meta-learning experiments to date, we evaluate the resulting Simple Neural AttentIve Learner (or SNAIL) on several heavily-benchmarked tasks. On all tasks, in both supervised and reinforcement learning, SNAIL attains state-of-the-art performance by significant margins.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1707.03141},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mishra et al_2018_a simple neural attentive meta-learner.pdf},
  journal = {arXiv:1707.03141 [cs, stat]},
  keywords = {attention,causal,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,meta learning,snail,Statistics - Machine Learning,temporal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{mishra20_MeasurementAIPolicy,
  title = {Measurement in {{AI Policy}}: {{Opportunities}} and {{Challenges}}},
  shorttitle = {Measurement in {{AI Policy}}},
  author = {Mishra, Saurabh and Clark, Jack and Perrault, C. Raymond},
  year = {2020},
  month = sep,
  abstract = {As artificial intelligence increasingly influences our world, it becomes crucial to assess its technical progress and societal impact. This paper surveys problems and opportunities in the measurement of AI systems and their impact, based on a workshop held at Stanford University in the fall of 2019. We identify six summary challenges inherent to measuring the progress and impact of AI, and summarize over 40 presentations and associated discussions from the workshop. We hope this can inspire research agendas in this crucial area.},
  archivePrefix = {arXiv},
  eprint = {2009.09071},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mishra et al_2020_measurement in ai policy.pdf},
  journal = {arXiv:2009.09071 [cs]},
  primaryClass = {cs}
}

@article{misra19_MishSelfRegularized,
  title = {Mish: {{A Self Regularized Non}}-{{Monotonic Neural Activation Function}}},
  shorttitle = {Mish},
  author = {Misra, Diganta},
  year = {2019},
  month = aug,
  abstract = {The concept of non-linearity in a Neural Network is introduced by an activation function which serves an integral role in the training and performance evaluation of the network. Over the years of theoretical research, many activation functions have been proposed, however, only a few are widely used in mostly all applications which include ReLU (Rectified Linear Unit), TanH (Tan Hyperbolic), Sigmoid, Leaky ReLU and Swish. In this work, a novel neural activation function called as Mish is proposed. The experiments show that Mish tends to work better than both ReLU and Swish along with other standard activation functions in many deep networks across challenging datasets. For instance, in Squeeze Excite Net- 18 for CIFAR 100 classification, the network with Mish had an increase in Top-1 test accuracy by 0.494\% and 1.671\% as compared to the same network with Swish and ReLU respectively. The similarity to Swish along with providing a boost in performance and its simplicity in implementation makes it easier for researchers and developers to use Mish in their Neural Network Models.},
  archivePrefix = {arXiv},
  eprint = {1908.08681},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/misra_2019_mish.pdf;/home/trung/Zotero/storage/Q8HGLSBW/1908.html},
  journal = {arXiv:1908.08681 [cs, stat]},
  keywords = {activation,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{misra19_SelfSupervisedLearningPretextInvariant,
  title = {Self-{{Supervised Learning}} of {{Pretext}}-{{Invariant Representations}}},
  author = {Misra, Ishan and {van der Maaten}, Laurens},
  year = {2019},
  month = dec,
  abstract = {The goal of self-supervised learning from images is to construct image representations that are semantically meaningful via pretext tasks that do not require semantic annotations for a large training set of images. Many pretext tasks lead to representations that are covariant with image transformations. We argue that, instead, semantic representations ought to be invariant under such transformations. Specifically, we develop Pretext-Invariant Representation Learning (PIRL, pronounced as ``pearl'') that learns invariant representations based on pretext tasks. We use PIRL with a commonly used pretext task that involves solving jigsaw puzzles. We find that PIRL substantially improves the semantic quality of the learned image representations. Our approach sets a new state-of-the-art in selfsupervised learning from images on several popular benchmarks for self-supervised learning. Despite being unsupervised, PIRL outperforms supervised pre-training in learning image representations for object detection. Altogether, our results demonstrate the potential of self-supervised learning of image representations with good invariance properties.},
  archivePrefix = {arXiv},
  eprint = {1912.01991},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/KJVADB4H/Misra and van der Maaten - 2019 - Self-Supervised Learning of Pretext-Invariant Repr.pdf},
  journal = {arXiv:1912.01991 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{mitrovic20_RepresentationLearningInvariant,
  title = {Representation {{Learning}} via {{Invariant Causal Mechanisms}}},
  author = {Mitrovic, Jovana and McWilliams, Brian and Walker, Jacob and Buesing, Lars and Blundell, Charles},
  year = {2020},
  month = oct,
  abstract = {Self-supervised learning has emerged as a strategy to reduce the reliance on costly supervised signal by pretraining representations only using unlabeled data. These methods combine heuristic proxy classification tasks with data augmentations and have achieved significant success, but our theoretical understanding of this success remains limited. In this paper we analyze self-supervised representation learning using a causal framework. We show how data augmentations can be more effectively utilized through explicit invariance constraints on the proxy classifiers employed during pretraining. Based on this, we propose a novel self-supervised objective, Representation Learning via Invariant Causal Mechanisms (ReLIC), that enforces invariant prediction of proxy targets across augmentations through an invariance regularizer which yields improved generalization guarantees. Further, using causality we generalize contrastive learning, a particular kind of self-supervised method, and provide an alternative theoretical explanation for the success of these methods. Empirically, ReLIC significantly outperforms competing methods in terms of robustness and out-of-distribution generalization on ImageNet, while also significantly outperforming these methods on Atari achieving above human-level performance on \$51\$ out of \$57\$ games.},
  archivePrefix = {arXiv},
  eprint = {2010.07922},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mitrovic et al_2020_representation learning via invariant causal mechanisms.pdf},
  journal = {arXiv:2010.07922 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@misc{mlk19_BriefHistoryDeep,
  title = {Brief {{History}} of {{Deep Learning}} from 1943-2019 [{{Timeline}}]},
  author = {{MLK}},
  year = {2019},
  month = nov,
  abstract = {Introduction The world right now is seeing a global AI revolution across all industry. And one of the driving factor of this AI revolution is Deep Learning. Thanks to giants like Google and Facebook, Deep Learning now~ has become a popular term and people might think that it is a recent discovery. But you might \ldots},
  annotation = {ZSCC: NoCitationData[s0]},
  chapter = {AI News \& Discussions},
  file = {/home/trung/Zotero/storage/TLI4Z4YV/brief-history-of-deep-learning.html},
  journal = {MLK - Machine Learning Knowledge},
  language = {en-US}
}

@article{mnih14_NeuralVariationalInference,
  title = {Neural {{Variational Inference}} and {{Learning}} in {{Belief Networks}}},
  author = {Mnih, Andriy and Gregor, Karol},
  year = {2014},
  month = jan,
  abstract = {Highly expressive directed latent variable models, such as sigmoid belief networks, are difficult to train on large datasets because exact inference in them is intractable and none of the approximate inference methods that have been applied to them scale well. We propose a fast non-iterative approximate inference method that uses a feedforward network to implement efficient exact sampling from the variational posterior. The model and this inference network are trained jointly by maximizing a variational lower bound on the log-likelihood. Although the naive estimator of the inference model gradient is too high-variance to be useful, we make it practical by applying several straightforward model-independent variance reduction techniques. Applying our approach to training sigmoid belief networks and deep autoregressive networks, we show that it outperforms the wake-sleep algorithm on MNIST and achieves state-of-the-art results on the Reuters RCV1 document dataset.},
  annotation = {ZSCC: 0000430},
  archivePrefix = {arXiv},
  eprint = {1402.0030},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mnih et al_2014_neural variational inference and learning in belief networks.pdf;/home/trung/Zotero/storage/H3AUZRKP/1402.html},
  journal = {arXiv:1402.0030 [cs, stat]},
  keywords = {belief,Computer Science - Machine Learning,discrete,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{mnih14_RecurrentModelsVisual,
  title = {Recurrent {{Models}} of {{Visual Attention}}},
  author = {Mnih, Volodymyr and Heess, Nicolas and Graves, Alex and Kavukcuoglu, Koray},
  year = {2014},
  month = jun,
  abstract = {Applying convolutional neural networks to large images is computationally expensive because the amount of computation scales linearly with the number of image pixels. We present a novel recurrent neural network model that is capable of extracting information from an image or video by adaptively selecting a sequence of regions or locations and only processing the selected regions at high resolution. Like convolutional neural networks, the proposed model has a degree of translation invariance built-in, but the amount of computation it performs can be controlled independently of the input image size. While the model is non-differentiable, it can be trained using reinforcement learning methods to learn task-specific policies. We evaluate our model on several image classification tasks, where it significantly outperforms a convolutional neural network baseline on cluttered images, and on a dynamic visual control problem, where it learns to track a simple object without an explicit training signal for doing so.},
  annotation = {ZSCC: 0000014},
  archivePrefix = {arXiv},
  eprint = {1406.6247},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mnih et al_2014_recurrent models of visual attention.pdf;/home/trung/Zotero/storage/KJ5RJEV8/1406.html},
  journal = {arXiv:1406.6247 [cs, stat]},
  keywords = {attention,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,hard attention,recurrent,soft attention,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{mogadala20_TrendsIntegrationVision,
  title = {Trends in {{Integration}} of {{Vision}} and {{Language Research}}: {{A Survey}} of {{Tasks}}, {{Datasets}}, and {{Methods}}},
  shorttitle = {Trends in {{Integration}} of {{Vision}} and {{Language Research}}},
  author = {Mogadala, Aditya and Kalimuthu, Marimuthu and Klakow, Dietrich},
  year = {2020},
  month = sep,
  abstract = {The interest in Artificial Intelligence (AI) and its applications has seen unprecedented growth in the last few years. This success can be partly attributed to the advancements made in the sub-fields of AI such as Machine Learning (ML), Computer Vision (CV), and Natural Language Processing (NLP). The largest of the growths in these fields has been made possible with deep learning, a sub-area of machine learning, which uses the principles of artificial neural networks. This has created significant interest in the integration of vision and language. The tasks are designed such that they perfectly embrace the ideas of deep learning. In this survey, we focus on ten prominent tasks that integrate language and vision by discussing their problem formulations, methods, existing datasets, evaluation measures, and compare the results obtained with corresponding state-of-the-art methods. Our efforts go beyond earlier surveys which are either task-specific or concentrate only on one type of visual content, i.e., image or video. Furthermore, we also provide some potential future directions in this field of research with an anticipation that this survey brings in innovative thoughts and ideas to address the existing challenges and build new applications.},
  archivePrefix = {arXiv},
  eprint = {1907.09358},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mogadala et al_2020_trends in integration of vision and language research.pdf},
  journal = {arXiv:1907.09358 [cs]},
  primaryClass = {cs}
}

@article{mohamed19_MonteCarloGradient,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2019},
  month = jun,
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical developments, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  archivePrefix = {arXiv},
  eprint = {1906.10652},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mohamed et al_2019_monte carlo gradient estimation in machine learning.pdf;/home/trung/Zotero/storage/CD4YU62X/1906.html},
  journal = {arXiv:1906.10652 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{mohamed20_MonteCarloGradient,
  title = {Monte {{Carlo Gradient Estimation}} in {{Machine Learning}}},
  author = {Mohamed, Shakir and Rosca, Mihaela and Figurnov, Michael and Mnih, Andriy},
  year = {2020},
  month = sep,
  abstract = {This paper is a broad and accessible survey of the methods we have at our disposal for Monte Carlo gradient estimation in machine learning and across the statistical sciences: the problem of computing the gradient of an expectation of a function with respect to parameters defining the distribution that is integrated; the problem of sensitivity analysis. In machine learning research, this gradient problem lies at the core of many learning problems, in supervised, unsupervised and reinforcement learning. We will generally seek to rewrite such gradients in a form that allows for Monte Carlo estimation, allowing them to be easily and efficiently used and analysed. We explore three strategies--the pathwise, score function, and measure-valued gradient estimators--exploring their historical development, derivation, and underlying assumptions. We describe their use in other fields, show how they are related and can be combined, and expand on their possible generalisations. Wherever Monte Carlo gradient estimators have been derived and deployed in the past, important advances have followed. A deeper and more widely-held understanding of this problem will lead to further advances, and it is these advances that we wish to support.},
  archivePrefix = {arXiv},
  eprint = {1906.10652},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mohamed et al_2020_monte carlo gradient estimation in machine learning.pdf},
  journal = {arXiv:1906.10652 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{mohammadifard15_effecttreenut,
  title = {The Effect of Tree Nut, Peanut, and Soy Nut Consumption on Blood Pressure: A Systematic Review and Meta-Analysis of Randomized Controlled Clinical Trials},
  shorttitle = {The Effect of Tree Nut, Peanut, and Soy Nut Consumption on Blood Pressure},
  author = {Mohammadifard, Noushin and {Salehi-Abargouei}, Amin and {Salas-Salvad{\'o}}, Jordi and {Guasch-Ferr{\'e}}, Marta and Humphries, Karin and Sarrafzadegan, Nizal},
  year = {2015},
  month = may,
  volume = {101},
  pages = {966--982},
  issn = {1938-3207},
  doi = {10.3945/ajcn.114.091595},
  abstract = {BACKGROUND: Although several studies have assessed the effects of nut consumption (tree nuts, peanuts, and soy nuts) on blood pressure (BP), the results are conflicting. OBJECTIVE: The aim was to conduct a systematic review and meta-analysis of published randomized controlled trials (RCTs) to estimate the effect of nut consumption on BP. DESIGN: The databases MEDLINE, SCOPUS, ISI Web of Science, and Google Scholar were searched for RCTs carried out between 1958 and October 2013 that reported the effect of consuming single or mixed nuts (including walnuts, almonds, pistachios, cashews, hazelnuts, macadamia nuts, pecans, peanuts, and soy nuts) on systolic BP (SBP) or diastolic BP (DBP) as primary or secondary outcomes in adult populations aged {$\geq$}18 y. Relevant articles were identified by screening the abstracts and titles and the full text. Studies that evaluated the effects for {$<$}2 wk or in which the control group ingested different healthy oils were excluded. Mean {$\pm$} SD changes in SBP and DBP in each treatment group were recorded for meta-analysis. RESULTS: Twenty-one RCTs met the inclusion criteria. Our findings suggest that nut consumption leads to a significant reduction in SBP in participants without type 2 diabetes [mean difference (MD): -1.29; 95\% CI: -2.35, -0.22; P = 0.02] but not in the total population. Subgroup analyses of different nut types suggest that pistachios, but not other nuts, significantly reduce SBP (MD: -1.82; 95\% CI: -2.97, -0.67; P = 0.002). Our study suggests that pistachios (MD: -0.80; 95\% CI: -1.43, -0.17; P = 0.01) and mixed nuts (MD: -1.19; 95\% CI: -2.35, -0.03; P = 0.04) have a significant reducing effect on DBP. We found no significant changes in DBP after the consumption of other nuts. CONCLUSIONS: Total nut consumption lowered SBP in participants without type 2 diabetes. Pistachios seemed to have the strongest effect on reducing SBP and DBP. Mixed nuts also reduced DBP.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/mohammadifard et al_2015_the effect of tree nut, peanut, and soy nut consumption on blood pressure.pdf},
  journal = {The American Journal of Clinical Nutrition},
  language = {eng},
  number = {5},
  pmid = {25809855}
}

@book{mohri18_Foundationsmachinelearning,
  title = {Foundations of Machine Learning},
  author = {Mohri, Mehryar and Rostamizadeh, Afshin and Talwalkar, Ameet},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  annotation = {ZSCC: 0002166},
  file = {/home/trung/GoogleDrive/Zotero/mohri et al_2018_foundations of machine learning.pdf},
  isbn = {978-0-262-03940-6},
  keywords = {Computer algorithms,Machine learning},
  language = {en},
  lccn = {Q325.5 .M64 2018},
  series = {Adaptive Computation and Machine Learning}
}

@article{molavipour20_ConditionalMutualInformation,
  title = {Conditional {{Mutual Information Neural Estimator}}},
  author = {Molavipour, Sina and Bassi, Germ{\'a}n and Skoglund, Mikael},
  year = {2020},
  month = feb,
  doi = {10.1109/ICASSP40776.2020.9053422},
  abstract = {Several recent works in communication systems have proposed to leverage the power of neural networks in the design of encoders and decoders. In this approach, these blocks can be tailored to maximize the transmission rate based on aggregated samples from the channel. Motivated by the fact that, in many communication schemes, the achievable transmission rate is determined by a conditional mutual information term, this paper focuses on neural-based estimators for this information-theoretic quantity. Our results are based on variational bounds for the KL-divergence and, in contrast to some previous works, we provide a mathematically rigorous lower bound. However, additional challenges with respect to the unconditional mutual information emerge due to the presence of a conditional density function which we address here.},
  archivePrefix = {arXiv},
  eprint = {1911.02277},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/molavipour et al_2020_conditional mutual information neural estimator.pdf},
  journal = {arXiv:1911.02277 [cs, math]},
  keywords = {information},
  primaryClass = {cs, math}
}

@article{molavipour20_NeuralEstimatorsConditional,
  title = {On {{Neural Estimators}} for {{Conditional Mutual Information Using Nearest Neighbors Sampling}}},
  author = {Molavipour, Sina and Bassi, Germ{\'a}n and Skoglund, Mikael},
  year = {2020},
  month = jun,
  abstract = {The estimation of mutual information (MI) or conditional mutual information (CMI) from a set of samples is a long-standing problem. A recent line of work in this area has leveraged the approximation power of artificial neural networks and has shown improvements over conventional methods. One important challenge in this new approach is the need to obtain, given the original dataset, a different set where the samples are distributed according to a specific product density function. This is particularly challenging when estimating CMI. In this paper, we introduce a new technique, based on k nearest neighbors (k-NN), to perform the resampling and derive high-confidence concentration bounds for the sample average. Then the technique is employed to train a neural network classifier and the CMI is estimated accordingly. We propose three estimators using this technique and prove their consistency, make a comparison between them and similar approaches in the literature, and experimentally show improvements in estimating the CMI in terms of accuracy and variance of the estimators.},
  archivePrefix = {arXiv},
  eprint = {2006.07225},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/molavipour et al_2020_on neural estimators for conditional mutual information using nearest neighbors sampling.pdf},
  journal = {arXiv:2006.07225 [cs, math]},
  keywords = {information},
  primaryClass = {cs, math}
}

@article{molavipour20_NeuralEstimatorsConditionala,
  title = {Neural {{Estimators}} for {{Conditional Mutual Information Using Nearest Neighbors Sampling}}},
  author = {Molavipour, Sina and Bassi, Germ{\'a}n and Skoglund, Mikael},
  year = {2020},
  month = oct,
  abstract = {The estimation of mutual information (MI) or conditional mutual information (CMI) from a set of samples is a long-standing problem. A recent line of work in this area has leveraged the approximation power of artificial neural networks and has shown improvements over conventional methods. One important challenge in this new approach is the need to obtain, given the original dataset, a different set where the samples are distributed according to a specific product density function. This is particularly challenging when estimating CMI. In this paper, we introduce a new technique, based on k nearest neighbors (k-NN), to perform the resampling and derive high-confidence concentration bounds for the sample average. Then the technique is employed to train a neural network classifier and the CMI is estimated accordingly. We propose three estimators using this technique and prove their consistency, make a comparison between them and similar approaches in the literature, and experimentally show improvements in estimating the CMI in terms of accuracy and variance of the estimators.},
  archivePrefix = {arXiv},
  eprint = {2006.07225},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/molavipour et al_2020_neural estimators for conditional mutual information using nearest neighbors sampling.pdf},
  journal = {arXiv:2006.07225 [cs, math]},
  keywords = {information},
  primaryClass = {cs, math}
}

@book{molnar00_InterpretableMachineLearning,
  title = {Interpretable {{Machine Learning}}},
  author = {Molnar, Christoph},
  abstract = {Machine learning algorithms usually operate as black boxes and it is unclear how they derived a certain decision. This book is a guide for practitioners to make machine learning decisions interpretable.},
  file = {/home/trung/Zotero/storage/4RVV9Y5L/interpretable-ml-book.html}
}

@article{mondal20_RegularizeNotRegularize,
  title = {To {{Regularize}} or {{Not To Regularize}}? {{The Bias Variance Trade}}-off in {{Regularized AEs}}},
  shorttitle = {To {{Regularize}} or {{Not To Regularize}}?},
  author = {Mondal, Arnab Kumar and Asnani, Himanshu and Singla, Parag and AP, Prathosh},
  year = {2020},
  month = jun,
  abstract = {Regularized Auto-Encoders (AE) form a rich class of methods within the landscape of neural generative models. They effectively model the joint-distribution between the data and a latent space using an Encoder-Decoder combination, with regularization imposed in terms of a prior over the latent space. Despite their advantages such as stability in training, the performance of AE based models has not reached that of the other models such as GANs. While several reasons including the presence of conflicting terms in the objective, distributional choices imposed on the Encoder and the Decoder, and dimensionality of the latent space have been identified as possible causes for the suboptimal performance, the role of the regularization (prior distribution) imposed has not been studied systematically. Motivated by this, we examine the effect of the latent prior on the generation quality of the AE models in this paper. We show that there is no single fixed prior which is optimal for all data distributions, given a Gaussian Decoder. Further, with finite data, we show that there exists a bias-variance trade-off that comes with prior imposition. As a remedy, we optimize a generalized ELBO objective, with an additional state space over the latent prior. We implicitly learn this flexible prior jointly with the AE training using an adversarial learning technique, which facilitates operation on different points of the bias-variance curve. Our experiments on multiple datasets show that the proposed method is the new state-of-the-art for AE based generative models.},
  archivePrefix = {arXiv},
  eprint = {2006.05838},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mondal et al_2020_to regularize or not to regularize.pdf},
  journal = {arXiv:2006.05838 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@inproceedings{monteiro19_EndtoEndLanguageIdentification,
  title = {End-to-{{End Language Identification Using}} a {{Residual Convolutional Neural Network}} with {{Attentive Temporal Pooling}}},
  booktitle = {2019 27th {{European Signal Processing Conference}} ({{EUSIPCO}})},
  author = {Monteiro, Jo{\~a}o and Alam, Jahangir and Bhattacharya, Gautam and Falk, Tiago H.},
  year = {2019},
  month = sep,
  pages = {1--5},
  issn = {2219-5491},
  doi = {10.23919/EUSIPCO.2019.8902777},
  abstract = {In this work, we tackle the problem of end-to-end language identification from speech. To this end, we propose the use of a residual convolutional neural network aiming at exploiting the ability of such architectures to take into account large contextual segments of input data. Moreover, in order for variable input lengths to be supported by the proposed setting, a self-attention mechanism is employed on top of the final convolutional layer. This results in a learnable temporal feature pooling scheme that allows for embedding varying duration utterances into a fixed dimension space. Evaluation is performed on data containing ten oriental languages under different test conditions, namely: short-duration recordings, confusing languages trials, as well as a set of trials in which non-target unseen languages are included. End-to-end evaluation of the proposed framework is thus shown to significantly outperform well-known benchmark methods under considered evaluation conditions.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/monteiro et al_2019_end-to-end language identification using a residual convolutional neural network with attentive temporal pooling.pdf;/home/trung/Zotero/storage/FUAKNCTE/8902777.html},
  keywords = {Attentive features pooling,Language identification,Residual convolutional neural networks}
}

@article{monti19_CausalDiscoveryGeneral,
  title = {Causal {{Discovery}} with {{General Non}}-{{Linear Relationships Using Non}}-{{Linear ICA}}},
  author = {Monti, Ricardo Pio and Zhang, Kun and Hyvarinen, Aapo},
  year = {2019},
  month = apr,
  abstract = {We consider the problem of inferring causal relationships between two or more passively observed variables. While the problem of such causal discovery has been extensively studied especially in the bivariate setting, the majority of current methods assume a linear causal relationship, and the few methods which consider non-linear dependencies usually make the assumption of additive noise. Here, we propose a framework through which we can perform causal discovery in the presence of general non-linear relationships. The proposed method is based on recent progress in non-linear independent component analysis and exploits the non-stationarity of observations in order to recover the underlying sources or latent disturbances. We show rigorously that in the case of bivariate causal discovery, such non-linear ICA can be used to infer the causal direction via a series of independence tests. We further propose an alternative measure of causal direction based on asymptotic approximations to the likelihood ratio, as well as an extension to multivariate causal discovery. We demonstrate the capabilities of the proposed method via a series of simulation studies and conclude with an application to neuroimaging data.},
  archivePrefix = {arXiv},
  eprint = {1904.09096},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Monti et al_2019_Causal Discovery with General Non-Linear Relationships Using Non-Linear ICA2.pdf;/home/trung/Zotero/storage/4YPSF95W/1904.html},
  journal = {arXiv:1904.09096 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,ica,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{moons17_MinimumEnergyQuantized,
  title = {Minimum {{Energy Quantized Neural Networks}}},
  author = {Moons, Bert and Goetschalckx, Koen and Van Berckelaer, Nick and Verhelst, Marian},
  year = {2017},
  month = nov,
  abstract = {This work targets the automated minimum-energy optimization of Quantized Neural Networks (QNNs) - networks using low precision weights and activations. These networks are trained from scratch at an arbitrary fixed point precision. At iso-accuracy, QNNs using fewer bits require deeper and wider network architectures than networks using higher precision operators, while they require less complex arithmetic and less bits per weights. This fundamental trade-off is analyzed and quantified to find the minimum energy QNN for any benchmark and hence optimize energy-efficiency. To this end, the energy consumption of inference is modeled for a generic hardware platform. This allows drawing several conclusions across different benchmarks. First, energy consumption varies orders of magnitude at iso-accuracy depending on the number of bits used in the QNN. Second, in a typical system, BinaryNets or int4 implementations lead to the minimum energy solution, outperforming int8 networks up to 2 - 10\texttimes{} at iso-accuracy. All code used for QNN training is available from https://github.com/BertMoons/.},
  annotation = {ZSCC: 0000030},
  archivePrefix = {arXiv},
  eprint = {1711.00215},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/moons et al_2017_minimum energy quantized neural networks.pdf},
  journal = {arXiv:1711.00215 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{moraffah19_Deepcausalrepresentation,
  title = {Deep Causal Representation Learning for Unsupervised Domain Adaptation},
  author = {Moraffah, Raha and Shu, Kai and Raglin, Adrienne and Liu, Huan},
  year = {2019},
  month = oct,
  abstract = {Studies show that the representations learned by deep neural networks can be transferred to similar prediction tasks in other domains for which we do not have enough labeled data. However, as we transition to higher layers in the model, the representations become more task-specific and less generalizable. Recent research on deep domain adaptation proposed to mitigate this problem by forcing the deep model to learn more transferable feature representations across domains. This is achieved by incorporating domain adaptation methods into deep learning pipeline. The majority of existing models learn the transferable feature representations which are highly correlated with the outcome. However, correlations are not always transferable. In this paper, we propose a novel deep causal representation learning framework for unsupervised domain adaptation, in which we propose to learn domain-invariant causal representations of the input from the source domain. We simulate a virtual target domain using reweighted samples from the source domain and estimate the causal effect of features on the outcomes. The extensive comparative study demonstrates the strengths of the proposed model for unsupervised domain adaptation via causal representations.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.12417},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Moraffah et al_2019_Deep causal representation learning for unsupervised domain adaptation2.pdf;/home/trung/Zotero/storage/3A3Q78JP/1910.html},
  journal = {arXiv:1910.12417 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{morcos18_importancesingledirections,
  title = {On the Importance of Single Directions for Generalization},
  author = {Morcos, Ari S. and Barrett, David G. T. and Rabinowitz, Neil C. and Botvinick, Matthew},
  year = {2018},
  month = may,
  abstract = {Despite their ability to memorize large datasets, deep neural networks often achieve good generalization performance. However, the differences between the learned solutions of networks which generalize and those which do not remain unclear. Additionally, the tuning properties of single directions (defined as the activation of a single unit or some linear combination of units in response to some input) have been highlighted, but their importance has not been evaluated. Here, we connect these lines of inquiry to demonstrate that a network's reliance on single directions is a good predictor of its generalization performance, across networks trained on datasets with different fractions of corrupted labels, across ensembles of networks trained on datasets with unmodified labels, across different hyperparameters, and over the course of training. While dropout only regularizes this quantity up to a point, batch normalization implicitly discourages single direction reliance, in part by decreasing the class selectivity of individual units. Finally, we find that class selectivity is a poor predictor of task importance, suggesting not only that networks which generalize well minimize their dependence on individual units by reducing their selectivity, but also that individually selective units may not be necessary for strong network performance.},
  annotation = {ZSCC: 0000080},
  archivePrefix = {arXiv},
  eprint = {1803.06959},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/morcos et al_2018_on the importance of single directions for generalization.pdf;/home/trung/Zotero/storage/MITHSGT2/1803.html},
  journal = {arXiv:1803.06959 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{moreno-torres12_unifyingviewdataset,
  title = {A Unifying View on Dataset Shift in Classification},
  author = {{Moreno-Torres}, Jose G. and Raeder, Troy and {Alaiz-Rodr{\'i}guez}, Roc{\'i}o and Chawla, Nitesh V. and Herrera, Francisco},
  year = {2012},
  month = jan,
  volume = {45},
  pages = {521--530},
  issn = {0031-3203},
  doi = {10.1016/j.patcog.2011.06.019},
  abstract = {The field of dataset shift has received a growing amount of interest in the last few years. The fact that most real-world applications have to cope with some form of shift makes its study highly relevant. The literature on the topic is mostly scattered, and different authors use different names to refer to the same concepts, or use the same name for different concepts. With this work, we attempt to present a unifying framework through the review and comparison of some of the most important works in the literature.},
  file = {/home/trung/GoogleDrive/Zotero/moreno-torres et al_2012_a unifying view on dataset shift in classification.pdf},
  journal = {Pattern Recognition},
  keywords = {Changing environments,Covariate shift,Data fracture,Dataset shift,Differing training and test populations,Non-stationary distributions,Sample selection bias},
  number = {1}
}

@article{moretti19_AutoencodingTopographicFactors,
  title = {Autoencoding {{Topographic Factors}}},
  author = {Moretti, Antonio and Stirn, Andrew and Marks, Gabriel and Pe'er, Itsik},
  year = {2019},
  month = jun,
  volume = {26},
  pages = {546--560},
  issn = {1557-8666},
  doi = {10.1089/cmb.2018.0176},
  abstract = {Topographic factor models separate overlapping signals into latent spatial functions to identify correlation structure across observations. These methods require the underlying structure to be held fixed and are not robust to deviations commonly found across images. We present autoencoding topographic factors, a novel variational inference scheme, to decompose irregular observations on a lattice into a superposition of low-rank sources. By exploiting recent developments in variational autoencoders, we replace fixed sources with a nonlinear mapping that parameterizes an unnormalized distribution on the lattice. In doing so, we permit sources to drift dynamically, filtering residual differences in location across comparable areas of interest. This gives an implicit mapping to a unique latent representation while simultaneously forcing the posterior to model group variability in spatial structure. Simulation results and applications to functional imaging demonstrate the effectiveness of our method and its ability to outperform existing spatial factor models.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/Zotero/storage/PZA3AK8T/Moretti et al. - 2019 - Autoencoding Topographic Factors.pdf},
  journal = {Journal of Computational Biology},
  language = {en},
  number = {6}
}

@article{morningstar20_DensityStatesEstimation,
  title = {Density of {{States Estimation}} for {{Out}}-of-{{Distribution Detection}}},
  author = {Morningstar, Warren R. and Ham, Cusuh and Gallagher, Andrew G. and Lakshminarayanan, Balaji and Alemi, Alexander A. and Dillon, Joshua V.},
  year = {2020},
  month = jun,
  abstract = {Perhaps surprisingly, recent studies have shown probabilistic model likelihoods have poor specificity for out-of-distribution (OOD) detection and often assign higher likelihoods to OOD data than in-distribution data. To ameliorate this issue we propose DoSE, the density of states estimator. Drawing on the statistical physics notion of ``density of states,'' the DoSE decision rule avoids direct comparison of model probabilities, and instead utilizes the ``probability of the model probability,'' or indeed the frequency of any reasonable statistic. The frequency is calculated using nonparametric density estimators (e.g., KDE and one-class SVM) which measure the typicality of various model statistics given the training data and from which we can flag test points with low typicality as anomalous. Unlike many other methods, DoSE requires neither labeled data nor OOD examples. DoSE is modular and can be trivially applied to any existing, trained model. We demonstrate DoSE's state-of-the-art performance against other unsupervised OOD detectors on previously established ``hard'' benchmarks.},
  archivePrefix = {arXiv},
  eprint = {2006.09273},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/morningstar et al_2020_density of states estimation for out-of-distribution detection.pdf},
  journal = {arXiv:2006.09273 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{morrow20_BenefitingDeepLatent,
  title = {Benefiting {{Deep Latent Variable Models}} via {{Learning}} the {{Prior}} and {{Removing Latent Regularization}}},
  author = {Morrow, Rogan and Chiu, Wei-Chen},
  year = {2020},
  month = jul,
  abstract = {There exist many forms of deep latent variable models, such as the variational autoencoder and adversarial autoencoder. Regardless of the specific class of model, there exists an implicit consensus that the latent distribution should be regularized towards the prior, even in the case where the prior distribution is learned. Upon investigating the effect of latent regularization on image generation our results indicate that in the case where a sufficiently expressive prior is learned, latent regularization is not necessary and may in fact be harmful insofar as image quality is concerned. We additionally investigate the benefit of learned priors on two common problems in computer vision: latent variable disentanglement, and diversity in image-to-image translation.},
  archivePrefix = {arXiv},
  eprint = {2007.03640},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/morrow et al_2020_benefiting deep latent variable models via learning the prior and removing latent regularization.pdf},
  journal = {arXiv:2007.03640 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@article{mosca00_RegularizingDeepLearning,
  title = {Regularizing {{Deep Learning Ensembles}} by {{Distillation}}},
  author = {Mosca, Alan and Magoulas, George D},
  pages = {5},
  abstract = {Ensemble methods are often utilised to improve the generalisation of a model, by exploiting the diversity of multiple underlying learners trained on data of the same problem. When applied to Deep Learning, this requires the use of very large models that have to be trained multiple times, and the requirements of using Deep Learning Ensembles at test-time can be considerably high. It has been shown that a Deep Neural Network can be approximated by a smaller network, if it has been trained to reproduce the output of the larger network, in a process called distillation. It has also been shown previously that an Ensemble can be approximated by an Artificial Neural Network, with the introduction of additional training pseudo-data. We observe that it is possible to apply the same principles to approximate an Ensemble of Deep Neural Networks with a single Deep Network of the same kind and with the same structure as the members of the Ensemble, without significant loss of generalisation, without additional synthetic training data, and without having to use special provisions to train the new network, such as using the soft target probabilities. This process leads to a form of model regularisation, because the learning capacity is reduced whilst maintaining similar generalisation results. This behaviour is corroborated by experimental results on popular benchmark datasets in computer vision.},
  file = {/home/trung/GoogleDrive/Zotero/mosca et al_regularizing deep learning ensembles by distillation.pdf},
  keywords = {distillation,ensemble,regularization},
  language = {en}
}

@article{mrabah19_AdversarialDeepEmbedded,
  title = {Adversarial {{Deep Embedded Clustering}}: On a Better Trade-off between {{Feature Randomness}} and {{Feature Drift}}},
  shorttitle = {Adversarial {{Deep Embedded Clustering}}},
  author = {Mrabah, Nairouz and Bouguessa, Mohamed and Ksantini, Riadh},
  year = {2019},
  month = sep,
  abstract = {Clustering using deep autoencoders has been thoroughly investigated in recent years. Current approaches rely on simultaneously learning embedded features and clustering the data points in the latent space. Although numerous deep clustering approaches outperform the shallow models in achieving favorable results on several high-semantic datasets, a critical weakness of such models has been overlooked. In the absence of concrete supervisory signals, the embedded clustering objective function may distort the latent space by learning from unreliable pseudo-labels. Thus, the network can learn non-representative features, which in turn undermines the discriminative ability, yielding worse pseudo-labels. In order to alleviate the effect of random discriminative features, modern autoencoder-based clustering papers propose to use the reconstruction loss for pretraining and as a regularizer during the clustering phase. Nevertheless, a clustering-reconstruction trade-off can cause the \textbackslash textit\{Feature Drift\} phenomena. In this paper, we propose ADEC (Adversarial Deep Embedded Clustering) a novel autoencoder-based clustering model, which addresses a dual problem, namely, \textbackslash textit\{Feature Randomness\} and \textbackslash textit\{Feature Drift\}, using adversarial training. We empirically demonstrate the suitability of our model on handling these problems using benchmark real datasets. Experimental results validate that our model outperforms state-of-the-art autoencoder-based clustering methods.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1909.11832},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mrabah et al_2019_adversarial deep embedded clustering.pdf},
  journal = {arXiv:1909.11832 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{mu18_LongTermEffectsDietary,
  title = {Long-{{Term Effects}} of {{Dietary Protein}} and {{Branched}}-{{Chain Amino Acids}} on {{Metabolism}} and {{Inflammation}} in {{Mice}}},
  author = {Mu, Wei-Chieh and VanHoosier, Erin and Elks, Carrie M. and Grant, Ryan W.},
  year = {2018},
  month = jul,
  volume = {10},
  issn = {2072-6643},
  doi = {10.3390/nu10070918},
  abstract = {Aging is the main factor involved in the onset of degenerative diseases. Dietary protein restriction has been shown to increase the lifespan of rodents and improve metabolic phenotype. Branched-chain amino acids (BCAA) can act as nutrient signals that increase the lifespan of mice after prolonged supplementation. It remains unclear whether the combination of protein restriction and BCAA supplementation improves metabolic and immunological profiles during aging. Here, we investigated how dietary protein levels and BCAA supplementation impact metabolism and immune profile during a 12-month intervention in adult male C57BL/6J mice. We found that protein restriction improved insulin tolerance and increased hepatic fibroblast growth factor 21 mRNA, circulating interleukin (IL)-5 concentration, and thermogenic uncoupling protein 1 in subcutaneous white fat. Surprisingly, BCAA supplementation conditionally increased body weight, lean mass, and fat mass, and deteriorated insulin intolerance during protein restriction, but not during protein sufficiency. BCAA also induced pro-inflammatory gene expression in visceral adipose tissue under both normal and low protein conditions. These results suggest that dietary protein levels and BCAA supplementation coordinate a complex regulation of metabolism and tissue inflammation during prolonged feeding.},
  file = {/home/trung/GoogleDrive/Zotero/mu et al_2018_long-term effects of dietary protein and branched-chain amino acids on metabolism and inflammation in mice.pdf},
  journal = {Nutrients},
  number = {7},
  pmcid = {PMC6073443},
  pmid = {30021962}
}

@article{mujika17_Multitasklearningdeep,
  title = {Multi-Task Learning with Deep Model Based Reinforcement Learning},
  author = {Mujika, Asier},
  year = {2017},
  month = may,
  abstract = {In recent years, model-free methods that use deep learning have achieved great success in many different reinforcement learning environments. Most successful approaches focus on solving a single task, while multi-task reinforcement learning remains an open problem. In this paper, we present a model based approach to deep reinforcement learning which we use to solve different tasks simultaneously. We show that our approach not only does not degrade but actually benefits from learning multiple tasks. For our model, we also present a new kind of recurrent neural network inspired by residual networks that decouples memory from computation allowing to model complex environments that do not require lots of memory.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1611.01457},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mujika_2017_multi-task learning with deep model based reinforcement learning.pdf},
  journal = {arXiv:1611.01457 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{mukherjee19_ProtectingGANsprivacy,
  title = {Protecting {{GANs}} against Privacy Attacks by Preventing Overfitting},
  author = {Mukherjee, Sumit and Xu, Yixi and Trivedi, Anusua and Ferres, Juan Lavista},
  year = {2019},
  month = dec,
  abstract = {Generative Adversarial Networks (GANs) have made releasing of synthetic images a viable approach to share data without releasing the original dataset. It has been shown that such synthetic data can be used for a variety of downstream tasks such as training classifiers that would otherwise require the original dataset to be shared. However, recent work has shown that the GAN models and their synthetically generated data can be used to infer the training set membership by an adversary who has access to the entire dataset and some auxiliary information. Here we develop a new GAN architecture (privGAN) which provides protection against this mode of attack while leading to negligible loss in downstream performances. Our architecture explicitly prevents overfitting to the training set thereby providing implicit protection against white-box attacks. The main contributions of this paper are: i) we propose a novel GAN architecture that can generate synthetic data in a privacy preserving manner and demonstrate the effectiveness of our model against white--box attacks on several benchmark datasets, ii) we provide a theoretical understanding of the optimal solution of the GAN loss function, iii) we demonstrate on two common benchmark datasets that synthetic images generated by privGAN lead to negligible loss in downstream performance when compared against non--private GANs. While we have focosued on benchmarking privGAN exclusively of image datasets, the architecture of privGAN is not exclusive to image datasets and can be easily extended to other types of datasets.},
  archivePrefix = {arXiv},
  eprint = {2001.00071},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mukherjee et al_2019_protecting gans against privacy attacks by preventing overfitting.pdf;/home/trung/Zotero/storage/RKB3X2TZ/2001.html},
  journal = {arXiv:2001.00071 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{mulcahy00_JohnConwayReminiscences,
  title = {John {{Conway Reminiscences}} about {{Dr}}. {{Matrix}} and {{Bourbaki}}},
  author = {Mulcahy, Colm},
  abstract = {Last week, life took me through Princeton, and I seized the opportunity to drop in to see resident English mathematician John Horton Conway.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/FLDFGGTB/john-conway-reminiscences-about-dr-matrix-and-bourbaki.html},
  howpublished = {https://blogs.scientificamerican.com/guest-blog/john-conway-reminiscences-about-dr-matrix-and-bourbaki/},
  journal = {Scientific American Blog Network},
  language = {en}
}

@article{mullainathan00_BiasedAlgorithmsAre,
  title = {Biased {{Algorithms Are Easier}} to {{Fix Than Biased People}}},
  author = {Mullainathan, Sendhil},
  pages = {3},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/mullainathan_biased algorithms are easier to fix than biased people.pdf},
  language = {en}
}

@article{mundt18_RethinkingLayerwiseFeature,
  title = {Rethinking {{Layer}}-Wise {{Feature Amounts}} in {{Convolutional Neural Network Architectures}}},
  author = {Mundt, Martin and Majumder, Sagnik and Weis, Tobias and Ramesh, Visvanathan},
  year = {2018},
  month = dec,
  abstract = {We characterize convolutional neural networks with respect to the relative amount of features per layer. Using a skew normal distribution as a parametrized framework, we investigate the common assumption of monotonously increasing feature-counts with higher layers of architecture designs. Our evaluation on models with VGG-type layers on the MNIST, Fashion-MNIST and CIFAR-10 image classification benchmarks provides evidence that motivates rethinking of our common assumption: architectures that favor larger early layers seem to yield better accuracy.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1812.05836},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mundt et al_2018_rethinking layer-wise feature amounts in convolutional neural network architectures.pdf;/home/trung/Zotero/storage/LJXMMDMX/1812.html},
  journal = {arXiv:1812.05836 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{mundt20_UnifiedProbabilisticDeep,
  title = {Unified {{Probabilistic Deep Continual Learning}} through {{Generative Replay}} and {{Open Set Recognition}}},
  author = {Mundt, Martin and Majumder, Sagnik and Pliushch, Iuliia and Ramesh, Visvanathan},
  year = {2020},
  month = feb,
  abstract = {We introduce a probabilistic approach to unify deep continual learning with open set recognition, based on variational Bayesian inference. Our single model combines a joint probabilistic encoder with a generative model and a linear classifier that get shared across sequentially arriving tasks. In order to successfully distinguish unseen unknown data from trained known tasks, we propose to bound the class specific approximate posterior by fitting regions of high density on the basis of correctly classified data points. These bounds are further used to significantly alleviate catastrophic forgetting by avoiding samples from low density areas in generative replay. Our approach requires no storing of old- or upfront knowledge of future data and is empirically validated on visual and audio tasks in class incremental, as well as cross-dataset scenarios across modalities.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1905.12019},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/mundt et al_2020_unified probabilistic deep continual learning through generative replay and open set recognition.pdf},
  journal = {arXiv:1905.12019 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{munjal19_ImplicitDiscriminatorVariational,
  title = {Implicit {{Discriminator}} in {{Variational Autoencoder}}},
  author = {Munjal, Prateek and Paul, Akanksha and Krishnan, Narayanan C.},
  year = {2019},
  month = sep,
  abstract = {Recently generative models have focused on combining the advantages of variational autoencoders (VAE) and generative adversarial networks (GAN) for good reconstruction and generative abilities. In this work we introduce a novel hybrid architecture, Implicit Discriminator in Variational Autoencoder (IDVAE), that combines a VAE and a GAN, which does not need an explicit discriminator network. The fundamental premise of the IDVAE architecture is that the encoder of a VAE and the discriminator of a GAN utilize common features and therefore can be trained as a shared network, while the decoder of the VAE and the generator of the GAN can be combined to learn a single network. This results in a simple two-tier architecture that has the properties of both a VAE and a GAN. The qualitative and quantitative experiments on real-world benchmark datasets demonstrates that IDVAE perform better than the state of the art hybrid approaches. We experimentally validate that IDVAE can be easily extended to work in a conditional setting and demonstrate its performance on complex datasets.},
  archivePrefix = {arXiv},
  eprint = {1909.13062},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/munjal et al_2019_implicit discriminator in variational autoencoder.pdf},
  journal = {arXiv:1909.13062 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{muppirala11_PredictingRNAProteinInteractions,
  title = {Predicting {{RNA}}-{{Protein Interactions Using Only Sequence Information}}},
  author = {Muppirala, Usha K and Honavar, Vasant G and Dobbs, Drena},
  year = {2011},
  month = dec,
  volume = {12},
  pages = {489},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-12-489},
  abstract = {Background: RNA-protein interactions (RPIs) play important roles in a wide variety of cellular processes, ranging from transcriptional and post-transcriptional regulation of gene expression to host defense against pathogens. High throughput experiments to identify RNA-protein interactions are beginning to provide valuable information about the complexity of RNA-protein interaction networks, but are expensive and time consuming. Hence, there is a need for reliable computational methods for predicting RNA-protein interactions. Results: We propose RPISeq, a family of classifiers for predicting RNA-protein interactions using only sequence information. Given the sequences of an RNA and a protein as input, RPIseq predicts whether or not the RNAprotein pair interact. The RNA sequence is encoded as a normalized vector of its ribonucleotide 4-mer composition, and the protein sequence is encoded as a normalized vector of its 3-mer composition, based on a 7-letter reduced alphabet representation. Two variants of RPISeq are presented: RPISeq-SVM, which uses a Support Vector Machine (SVM) classifier and RPISeq-RF, which uses a Random Forest classifier. On two non-redundant benchmark datasets extracted from the Protein-RNA Interface Database (PRIDB), RPISeq achieved an AUC (Area Under the Receiver Operating Characteristic (ROC) curve) of 0.96 and 0.92. On a third dataset containing only mRNA-protein interactions, the performance of RPISeq was competitive with that of a published method that requires information regarding many different features (e.g., mRNA half-life, GO annotations) of the putative RNA and protein partners. In addition, RPISeq classifiers trained using the PRIDB data correctly predicted the majority (57-99\%) of non-coding RNA-protein interactions in NPInter-derived networks from E. coli, S. cerevisiae, D. melanogaster, M. musculus, and H. sapiens. Conclusions: Our experiments with RPISeq demonstrate that RNA-protein interactions can be reliably predicted using only sequence-derived information. RPISeq offers an inexpensive method for computational construction of RNA-protein interaction networks, and should provide useful insights into the function of non-coding RNAs. RPISeq is freely available as a web-based server at http://pridb.gdcb.iastate.edu/RPISeq/.},
  file = {/home/trung/GoogleDrive/Zotero/muppirala et al_2011_predicting rna-protein interactions using only sequence information.pdf},
  journal = {BMC Bioinformatics},
  language = {en},
  number = {1}
}

@article{murin19_Antibodyresponsesviral,
  title = {Antibody Responses to Viral Infections: A Structural Perspective across Three Different Enveloped Viruses},
  shorttitle = {Antibody Responses to Viral Infections},
  author = {Murin, Charles D. and Wilson, Ian A. and Ward, Andrew B.},
  year = {2019},
  month = may,
  volume = {4},
  pages = {734--747},
  publisher = {{Nature Publishing Group}},
  issn = {2058-5276},
  doi = {10.1038/s41564-019-0392-y},
  abstract = {This Review summarizes recent advances in our understanding of neutralizing antibody responses to enveloped viruses with different pathogenesis and discusses how this information is used to inform design of vaccines, therapeutics and diagnostics.},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
  file = {/home/trung/GoogleDrive/Zotero/murin et al_2019_antibody responses to viral infections.pdf;/home/trung/Zotero/storage/QBCZHU3P/s41564-019-0392-y.html},
  journal = {Nature Microbiology},
  language = {en},
  number = {5}
}

@phdthesis{murphy02_DynamicBayesianNetworks,
  title = {Dynamic {{Bayesian Networks}}: {{Representation}}, {{Inference}} and {{Learning}}},
  author = {Murphy, Kevin},
  year = {2002},
  file = {/home/trung/GoogleDrive/Zotero/murphy_2002_dynamic bayesian networks.pdf},
  school = {UC Berkeley}
}

@article{muthirayan19_ImprovedAttentionModels,
  title = {Improved {{Attention Models}} for {{Memory Augmented Neural Network Adaptive Controllers}}},
  author = {Muthirayan, Deepan and Nivison, Scott and Khargonekar, Pramod P.},
  year = {2019},
  month = oct,
  abstract = {We introduced a \{\textbackslash it working memory\} augmented adaptive controller in our recent work. The controller uses attention to read from and write to the working memory. Attention allows the controller to read specific information that is relevant and update its working memory with information based on its relevance. The retrieved information is used to modify the final control input computed by the controller. We showed that this modification speeds up learning. In the above work, we used a soft-attention mechanism for the adaptive controller. Controllers that use soft attention or hard attention mechanisms are limited either because they can forget the information or fail to shift attention when the information they are reading becomes less relevant. We propose an attention mechanism that comprises of (i) a hard attention mechanism and additionally (ii) an attention reallocation mechanism. The attention reallocation enables the controller to reallocate attention to a different location when the relevance of the location it is reading from diminishes. The reallocation also ensures that the information stored in the memory before the shift in attention is retained which can be lost in both soft and hard attention mechanisms. We illustrate through simulations that the memory that uses the proposed attention mechanism stores a more accurate representation of the variations in the hidden layer values of the neural network (NN). Also, through detailed simulations of various scenarios for two link robot and three link robot arm systems we illustrate the effectiveness of the proposed attention mechanism.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.01189},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/muthirayan et al_2019_improved attention models for memory augmented neural network adaptive controllers.pdf;/home/trung/Zotero/storage/9KX68KYX/1910.html},
  journal = {arXiv:1910.01189 [cs, eess]},
  keywords = {attention,Electrical Engineering and Systems Science - Systems and Control,memory},
  primaryClass = {cs, eess}
}

@inproceedings{nachmani19_UnsupervisedSingingVoice,
  title = {Unsupervised {{Singing Voice Conversion}}},
  booktitle = {Interspeech 2019},
  author = {Nachmani, Eliya and Wolf, Lior},
  year = {2019},
  month = sep,
  pages = {2583--2587},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1761},
  abstract = {We present a deep learning method for singing voice conversion. The proposed network is not conditioned on the text or on the notes, and it directly converts the audio of one singer to the voice of another. Training is performed without any form of supervision: no lyrics or any kind of phonetic features, no notes, and no matching samples between singers. The proposed network employs a single CNN encoder for all singers, a single WaveNet decoder, and a classifier that enforces the latent representation to be singer-agnostic. Each singer is represented by one embedding vector, which the decoder is conditioned on. In order to deal with relatively small datasets, we propose a new data augmentation scheme, as well as new training losses and protocols that are based on backtranslation. Our evaluation presents evidence that the conversion produces natural signing voices that are highly recognizable as the target singer.},
  file = {/home/trung/GoogleDrive/Zotero/nachmani et al_2019_unsupervised singing voice conversion.pdf},
  keywords = {voice conversion},
  language = {en}
}

@article{naesseth17_VariationalSequentialMonte,
  title = {Variational {{Sequential Monte Carlo}}},
  author = {Naesseth, Christian A. and Linderman, Scott W. and Ranganath, Rajesh and Blei, David M.},
  year = {2017},
  month = may,
  abstract = {Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.},
  archivePrefix = {arXiv},
  eprint = {1705.11140},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/naesseth et al_2017_variational sequential monte carlo.pdf;/home/trung/Zotero/storage/TDM4MYPZ/1705.html},
  journal = {arXiv:1705.11140 [stat]},
  keywords = {sequential,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,variational},
  primaryClass = {stat}
}

@article{naesseth20_ReparameterizationGradientsAcceptanceRejection,
  title = {Reparameterization {{Gradients}} through {{Acceptance}}-{{Rejection Sampling Algorithms}}},
  author = {Naesseth, Christian A. and Ruiz, Francisco J. R. and Linderman, Scott W. and Blei, David M.},
  year = {2020},
  month = feb,
  abstract = {Variational inference using the reparameterization trick has enabled large-scale approximate Bayesian inference in complex probabilistic models, leveraging stochastic optimization to sidestep intractable expectations. The reparameterization trick is applicable when we can simulate a random variable by applying a differentiable deterministic function on an auxiliary random variable whose distribution is fixed. For many distributions of interest (such as the gamma or Dirichlet), simulation of random variables relies on acceptance-rejection sampling. The discontinuity introduced by the accept\textendash reject step means that standard reparameterization tricks are not applicable. We propose a new method that lets us leverage reparameterization gradients even when variables are outputs of a acceptance-rejection sampling algorithm. Our approach enables reparameterization on a larger class of variational distributions. In several studies of real and synthetic data, we show that the variance of the estimator of the gradient is significantly lower than other state-of-the-art methods. This leads to faster convergence of stochastic gradient variational inference.},
  archivePrefix = {arXiv},
  eprint = {1610.05683},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/naesseth et al_2020_reparameterization gradients through acceptance-rejection sampling algorithms.pdf},
  journal = {arXiv:1610.05683 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{nagano19_WrappedNormalDistribution,
  title = {A {{Wrapped Normal Distribution}} on {{Hyperbolic Space}} for {{Gradient}}-{{Based Learning}}},
  author = {Nagano, Yoshihiro and Yamaguchi, Shoichiro and Fujita, Yasuhiro and Koyama, Masanori},
  year = {2019},
  month = may,
  abstract = {Hyperbolic space is a geometry that is known to be well-suited for representation learning of data with an underlying hierarchical structure. In this paper, we present a novel hyperbolic distribution called \textbackslash textit\{pseudo-hyperbolic Gaussian\}, a Gaussian-like distribution on hyperbolic space whose density can be evaluated analytically and differentiated with respect to the parameters. Our distribution enables the gradient-based learning of the probabilistic models on hyperbolic space that could never have been considered before. Also, we can sample from this hyperbolic probability distribution without resorting to auxiliary means like rejection sampling. As applications of our distribution, we develop a hyperbolic-analog of variational autoencoder and a method of probabilistic word embedding on hyperbolic space. We demonstrate the efficacy of our distribution on various datasets including MNIST, Atari 2600 Breakout, and WordNet.},
  archivePrefix = {arXiv},
  eprint = {1902.02992},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nagano et al_2019_a wrapped normal distribution on hyperbolic space for gradient-based learning.pdf},
  journal = {arXiv:1902.02992 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{nagrani19_VoxCelebLargescaleSpeaker,
  title = {{{VoxCeleb}}: {{Large}}-Scale {{Speaker Verification}} in the {{Wild}}},
  shorttitle = {{{VoxCeleb}}},
  author = {Nagrani, Arsha and Chung, Joon Son and Xie, Weidi and Zisserman, Andrew},
  year = {2019},
  month = oct,
  pages = {101027},
  issn = {08852308},
  doi = {10.1016/j.csl.2019.101027},
  abstract = {The objective of this work is speaker recognition under noisy and unconstrained conditions. We make two key contributions. First, we introduce a very largescale audio-visual dataset collected from open source media using a fully auotmated pipeline. Most existing datasets for speaker identification contain samples obtained under quite constrained conditions, and usually require manual annotations, hence are limited in size. We propose a pipeline based on computer vision techniques to create the dataset from open-source media. Our pipeline involves obtaining videos from YouTube; performing active speaker verification using a two-stream synchronization Convolutional Neural Network (CNN), and confirming the identity of the speaker using CNN based facial recognition. We use this pipeline to curate VoxCeleb which contains contains over a million `realworld' utterances from over 6,000 speakers. This is several times larger than any publicly available speaker recognition dataset. Second, we develop and compare different CNN architectures with various aggregation methods and training loss functions that can effectively recognise identities from voice under various conditions. The models trained on our dataset surpass the performance of previous works by a significant margin.},
  file = {/home/trung/GoogleDrive/Zotero/nagrani et al_2019_voxceleb.pdf},
  journal = {Computer Speech \& Language},
  language = {en}
}

@article{nair19_CausalInductionVisual,
  title = {Causal {{Induction}} from {{Visual Observations}} for {{Goal Directed Tasks}}},
  author = {Nair, Suraj and Zhu, Yuke and Savarese, Silvio and {Fei-Fei}, Li},
  year = {2019},
  month = oct,
  abstract = {Causal reasoning has been an indispensable capability for humans and other intelligent animals to interact with the physical world. In this work, we propose to endow an artificial agent with the capability of causal reasoning for completing goal-directed tasks. We develop learning-based approaches to inducing causal knowledge in the form of directed acyclic graphs, which can be used to contextualize a learned goal-conditional policy to perform tasks in novel environments with latent causal structures. We leverage attention mechanisms in our causal induction model and goal-conditional policy, enabling us to incrementally generate the causal graph from the agent's visual observations and to selectively use the induced graph for determining actions. Our experiments show that our method effectively generalizes towards completing new tasks in novel environments with previously unseen causal structures.},
  archivePrefix = {arXiv},
  eprint = {1910.01751},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nair et al_2019_causal induction from visual observations for goal directed tasks.pdf;/home/trung/Zotero/storage/T67ZYAUI/1910.html},
  journal = {arXiv:1910.01751 [cs, stat]},
  keywords = {causal,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{nair19_RealMixRealisticSemiSupervised,
  title = {{{RealMix}}: {{Towards Realistic Semi}}-{{Supervised Deep Learning Algorithms}}},
  shorttitle = {{{RealMix}}},
  author = {Nair, Varun and Alonso, Javier Fuentes and Beltramelli, Tony},
  year = {2019},
  month = dec,
  abstract = {Semi-Supervised Learning (SSL) algorithms have shown great potential in training regimes when access to labeled data is scarce but access to unlabeled data is plentiful. However, our experiments illustrate several shortcomings that prior SSL algorithms suffer from. In particular, poor performance when unlabeled and labeled data distributions differ. To address these observations, we develop RealMix, which achieves state-of-the-art results on standard benchmark datasets across different labeled and unlabeled set sizes while overcoming the aforementioned challenges. Notably, RealMix achieves an error rate of 9.79\% on CIFAR10 with 250 labels and is the only SSL method tested able to surpass baseline performance when there is significant mismatch in the labeled and unlabeled data distributions. RealMix demonstrates how SSL can be used in real world situations with limited access to both data and compute and guides further research in SSL with practical applicability in mind.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.08766},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nair et al_2019_realmix.pdf;/home/trung/Zotero/storage/WMS9FXQB/1912.html},
  journal = {arXiv:1912.08766 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{nakagawa20_QuantitativeUnderstandingVAE,
  title = {Quantitative {{Understanding}} of {{VAE}} by {{Interpreting ELBO}} as {{Rate Distortion Cost}} of {{Transform Coding}}},
  author = {Nakagawa, Akira and Kato, Keizo},
  year = {2020},
  month = jul,
  abstract = {VAE (Variational autoencoder) estimates the posterior parameters (mean and variance) of latent variables corresponding to each input data. While it is used for many tasks, the transparency of the model is still an underlying issue. This paper provides a quantitative understanding of VAE property by interpreting ELBO maximization as Rate-distortion optimization of transform coding. According to the Rate-distortion theory, the optimal transform coding is achieved by using PCA-like orthonormal (orthogonal and unit norm) transform. From this analogy, we show theoretically and experimentally that VAE can be mapped to an implicit orthonormal transform with a scale factor derived from the posterior parameter. As a result, the quantitative importance of each latent variable can be evaluated like the eigenvalue of PCA. We can also estimate the data probabilities in the input space from the prior, loss metrics, and corresponding posterior parameters.},
  archivePrefix = {arXiv},
  eprint = {2007.15190},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nakagawa et al_2020_quantitative understanding of vae by interpreting elbo as rate distortion cost of transform coding.pdf},
  journal = {arXiv:2007.15190 [cs, math, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, math, stat}
}

@article{nakkiran19_DeepDoubleDescent,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  year = {2019},
  month = dec,
  abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/Nakkiran et al_2019_Deep Double Descent2.pdf;/home/trung/GoogleDrive/Zotero/nakkiran et al_2019_deep double descent.pdf;/home/trung/Zotero/storage/AYX8H6ID/1912.html;/home/trung/Zotero/storage/PUY9KQAG/1912.html},
  journal = {arXiv:1912.02292 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,disentanglement,favorite,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{nakkiran20_DistributionalGeneralizationNew,
  title = {Distributional {{Generalization}}: {{A New Kind}} of {{Generalization}}},
  shorttitle = {Distributional {{Generalization}}},
  author = {Nakkiran, Preetum and Bansal, Yamini},
  year = {2020},
  month = sep,
  abstract = {We introduce a new notion of generalization-- Distributional Generalization-- which roughly states that outputs of a classifier at train and test time are close *as distributions*, as opposed to close in just their average error. For example, if we mislabel 30\% of dogs as cats in the train set of CIFAR-10, then a ResNet trained to interpolation will in fact mislabel roughly 30\% of dogs as cats on the *test set* as well, while leaving other classes unaffected. This behavior is not captured by classical generalization, which would only consider the average error and not the distribution of errors over the input domain. This example is a specific instance of our much more general conjectures which apply even on distributions where the Bayes risk is zero. Our conjectures characterize the form of distributional generalization that can be expected, in terms of problem parameters (model architecture, training procedure, number of samples, data distribution). We verify the quantitative predictions of these conjectures across a variety of domains in machine learning, including neural networks, kernel machines, and decision trees. These empirical observations are independently interesting, and form a more fine-grained characterization of interpolating classifiers beyond just their test error.},
  archivePrefix = {arXiv},
  eprint = {2009.08092},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nakkiran et al_2020_distributional generalization.pdf},
  journal = {arXiv:2009.08092 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{nalisnick00_ApproximateInferenceDeep,
  title = {Approximate {{Inference}} for {{Deep Latent Gaussian Mixtures}}},
  author = {Nalisnick, Eric and Hertel, Lars and Smyth, Padhraic},
  pages = {4},
  file = {/home/trung/GoogleDrive/Zotero/nalisnick et al_approximate inference for deep latent gaussian mixtures.pdf},
  language = {en}
}

@article{nalisnick17_StickBreakingVariationalAutoencoders,
  title = {Stick-{{Breaking Variational Autoencoders}}},
  author = {Nalisnick, Eric and Smyth, Padhraic},
  year = {2017},
  month = apr,
  abstract = {We extend Stochastic Gradient Variational Bayes to perform posterior inference for the weights of Stick-Breaking processes. This development allows us to define a Stick-Breaking Variational Autoencoder (SB-VAE), a Bayesian nonparametric version of the variational autoencoder that has a latent representation with stochastic dimensionality. We experimentally demonstrate that the SB-VAE, and a semisupervised variant, learn highly discriminative latent representations that often outperform the Gaussian VAE's.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1605.06197},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nalisnick et al_2017_stick-breaking variational autoencoders.pdf},
  journal = {arXiv:1605.06197 [stat]},
  language = {en},
  primaryClass = {stat}
}

@inproceedings{nalisnick19_deepgenerativemodels,
  title = {Do Deep Generative Models Know What They Don't Know?},
  booktitle = {International Conference on Learning Representations},
  author = {Nalisnick, Eric and Matsukawa, Akihiro and Teh, Yee Whye and Gorur, Dilan and Lakshminarayanan, Balaji},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/nalisnick et al_2019_do deep generative models know what they don't know.pdf}
}

@article{nalisnick19_DropoutStructuredShrinkage,
  title = {Dropout as a {{Structured Shrinkage Prior}}},
  author = {Nalisnick, Eric and {Hern{\'a}ndez-Lobato}, Jos{\'e} Miguel and Smyth, Padhraic},
  year = {2019},
  month = may,
  abstract = {Dropout regularization of deep neural networks has been a mysterious yet effective tool to prevent overfitting. Explanations for its success range from the prevention of "co-adapted" weights to it being a form of cheap Bayesian inference. We propose a novel framework for understanding multiplicative noise in neural networks, considering continuous distributions as well as Bernoulli noise (i.e. dropout). We show that multiplicative noise induces structured shrinkage priors on a network's weights. We derive the equivalence through reparametrization properties of scale mixtures and without invoking any approximations. Given the equivalence, we then show that dropout's Monte Carlo training objective approximates marginal MAP estimation. We leverage these insights to propose a novel shrinkage framework for resnets, terming the prior automatic depth determination as it is the natural analog of automatic relevance determination for network depth. Lastly, we investigate two inference strategies that improve upon the aforementioned MAP approximation in regression benchmarks.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1810.04045},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nalisnick et al_2019_dropout as a structured shrinkage prior.pdf},
  journal = {arXiv:1810.04045 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{narwane19_MachineLearningClass,
  title = {Machine {{Learning}} and {{Class Imbalance}}: {{A Literature Survey}}},
  shorttitle = {Machine {{Learning}} and {{Class Imbalance}}},
  author = {Narwane, Swati and Sawarkar, Sudhir},
  year = {2019},
  month = oct,
  volume = {12},
  doi = {10.26488/IEJ.12.10.1202},
  abstract = {The rapid growth in technologies and inexpensive internet connection has increased the volume of data generated. The data generated can be used to derive lots of information and patterns. Data sets are an essential part of the Machine Learning (ML) technique. But modern data sets are suffering from class imbalance. ML does not work very well with unbalanced data sets. In this context, this paper aims to provide a systematic literature review of unbalanced data sets for ML. The collected papers on class imbalance problem for ML were 4 major categories like binary class imbalance, multi-class imbalance, binary and multi-class imbalance, and rare events class imbalance. The survey focused on, various issues in class imbalance for ML. The purpose of the present paper is to help the scholars and readers in understanding the impact of the class imbalance for ML. This article contributes to the role of unbalanced data sets and their impact on the predictive systems.},
  file = {/home/trung/GoogleDrive/Zotero/narwane et al_2019_machine learning and class imbalance.pdf},
  journal = {Industrial Engineering Journal},
  language = {en},
  number = {10}
}

@article{nasersharif20_InformationTheoreticDiscussionConvolutional,
  title = {An {{Information}}-{{Theoretic Discussion}} of {{Convolutional Bottleneck Features}} for {{Robust Speech Recognition}}},
  author = {Nasersharif, B and Naderi, N},
  year = {2020},
  pages = {9},
  abstract = {Convolutional Neural Networks (CNNs) have been shown their performance in speech recognition systems for extracting features, and also acoustic modeling. In addition, CNNs have been used for robust speech recognition and competitive results have been reported. Convolutive Bottleneck Network (CBN) is a kind of CNNs which has a bottleneck layer among its fully connected layers. The bottleneck features extracted by CBNs contain discriminative and rich context information. In this paper, we discuss these bottleneck features from an information theory viewpoint and use them as robust features for noisy speech recognition. In the proposed method, CBN inputs are the noisy logarithm of Mel filter bank energies (LMFBs) in a number of neighbor frames and its outputs are corresponding phone labels. In such a system, we showed that the mutual information between the bottleneck layer and labels are higher than the mutual information between noisy input features and labels. Thus, the bottleneck features are a denoised compressed form of input features which are more representative than input features for discriminating phone classes. Experimental results on the Aurora2 database show that bottleneck features extracted by CBN outperform some conventional speech features and also robust features extracted by CNN.},
  file = {/home/trung/GoogleDrive/Zotero/nasersharif et al_2020_an information-theoretic discussion of convolutional bottleneck features for robust speech recognition.pdf},
  keywords = {_tablet,information},
  language = {en}
}

@article{nash19_AutoregressiveEnergyMachines,
  title = {Autoregressive {{Energy Machines}}},
  author = {Nash, Charlie and Durkan, Conor},
  year = {2019},
  month = apr,
  abstract = {Neural density estimators are flexible families of parametric models which have seen widespread use in unsupervised machine learning in recent years. Maximum-likelihood training typically dictates that these models be constrained to specify an explicit density. However, this limitation can be overcome by instead using a neural network to specify an energy function, or unnormalized density, which can subsequently be normalized to obtain a valid distribution. The challenge with this approach lies in accurately estimating the normalizing constant of the high-dimensional energy function. We propose the Autoregressive Energy Machine, an energy-based model which simultaneously learns an unnormalized density and computes an importance-sampling estimate of the normalizing constant for each conditional in an autoregressive decomposition. The Autoregressive Energy Machine achieves state-of-the-art performance on a suite of density-estimation tasks.},
  archivePrefix = {arXiv},
  eprint = {1904.05626},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nash et al_2019_autoregressive energy machines.pdf},
  journal = {arXiv:1904.05626 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{naud20_ManifoldsUnsupervisedVisual,
  title = {Manifolds for {{Unsupervised Visual Anomaly Detection}}},
  author = {Naud, Louise and Lavin, Alexander},
  year = {2020},
  month = jun,
  abstract = {Anomalies are by definition rare, thus labeled examples are very limited or nonexistent, and likely do not cover unforeseen scenarios. Unsupervised learning methods that don't necessarily encounter anomalies in training would be immensely useful. Generative vision models can be useful in this regard but do not sufficiently represent normal and abnormal data distributions. To this end, we propose constant curvature manifolds for embedding data distributions in unsupervised visual anomaly detection. Through theoretical and empirical explorations of manifold shapes, we develop a novel hyperspherical Variational Auto-Encoder (VAE) via stereographic projections with a gyroplane layer - a complete equivalent to the Poincar\textbackslash 'e VAE. This approach with manifold projections is beneficial in terms of model generalization and can yield more interpretable representations. We present state-of-the-art results on visual anomaly benchmarks in precision manufacturing and inspection, demonstrating real-world utility in industrial AI scenarios. We further demonstrate the approach on the challenging problem of histopathology: our unsupervised approach effectively detects cancerous brain tissue from noisy whole-slide images, learning a smooth, latent organization of tissue types that provides an interpretable decisions tool for medical professionals.},
  archivePrefix = {arXiv},
  eprint = {2006.11364},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/naud et al_2020_manifolds for unsupervised visual anomaly detection.pdf},
  journal = {arXiv:2006.11364 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{naveed11_Searchingmicroblogscoping,
  title = {Searching Microblogs: Coping with Sparsity and Document Quality},
  shorttitle = {Searching Microblogs},
  booktitle = {Proceedings of the 20th {{ACM}} International Conference on {{Information}} and Knowledge Management - {{CIKM}} '11},
  author = {Naveed, Nasir and Gottron, Thomas and Kunegis, J{\'e}r{\^o}me and Alhadi, Arifah Che},
  year = {2011},
  pages = {183},
  publisher = {{ACM Press}},
  address = {{Glasgow, Scotland, UK}},
  doi = {10.1145/2063576.2063607},
  abstract = {Two of the main challenges in retrieval on microblogs are the inherent sparsity of the documents and difficulties in assessing their quality. The feature sparsity is immanent to the restriction of the medium to short texts. Quality assessment is necessary as the microblog documents range from spam over trivia and personal chatter to news broadcasts, information dissemination and reports of current hot topics. In this paper we analyze how these challenges can influence standard retrieval models and propose methods to overcome the problems they pose. We consider the sparsity's effect on document length normalization and introduce interestingness as static quality measure. Our results show that deliberately ignoring length normalization yields better retrieval results in general and that interestingness improves retrieval for underspecified queries.},
  file = {/home/trung/GoogleDrive/Zotero/naveed et al_2011_searching microblogs.pdf},
  isbn = {978-1-4503-0717-8},
  language = {en}
}

@article{nazabal00_HandlingIncompleteHeterogeneous,
  title = {Handling {{Incomplete Heterogeneous Data}} Using {{VAEs}}},
  author = {Nazabal, Alfredo and Olmos, Pablo M and Ghahramani, Zoubin and Valera, Isabel},
  pages = {19},
  abstract = {Variational autoencoders (VAEs), as well as other generative models, have been shown to be efficient and accurate to capture the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications.},
  file = {/home/trung/GoogleDrive/Zotero/nazabal et al_handling incomplete heterogeneous data using vaes.pdf},
  language = {en}
}

@article{nazabal20_DataEngineeringData,
  title = {Data {{Engineering}} for {{Data Analytics}}: {{A Classification}} of the {{Issues}}, and {{Case Studies}}},
  shorttitle = {Data {{Engineering}} for {{Data Analytics}}},
  author = {Nazabal, Alfredo and Williams, Christopher K. I. and Colavizza, Giovanni and Smith, Camila Rangel and Williams, Angus},
  year = {2020},
  month = apr,
  abstract = {Consider the situation where a data analyst wishes to carry out an analysis on a given dataset. It is widely recognized that most of the analyst's time will be taken up with \textbackslash emph\{data engineering\} tasks such as acquiring, understanding, cleaning and preparing the data. In this paper we provide a description and classification of such tasks into high-levels groups, namely data organization, data quality and feature engineering. We also make available four datasets and example analyses that exhibit a wide variety of these problems, to help encourage the development of tools and techniques to help reduce this burden and push forward research towards the automation or semi-automation of the data engineering process.},
  archivePrefix = {arXiv},
  eprint = {2004.12929},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nazabal et al_2020_data engineering for data analytics.pdf},
  journal = {arXiv:2004.12929 [cs]},
  primaryClass = {cs}
}

@misc{neal00_IntroductionCausalInference,
  title = {Introduction to {{Causal Inference}}},
  author = {Neal, Brady},
  abstract = {Upcoming online causal inference course to begin in September 2020.},
  howpublished = {https://www.bradyneal.com/causal-inference-course},
  language = {en}
}

@book{neal96_BayesianLearningNeural,
  title = {Bayesian {{Learning}} for {{Neural Networks}}},
  author = {Neal, Radford M.},
  editor = {Bickel, P. and Diggle, P. and Fienberg, S. and Krickeberg, K. and Olkin, I. and Wermuth, N. and Zeger, S.},
  year = {1996},
  volume = {118},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-0745-0},
  abstract = {Two features distinguish the Bayesian approach to learning models from data. First, beliefs derived from background knowledge are used to select a prior probability distribution for the model parameters. Second, predictions of future observations are made by integrating the model's predictions with respect to the posterior parameter distribution obtained by updating this prior to take account of the data. For neural network models, both these aspects present di culties | the prior over network parameters has no obvious relation to our prior knowledge, and integration over the posterior is computationally very demanding.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/neal_1996_bayesian learning for neural networks.pdf},
  isbn = {978-0-387-94724-2 978-1-4612-0745-0},
  language = {en},
  series = {Lecture {{Notes}} in {{Statistics}}}
}

@article{needham00_GraphAlgorithms,
  title = {Graph {{Algorithms}}},
  author = {Needham, Mark and Hodler, Amy E},
  pages = {266},
  file = {/home/trung/GoogleDrive/Zotero/needham et al_graph algorithms.pdf},
  language = {en}
}

@article{nehlich09_Establishingcollagenquality,
  title = {Establishing Collagen Quality Criteria for Sulphur Isotope Analysis of Archaeological Bone Collagen},
  author = {Nehlich, Olaf and Richards, Michael P.},
  year = {2009},
  month = mar,
  volume = {1},
  pages = {59--75},
  issn = {1866-9557, 1866-9565},
  doi = {10.1007/s12520-009-0003-6},
  abstract = {Sulphur isotope measurements of bone collagen from archaeological sites are beginning to be applied more often, yet there are no clear criteria to assess the quality of the collagen and therefore the validity of the sulphur isotope values. We provide elemental data from different methods (DNA sequences, amino acid sequences and mass spectrometric measurements) which are used to establish a reliable system of quality criteria for sulphur isotope analyses of bone collagen. The difference in the amount of sulphur from fish and mammalian collagen type I led to the suggestion to use different criteria to assess the in vivo character of the collagen between these two categories. For establishing quality ranges, the bone collagen of 140 modern animals were analysed. The amount of sulphur in fish and mammalian bone collagen is 0.63{$\pm$}0.08\% and 0.28 {$\pm$}0.07\%, respectively. Based on these results we define for mammalian bone collagen an atomic C:S ratio of 600{$\pm$}300 and an atomic N:S ratio of 200{$\pm$}100, and for fish bone an atomic C:S ratio of 175{$\pm$}50 and an atomic N:S ratio of 60{$\pm$} 20. These quality criteria were then applied to 305 specimens from different archaeological contexts.},
  annotation = {ZSCC: 0000161},
  file = {/home/trung/Zotero/storage/ZYRGJHVK/Nehlich and Richards - 2009 - Establishing collagen quality criteria for sulphur.pdf},
  journal = {Archaeological and Anthropological Sciences},
  language = {en},
  number = {1}
}

@article{neyshabur20_Whatbeingtransferred,
  title = {What Is Being Transferred in Transfer Learning?},
  author = {Neyshabur, Behnam and Sedghi, Hanie and Zhang, Chiyuan},
  year = {2020},
  month = aug,
  abstract = {One desired capability for machines is the ability to transfer their knowledge of one domain to another where data is (usually) scarce. Despite ample adaptation of transfer learning in various deep learning applications, we yet do not understand what enables a successful transfer and which part of the network is responsible for that. In this paper, we provide new tools and analyses to address these fundamental questions. Through a series of analyses on transferring to block-shuffled images, we separate the effect of feature reuse from learning low-level statistics of data and show that some benefit of transfer learning comes from the latter. We present that when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space.},
  archivePrefix = {arXiv},
  eprint = {2008.11687},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/neyshabur et al_2020_what is being transferred in transfer learning.pdf},
  journal = {arXiv:2008.11687 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ng20_SSMBASelfSupervisedManifold,
  title = {{{SSMBA}}: {{Self}}-{{Supervised Manifold Based Data Augmentation}} for {{Improving Out}}-of-{{Domain Robustness}}},
  shorttitle = {{{SSMBA}}},
  author = {Ng, Nathan and Cho, Kyunghyun and Ghassemi, Marzyeh},
  year = {2020},
  month = sep,
  abstract = {Models that perform well on a training domain often fail to generalize to out-of-domain (OOD) examples. Data augmentation is a common method used to prevent overfitting and improve OOD generalization. However, in natural language, it is difficult to generate new examples that stay on the underlying data manifold. We introduce SSMBA, a data augmentation method for generating synthetic training examples by using a pair of corruption and reconstruction functions to move randomly on a data manifold. We investigate the use of SSMBA in the natural language domain, leveraging the manifold assumption to reconstruct corrupted text with masked language models. In experiments on robustness benchmarks across 3 tasks and 9 datasets, SSMBA consistently outperforms existing data augmentation methods and baseline models on both in-domain and OOD data, achieving gains of 0.8\% accuracy on OOD Amazon reviews, 1.8\% accuracy on OOD MNLI, and 1.4 BLEU on in-domain IWSLT14 German-English.},
  archivePrefix = {arXiv},
  eprint = {2009.10195},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ng et al_2020_ssmba.pdf},
  journal = {arXiv:2009.10195 [cs, stat]},
  keywords = {self-supervised},
  primaryClass = {cs, stat}
}

@article{ngiam00_MultimodalDeepLearning,
  title = {Multimodal {{Deep Learning}}},
  author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
  pages = {8},
  abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned if multiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evaluate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our models are validated on the CUAVE and AVLetters datasets on audio-visual speech classification, demonstrating best published visual speech classification on AVLetters and effective shared representation learning.},
  annotation = {ZSCC: 0001953},
  file = {/home/trung/GoogleDrive/Zotero/ngiam et al_multimodal deep learning.pdf},
  keywords = {favorite},
  language = {en}
}

@inproceedings{ngiam11_Learningdeepenergy,
  title = {Learning Deep Energy Models},
  booktitle = {{{ICML}}},
  author = {Ngiam, Jiquan and Chen, Zhenghao and Koh, Pang Wei and Ng, Andrew Y.},
  year = {2011},
  pages = {1105--1112},
  cdate = {1293840000000},
  crossref = {conf/icml/2011},
  file = {/home/trung/GoogleDrive/Zotero/ngiam et al_2011_learning deep energy models.pdf}
}

@inproceedings{ngo16_DeepLanguagecomprehensive,
  title = {Deep {{Language}}: A Comprehensive Deep Learning Approach to End-to-End Language Recognition},
  shorttitle = {Deep {{Language}}},
  booktitle = {Odyssey 2016},
  author = {Ngo, Trung and Hautam{\"a}ki, Ville and Lee, Kong Aik},
  year = {2016},
  month = jun,
  pages = {109--116},
  doi = {10.21437/Odyssey.2016-16},
  abstract = {This work explores the use of various Deep Neural Network (DNN) architectures for an end-to-end language identification (LID) task. The approach has been proven to significantly improve the state-of-art in many domains include speech recognition, computer vision and genomics. As an end-to-end system, deep learning removes the burden of hand crafting the feature extraction is conventional approach in LID. This versatility is achieved by training a very deep network to learn distributed representations of speech features with multiple levels of abstraction. In this paper, we show that an end-to-end deep learning system can be used to recognize language from speech utterances with various lengths. Our results show that a combination of three deep architectures: feed-forward network, convolutional network and recurrent network can achieve the best performance compared to other network designs. Additionally, we compare our network performance to state-of-the-art BNF-based i-vector system on NIST 2015 Language Recognition Evaluation corpus. Key to our approach is that we effectively address computational and regularization issues into the network structure to build deeper architecture compare to any previous DNN approaches to language recognition task.},
  file = {/home/trung/GoogleDrive/Zotero/ngo et al_2016_deep language.pdf},
  language = {en}
}

@article{ngotrong19_SemisupervisedGenerativeAutoencoder,
  title = {Semisupervised {{Generative Autoencoder}} for {{Single}}-{{Cell Data}}},
  author = {Ngo Trong, Trung and Mehtonen, Juha and Gonz{\'a}lez, Gerardo and Kramer, Roger and Hautam{\"a}ki, Ville and Hein{\"a}niemi, Merja},
  year = {2019},
  month = dec,
  doi = {10.1089/cmb.2019.0337},
  abstract = {Single-cell transcriptomics offers a tool to study the diversity of cell phenotypes through snapshots of the abundance of mRNA in individual cells. Often there is additional information available besides the single-cell gene expression counts, such as bulk transcriptome data from the same tissue, or quantification of surface protein levels from the same cells. In this study, we propose models based on the Bayesian deep learning approach, where protein quantification, available as CITE-seq counts, from the same cells is used to constrain the learning process, thus forming a SemI-SUpervised generative Autoencoder (SISUA) model. The generative model is based on the deep variational autoencoder (VAE) neural network architecture.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/ngo trong et al_2019_semisupervised generative autoencoder for single-cell data.pdf;/home/trung/Zotero/storage/UBJM9MAY/cmb.2019.html},
  journal = {Journal of Computational Biology}
}

@article{ngotrong19_SISUASemiSupervisedGenerative,
  title = {{{SISUA}}: {{Semi}}-{{Supervised Generative Autoencoder}} for {{Single Cell Data}}},
  shorttitle = {{{SISUA}}},
  author = {Ngo Trong, Trung and Kramer, Roger and Mehtonen, Juha and Gonz{\'a}lez, Gerardo and Hautam{\"a}ki, Ville and Hein{\"a}niemi, Merja},
  year = {2019},
  month = may,
  doi = {10.1101/631382},
  abstract = {Single-cell transcriptomics offers a tool to study the diversity of cell phenotypes through snapshots of the abundance of mRNA in individual cells. Often there is additional information available besides the single cell gene expression counts, such as bulk transcriptome data from the same tissue, or quantification of surface protein levels from the same cells. In this study, we propose models based on the Bayesian generative approach, where protein quantification available as CITEseq counts from the same cells are used to constrain the learning process, thus forming a semisupervised model. The generative model is based on the deep variational autoencoder (VAE) neural network architecture.},
  file = {/home/trung/GoogleDrive/Zotero/ngo trong et al_2019_sisua.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{nguyen-phuoc19_HoloGANUnsupervisedlearning,
  title = {{{HoloGAN}}: {{Unsupervised}} Learning of {{3D}} Representations from Natural Images},
  shorttitle = {{{HoloGAN}}},
  author = {{Nguyen-Phuoc}, Thu and Li, Chuan and Theis, Lucas and Richardt, Christian and Yang, Yong-Liang},
  year = {2019},
  month = oct,
  abstract = {We propose a novel generative adversarial network (GAN) for the task of unsupervised learning of 3D representations from natural images. Most generative models rely on 2D kernels to generate images and make few assumptions about the 3D world. These models therefore tend to create blurry images or artefacts in tasks that require a strong 3D understanding, such as novel-view synthesis. HoloGAN instead learns a 3D representation of the world, and to render this representation in a realistic manner. Unlike other GANs, HoloGAN provides explicit control over the pose of generated objects through rigid-body transformations of the learnt 3D features. Our experiments show that using explicit 3D features enables HoloGAN to disentangle 3D pose and identity, which is further decomposed into shape and appearance, while still being able to generate images with similar or higher visual quality than other generative models. HoloGAN can be trained end-to-end from unlabelled 2D images only. In particular, we do not require pose labels, 3D shapes, or multiple views of the same objects. This shows that HoloGAN is the first generative model that learns 3D representations from natural images in an entirely unsupervised manner.},
  annotation = {ZSCC: 0000005},
  archivePrefix = {arXiv},
  eprint = {1904.01326},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nguyen-phuoc et al_2019_hologan.pdf},
  journal = {arXiv:1904.01326 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{nguyen00_ConnectedSublevelSets,
  title = {On {{Connected Sublevel Sets}} in {{Deep Learning}}},
  author = {Nguyen, Quynh},
  pages = {16},
  abstract = {This paper shows that every sublevel set of the loss function of a class of deep overparameterized neural nets with piecewise linear activation functions is connected and unbounded. This implies that the loss has no bad local valleys and all of its global minima are connected within a unique and potentially very large global valley.},
  file = {/home/trung/GoogleDrive/Zotero/nguyen_on connected sublevel sets in deep learning.pdf},
  language = {en}
}

@article{nguyen00_HyperVAEMinimumDescription,
  title = {{{HyperVAE}}: {{A Minimum Description Length Variational Hyper}}-{{Encoding Network}}},
  author = {Nguyen, Phuoc and Tran, Truyen and Gupta, Sunil and Rana, Santu and Dam, Hieu-Chi},
  pages = {7},
  abstract = {We propose a framework called HyperVAE for encoding distributions of distributions. When a target distribution is modeled by a VAE, its neural network parameters \texttheta{} is drawn from a distribution p(\texttheta ) which is modeled by a hyper-level VAE. We propose a variational inference using Gaussian mixture models to implicitly encode the parameters \texttheta{} into a low dimensional Gaussian distribution. Given a target distribution, we predict the posterior distribution of the latent code, then use a matrix-network decoder to generate a posterior distribution q(\texttheta ). HyperVAE can encode the parameters \texttheta{} in full in contrast to common hyper-networks practices, which generate only the scale and bias vectors as target-network parameters. Thus HyperVAE preserves much more information about the model for each task in the latent space. We discuss HyperVAE using the minimum description length (MDL) principle and show that it helps HyperVAE to generalize. We evaluate HyperVAE in density estimation tasks, outlier detection and discovery of novel design classes, demonstrating its efficacy.},
  file = {/home/trung/GoogleDrive/Zotero/nguyen et al_hypervae.pdf},
  language = {en}
}

@article{nguyen00_TransformersTearsImproving,
  title = {Transformers without {{Tears}}: {{Improving}} the {{Normalization}} of {{Self}}-{{Attention}}},
  shorttitle = {Transformers without {{Tears}}},
  author = {Nguyen, Toan Q. and Salazar, Julian},
  doi = {10.5281/zenodo.3525484},
  abstract = {We evaluate three simple, normalization-centric changes to improve Transformer training. First, we show that pre-norm residual connections (PreNorm) and smaller initializations enable warmup-free, validation-based training with large learning rates. Second, we propose \$\textbackslash ell\_2\$ normalization with a single scale parameter (ScaleNorm) for faster training and better performance. Finally, we reaffirm the effectiveness of normalizing word embeddings to a fixed length (FixNorm). On five low-resource translation pairs from TED Talks-based corpora, these changes always converge, giving an average +1.1 BLEU over state-of-the-art bilingual baselines and a new 32.8 BLEU on IWSLT'15 English-Vietnamese. We observe sharper performance curves, more consistent gradient norms, and a linear relationship between activation scaling and decoder depth. Surprisingly, in the high-resource setting (WMT'14 English-German), ScaleNorm and FixNorm remain competitive but PreNorm degrades performance.},
  archivePrefix = {arXiv},
  eprint = {1910.05895},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nguyen et al_transformers without tears.pdf},
  journal = {arXiv:1910.05895 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{nguyen10_EstimatingDivergenceFunctionals,
  title = {Estimating {{Divergence Functionals}} and the {{Likelihood Ratio}} by {{Convex Risk Minimization}}},
  author = {Nguyen, XuanLong and Wainwright, Martin J. and Jordan, Michael I.},
  year = {2010},
  month = nov,
  volume = {56},
  pages = {5847--5861},
  issn = {0018-9448, 1557-9654},
  doi = {10.1109/TIT.2010.2068870},
  abstract = {We develop and analyze -estimation methods for divergence functionals and the likelihood ratios of two probability distributions. Our method is based on a nonasymptotic variational characterization of -divergences, which allows the problem of estimating divergences to be tackled via convex empirical risk optimization. The resulting estimators are simple to implement, requiring only the solution of standard convex programs. We present an analysis of consistency and convergence for these estimators. Given conditions only on the ratios of densities, we show that our estimators can achieve optimal minimax rates for the likelihood ratio and the divergence functionals in certain regimes. We derive an efficient optimization algorithm for computing our estimates, and illustrate their convergence behavior and practical viability by simulations.},
  annotation = {ZSCC: 0000317},
  file = {/home/trung/Zotero/storage/N28EM7M8/Nguyen et al. - 2010 - Estimating Divergence Functionals and the Likeliho.pdf},
  journal = {IEEE Transactions on Information Theory},
  language = {en},
  number = {11}
}

@article{nguyen18_SingleCellRNA,
  title = {Single {{Cell RNA Sequencing}} of {{Rare Immune Cell Populations}}},
  author = {Nguyen, Akira and Khoo, Weng Hua and Moran, Imogen and Croucher, Peter I. and Phan, Tri Giang},
  year = {2018},
  month = jul,
  volume = {9},
  issn = {1664-3224},
  doi = {10.3389/fimmu.2018.01553},
  abstract = {Single-cell RNA sequencing (scRNA-Seq) is transforming our ability to characterize cells, particularly rare cells that are often overlooked in bulk population analytical approaches. This has lead to the discovery of new cell types and cellular states that echo the underlying heterogeneity and plasticity in the immune system. Technologies for the capture, sequencing, and bioinformatic analysis of single cells are rapidly improving, and scRNA-Seq is now becoming much more accessible to non-specialized laboratories. Here, we describe our experiences in adopting scRNA-Seq to the study of rare immune cells in their microanatomical niches.},
  annotation = {ZSCC: 0000026},
  file = {/home/trung/GoogleDrive/Zotero/nguyen et al_2018_single cell rna sequencing of rare immune cell populations.pdf},
  journal = {Frontiers in Immunology},
  pmcid = {PMC6039576},
  pmid = {30022984}
}

@article{nguyen19_Howexplainneural,
  title = {How to Explain Neural Network Decisions?},
  author = {Nguyen, Anh},
  year = {2019},
  pages = {46},
  file = {/home/trung/GoogleDrive/Zotero/nguyen_2019_how to explain neural network decisions.pdf},
  language = {en}
}

@article{nguyen19_UnderstandingCatastrophicForgetting,
  title = {Toward {{Understanding Catastrophic Forgetting}} in {{Continual Learning}}},
  author = {Nguyen, Cuong V. and Achille, Alessandro and Lam, Michael and Hassner, Tal and Mahadevan, Vijay and Soatto, Stefano},
  year = {2019},
  month = aug,
  abstract = {We study the relationship between catastrophic forgetting and properties of task sequences. In particular, given a sequence of tasks, we would like to understand which properties of this sequence influence the error rates of continual learning algorithms trained on the sequence. To this end, we propose a new procedure that makes use of recent developments in task space modeling as well as correlation analysis to specify and analyze the properties we are interested in. As an application, we apply our procedure to study two properties of a task sequence: (1) total complexity and (2) sequential heterogeneity. We show that error rates are strongly and positively correlated to a task sequence's total complexity for some state-of-the-art algorithms. We also show that, surprisingly, the error rates have no or even negative correlations in some cases to sequential heterogeneity. Our findings suggest directions for improving continual learning benchmarks and methods.},
  archivePrefix = {arXiv},
  eprint = {1908.01091},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/NN5Y6LWH/Nguyen et al. - 2019 - Toward Understanding Catastrophic Forgetting in Co.pdf},
  journal = {arXiv:1908.01091 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{nguyen19_UseCapsuleNetwork,
  title = {Use of a {{Capsule Network}} to {{Detect Fake Images}} and {{Videos}}},
  author = {Nguyen, Huy H. and Yamagishi, Junichi and Echizen, Isao},
  year = {2019},
  month = oct,
  abstract = {The revolution in computer hardware, especially in graphics processing units and tensor processing units, has enabled significant advances in computer graphics and artificial intelligence algorithms. In addition to their many beneficial applications in daily life and business, computer-generated/manipulated images and videos can be used for malicious purposes that violate security systems, privacy, and social trust. The deepfake phenomenon and its variations enable a normal user to use his or her personal computer to easily create fake videos of anybody from a short real online video. Several countermeasures have been introduced to deal with attacks using such videos. However, most of them are targeted at certain domains and are ineffective when applied to other domains or new attacks. In this paper, we introduce a capsule network that can detect various kinds of attacks, from presentation attacks using printed images and replayed videos to attacks using fake videos created using deep learning. It uses many fewer parameters than traditional convolutional neural networks with similar performance. Moreover, we explain, for the first time ever in the literature, the theory behind the application of capsule networks to the forensics problem through detailed analysis and visualization.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.12467},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nguyen et al_2019_use of a capsule network to detect fake images and videos.pdf;/home/trung/Zotero/storage/C597TKV4/1910.html},
  journal = {arXiv:1910.12467 [cs]},
  keywords = {capsule,Computer Science - Computer Vision and Pattern Recognition,deepfake},
  primaryClass = {cs}
}

@article{nguyen20_SemixupOutofManifoldRegularization,
  title = {Semixup: {{In}}- and {{Out}}-of-{{Manifold Regularization}} for {{Deep Semi}}-{{Supervised Knee Osteoarthritis Severity Grading}} from {{Plain Radiographs}}},
  shorttitle = {Semixup},
  author = {Nguyen, Huy Hoang and Saarakkala, Simo and Blaschko, Matthew and Tiulpin, Aleksei},
  year = {2020},
  month = aug,
  abstract = {Knee osteoarthritis (OA) is one of the highest disability factors in the world. This musculoskeletal disorder is assessed from clinical symptoms, and typically confirmed via radiographic assessment. This visual assessment done by a radiologist requires experience, and suffers from moderate to high inter-observer variability. The recent literature has shown that deep learning methods can reliably perform the OA severity assessment according to the gold standard Kellgren-Lawrence (KL) grading system. However, these methods require large amounts of labeled data, which are costly to obtain. In this study, we propose the Semixup algorithm, a semi-supervised learning (SSL) approach to leverage unlabeled data. Semixup relies on consistency regularization using in- and out-of-manifold samples, together with interpolated consistency. On an independent test set, our method significantly outperformed other state-of-the-art SSL methods in most cases. Finally, when compared to a welltuned fully supervised baseline that yielded a balanced accuracy (BA) of 70.9 {$\pm$} 0.8\% on the test set, Semixup had comparable performance \textendash{} BA of 71 {$\pm$} 0.8\% (p = 0.368) while requiring 6 times less labeled data. These results show that our proposed SSL method allows building fully automatic OA severity assessment tools with datasets that are available outside research settings.},
  archivePrefix = {arXiv},
  eprint = {2003.01944},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nguyen et al_2020_semixup.pdf},
  journal = {arXiv:2003.01944 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{nguyen20_VariationalDeepLearning,
  title = {Variational {{Deep Learning}} for the {{Identification}} and {{Reconstruction}} of {{Chaotic}} and {{Stochastic Dynamical Systems}} from {{Noisy}} and {{Partial Observations}}},
  author = {Nguyen, Duong and Ouala, Said and Drumetz, Lucas and Fablet, Ronan},
  year = {2020},
  month = sep,
  abstract = {The data-driven recovery of the unknown governing equations of dynamical systems has recently received an increasing interest. However, the identification of the governing equations remains challenging when dealing with noisy and partial observations. Here, we address this challenge and investigate variational deep learning schemes. Within the proposed framework, we jointly learn an inference model to reconstruct the true states of the system from series of noisy and partial data and the governing equations of these states. In doing so, this framework bridges classical data assimilation and state-of-the-art machine learning techniques and we show that it generalizes state-of-the-art methods. Importantly, both the inference model and the governing equations embed stochastic components to account for stochastic variabilities, model errors and reconstruction uncertainties. Various experiments on chaotic and stochastic dynamical systems support the relevance of our scheme w.r.t. state-of-the-art approaches.},
  archivePrefix = {arXiv},
  eprint = {2009.02296},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nguyen et al_2020_variational deep learning for the identification and reconstruction of chaotic and stochastic dynamical systems from noisy and partial observations.pdf},
  journal = {arXiv:2009.02296 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{nickel17_PoincarEmbeddingsLearning,
  title = {Poincar\textbackslash 'e {{Embeddings}} for {{Learning Hierarchical Representations}}},
  author = {Nickel, Maximilian and Kiela, Douwe},
  year = {2017},
  month = may,
  abstract = {Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, while complex symbolic datasets often exhibit a latent hierarchical structure, state-of-the-art methods typically learn embeddings in Euclidean vector spaces, which do not account for this property. For this purpose, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space -- or more precisely into an n-dimensional Poincar\textbackslash 'e ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We introduce an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincar\textbackslash 'e embeddings outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability.},
  archivePrefix = {arXiv},
  eprint = {1705.08039},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nickel et al_2017_poincar-'e embeddings for learning hierarchical representations.pdf},
  journal = {arXiv:1705.08039 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{nie20_ImprovedSemiSupervisedVAE,
  title = {An {{Improved Semi}}-{{Supervised VAE}} for {{Learning Disentangled Representations}}},
  author = {Nie, Weili and Wang, Zichao and Patel, Ankit B. and Baraniuk, Richard G.},
  year = {2020},
  month = jun,
  abstract = {Learning interpretable and disentangled representations is a crucial yet challenging task in representation learning. In this work, we focus on semi-supervised disentanglement learning and extend work by Locatello et al. (2019) by introducing another source of supervision that we denote as label replacement. Specifically, during training, we replace the inferred representation associated with a data point with its ground-truth representation whenever it is available. Our extension is theoretically inspired by our proposed general framework of semi-supervised disentanglement learning in the context of VAEs which naturally motivates the supervised terms commonly used in existing semi-supervised VAEs (but not for disentanglement learning). Extensive experiments on synthetic and real datasets demonstrate both quantitatively and qualitatively the ability of our extension to significantly and consistently improve disentanglement with very limited supervision.},
  archivePrefix = {arXiv},
  eprint = {2006.07460},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2020/false},
  journal = {arXiv:2006.07460 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{nie20_ImprovedSemiSupervisedVAEa,
  title = {An {{Improved Semi}}-{{Supervised VAE}} for {{Learning Disentangled Representations}}},
  author = {Nie, Weili and Wang, Zichao and Patel, Ankit B. and Baraniuk, Richard G.},
  year = {2020},
  month = jun,
  abstract = {Learning interpretable and disentangled representations is a crucial yet challenging task in representation learning. In this work, we focus on semi-supervised disentanglement learning and extend work by Locatello et al. (2019) by introducing another source of supervision that we denote as label replacement. Specifically, during training, we replace the inferred representation associated with a data point with its ground-truth representation whenever it is available. Our extension is theoretically inspired by our proposed general framework of semi-supervised disentanglement learning in the context of VAEs which naturally motivates the supervised terms commonly used in existing semi-supervised VAEs (but not for disentanglement learning). Extensive experiments on synthetic and real datasets demonstrate both quantitatively and qualitatively the ability of our extension to significantly and consistently improve disentanglement with very limited supervision.},
  archivePrefix = {arXiv},
  eprint = {2006.07460},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nie et al_2020_an improved semi-supervised vae for learning disentangled representations.pdf},
  journal = {arXiv:2006.07460 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{nie20_SemiSupervisedStyleGANDisentanglement,
  title = {Semi-{{Supervised StyleGAN}} for {{Disentanglement Learning}}},
  author = {Nie, Weili and Karras, Tero and Garg, Animesh and Debnath, Shoubhik and Patney, Anjul and Patel, Ankit B. and Anandkumar, Anima},
  year = {2020},
  month = apr,
  abstract = {Disentanglement learning is crucial for obtaining disentangled representations and controllable generation. Current disentanglement methods face several inherent limitations: difficulty with high-resolution images, primarily on learning disentangled representations, and non-identifiability due to the unsupervised setting. To alleviate these limitations, we design new architectures and loss functions based on StyleGAN (Karras et al., 2019), for semi-supervised high-resolution disentanglement learning. We create two complex high-resolution synthetic datasets for systematic testing. We investigate the impact of limited supervision and find that using only 0.25\%\textasciitilde 2.5\% of labeled data is sufficient for good disentanglement on both synthetic and real datasets. We propose new metrics to quantify generator controllability, and observe there may exist a crucial trade-off between disentangled representation learning and controllable generation. We also consider semantic fine-grained image editing to achieve better generalization to unseen images.},
  archivePrefix = {arXiv},
  eprint = {2003.03461},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nie et al_2020_semi-supervised stylegan for disentanglement learning.pdf},
  journal = {arXiv:2003.03461 [cs]},
  primaryClass = {cs}
}

@article{nielsen20_SurVAEFlowsSurjections,
  title = {{{SurVAE Flows}}: {{Surjections}} to {{Bridge}} the {{Gap}} between {{VAEs}} and {{Flows}}},
  shorttitle = {{{SurVAE Flows}}},
  author = {Nielsen, Didrik and Jaini, Priyank and Hoogeboom, Emiel and Winther, Ole and Welling, Max},
  year = {2020},
  month = jul,
  abstract = {Normalizing flows and variational autoencoders are powerful generative models that can represent complicated density functions. However, they both impose constraints on the models: Normalizing flows use bijective transformations to model densities whereas VAEs learn stochastic transformations that are non-invertible and thus typically do not provide tractable estimates of the marginal likelihood. In this paper, we introduce SurVAE Flows: A modular framework of composable transformations that encompasses VAEs and normalizing flows. SurVAE Flows bridge the gap between normalizing flows and VAEs with surjective transformations, wherein the transformations are deterministic in one direction \textendash{} thereby allowing exact likelihood computation, and stochastic in the reverse direction \textendash{} hence providing a lower bound on the corresponding likelihood. We show that several recently proposed methods, including dequantization and augmented normalizing flows, can be expressed as SurVAE Flows. Finally, we introduce common operations such as the max value, the absolute value, sorting and stochastic permutation as composable layers in SurVAE Flows.},
  archivePrefix = {arXiv},
  eprint = {2007.02731},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nielsen et al_2020_survae flows.pdf},
  journal = {arXiv:2007.02731 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{nieuwenhuis18_TotalNutTree,
  title = {Total {{Nut}}, {{Tree Nut}}, {{Peanut}}, and {{Peanut Butter Consumption}} and the {{Risk}} of {{Pancreatic Cancer}} in the {{Netherlands Cohort Study}}},
  author = {Nieuwenhuis, Lisette and {van den Brandt}, Piet A.},
  year = {2018},
  month = mar,
  volume = {27},
  pages = {274--284},
  issn = {1055-9965, 1538-7755},
  doi = {10.1158/1055-9965.EPI-17-0448},
  abstract = {Background: Nut intake has been associated with decreased cancer-related mortality, but few studies have examined the potential of nuts in the chemoprevention of pancreatic cancer. We prospectively investigated the association of total nut, tree nut, peanut, and peanut butter consumption with pancreatic cancer risk.},
  annotation = {ZSCC: 0000005},
  file = {/home/trung/GoogleDrive/Zotero/nieuwenhuis et al_2018_total nut, tree nut, peanut, and peanut butter consumption and the risk of pancreatic cancer in the netherlands cohort study.pdf},
  journal = {Cancer Epidemiology Biomarkers \& Prevention},
  language = {en},
  number = {3}
}

@article{ning18_NonparametricTopicModeling,
  title = {Nonparametric {{Topic Modeling}} with {{Neural Inference}}},
  author = {Ning, Xuefei and Zheng, Yin and Jiang, Zhuxi and Wang, Yu and Yang, Huazhong and Huang, Junzhou},
  year = {2018},
  month = jun,
  abstract = {This work focuses on combining nonparametric topic models with Auto-Encoding Variational Bayes (AEVB). Specifically, we first propose iTM-VAE, where the topics are treated as trainable parameters and the document-specific topic proportions are obtained by a stick-breaking construction. The inference of iTM-VAE is modeled by neural networks such that it can be computed in a simple feed-forward manner. We also describe how to introduce a hyper-prior into iTM-VAE so as to model the uncertainty of the prior parameter. Actually, the hyper-prior technique is quite general and we show that it can be applied to other AEVB based models to alleviate the collapse-to-prior problem elegantly. Moreover, we also propose HiTM-VAE, where the document-specific topic distributions are generated in a hierarchical manner. HiTM-VAE is even more flexible and can generate topic distributions with better variability. Experimental results on 20News and Reuters RCV1-V2 datasets show that the proposed models outperform the state-of-theart baselines significantly. The advantages of the hyper-prior technique and the hierarchical model construction are also confirmed by experiments.},
  archivePrefix = {arXiv},
  eprint = {1806.06583},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ning et al_2018_nonparametric topic modeling with neural inference.pdf},
  journal = {arXiv:1806.06583 [cs]},
  language = {en},
  primaryClass = {cs}
}

@incollection{NIPS2019_8387,
  title = {Explicit Disentanglement of Appearance and Perspective in Generative Models},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Skafte, Nicki and Hauberg, S{\o}ren},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {1018--1028},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/skafte et al_2019_explicit disentanglement of appearance and perspective in generative models.pdf},
  keywords = {disentanglement}
}

@incollection{NIPS2019_8547,
  title = {Identifying Causal Effects via Context-Specific Independence Relations},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Tikka, Santtu and Hyttinen, Antti and Karvanen, Juha},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and {dAlch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {2804--2814},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/tikka et al_2019_identifying causal effects via context-specific independence relations.pdf},
  keywords = {causal}
}

@article{niu11_HOGWILDLockFreeApproach,
  title = {{{HOGWILD}}!: {{A Lock}}-{{Free Approach}} to {{Parallelizing Stochastic Gradient Descent}}},
  shorttitle = {{{HOGWILD}}!},
  author = {Niu, Feng and Recht, Benjamin and Re, Christopher and Wright, Stephen J.},
  year = {2011},
  month = nov,
  abstract = {Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude.},
  archivePrefix = {arXiv},
  eprint = {1106.5730},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/niu et al_2011_hogwild.pdf},
  journal = {arXiv:1106.5730 [cs, math]},
  keywords = {favorite},
  primaryClass = {cs, math}
}

@article{norlander19_Latentspaceconditioning,
  title = {Latent Space Conditioning for Improved Classification and Anomaly Detection},
  author = {Norlander, Erik and Sopasakis, Alexandros},
  year = {2019},
  month = nov,
  abstract = {We propose a new type of variational autoencoder to perform improved pre-processing for clustering and anomaly detection on data with a given label. Anomalies however are not known or labeled. We call our method conditional latent space variational autonencoder since it separates the latent space by conditioning on information within the data. The method fits one prior distribution to each class in the dataset, effectively expanding the prior distribution to include a Gaussian mixture model. Our approach is compared against the capabilities of a typical variational autoencoder by measuring their V-score during cluster formation with respect to the k-means and EM algorithms. For anomaly detection, we use a new metric composed of the mass-volume and excess-mass curves which can work in an unsupervised setting. We compare the results between established methods such as as isolation forest, local outlier factor and one-class support vector machine.},
  archivePrefix = {arXiv},
  eprint = {1911.10599},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/norlander et al_2019_latent space conditioning for improved classification and anomaly detection.pdf},
  journal = {arXiv:1911.10599 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{norouzi20_ExemplarbasedGeneration,
  title = {Exemplar Based {{Generation}} and {{Data Augmentation}} Using {{Exemplar VAEs}}},
  author = {Norouzi, Sajad and Fleet, David J. and Norouzi, Mohammad},
  year = {2020},
  month = jul,
  abstract = {This paper combines the advantages of parametric and non-parametric, exemplar based generative models using variational inference, yielding a new generative model called Exemplar VAE. This is a variant of VAE with a non-parametric Parzen window prior in the latent space. To sample from it, one first draws a random exemplar from training data, then stochastically transforms the exemplar into a latent code and a new observation. We propose Retrieval Augmented Training (RAT) that uses approximate nearest neighbor search in the latent space to speed up training based on a novel lower bound on log marginal likelihood. To enhance generalization, model parameters are learned using exemplar leave-one-out and subsampling. Experiments demonstrate the effectiveness of Exemplar VAEs on density estimation and representation learning. Further, generative data augmentation using Exemplar VAEs on permutation invariant MNIST and Fashion MNIST reduces classification error from 1.23 to 0.69 and 8.56 to 8.16.},
  archivePrefix = {arXiv},
  eprint = {2004.04795},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/norouzi et al_2020_exemplar based generation and data augmentation using exemplar vaes.pdf},
  journal = {arXiv:2004.04795 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{norouzi20_sajadnposteriorcollapselist,
  title = {Sajadn/Posterior-Collapse-List},
  author = {Norouzi, Sajad},
  year = {2020},
  month = jan,
  abstract = {A curated list of techniques to avoid posterior collapse},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{norris00_AcademicWritingEnglish,
  title = {Academic {{Writing}} in {{English}}},
  author = {Norris, Carolyn Brimley},
  pages = {86},
  annotation = {ZSCC: 0000033},
  file = {/home/trung/Zotero/storage/GAS26CWY/Norris - Academic Writing in English.pdf},
  language = {en}
}

@article{nozawa20_PACBayesianContrastiveUnsupervised,
  title = {{{PAC}}-{{Bayesian Contrastive Unsupervised Representation Learning}}},
  author = {Nozawa, Kento and Germain, Pascal and Guedj, Benjamin},
  year = {2020},
  month = mar,
  abstract = {Contrastive unsupervised representation learning (CURL) is the state-of-the-art technique to learn representations (as a set of features) from unlabelled data. While CURL has collected several empirical successes recently, theoretical understanding of its performance was still missing. In a recent work, Arora et al. (2019) provide the first generalisation bounds for CURL, relying on a Rademacher complexity. We extend their framework to the flexible PAC-Bayes setting, allowing us to deal with the non-iid setting. We present PAC-Bayesian generalisation bounds for CURL, which are then used to derive a new representation learning algorithm. Numerical experiments on real-life datasets illustrate that our algorithm achieves competitive accuracy, and yields non-vacuous generalisation bounds.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.04464},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/nozawa et al_2020_pac-bayesian contrastive unsupervised representation learning.pdf;/home/trung/Zotero/storage/792HPBDW/1910.html},
  journal = {arXiv:1910.04464 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{ntranos19_discriminativelearningapproach,
  title = {A Discriminative Learning Approach to Differential Expression Analysis for Single-Cell {{RNA}}-Seq},
  author = {Ntranos, Vasilis and Yi, Lynn and Melsted, P{\'a}ll and Pachter, Lior},
  year = {2019},
  month = feb,
  volume = {16},
  pages = {163--166},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-018-0303-9},
  annotation = {ZSCC: 0000018},
  file = {/home/trung/Zotero/storage/VD3CY49X/Ntranos et al. - 2019 - A discriminative learning approach to differential.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {2}
}

@article{nugroho20_surveyrecentmethods,
  title = {A Survey of Recent Methods on Deriving Topics from {{Twitter}}: Algorithm to Evaluation},
  shorttitle = {A Survey of Recent Methods on Deriving Topics from {{Twitter}}},
  author = {Nugroho, Robertus and Paris, Cecile and Nepal, Surya and Yang, Jian and Zhao, Weiliang},
  year = {2020},
  month = jul,
  volume = {62},
  pages = {2485--2519},
  issn = {0219-1377, 0219-3116},
  doi = {10.1007/s10115-019-01429-z},
  abstract = {In recent years, studies related to topic derivation in Twitter have gained a lot of interest from businesses and academics. The interconnection between users and information has made social media, especially Twitter, an ultimate platform for propagation of information about events in real time. Many applications require topic derivation from this social media platform. These include, for example, disaster management, outbreak detection, situation awareness, surveillance, and market analysis. Deriving topics from Twitter is challenging due to the short content of the individual posts. The environment itself is also highly dynamic. This paper presents a review of recent methods proposed to derive topics from social media platform from algorithms to evaluations. With regard to algorithms, we classify them based on the features they exploit, such as content, social interactions, and temporal aspects. In terms of evaluations, we discuss the datasets and metrics generally used to evaluate the methods. Finally, we highlight the gaps in the research this far and the problems that remain to be addressed.},
  file = {/home/trung/GoogleDrive/Zotero/nugroho et al_2020_a survey of recent methods on deriving topics from twitter.pdf},
  journal = {Knowledge and Information Systems},
  language = {en},
  number = {7}
}

@article{obermeyer19_FunctionalTensorsProbabilistic,
  title = {Functional {{Tensors}} for {{Probabilistic Programming}}},
  author = {Obermeyer, Fritz and Bingham, Eli and Jankowiak, Martin and Phan, Du and Chen, Jonathan P.},
  year = {2019},
  month = oct,
  abstract = {It is a significant challenge to design probabilistic programming systems that can accommodate a wide variety of inference strategies within a unified framework. Noting that the versatility of modern automatic differentiation frameworks is based in large part on the unifying concept of tensors, we describe a software abstraction --functional tensors-- that captures many of the benefits of tensors, while also being able to describe continuous probability distributions. Moreover, functional tensors are a natural candidate for generalized variable elimination and parallel-scan filtering algorithms that enable parallel exact inference for a large family of tractable modeling motifs. We demonstrate the versatility of functional tensors by integrating them into the modeling frontend and inference backend of the Pyro programming language. In experiments we show that the resulting framework enables a large variety of inference strategies, including those that mix exact and approximate inference.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1910.10775},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/obermeyer et al_2019_functional tensors for probabilistic programming.pdf;/home/trung/Zotero/storage/H2FBY94N/1910.html},
  journal = {arXiv:1910.10775 [cs, stat]},
  keywords = {Computer Science - Machine Learning,probabilistic programming,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{obrien18_LanguageFakeNews,
  title = {The {{Language}} of {{Fake News}}: {{Opening}} the {{Black}}-{{Box}} of {{Deep Learning Based Detectors}}},
  author = {O'Brien, Nicole and Latessa, Sophia and Evangelopoulos, Georgios and Boix, Xavier},
  year = {2018},
  pages = {5},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/GoogleDrive/Zotero/o’brien et al_2018_the language of fake news.pdf},
  language = {en}
}

@article{odena16_ConditionalImageSynthesis,
  title = {Conditional {{Image Synthesis With Auxiliary Classifier GANs}}},
  author = {Odena, Augustus and Olah, Christopher and Shlens, Jonathon},
  year = {2016},
  month = oct,
  abstract = {In this paper we introduce new methods for the improved training of generative adversarial networks (GANs) for image synthesis. We construct a variant of GANs employing label conditioning that results in 128 \texttimes{} 128 resolution image samples exhibiting global coherence. We expand on previous work for image quality assessment to provide two new analyses for assessing the discriminability and diversity of samples from class-conditional image synthesis models. These analyses demonstrate that high resolution samples provide class information not present in low resolution samples. Across 1000 ImageNet classes, 128 \texttimes{} 128 samples are more than twice as discriminable as artificially resized 32 \texttimes{} 32 samples. In addition, 84.7\% of the classes have samples exhibiting diversity comparable to real ImageNet data.},
  archivePrefix = {arXiv},
  eprint = {1610.09585},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/odena et al_2016_conditional image synthesis with auxiliary classifier gans.pdf},
  journal = {arXiv:1610.09585 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{oh19_CombinatorialBayesianOptimization,
  title = {Combinatorial {{Bayesian Optimization}} Using {{Graph Representations}}},
  author = {Oh, Changyong and Tomczak, Jakub M. and Gavves, Efstratios and Welling, Max},
  year = {2019},
  month = feb,
  abstract = {This paper focuses on Bayesian Optimization - typically considered with continuous inputs - for discrete search input spaces, including integer, categorical or graph structured input variables. In Gaussian process-based Bayesian Optimization a problem arises, as it is not straightforward to define a proper kernel on discrete input structures, where no natural notion of smoothness or similarity could be provided. We propose COMBO, a method that represents values of discrete variables as vertices of a graph and then use the diffusion kernel on that graph. As the graph size explodes with the number of categorical variables and categories, we propose the graph Cartesian product to decompose the graph into smaller sub-graphs, enabling kernel computation in linear time with respect to the number of input variables. Moreover, in our formulation we learn a scale parameter per subgraph. In empirical studies on four discrete optimization problems we demonstrate that our method is on par or outperforms the state-of-the-art in discrete Bayesian optimization.},
  archivePrefix = {arXiv},
  eprint = {1902.00448},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/oh et al_2019_combinatorial bayesian optimization using graph representations.pdf;/home/trung/Zotero/storage/C9ZEPBZJ/1902.html},
  journal = {arXiv:1902.00448 [cs, stat]},
  keywords = {Computer Science - Machine Learning,graph,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{ohagan04_Dicingunknown,
  title = {Dicing with the Unknown},
  author = {O'Hagan, Tony},
  year = {2004},
  month = sep,
  volume = {1},
  pages = {132--133},
  issn = {17409705},
  doi = {10.1111/j.1740-9713.2004.00050.x},
  journal = {Significance},
  language = {en},
  number = {3}
}

@article{okabe18_AttentiveStatisticsPooling,
  title = {Attentive {{Statistics Pooling}} for {{Deep Speaker Embedding}}},
  author = {Okabe, Koji and Koshinaka, Takafumi and Shinoda, Koichi},
  year = {2018},
  month = sep,
  pages = {2252--2256},
  doi = {10.21437/Interspeech.2018-993},
  abstract = {This paper proposes attentive statistics pooling for deep speaker embedding in text-independent speaker verification. In conventional speaker embedding, frame-level features are averaged over all the frames of a single utterance to form an utterance-level feature. Our method utilizes an attention mechanism to give different weights to different frames and generates not only weighted means but also weighted standard deviations. In this way, it can capture long-term variations in speaker characteristics more effectively. An evaluation on the NIST SRE 2012 and the VoxCeleb data sets shows that it reduces equal error rates (EERs) from the conventional method by 7.5\% and 8.1\%, respectively.},
  archivePrefix = {arXiv},
  eprint = {1803.10963},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/okabe et al_2018_attentive statistics pooling for deep speaker embedding.pdf;/home/trung/Zotero/storage/TR3IWRAN/1803.html},
  journal = {Interspeech 2018},
  keywords = {attention,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing}
}

@article{olmschenk18_Generalizingsemisupervisedgenerative,
  title = {Generalizing Semi-Supervised Generative Adversarial Networks to Regression Using Feature Contrasting},
  author = {Olmschenk, Greg and Zhu, Zhigang and Tang, Hao},
  year = {2018},
  month = nov,
  abstract = {In this work, we generalize semi-supervised generative adversarial networks (GANs) from classification problems to regression problems. In the last few years, the importance of improving the training of neural networks using semi-supervised training has been demonstrated for classification problems. We present a novel loss function, called feature contrasting, resulting in a discriminator which can distinguish between fake and real data based on feature statistics. This method avoids potential biases and limitations of alternative approaches. The generalization of semi-supervised GANs to the regime of regression problems of opens their use to countless applications as well as providing an avenue for a deeper understanding of how GANs function. We first demonstrate the capabilities of semi-supervised regression GANs on a toy dataset which allows for a detailed understanding of how they operate in various circumstances. This toy dataset is used to provide a theoretical basis of the semi-supervised regression GAN. We then apply the semi-supervised regression GANs to a number of real-world computer vision applications: age estimation, driving steering angle prediction, and crowd counting from single images. We perform extensive tests of what accuracy can be achieved with significantly reduced annotated data. Through the combination of the theoretical example and real-world scenarios, we demonstrate how semi-supervised GANs can be generalized to regression problems.},
  archivePrefix = {arXiv},
  eprint = {1811.11269},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/olmschenk et al_2018_generalizing semi-supervised generative adversarial networks to regression using feature contrasting.pdf},
  journal = {arXiv:1811.11269 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{oord16_ConditionalImageGeneration,
  title = {Conditional {{Image Generation}} with {{PixelCNN Decoders}}},
  author = {van den Oord, Aaron and Kalchbrenner, Nal and Vinyals, Oriol and Espeholt, Lasse and Graves, Alex and Kavukcuoglu, Koray},
  year = {2016},
  month = jun,
  abstract = {This work explores conditional image generation with a new image density model based on the PixelCNN architecture. The model can be conditioned on any vector, including descriptive labels or tags, or latent embeddings created by other networks. When conditioned on class labels from the ImageNet database, the model is able to generate diverse, realistic scenes representing distinct animals, objects, landscapes and structures. When conditioned on an embedding produced by a convolutional network given a single image of an unseen face, it generates a variety of new portraits of the same person with different facial expressions, poses and lighting conditions. We also show that conditional PixelCNN can serve as a powerful decoder in an image autoencoder. Additionally, the gated convolutional layers in the proposed model improve the log-likelihood of PixelCNN to match the state-of-the-art performance of PixelRNN on ImageNet, with greatly reduced computational cost.},
  archivePrefix = {arXiv},
  eprint = {1606.05328},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/oord et al_2016_conditional image generation with pixelcnn decoders.pdf},
  journal = {arXiv:1606.05328 [cs]},
  primaryClass = {cs}
}

@article{oord16_WaveNetGenerativeModel,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archivePrefix = {arXiv},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/oord et al_2016_wavenet.pdf;/home/trung/Zotero/storage/VZRNUGMT/1609.html},
  journal = {arXiv:1609.03499 [cs]},
  keywords = {causal,Computer Science - Machine Learning,Computer Science - Sound},
  primaryClass = {cs}
}

@article{oord16_WaveNetGenerativeModela,
  title = {{{WaveNet}}: {{A Generative Model}} for {{Raw Audio}}},
  shorttitle = {{{WaveNet}}},
  author = {van den Oord, Aaron and Dieleman, Sander and Zen, Heiga and Simonyan, Karen and Vinyals, Oriol and Graves, Alex and Kalchbrenner, Nal and Senior, Andrew and Kavukcuoglu, Koray},
  year = {2016},
  month = sep,
  abstract = {This paper introduces WaveNet, a deep neural network for generating raw audio waveforms. The model is fully probabilistic and autoregressive, with the predictive distribution for each audio sample conditioned on all previous ones; nonetheless we show that it can be efficiently trained on data with tens of thousands of samples per second of audio. When applied to text-to-speech, it yields state-of-the-art performance, with human listeners rating it as significantly more natural sounding than the best parametric and concatenative systems for both English and Mandarin. A single WaveNet can capture the characteristics of many different speakers with equal fidelity, and can switch between them by conditioning on the speaker identity. When trained to model music, we find that it generates novel and often highly realistic musical fragments. We also show that it can be employed as a discriminative model, returning promising results for phoneme recognition.},
  archivePrefix = {arXiv},
  eprint = {1609.03499},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/oord et al_2016_wavenet2.pdf},
  journal = {arXiv:1609.03499 [cs]},
  primaryClass = {cs}
}

@article{oord17_ParallelWaveNetFast,
  title = {Parallel {{WaveNet}}: {{Fast High}}-{{Fidelity Speech Synthesis}}},
  shorttitle = {Parallel {{WaveNet}}},
  author = {van den Oord, Aaron and Li, Yazhe and Babuschkin, Igor and Simonyan, Karen and Vinyals, Oriol and Kavukcuoglu, Koray and van den Driessche, George and Lockhart, Edward and Cobo, Luis C. and Stimberg, Florian and Casagrande, Norman and Grewe, Dominik and Noury, Seb and Dieleman, Sander and Elsen, Erich and Kalchbrenner, Nal and Zen, Heiga and Graves, Alex and King, Helen and Walters, Tom and Belov, Dan and Hassabis, Demis},
  year = {2017},
  month = nov,
  abstract = {The recently-developed WaveNet architecture is the current state of the art in realistic speech synthesis, consistently rated as more natural sounding for many different languages than any previous system. However, because WaveNet relies on sequential generation of one audio sample at a time, it is poorly suited to today's massively parallel computers, and therefore hard to deploy in a real-time production setting. This paper introduces Probability Density Distillation, a new method for training a parallel feed-forward network from a trained WaveNet with no significant difference in quality. The resulting system is capable of generating high-fidelity speech samples at more than 20 times faster than real-time, and is deployed online by Google Assistant, including serving multiple English and Japanese voices.},
  archivePrefix = {arXiv},
  eprint = {1711.10433},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/oord et al_2017_parallel wavenet.pdf},
  journal = {arXiv:1711.10433 [cs]},
  primaryClass = {cs}
}

@article{oord18_NeuralDiscreteRepresentation,
  title = {Neural {{Discrete Representation Learning}}},
  author = {van den Oord, Aaron and Vinyals, Oriol and Kavukcuoglu, Koray},
  year = {2018},
  month = may,
  abstract = {Learning useful representations without supervision remains a key challenge in machine learning. In this paper, we propose a simple yet powerful generative model that learns such discrete representations. Our model, the Vector Quantised-Variational AutoEncoder (VQ-VAE), differs from VAEs in two key ways: the encoder network outputs discrete, rather than continuous, codes; and the prior is learnt rather than static. In order to learn a discrete latent representation, we incorporate ideas from vector quantisation (VQ). Using the VQ method allows the model to circumvent issues of "posterior collapse" -- where the latents are ignored when they are paired with a powerful autoregressive decoder -- typically observed in the VAE framework. Pairing these representations with an autoregressive prior, the model can generate high quality images, videos, and speech as well as doing high quality speaker conversion and unsupervised learning of phonemes, providing further evidence of the utility of the learnt representations.},
  archivePrefix = {arXiv},
  eprint = {1711.00937},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/oord et al_2018_neural discrete representation learning.pdf;/home/trung/GoogleDrive/Zotero/oord et al_2018_neural discrete representation learning2.pdf;/home/trung/Zotero/storage/8CCP5RX8/1711.html},
  journal = {arXiv:1711.00937 [cs]},
  primaryClass = {cs}
}

@article{oord18_RepresentationLearningContrastive,
  title = {Representation {{Learning}} with {{Contrastive Predictive Coding}}},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  month = jul,
  abstract = {While supervised learning has enabled great progress in many applications, unsupervised learning has not seen such widespread adoption, and remains an important and challenging endeavor for artificial intelligence. In this work, we propose a universal unsupervised learning approach to extract useful representations from high-dimensional data, which we call Contrastive Predictive Coding. The key insight of our model is to learn such representations by predicting the future in latent space by using powerful autoregressive models. We use a probabilistic contrastive loss which induces the latent space to capture information that is maximally useful to predict future samples. It also makes the model tractable by using negative sampling. While most prior work has focused on evaluating representations for a particular modality, we demonstrate that our approach is able to learn useful representations achieving strong performance on four distinct domains: speech, images, text and reinforcement learning in 3D environments.},
  annotation = {ZSCC: 0000086},
  archivePrefix = {arXiv},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/oord et al_2018_representation learning with contrastive predictive coding.pdf;/home/trung/Zotero/storage/QP35IVYI/1807.html},
  journal = {arXiv:1807.03748 [cs, stat]},
  keywords = {Computer Science - Machine Learning,constrastive,regularization,representation,Statistics - Machine Learning,unsupervised},
  primaryClass = {cs, stat}
}

@article{orabona19_ModernIntroductionOnline,
  title = {A {{Modern Introduction}} to {{Online Learning}}},
  author = {Orabona, Francesco},
  year = {2019},
  month = dec,
  abstract = {In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously explained. Moreover, all the proofs have been carefully chosen to be as simple and as short as possible.},
  archivePrefix = {arXiv},
  eprint = {1912.13213},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/orabona_2019_a modern introduction to online learning.pdf},
  journal = {arXiv:1912.13213 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{orabona20_ModernIntroductionOnline,
  title = {A {{Modern Introduction}} to {{Online Learning}}},
  author = {Orabona, Francesco},
  year = {2020},
  month = may,
  abstract = {In this monograph, I introduce the basic concepts of Online Learning through a modern view of Online Convex Optimization. Here, online learning refers to the framework of regret minimization under worst-case assumptions. I present first-order and second-order algorithms for online learning with convex losses, in Euclidean and non-Euclidean settings. All the algorithms are clearly presented as instantiation of Online Mirror Descent or Follow-The-Regularized-Leader and their variants. Particular attention is given to the issue of tuning the parameters of the algorithms and learning in unbounded domains, through adaptive and parameter-free online learning algorithms. Non-convex losses are dealt through convex surrogate losses and through randomization. The bandit setting is also briefly discussed, touching on the problem of adversarial and stochastic multi-armed bandits. These notes do not require prior knowledge of convex analysis and all the required mathematical tools are rigorously explained. Moreover, all the proofs have been carefully chosen to be as simple and as short as possible.},
  archivePrefix = {arXiv},
  eprint = {1912.13213},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/orabona_2020_a modern introduction to online learning.pdf},
  journal = {arXiv:1912.13213 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@misc{oremus19_RealMoralDilemma,
  title = {The {{Real Moral Dilemma}} of {{Self}}-{{Driving Cars}}},
  author = {Oremus, Will},
  year = {2019},
  month = nov,
  abstract = {It has nothing to do with the `trolley problem'},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/VTYLDKEF/the-real-moral-dilemma-of-self-driving-cars-ab6bb5f216b.html},
  howpublished = {https://onezero.medium.com/the-real-moral-dilemma-of-self-driving-cars-ab6bb5f216b},
  journal = {Medium},
  language = {en}
}

@article{osawa19_PracticalDeepLearning,
  title = {Practical {{Deep Learning}} with {{Bayesian Principles}}},
  author = {Osawa, Kazuki and Swaroop, Siddharth and Jain, Anirudh and Eschenhagen, Runa and Turner, Richard E. and Yokota, Rio and Khan, Mohammad Emtiyaz},
  year = {2019},
  month = oct,
  abstract = {Bayesian methods promise to fix many shortcomings of deep learning, but they are impractical and rarely match the performance of standard methods, let alone improve them. In this paper, we demonstrate practical training of deep networks with natural-gradient variational inference. By applying techniques such as batch normalisation, data augmentation, and distributed training, we achieve similar performance in about the same number of epochs as the Adam optimiser, even on large datasets such as ImageNet. Importantly, the benefits of Bayesian principles are preserved: predictive probabilities are well-calibrated, uncertainties on outof-distribution data are improved, and continual-learning performance is boosted. This work enables practical deep learning while preserving benefits of Bayesian principles. A PyTorch implementation1 is available as a plug-and-play optimiser.},
  archivePrefix = {arXiv},
  eprint = {1906.02506},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/osawa et al_2019_practical deep learning with bayesian principles.pdf},
  journal = {arXiv:1906.02506 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ostrovski18_AutoregressiveQuantileNetworks,
  title = {Autoregressive {{Quantile Networks}} for {{Generative Modeling}}},
  author = {Ostrovski, Georg and Dabney, Will and Munos, R{\'e}mi},
  year = {2018},
  month = jun,
  abstract = {We introduce autoregressive implicit quantile networks (AIQN), a fundamentally different approach to generative modeling than those commonly used, that implicitly captures the distribution using quantile regression. AIQN is able to achieve superior perceptual quality and improvements in evaluation metrics, without incurring a loss of sample diversity. The method can be applied to many existing models and architectures. In this work we extend the PixelCNN model with AIQN and demonstrate results on CIFAR-10 and ImageNet using Inception score, FID, non-cherry-picked samples, and inpainting results. We consistently observe that AIQN yields a highly stable algorithm that improves perceptual quality while maintaining a highly diverse distribution.},
  annotation = {ZSCC: 0000018},
  archivePrefix = {arXiv},
  eprint = {1806.05575},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ostrovski et al_2018_autoregressive quantile networks for generative modeling.pdf;/home/trung/Zotero/storage/RW8UQWYH/1806.html},
  journal = {arXiv:1806.05575 [cs, stat]},
  primaryClass = {cs, stat}
}

@misc{ou19_ReviewLearningDeep,
  title = {A {{Review}} of {{Learning}} with {{Deep Generative Models}}},
  author = {Ou, Zhijian},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/ou_2019_a review of learning with deep generative models.pdf}
}

@article{ovadia19_CanYouTrust,
  title = {Can {{You Trust Your Model}}'s {{Uncertainty}}? {{Evaluating Predictive Uncertainty Under Dataset Shift}}},
  shorttitle = {Can {{You Trust Your Model}}'s {{Uncertainty}}?},
  author = {Ovadia, Yaniv and Fertig, Emily and Ren, Jie and Nado, Zachary and Sculley, D. and Nowozin, Sebastian and Dillon, Joshua V. and Lakshminarayanan, Balaji and Snoek, Jasper},
  year = {2019},
  month = dec,
  abstract = {Modern machine learning methods including deep learning have achieved great success in predictive accuracy for supervised learning tasks, but may still fall short in giving useful estimates of their predictive uncertainty. Quantifying uncertainty is especially critical in real-world settings, which often involve input distributions that are shifted from the training distribution due to a variety of factors including sample bias and non-stationarity. In such settings, well calibrated uncertainty estimates convey information about when a model's output should (or should not) be trusted. Many probabilistic deep learning methods, including Bayesian-and nonBayesian methods, have been proposed in the literature for quantifying predictive uncertainty, but to our knowledge there has not previously been a rigorous largescale empirical comparison of these methods under dataset shift. We present a largescale benchmark of existing state-of-the-art methods on classification problems and investigate the effect of dataset shift on accuracy and calibration. We find that traditional post-hoc calibration does indeed fall short, as do several other previous methods. However, some methods that marginalize over models give surprisingly strong results across a broad spectrum of tasks.},
  archivePrefix = {arXiv},
  eprint = {1906.02530},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ovadia et al_2019_can you trust your model's uncertainty.pdf},
  journal = {arXiv:1906.02530 [cs, stat]},
  keywords = {_tablet,disentanglement,favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{p.esser20_DisentanglingInvertibleInterpretation,
  title = {A {{Disentangling Invertible Interpretation Network}} for {{Explaining Latent Representations}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {{P. Esser} and {R. Rombach} and {B. Ommer}},
  year = {2020},
  month = jun,
  pages = {9220--9229},
  file = {/home/trung/GoogleDrive/Zotero/p. esser et al_2020_a disentangling invertible interpretation network for explaining latent representations.pdf},
  isbn = {2575-7075},
  keywords = {disentanglement}
}

@inproceedings{padi19_AttentionBasedHybrid,
  title = {Attention {{Based Hybrid}} I-{{Vector BLSTM Model}} for {{Language Recognition}}},
  booktitle = {Interspeech 2019},
  author = {Padi, Bharat and Mohan, Anand and Ganapathy, Sriram},
  year = {2019},
  month = sep,
  pages = {1263--1267},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2371},
  abstract = {In this paper, a hybrid i-vector neural network framework (iBLSTM) which models the sequence information present in a series of short segment i-vectors for the task of spoken language recognition (LRE) is proposed. A sequence of short segment i-vectors are extracted for every speech utterance and are then modeled using a bidirectional long short-term memory (BLSTM) recurrent neural network (RNN). Attention mechanism inside the neural network relevantly weights segments of the speech utterance and the model learns to give higher weights to parts of speech data which are more helpful to the classification task. The proposed framework performs better in short duration and noisy environments when compared with the conventional i-vector system. Experiments are performed on clean, noisy and multi-speaker speech data from NIST LRE 2017 and RATS language recognition corpus. In these experiments, the proposed approach yields significant improvements (relative improvements of 7.6 - 13\% in terms of accuracy for noisy conditions) over the conventional i-vector based language recognition approach and also over an end-to-end LSTM-RNN based approach.},
  file = {/home/trung/GoogleDrive/Zotero/padi et al_2019_attention based hybrid i-vector blstm model for language recognition.pdf},
  keywords = {attention},
  language = {en}
}

@article{painter20_LinearDisentangledRepresentations,
  title = {Linear {{Disentangled Representations}} and {{Unsupervised Action Estimation}}},
  author = {Painter, Matthew and Hare, Jonathon and {Prugel-Bennett}, Adam},
  year = {2020},
  month = aug,
  abstract = {Disentangled representation learning has seen a surge in interest over recent times, generally focusing on new models to optimise one of many disparate disentanglement metrics. It was only with Symmetry Based Disentangled Representation Learning that a robust mathematical framework was introduced to define precisely what is meant by a "linear disentangled representation". This framework determines that such representations would depend on a particular decomposition of the symmetry group acting on the data, showing that actions would manifest through irreducible group representations acting on independent representational subspaces. ForwardVAE subsequently proposed the first model to induce and demonstrate a linear disentangled representation in a VAE model. In this work we empirically show that linear disentangled representations are not present in standard VAE models and that they instead require altering the loss landscape to induce them. We proceed to show that such representations are a desirable property with regard to classical disentanglement metrics. Finally we propose a method to induce irreducible representations which forgoes the need for labelled action sequences, as was required by prior work. We explore a number of properties of this method, including the ability to learn from action sequences without knowledge of intermediate states.},
  archivePrefix = {arXiv},
  eprint = {2008.07922},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/painter et al_2020_linear disentangled representations and unsupervised action estimation.pdf},
  journal = {arXiv:2008.07922 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{paisley00_TutorialDirichletProcess,
  title = {A {{Tutorial}} on the {{Dirichlet Process}} for {{Engineers Technical Report}}},
  author = {Paisley, John},
  pages = {23},
  abstract = {This document provides a review of the Dirichlet process originally given in the author's preliminary exam paper and is presented here as a tutorial. No motivation is given (in what I've excerpted here), and this document is intended to be a mathematical tutorial that is still accessible to the engineer.},
  file = {/home/trung/GoogleDrive/Zotero/paisley_a tutorial on the dirichlet process for engineers technical report.pdf},
  language = {en}
}

@article{pal20_GameTheoreticAnalysis,
  title = {A {{Game Theoretic Analysis}} of {{Additive Adversarial Attacks}} and {{Defenses}}},
  author = {Pal, Ambar and Vidal, Ren{\'e}},
  year = {2020},
  month = sep,
  abstract = {Research in adversarial learning follows a cat and mouse game between attackers and defenders where attacks are proposed, they are mitigated by new defenses, and subsequently new attacks are proposed that break earlier defenses, and so on. However, it has remained unclear as to whether there are conditions under which no better attacks or defenses can be proposed. In this paper, we propose a game-theoretic framework for studying attacks and defenses which exist in equilibrium. Under a locally linear decision boundary model for the underlying binary classifier, we prove that the Fast Gradient Method attack and the Randomized Smoothing defense form a Nash Equilibrium. We then show how this equilibrium defense can be approximated given finitely many samples from a data-generating distribution, and derive a generalization bound for the performance of our approximation.},
  archivePrefix = {arXiv},
  eprint = {2009.06530},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pal et al_2020_a game theoretic analysis of additive adversarial attacks and defenses.pdf},
  journal = {arXiv:2009.06530 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{paleyes20_ChallengesDeployingMachine,
  title = {Challenges in {{Deploying Machine Learning}}: A {{Survey}} of {{Case Studies}}},
  shorttitle = {Challenges in {{Deploying Machine Learning}}},
  author = {Paleyes, Andrei and Urma, Raoul-Gabriel and Lawrence, Neil D.},
  year = {2020},
  month = nov,
  abstract = {In recent years, machine learning has received increased interest both as an academic research field and as a solution for real-world business problems. However, the deployment of machine learning models in production systems can present a number of issues and concerns. This survey reviews published reports of deploying machine learning solutions in a variety of use cases, industries and applications and extracts practical considerations corresponding to stages of the machine learning deployment workflow. Our survey shows that practitioners face challenges at each stage of the deployment. The goal of this paper is to layout a research agenda to explore approaches addressing these challenges.},
  archivePrefix = {arXiv},
  eprint = {2011.09926},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/paleyes et al_2020_challenges in deploying machine learning.pdf},
  journal = {arXiv:2011.09926 [cs]},
  primaryClass = {cs}
}

@incollection{palmer04_PerspectivesSparseBayesian,
  title = {Perspectives on {{Sparse Bayesian Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 16},
  author = {Palmer, Jason and Rao, Bhaskar D. and Wipf, David P.},
  editor = {Thrun, S. and Saul, L. K. and Sch{\"o}lkopf, B.},
  year = {2004},
  pages = {249--256},
  publisher = {{MIT Press}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/palmer et al_2004_perspectives on sparse bayesian learning.pdf;/home/trung/Zotero/storage/WEIISFZV/2393-perspectives-on-sparse-bayesian-learning.html},
  keywords = {favorite,sparse,variational}
}

@article{panch19_inconvenienttruthAI,
  title = {The ``Inconvenient Truth'' about {{AI}} in Healthcare},
  author = {Panch, Trishan and Mattie, Heather and Celi, Leo Anthony},
  year = {2019},
  month = dec,
  volume = {2},
  pages = {77},
  issn = {2398-6352},
  doi = {10.1038/s41746-019-0155-4},
  file = {/home/trung/Zotero/storage/KFE8GQ3I/Panch et al. - 2019 - The “inconvenient truth” about AI in healthcare.pdf},
  journal = {npj Digital Medicine},
  language = {en},
  number = {1}
}

@article{pande20_ImportanceLocalInformation,
  title = {On the {{Importance}} of {{Local Information}} in {{Transformer Based Models}}},
  author = {Pande, Madhura and Budhraja, Aakriti and Nema, Preksha and Kumar, Pratyush and Khapra, Mitesh M.},
  year = {2020},
  month = aug,
  abstract = {The self-attention module is a key component of Transformer-based models, wherein each token pays attention to every other token. Recent studies have shown that these heads exhibit syntactic, semantic, or local behaviour. Some studies have also identified promise in restricting this attention to be local, i.e., a token attending to other tokens only in a small neighbourhood around it. However, no conclusive evidence exists that such local attention alone is sufficient to achieve high accuracy on multiple NLP tasks. In this work, we systematically analyse the role of locality information in learnt models and contrast it with the role of syntactic information. More specifically, we first do a sensitivity analysis and show that, at every layer, the representation of a token is much more sensitive to tokens in a small neighborhood around it than to tokens which are syntactically related to it. We then define an attention bias metric to determine whether a head pays more attention to local tokens or to syntactically related tokens. We show that a larger fraction of heads have a locality bias as compared to a syntactic bias. Having established the importance of local attention heads, we train and evaluate models where varying fractions of the attention heads are constrained to be local. Such models would be more efficient as they would have fewer computations in the attention layer. We evaluate these models on 4 GLUE datasets (QQP, SST-2, MRPC, QNLI) and 2 MT datasets (En-De, En-Ru) and clearly demonstrate that such constrained models have comparable performance to the unconstrained models. Through this systematic evaluation we establish that attention in Transformer-based models can be constrained to be local without affecting performance.},
  archivePrefix = {arXiv},
  eprint = {2008.05828},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pande et al_2020_on the importance of local information in transformer based models.pdf},
  journal = {arXiv:2008.05828 [cs]},
  primaryClass = {cs}
}

@article{pandey20_DisentangledRepresentationLearning,
  title = {Disentangled {{Representation Learning}} and {{Generation}} with {{Manifold Optimization}}},
  author = {Pandey, Arun and Fanuel, Michael and Schreurs, Joachim and Suykens, Johan A. K.},
  year = {2020},
  month = jun,
  abstract = {Disentanglement is an enjoyable property in representation learning which increases the interpretability of generative models such as Variational Auto-Encoders (VAE), Generative Adversarial Models and their many variants. In the context of latent space models, this work presents a representation learning framework that explicitly promotes disentanglement thanks to the combination of an auto-encoder with Principal Component Analysis (PCA) in latent space. The proposed objective is the sum of an auto-encoder error term along with a PCA reconstruction error in the feature space. This has an interpretation of a Restricted Kernel Machine with an interconnection matrix on the Stiefel manifold. The construction encourages a matching between the principal directions in latent space and the directions of orthogonal variation in data space. The training algorithm involves a stochastic optimization method on the Stiefel manifold, which increases only marginally the computing time compared to an analogous VAE. Our theoretical discussion and various experiments show that the proposed model improves over many VAE variants along with special emphasis on disentanglement learning.},
  archivePrefix = {arXiv},
  eprint = {2006.07046},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pandey et al_2020_disentangled representation learning and generation with manifold optimization.pdf},
  journal = {arXiv:2006.07046 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{pang20_BagTricksAdversarial,
  title = {Bag of {{Tricks}} for {{Adversarial Training}}},
  author = {Pang, Tianyu and Yang, Xiao and Dong, Yinpeng and Su, Hang and Zhu, Jun},
  year = {2020},
  month = oct,
  abstract = {Adversarial training (AT) is one of the most effective strategies for promoting model robustness. However, recent benchmarks show that most of the proposed improvements on AT are less effective than simply early stopping the training procedure. This counter-intuitive fact motivates us to investigate the implementation details of tens of AT methods. Surprisingly, we find that the basic training settings (e.g., weight decay, learning rate schedule, etc.) used in these methods are highly inconsistent, which could largely affect the model performance as shown in our experiments. For example, a slightly different value of weight decay can reduce the model robust accuracy by more than 7\%, which is probable to override the potential promotion induced by the proposed methods. In this work, we provide comprehensive evaluations on the effects of basic training tricks and hyperparameter settings for adversarially trained models. We provide a reasonable baseline setting and re-implement previous defenses to achieve new state-of-the-art results1.},
  archivePrefix = {arXiv},
  eprint = {2010.00467},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pang et al_2020_bag of tricks for adversarial training.pdf},
  journal = {arXiv:2010.00467 [cs, stat]},
  keywords = {early stopping},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pang20_LearningLatentSpace,
  title = {Learning {{Latent Space Energy}}-{{Based Prior Model}}},
  author = {Pang, Bo and Han, Tian and Nijkamp, Erik and Zhu, Song-Chun and Wu, Ying Nian},
  year = {2020},
  month = jun,
  abstract = {The generator model assumes that the observed example is generated by a low-dimensional latent vector via a top-down network, and the latent vector follows a simple and known prior distribution, such as uniform or Gaussian white noise distribution. While we can learn an expressive top-down network to map the prior distribution to the data distribution, we can also learn an expressive prior model instead of assuming a given prior distribution. This follows the philosophy of empirical Bayes where the prior model is learned from the observed data. We propose to learn an energy-based prior model for the latent vector, where the energy function is parametrized by a very simple multi-layer perceptron. Due to the low-dimensionality of the latent space, learning a latent space energy-based prior model proves to be both feasible and desirable. In this paper, we develop the maximum likelihood learning algorithm and its variation based on short-run Markov chain Monte Carlo sampling from the prior and the posterior distributions of the latent vector, and we show that the learned model exhibits strong performance in terms of image and text generation and anomaly detection.},
  archivePrefix = {arXiv},
  eprint = {2006.08205},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pang et al_2020_learning latent space energy-based prior model.pdf},
  journal = {arXiv:2006.08205 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{papamakarios18_MaskedAutoregressiveFlow,
  title = {Masked {{Autoregressive Flow}} for {{Density Estimation}}},
  author = {Papamakarios, George and Pavlakou, Theo and Murray, Iain},
  year = {2018},
  month = jun,
  abstract = {Autoregressive models are among the best performing neural density estimators. We describe an approach for increasing the flexibility of an autoregressive model, based on modelling the random numbers that the model uses internally when generating data. By constructing a stack of autoregressive models, each modelling the random numbers of the next model in the stack, we obtain a type of normalizing flow suitable for density estimation, which we call Masked Autoregressive Flow. This type of flow is closely related to Inverse Autoregressive Flow and is a generalization of Real NVP. Masked Autoregressive Flow achieves state-of-the-art performance in a range of general-purpose density estimation tasks.},
  archivePrefix = {arXiv},
  eprint = {1705.07057},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/papamakarios et al_2018_masked autoregressive flow for density estimation.pdf},
  journal = {arXiv:1705.07057 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{papamakarios19_NeuralDensityEstimation,
  title = {Neural {{Density Estimation}} and {{Likelihood}}-Free {{Inference}}},
  author = {Papamakarios, George},
  year = {2019},
  month = oct,
  abstract = {I consider two problems in machine learning and statistics: the problem of estimating the joint probability density of a collection of random variables, known as density estimation, and the problem of inferring model parameters when their likelihood is intractable, known as likelihood-free inference. The contribution of the thesis is a set of new methods for addressing these problems that are based on recent advances in neural networks and deep learning.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1910.13233},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/papamakarios_2019_neural density estimation and likelihood-free inference.pdf},
  journal = {arXiv:1910.13233 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{papamakarios19_NormalizingFlowsProbabilistic,
  title = {Normalizing {{Flows}} for {{Probabilistic Modeling}} and {{Inference}}},
  author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
  year = {2019},
  month = dec,
  abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1912.02762},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/papamakarios et al_2019_normalizing flows for probabilistic modeling and inference.pdf},
  journal = {arXiv:1912.02762 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{pappagari19_HierarchicalTransformersLong,
  title = {Hierarchical {{Transformers}} for {{Long Document Classification}}},
  author = {Pappagari, Raghavendra and {\.Z}elasko, Piotr and Villalba, Jes{\'u}s and Carmiel, Yishay and Dehak, Najim},
  year = {2019},
  month = oct,
  abstract = {BERT, which stands for Bidirectional Encoder Representations from Transformers, is a recently introduced language representation model based upon the transfer learning paradigm. We extend its fine-tuning procedure to address one of its major limitations - applicability to inputs longer than a few hundred words, such as transcripts of human call conversations. Our method is conceptually simple. We segment the input into smaller chunks and feed each of them into the base model. Then, we propagate each output through a single recurrent layer, or another transformer, followed by a softmax activation. We obtain the final classification decision after the last segment has been consumed. We show that both BERT extensions are quick to fine-tune and converge after as little as 1 epoch of training on a small, domain-specific data set. We successfully apply them in three different tasks involving customer call satisfaction prediction and topic classification, and obtain a significant improvement over the baseline models in two of them.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.10781},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pappagari et al_2019_hierarchical transformers for long document classification.pdf;/home/trung/Zotero/storage/VK3VYXQ7/1910.html},
  journal = {arXiv:1910.10781 [cs, stat]},
  keywords = {bert,Computer Science - Computation and Language,Computer Science - Machine Learning,hierarchical,Statistics - Machine Learning,transformer},
  primaryClass = {cs, stat}
}

@article{paquet18_FactorialMixturePrior,
  title = {A {{Factorial Mixture Prior}} for {{Compositional Deep Generative Models}}},
  author = {Paquet, Ulrich and Ghaisas, Sumedh K. and Tieleman, Olivier},
  year = {2018},
  month = dec,
  abstract = {We assume that a high-dimensional datum, like an image, is a compositional expression of a set of properties, with a complicated non-linear relationship between the datum and its properties. This paper proposes a factorial mixture prior for capturing latent properties, thereby adding structured compositionality to deep generative models. The prior treats a latent vector as belonging to Cartesian product of subspaces, each of which is quantized separately with a Gaussian mixture model. Some mixture components can be set to represent properties as observed random variables whenever labeled properties are present. Through a combination of stochastic variational inference and gradient descent, a method for learning how to infer discrete properties in an unsupervised or semi-supervised way is outlined and empirically evaluated.},
  archivePrefix = {arXiv},
  eprint = {1812.07480},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/paquet et al_2018_a factorial mixture prior for compositional deep generative models.pdf},
  journal = {arXiv:1812.07480 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{parascandolo18_LearningIndependentCausal,
  title = {Learning {{Independent Causal Mechanisms}}},
  author = {Parascandolo, Giambattista and Kilbertus, Niki and {Rojas-Carulla}, Mateo and Sch{\"o}lkopf, Bernhard},
  year = {2018},
  month = sep,
  abstract = {Statistical learning relies upon data sampled from a distribution, and we usually do not care what actually generated it in the first place. From the point of view of causal modeling, the structure of each distribution is induced by physical mechanisms that give rise to dependences between observables. Mechanisms, however, can be meaningful autonomous modules of generative models that make sense beyond a particular entailed data distribution, lending themselves to transfer between problems. We develop an algorithm to recover a set of independent (inverse) mechanisms from a set of transformed data points. The approach is unsupervised and based on a set of experts that compete for data generated by the mechanisms, driving specialization. We analyze the proposed method in a series of experiments on image data. Each expert learns to map a subset of the transformed data back to a reference distribution. The learned mechanisms generalize to novel domains. We discuss implications for transfer learning and links to recent trends in generative modeling.},
  archivePrefix = {arXiv},
  eprint = {1712.00961},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/parascandolo et al_2018_learning independent causal mechanisms.pdf},
  journal = {arXiv:1712.00961 [cs, stat]},
  keywords = {_tablet,causal,favorite},
  primaryClass = {cs, stat}
}

@article{parascandolo20_Learningexplanationsthat,
  title = {Learning Explanations That Are Hard to Vary},
  author = {Parascandolo, Giambattista and Neitz, Alexander and Orvieto, Antonio and Gresele, Luigi and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = sep,
  abstract = {In this paper, we investigate the principle that `good explanations are hard to vary' in the context of deep learning. We show that averaging gradients across examples -- akin to a logical OR of patterns -- can favor memorization and `patchwork' solutions that sew together different strategies, instead of identifying invariances. To inspect this, we first formalize a notion of consistency for minima of the loss surface, which measures to what extent a minimum appears only when examples are pooled. We then propose and experimentally validate a simple alternative algorithm based on a logical AND, that focuses on invariances and prevents memorization in a set of real-world tasks. Finally, using a synthetic dataset with a clear distinction between invariant and spurious mechanisms, we dissect learning signals and compare this approach to well-established regularizers.},
  archivePrefix = {arXiv},
  eprint = {2009.00329},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/parascandolo et al_2020_learning explanations that are hard to vary.pdf},
  journal = {arXiv:2009.00329 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{parbhoo18_CauseEffectDeepInformation,
  title = {Cause-{{Effect Deep Information Bottleneck For Incomplete Covariates}}},
  author = {Parbhoo, Sonali and Wieser, Mario and Roth, Volker},
  year = {2018},
  month = oct,
  abstract = {Estimating the causal effects of an intervention in the presence of confounding is a frequently occurring problem in applications such as medicine. The task is challenging since there may be multiple confounding factors, some of which may be missing, and inferences must be made from high-dimensional, noisy measurements. In this paper, we propose a decision-theoretic approach to estimate the causal effects of interventions where a subset of the covariates is unavailable for some patients during testing. Our approach uses the information bottleneck principle to perform a discrete, low-dimensional sufficient reduction of the covariate data to estimate a distribution over confounders. In doing so, we can estimate the causal effect of an intervention where only partial covariate information is available. Our results on a causal inference benchmark and a real application for treating sepsis show that our method achieves state-of-the-art performance, without sacrificing interpretability.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1807.02326},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/parbhoo et al_2018_cause-effect deep information bottleneck for incomplete covariates.pdf;/home/trung/Zotero/storage/IXTYIRQW/1807.html},
  journal = {arXiv:1807.02326 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{park19_EffectNetworkWidth,
  title = {The {{Effect}} of {{Network Width}} on {{Stochastic Gradient Descent}} and {{Generalization}}: An {{Empirical Study}}},
  shorttitle = {The {{Effect}} of {{Network Width}} on {{Stochastic Gradient Descent}} and {{Generalization}}},
  author = {Park, Daniel S. and {Sohl-Dickstein}, Jascha and Le, Quoc V. and Smith, Samuel L.},
  year = {2019},
  month = may,
  abstract = {We investigate how the final parameters found by stochastic gradient descent are influenced by over-parameterization. We generate families of models by increasing the number of channels in a base network, and then perform a large hyper-parameter search to study how the test error depends on learning rate, batch size, and network width. We find that the optimal SGD hyper-parameters are determined by a "normalized noise scale," which is a function of the batch size, learning rate, and initialization conditions. In the absence of batch normalization, the optimal normalized noise scale is directly proportional to width. Wider networks, with their higher optimal noise scale, also achieve higher test accuracy. These observations hold for MLPs, ConvNets, and ResNets, and for two different parameterization schemes ("Standard" and "NTK"). We observe a similar trend with batch normalization for ResNets. Surprisingly, since the largest stable learning rate is bounded, the largest batch size consistent with the optimal normalized noise scale decreases as the width increases.},
  annotation = {ZSCC: 0000004},
  archivePrefix = {arXiv},
  eprint = {1905.03776},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/park et al_2019_the effect of network width on stochastic gradient descent and generalization.pdf;/home/trung/Zotero/storage/23F2M323/1905.html},
  journal = {arXiv:1905.03776 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{park19_SpecAugmentSimpleData,
  title = {{{SpecAugment}}: {{A Simple Data Augmentation Method}} for {{Automatic Speech Recognition}}},
  shorttitle = {{{SpecAugment}}},
  author = {Park, Daniel S. and Chan, William and Zhang, Yu and Chiu, Chung-Cheng and Zoph, Barret and Cubuk, Ekin D. and Le, Quoc V.},
  year = {2019},
  month = apr,
  abstract = {We present SpecAugment, a simple data augmentation method for speech recognition. SpecAugment is applied directly to the feature inputs of a neural network (i.e., filter bank coefficients). The augmentation policy consists of warping the features, masking blocks of frequency channels, and masking blocks of time steps. We apply SpecAugment on Listen, Attend and Spell networks for end-to-end speech recognition tasks. We achieve state-of-the-art performance on the LibriSpeech 960h and Swichboard 300h tasks, outperforming all prior work. On LibriSpeech, we achieve 6.8\% WER on test-other without the use of a language model, and 5.8\% WER with shallow fusion with a language model. This compares to the previous state-of-the-art hybrid system of 7.5\% WER. For Switchboard, we achieve 7.2\%/14.6\% on the Switchboard/CallHome portion of the Hub5'00 test set without the use of a language model, and 6.8\%/14.1\% with shallow fusion, which compares to the previous state-of-the-art hybrid system at 8.3\%/17.3\% WER.},
  archivePrefix = {arXiv},
  eprint = {1904.08779},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/park et al_2019_specaugment.pdf;/home/trung/Zotero/storage/VQ75E7LL/1904.html},
  journal = {arXiv:1904.08779 [cs, eess, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,data augmentation,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{park20_MomentMatchingGraphNetworksCausal,
  title = {Moment-{{Matching Graph}}-{{Networks}} for {{Causal Inference}}},
  author = {Park, Michael},
  year = {2020},
  month = jul,
  abstract = {In this note we explore a fully unsupervised deep-learning framework for simulating non-linear structural equation models from observational training data. The main contribution of this note is an architecture for applying moment-matching loss functions to the edges of a causal Bayesian graph, resulting in a generative conditional-moment-matching graph-neural-network. This framework thus enables automated sampling of latent space conditional probability distributions for various graphical interventions, and is capable of generating out-of-sample interventional probabilities that are often faithful to the ground truth distributions well beyond the range contained in the training set. These methods could in principle be used in conjunction with any existing autoencoder that produces a latent space representation containing causal graph structures.},
  archivePrefix = {arXiv},
  eprint = {2007.10507},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/park_2020_moment-matching graph-networks for causal inference.pdf},
  journal = {arXiv:2007.10507 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{parker11_Moodeffectsamino,
  title = {Mood Effects of the Amino Acids Tryptophan and Tyrosine},
  author = {Parker, G. and Brotchie, H.},
  year = {2011},
  volume = {124},
  pages = {417--426},
  issn = {1600-0447},
  doi = {10.1111/j.1600-0447.2011.01706.x},
  abstract = {Parker G, Brotchie H. Mood effects of the amino acids tryptophan and tyrosine. Objective: Reflecting increased scientific interest in any nutritional contribution to the onset and treatment of mood disorders, we overview research into two neurotransmitter precursors \textendash{} the amino acids tryptophan and tyrosine \textendash{} particularly examining whether any deficiency increases risk to depression and whether those amino acids have any antidepressant properties. Method: The theoretical relevance of the two amino acids was overviewed by considering published risk and intervention studies, technical papers and reviews. Results: There is some limited evidence, suggesting that depressed patients, especially those with a melancholic depression, have decreased tryptophan levels. Whether such findings reflect a causal contribution or are a consequence of a depressed state remains an open question. There is a small database supporting tryptophan preparations as benefitting depressed mood states. There is no clear evidence as to whether tyrosine deficiency contributes to depression, while the only randomized double-blind study examining tyrosine supplementation did not show antidepressant benefit. Conclusion: Acute tryptophan depletion continues to provide a research tool for investigating the relevance of serotonin to depression onset. There is limited evidence that tryptophan loading is effective as a treatment for depression through its action of increasing serotonin production. Most clinical studies are dated, involve small sample sizes and/or were not placebo controlled. The development of the new serotonin reuptake inhibitor drugs seemingly signalled an end to pursuing such means of promoting increased serotonin as a treatment for depression. The evidence for tyrosine loading promoting catecholamine production as a possible treatment for depression appears even less promising, and depletion studies less informative.},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1600-0447.2011.01706.x},
  copyright = {\textcopyright{} 2011 John Wiley \& Sons A/S},
  file = {/home/trung/GoogleDrive/Zotero/parker et al_2011_mood effects of the amino acids tryptophan and tyrosine.pdf},
  journal = {Acta Psychiatrica Scandinavica},
  language = {en},
  number = {6}
}

@article{parr18_MatrixCalculusYou,
  title = {The {{Matrix Calculus You Need For Deep Learning}}},
  author = {Parr, Terence and Howard, Jeremy},
  year = {2018},
  month = jul,
  abstract = {This paper is an attempt to explain all the matrix calculus you need in order to understand the training of deep neural networks. We assume no math knowledge beyond what you learned in calculus 1, and provide links to help you refresh the necessary math where needed. Note that you do not need to understand this material before you start learning to train and use deep learning in practice; rather, this material is for those who are already familiar with the basics of neural networks, and wish to deepen their understanding of the underlying math. Don't worry if you get stuck at some point along the way\textemdash just go back and reread the previous section, and try writing down and working through some examples. And if you're still stuck, we're happy to answer your questions in the Theory category at forums.fast.ai. Note: There is a reference section at the end of the paper summarizing all the key matrix calculus rules and terminology discussed here.},
  annotation = {ZSCC: 0000007},
  archivePrefix = {arXiv},
  eprint = {1802.01528},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/parr et al_2018_the matrix calculus you need for deep learning.pdf;/home/trung/Zotero/storage/KV7HKJEI/index.html},
  journal = {arXiv:1802.01528 [cs, stat]},
  keywords = {_tablet,favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pascanu00_difficultytrainingrecurrent,
  title = {On the Difficulty of Training Recurrent Neural Networks},
  author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
  pages = {9},
  abstract = {There are two widely known issues with properly training recurrent neural networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
  annotation = {ZSCC: 0002304},
  file = {/home/trung/GoogleDrive/Zotero/pascanu et al_on the diﬃculty of training recurrent neural networks.pdf},
  language = {en}
}

@article{patel19_BayesianInferenceGenerative,
  title = {Bayesian {{Inference}} with {{Generative Adversarial Network Priors}}},
  author = {Patel, Dhruv and Oberai, Assad A.},
  year = {2019},
  month = jul,
  abstract = {Bayesian inference is used extensively to infer and to quantify the uncertainty in a field of interest from a measurement of a related field when the two are linked by a physical model. Despite its many applications, Bayesian inference faces challenges when inferring fields that have discrete representations of large dimension, and/or have prior distributions that are difficult to represent mathematically. In this manuscript we consider the use of Generative Adversarial Networks (GANs) in addressing these challenges. A GAN is a type of deep neural network equipped with the ability to learn the distribution implied by multiple samples of a given field. Once trained on these samples, the generator component of a GAN maps the iid components of a low-dimensional latent vector to an approximation of the distribution of the field of interest. In this work we demonstrate how this approximate distribution may be used as a prior in a Bayesian update, and how it addresses the challenges associated with characterizing complex prior distributions and the large dimension of the inferred field. We demonstrate the efficacy of this approach by applying it to the problem of inferring and quantifying uncertainty in the initial temperature field in a heat conduction problem from a noisy measurement of the temperature at later time.},
  archivePrefix = {arXiv},
  eprint = {1907.09987},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/patel et al_2019_bayesian inference with generative adversarial network priors.pdf;/home/trung/Zotero/storage/M8DG2K4Z/1907.html},
  journal = {arXiv:1907.09987 [physics, stat]},
  keywords = {adversarial,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,gan,Physics - Computational Physics,Statistics - Machine Learning,varational},
  primaryClass = {physics, stat}
}

@article{pathak16_ContextEncodersFeature,
  title = {Context {{Encoders}}: {{Feature Learning}} by {{Inpainting}}},
  shorttitle = {Context {{Encoders}}},
  author = {Pathak, Deepak and Krahenbuhl, Philipp and Donahue, Jeff and Darrell, Trevor and Efros, Alexei A.},
  year = {2016},
  month = nov,
  abstract = {We present an unsupervised visual feature learning algorithm driven by context-based pixel prediction. By analogy with auto-encoders, we propose Context Encoders -- a convolutional neural network trained to generate the contents of an arbitrary image region conditioned on its surroundings. In order to succeed at this task, context encoders need to both understand the content of the entire image, as well as produce a plausible hypothesis for the missing part(s). When training context encoders, we have experimented with both a standard pixel-wise reconstruction loss, as well as a reconstruction plus an adversarial loss. The latter produces much sharper results because it can better handle multiple modes in the output. We found that a context encoder learns a representation that captures not just appearance but also the semantics of visual structures. We quantitatively demonstrate the effectiveness of our learned features for CNN pre-training on classification, detection, and segmentation tasks. Furthermore, context encoders can be used for semantic inpainting tasks, either stand-alone or as initialization for non-parametric methods.},
  annotation = {ZSCC: 0001479},
  archivePrefix = {arXiv},
  eprint = {1604.07379},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pathak et al_2016_context encoders.pdf;/home/trung/Zotero/storage/YBLG7XM7/1604.html},
  journal = {arXiv:1604.07379 [cs]},
  primaryClass = {cs}
}

@article{pati20_AttributebasedRegularizationLatent,
  title = {Attribute-Based {{Regularization}} of {{Latent Spaces}} for {{Variational Auto}}-{{Encoders}}},
  author = {Pati, Ashis and Lerch, Alexander},
  year = {2020},
  month = jul,
  abstract = {Selective manipulation of data attributes using deep generative models is an active area of research. In this paper, we present a novel method to structure the latent space of a Variational Auto-Encoder (VAE) to encode different continuous-valued attributes explicitly. This is accomplished by using an attribute regularization loss which enforces a monotonic relationship between the attribute values and the latent code of the dimension along which the attribute is to be encoded. Consequently, post-training, the model can be used to manipulate the attribute by simply changing the latent code of the corresponding regularized dimension. The results obtained from several quantitative and qualitative experiments show that the proposed method leads to disentangled and interpretable latent spaces that can be used to effectively manipulate a wide range of data attributes spanning image and symbolic music domains.},
  archivePrefix = {arXiv},
  eprint = {2004.05485},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pati et al_2020_attribute-based regularization of latent spaces for variational auto-encoders.pdf},
  journal = {arXiv:2004.05485 [cs, stat]},
  keywords = {_tablet,disentanglement},
  primaryClass = {cs, stat}
}

@article{pati20_dMelodiesMusicDataset,
  title = {{{dMelodies}}: {{A Music Dataset}} for {{Disentanglement Learning}}},
  shorttitle = {{{dMelodies}}},
  author = {Pati, Ashis and Gururani, Siddharth and Lerch, Alexander},
  year = {2020},
  month = jul,
  abstract = {Representation learning focused on disentangling the underlying factors of variation in given data has become an important area of research in machine learning. However, most of the studies in this area have relied on datasets from the computer vision domain and thus, have not been readily extended to music. In this paper, we present a new symbolic music dataset that will help researchers working on disentanglement problems demonstrate the efficacy of their algorithms on diverse domains. This will also provide a means for evaluating algorithms specifically designed for music. To this end, we create a dataset comprising of 2-bar monophonic melodies where each melody is the result of a unique combination of nine latent factors that span ordinal, categorical, and binary types. The dataset is large enough ({$\approx$} 1.3 million data points) to train and test deep networks for disentanglement learning. In addition, we present benchmarking experiments using popular unsupervised disentanglement algorithms on this dataset and compare the results with those obtained on an image-based dataset.},
  archivePrefix = {arXiv},
  eprint = {2007.15067},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pati et al_2020_dmelodies.pdf},
  journal = {arXiv:2007.15067 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{patrini00_2018statedeepfakes,
  title = {2018 the State of Deepfakes},
  author = {Patrini, Giorgio},
  pages = {16},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/patrini_2018 the state of deepfakes.pdf},
  language = {en}
}

@article{patruno20_reviewcomputationalstrategies,
  title = {A Review of Computational Strategies for Denoising and Imputation of Single-Cell Transcriptomic Data},
  author = {Patruno, Lucrezia and Maspero, Davide and Craighero, Francesco and Angaroni, Fabrizio and Antoniotti, Marco and Graudenzi, Alex},
  year = {2020},
  month = oct,
  issn = {1477-4054},
  doi = {10.1093/bib/bbaa222},
  abstract = {The advancements of single-cell sequencing methods have paved the way for the characterization of cellular states at unprecedented resolution, revolutionizing the investigation on complex biological systems. Yet, single-cell sequencing experiments are hindered by several technical issues, which cause output data to be noisy, impacting the reliability of downstream analyses. Therefore, a growing number of data science methods has been proposed to recover lost or corrupted information from single-cell sequencing data. To date, however, no quantitative benchmarks have been proposed to evaluate such methods.We present a comprehensive analysis of the state-of-the-art computational approaches for denoising and imputation of single-cell transcriptomic data, comparing their performance in different experimental scenarios. In detail, we compared 19 denoising and imputation methods, on both simulated and real-world datasets, with respect to several performance metrics related to imputation of dropout events, recovery of true expression profiles, characterization of cell similarity, identification of differentially expressed genes and computation time. The effectiveness and scalability of all methods were assessed with regard to distinct sequencing protocols, sample size and different levels of biological variability and technical noise. As a result, we identify a subset of versatile approaches exhibiting solid performances on most tests and show that certain algorithmic families prove effective on specific tasks but inefficient on others. Finally, most methods appear to benefit from the introduction of appropriate assumptions on noise distribution of biological processes.},
  file = {/home/trung/GoogleDrive/Zotero/patruno et al_2020_a review of computational strategies for denoising and imputation of single-cell transcriptomic data.pdf},
  journal = {Briefings in Bioinformatics},
  number = {bbaa222}
}

@article{pattanayak00_ProDeepLearning,
  title = {Pro {{Deep Learning}} with {{TensorFlow}}},
  author = {Pattanayak, Santanu},
  pages = {412},
  annotation = {ZSCC: 0000023},
  file = {/home/trung/GoogleDrive/Zotero/pattanayak_pro deep learning with tensorflow.pdf},
  language = {en}
}

@article{patterson00_DeepLearningPractitioner,
  title = {Deep {{Learning}} a {{Practitioner}}'s {{Approach}}},
  author = {Patterson, Josh and Gibson, Adam},
  pages = {532},
  annotation = {ZSCC: 0000280},
  file = {/home/trung/GoogleDrive/Zotero/patterson et al_deep learning a practitioner's approach.pdf},
  language = {en}
}

@article{paul19_SignificantAmountsFunctional,
  title = {Significant {{Amounts}} of {{Functional Collagen Peptides Can Be Incorporated}} in the {{Diet While Maintaining Indispensable Amino Acid Balance}}},
  author = {Paul, Cristiana and Leser, Suzane and Oesser, Steffen},
  year = {2019},
  month = may,
  volume = {11},
  pages = {1079},
  issn = {2072-6643},
  doi = {10.3390/nu11051079},
  abstract = {The results of twenty years of research indicate that the inclusion of collagen peptides in the diet can lead to various improvements in health. According to the current protein quality evaluation method PDCAAS (Protein Digestibility-corrected Amino Acid Score), collagen protein lacks one indispensable amino acid (tryptophan) and is therefore categorized as an incomplete protein source. Collagen protein displays a low indispensable amino acid profile, yet as a functional food, collagen is a source of physiologically active peptides and conditionally indispensable amino acids that have the potential to optimize health and address physiological needs posed by aging and exercise. The objective of this study was to determine the maximum level of dietary collagen peptides that can be incorporated in the Western pattern diet while maintaining its indispensable amino acid balance. Iterative PDCAAS calculations showed that a level as high as 36\% of collagen peptides can be used as protein substitution in the daily diet while ensuring indispensable amino acid requirements are met. This study suggests that the effective amounts of functional collagen peptides (2.5 to 15 g per day) observed in the literature are below the maximum level of collagen that may be incorporated in the standard American diet.},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/Zotero/storage/AGYYCVZC/Paul et al. - 2019 - Significant Amounts of Functional Collagen Peptide.pdf},
  journal = {Nutrients},
  language = {en},
  number = {5}
}

@misc{pavlus00_MachinesBeatHumans,
  title = {Machines {{Beat Humans}} on a {{Reading Test}}. {{But Do They Understand}}?},
  author = {Pavlus, John},
  abstract = {A tool known as BERT can now beat humans on advanced reading-comprehension tests. But it's also revealed how far AI has to go.},
  howpublished = {https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/},
  journal = {Quanta Magazine}
}

@misc{pavlus00_MachinesBeatHumansa,
  title = {Machines {{Beat Humans}} on a {{Reading Test}}. {{But Do They Understand}}?},
  author = {Pavlus, John},
  abstract = {A tool known as BERT can now beat humans on advanced reading-comprehension tests. But it's also revealed how far AI has to go.},
  howpublished = {https://www.quantamagazine.org/machines-beat-humans-on-a-reading-test-but-do-they-understand-20191017/},
  journal = {Quanta Magazine},
  keywords = {favorite},
  language = {en}
}

@article{pawlowski17_ImplicitWeightUncertainty,
  title = {Implicit {{Weight Uncertainty}} in {{Neural Networks}}},
  author = {Pawlowski, Nick and Brock, Andrew and Lee, Matthew C. H. and Rajchl, Martin and Glocker, Ben},
  year = {2017},
  month = nov,
  abstract = {Modern neural networks tend to be overconfident on unseen, noisy or incorrectly labelled data and do not produce meaningful uncertainty measures. Bayesian deep learning aims to address this shortcoming with variational approximations (such as Bayes by Backprop or Multiplicative Normalising Flows). However, current approaches have limitations regarding flexibility and scalability. We introduce Bayes by Hypernet (BbH), a new method of variational approximation that interprets hypernetworks as implicit distributions. It naturally uses neural networks to model arbitrarily complex distributions and scales to modern deep learning architectures. In our experiments, we demonstrate that our method achieves competitive accuracies and predictive uncertainties on MNIST and a CIFAR5 task, while being the most robust against adversarial attacks.},
  archivePrefix = {arXiv},
  eprint = {1711.01297},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pawlowski et al_2017_implicit weight uncertainty in neural networks.pdf;/home/trung/Zotero/storage/6ZAZVWSS/1711.html},
  journal = {arXiv:1711.01297 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{pcawgdriversandfunctionalinterpretationworkinggroup20_Integrativepathwayenrichment,
  title = {Integrative Pathway Enrichment Analysis of Multivariate Omics Data},
  author = {{PCAWG Drivers and Functional Interpretation Working Group} and {PCAWG Consortium} and Paczkowska, Marta and Barenboim, Jonathan and Sintupisut, Nardnisa and Fox, Natalie S. and Zhu, Helen and {Abd-Rabbo}, Diala and Mee, Miles W. and Boutros, Paul C. and Reimand, J{\"u}ri},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {735},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-13983-9},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/MGEGB9PU/PCAWG Drivers and Functional Interpretation Working Group et al. - 2020 - Integrative pathway enrichment analysis of multiva.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{pcawgevolutionandheterogeneityworkinggroup20_Inferringstructuralvariant,
  title = {Inferring Structural Variant Cancer Cell Fraction},
  author = {{PCAWG Evolution and Heterogeneity Working Group} and {PCAWG Consortium} and Cmero, Marek and Yuan, Ke and Ong, Cheng Soon and Schr{\"o}der, Jan and Corcoran, Niall M. and Papenfuss, Tony and Hovens, Christopher M. and Markowetz, Florian and Macintyre, Geoff},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {730},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14351-8},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/ZV2F5EA5/PCAWG Evolution and Heterogeneity Working Group et al. - 2020 - Inferring structural variant cancer cell fraction.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{pcawgevolutionheterogeneityworkinggroup20_evolutionaryhistory658,
  title = {The Evolutionary History of 2,658 Cancers},
  author = {{PCAWG Evolution \& Heterogeneity Working Group} and {PCAWG Consortium} and Gerstung, Moritz and Jolly, Clemency and Leshchiner, Ignaty and Dentro, Stefan C. and Gonzalez, Santiago and Rosebrock, Daniel and Mitchell, Thomas J. and Rubanova, Yulia and Anur, Pavana and Yu, Kaixian and Tarabichi, Maxime and Deshwar, Amit and Wintersinger, Jeff and Kleinheinz, Kortine and {V{\'a}zquez-Garc{\'i}a}, Ignacio and Haase, Kerstin and Jerman, Lara and Sengupta, Subhajit and Macintyre, Geoff and Malikic, Salem and Donmez, Nilgun and Livitz, Dimitri G. and Cmero, Marek and Demeulemeester, Jonas and Schumacher, Steven and Fan, Yu and Yao, Xiaotong and Lee, Juhee and Schlesner, Matthias and Boutros, Paul C. and Bowtell, David D. and Zhu, Hongtu and Getz, Gad and Imielinski, Marcin and Beroukhim, Rameen and Sahinalp, S. Cenk and Ji, Yuan and Peifer, Martin and Markowetz, Florian and Mustonen, Ville and Yuan, Ke and Wang, Wenyi and Morris, Quaid D. and Spellman, Paul T. and Wedge, David C. and Van Loo, Peter},
  year = {2020},
  month = feb,
  volume = {578},
  pages = {122--128},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1907-7},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/DF7XUDFX/PCAWG Evolution & Heterogeneity Working Group et al. - 2020 - The evolutionary history of 2,658 cancers.pdf},
  journal = {Nature},
  language = {en},
  number = {7793}
}

@article{pcawgtranscriptomecoregroup20_GenomicbasisRNA,
  title = {Genomic Basis for {{RNA}} Alterations in Cancer},
  author = {{PCAWG Transcriptome Core Group} and {PCAWG Transcriptome Working Group} and {PCAWG Consortium} and Calabrese, Claudia and Davidson, Natalie R. and Demircio{\u g}lu, Deniz and Fonseca, Nuno A. and He, Yao and Kahles, Andr{\'e} and Lehmann, Kjong-Van and Liu, Fenglin and Shiraishi, Yuichi and Soulette, Cameron M. and Urban, Lara and Greger, Liliana and Li, Siliang and Liu, Dongbing and Perry, Marc D. and Xiang, Qian and Zhang, Fan and Zhang, Junjun and Bailey, Peter and Erkek, Serap and Hoadley, Katherine A. and Hou, Yong and Huska, Matthew R. and Kilpinen, Helena and Korbel, Jan O. and Marin, Maximillian G. and Markowski, Julia and Nandi, Tannistha and {Pan-Hammarstr{\"o}m}, Qiang and Pedamallu, Chandra Sekhar and Siebert, Reiner and Stark, Stefan G. and Su, Hong and Tan, Patrick and Waszak, Sebastian M. and Yung, Christina and Zhu, Shida and Awadalla, Philip and Creighton, Chad J. and Meyerson, Matthew and Ouellette, B. F. Francis and Wu, Kui and Yang, Huanming and Brazma, Alvis and Brooks, Angela N. and G{\"o}ke, Jonathan and R{\"a}tsch, Gunnar and Schwarz, Roland F. and Stegle, Oliver and Zhang, Zemin},
  year = {2020},
  month = feb,
  volume = {578},
  pages = {129--136},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-1970-0},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/EKIPBZYX/PCAWG Transcriptome Core Group et al. - 2020 - Genomic basis for RNA alterations in cancer.pdf},
  journal = {Nature},
  language = {en},
  number = {7793}
}

@incollection{pearl01_BayesianismCausalityWhy,
  title = {Bayesianism and {{Causality}}, or, {{Why I}} Am {{Only}} a {{Half}}-{{Bayesian}}},
  booktitle = {Foundations of {{Bayesianism}}},
  author = {Pearl, Judea},
  editor = {Gabbay, Dov M. and Barwise, Jon and Corfield, David and Williamson, Jon},
  year = {2001},
  volume = {24},
  pages = {19--36},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-017-1586-7_2},
  annotation = {ZSCC: NoCitationData[s1]},
  file = {/home/trung/GoogleDrive/Zotero/pearl_2001_bayesianism and causality, or, why i am only a half-bayesian.pdf},
  isbn = {978-90-481-5920-8 978-94-017-1586-7},
  keywords = {causal},
  language = {en}
}

@article{pearl13_CurseFreeWillParadox,
  title = {The {{Curse}} of {{Free}}-{{Will}} and the {{Paradox}} of {{Inevitable Regret}}},
  author = {Pearl, Judea},
  year = {2013},
  month = jan,
  volume = {1},
  issn = {2193-3677, 2193-3685},
  doi = {10.1515/jci-2013-0027},
  abstract = {The paradox described below aims to clarify the principles by which empirical data are harnessed to guide decision making. It is motivated by the practical question of whether empirical assessments of the effect of treatment on the treated (ETT) can be useful for either policy evaluation or personal decisions.},
  annotation = {ZSCC: 0000010},
  file = {/home/trung/GoogleDrive/Zotero/pearl_2013_the curse of free-will and the paradox of inevitable regret.pdf},
  journal = {Journal of Causal Inference},
  keywords = {causal},
  language = {en},
  number = {2}
}

@article{pearl18_WhatGainedLearning,
  title = {What Is {{Gained}} from {{Past Learning}}},
  author = {Pearl, Judea},
  year = {2018},
  month = mar,
  volume = {6},
  issn = {2193-3685},
  doi = {10.1515/jci-2018-0005},
  abstract = {This note reviews basic techniques of linear path analysis and demonstrates, using simple examples, how causal phenomena of non-trivial character can be understood, exemplified and analyzed using diagrams and a few algebraic steps. The techniques allow for swift assessment of how various features of the model impact the phenomenon under investigation. This includes: Simpson's paradox, case-control bias, selection bias, missing data, collider bias, reverse regression, bias amplification, near instruments, and measurement errors.},
  file = {/home/trung/GoogleDrive/Zotero/pearl_2018_what is gained from past learning.pdf},
  journal = {Journal of Causal Inference},
  keywords = {causal},
  language = {en},
  number = {1}
}

@misc{pechyonkin18_UnderstandingHintonCapsule,
  title = {Understanding {{Hinton}}'s {{Capsule Networks}}. {{Part I}}: {{Intuition}}.},
  shorttitle = {Understanding {{Hinton}}'s {{Capsule Networks}}. {{Part I}}},
  author = {Pechyonkin, Max},
  year = {2018},
  month = dec,
  abstract = {Part of Understanding Hinton's Capsule Networks Series:},
  file = {/home/trung/Zotero/storage/4JLXZWNP/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b.html},
  howpublished = {https://medium.com/ai\%C2\%B3-theory-practice-business/understanding-hintons-capsule-networks-part-i-intuition-b4b559d1159b},
  journal = {Medium},
  language = {en}
}

@article{peddinti00_timedelayneural,
  title = {A Time Delay Neural Network Architecture for Efficient Modeling of Long Temporal Contexts},
  author = {Peddinti, Vijayaditya and Povey, Daniel and Khudanpur, Sanjeev},
  pages = {5},
  abstract = {Recurrent neural network architectures have been shown to efficiently model long term temporal dependencies between acoustic events. However the training time of recurrent networks is higher than feedforward networks due to the sequential nature of the learning algorithm. In this paper we propose a time delay neural network architecture which models long term temporal dependencies with training times comparable to standard feed-forward DNNs. The network uses sub-sampling to reduce computation during training. On the Switchboard task we show a relative improvement of 6\% over the baseline DNN model. We present results on several LVCSR tasks with training data ranging from 3 to 1800 hours to show the effectiveness of the TDNN architecture in learning wider temporal dependencies in both small and large data scenarios.},
  file = {/home/trung/GoogleDrive/Zotero/peddinti et al_a time delay neural network architecture for efﬁcient modeling of long temporal contexts.pdf},
  language = {en}
}

@misc{pedregosa00_birdeyeviewoptimization,
  title = {A Bird-Eye View of Optimization Algorithms},
  author = {Pedregosa, Fabian},
  abstract = {A bird-eye view of optimization algorithms.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/YZ6MJRWK/eecs227at.html},
  howpublished = {http://distill.pub/},
  journal = {Distill},
  language = {en}
}

@article{peeples20_HistogramLayersTexture,
  title = {Histogram {{Layers}} for {{Texture Analysis}}},
  author = {Peeples, Joshua and Xu, Weihuang and Zare, Alina},
  year = {2020},
  month = jan,
  abstract = {We present a histogram layer for artificial neural networks (ANNs). An essential aspect of texture analysis is the extraction of features that describe the distribution of values in local spatial regions. The proposed histogram layer leverages the spatial distribution of features for texture analysis and parameters for the layer are estimated during backpropagation. We compare our method with state-of-the-art texture encoding methods such as the Deep Encoding Network (DEP) and Deep Texture Encoding Network (DeepTEN) on three texture datasets: (1) the Describable Texture Dataset (DTD); (2) an extension of the ground terrain in outdoor scenes (GTOS-mobile); (3) and a subset of the Materials in Context (MINC-2500) dataset. Results indicate that the inclusion of the proposed histogram layer improves performance. The source code for the histogram layer is publicly available.},
  archivePrefix = {arXiv},
  eprint = {2001.00215},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/peeples et al_2020_histogram layers for texture analysis.pdf;/home/trung/Zotero/storage/BK9EJKH4/2001.html},
  journal = {arXiv:2001.00215 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{pelsmaeker19_EffectiveEstimationDeep,
  title = {Effective {{Estimation}} of {{Deep Generative Language Models}}},
  author = {Pelsmaeker, Tom and Aziz, Wilker},
  year = {2019},
  month = sep,
  abstract = {Advances in variational inference enable parameterisation of probabilistic models by deep neural networks. This combines the statistical transparency of the probabilistic modelling framework with the representational power of deep learning. Yet, it seems difficult to effectively estimate such models in the context of language modelling. Even models based on rather simple generative stories struggle to make use of additional structure due to a problem known as posterior collapse. We concentrate on one such model, namely, a variational auto-encoder, which we argue is an important building block in hierarchical probabilistic models of language. This paper contributes a sober view of the problem, a survey of techniques to address it, novel techniques, and extensions to the model. Our experiments on modelling written English text support a number of recommendations that should help researchers interested in this exciting field.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1904.08194},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pelsmaeker et al_2019_effective estimation of deep generative language models.pdf},
  journal = {arXiv:1904.08194 [cs]},
  primaryClass = {cs}
}

@article{peng15_Currentstatusemergency,
  title = {Current Status of Emergency Department Triage in Mainland {{China}}: {{A}} Narrative Review of the Literature: {{Emergency}} Triage Status in {{China}}},
  shorttitle = {Current Status of Emergency Department Triage in Mainland {{China}}},
  author = {Peng, Lingli and Hammad, Karen},
  year = {2015},
  month = jun,
  volume = {17},
  pages = {148--158},
  issn = {14410745},
  doi = {10.1111/nhs.12159},
  annotation = {ZSCC: 0000010},
  file = {/home/trung/GoogleDrive/Zotero/peng et al_2015_current status of emergency department triage in mainland china.pdf},
  journal = {Nursing \& Health Sciences},
  language = {en},
  number = {2}
}

@article{peng19_Mixturefactorizedautoencoder,
  title = {Mixture Factorized Auto-Encoder for Unsupervised Hierarchical Deep Factorization of Speech Signal},
  author = {Peng, Zhiyuan and Feng, Siyuan and Lee, Tan},
  year = {2019},
  month = oct,
  abstract = {Speech signal is constituted and contributed by various informative factors, such as linguistic content and speaker characteristic. There have been notable recent studies attempting to factorize speech signal into these individual factors without requiring any annotation. These studies typically assume continuous representation for linguistic content, which is not in accordance with general linguistic knowledge and may make the extraction of speaker information less successful. This paper proposes the mixture factorized auto-encoder (mFAE) for unsupervised deep factorization. The encoder part of mFAE comprises a frame tokenizer and an utterance embedder. The frame tokenizer models linguistic content of input speech with a discrete categorical distribution. It performs frame clustering by assigning each frame a soft mixture label. The utterance embedder generates an utterance-level vector representation. A frame decoder serves to reconstruct speech features from the encoders'outputs. The mFAE is evaluated on speaker verification (SV) task and unsupervised subword modeling (USM) task. The SV experiments on VoxCeleb 1 show that the utterance embedder is capable of extracting speaker-discriminative embeddings with performance comparable to a x-vector baseline. The USM experiments on ZeroSpeech 2017 dataset verify that the frame tokenizer is able to capture linguistic content and the utterance embedder can acquire speaker-related information.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1911.01806},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/peng et al_2019_mixture factorized auto-encoder for unsupervised hierarchical deep factorization of speech signal.pdf;/home/trung/Zotero/storage/K5XSNDYW/1911.html},
  journal = {arXiv:1911.01806 [cs, eess]},
  primaryClass = {cs, eess}
}

@book{percival14_TestdrivendevelopmentPython,
  title = {Test-Driven Development with {{Python}}},
  author = {Percival, Harry},
  year = {2014},
  edition = {First edition},
  publisher = {{O'Reilly Media}},
  address = {{Sebastopol, CA}},
  annotation = {OCLC: ocn844460905},
  file = {/home/trung/GoogleDrive/Zotero/percival_2014_test-driven development with python.pdf},
  isbn = {978-1-4493-6482-3},
  keywords = {Application software,Development,Object-oriented programming (Computer science),Python (Computer program language),Web site development},
  language = {en},
  lccn = {QA76.73.P98 P46 2014}
}

@article{perez-carrasco19_AdversarialVariationalDomain,
  title = {Adversarial {{Variational Domain Adaptation}}},
  author = {{P{\'e}rez-Carrasco}, Manuel and {Cabrera-Vives}, Guillermo and Protopapas, Pavlos and Astorga, Nicol{\'a}s and Belhaj, Marouan},
  year = {2019},
  month = sep,
  abstract = {In this work we address the problem of transferring knowledge obtained from a vast annotated source domain to a low labeled or unlabeled target domain. We propose Adversarial Variational Domain Adaptation (AVDA), a semi-supervised domain adaptation method based on deep variational embedded representations. We use approximate inference and adversarial methods to map samples from source and target domains into an aligned semantic embedding. We show that on a semi-supervised few-shot scenario, our approach can be used to obtain a significant speed-up in performance when using an increasing number of labels on the target domain.},
  archivePrefix = {arXiv},
  eprint = {1909.11651},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pérez-carrasco et al_2019_adversarial variational domain adaptation.pdf;/home/trung/Zotero/storage/FEGF8DAN/1909.html},
  journal = {arXiv:1909.11651 [cs, stat]},
  keywords = {adversarial,Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{perez19_TuringCompletenessModern,
  title = {On the {{Turing Completeness}} of {{Modern Neural Network Architectures}}},
  author = {P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  year = {2019},
  month = jan,
  abstract = {Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser \& Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.},
  archivePrefix = {arXiv},
  eprint = {1901.03429},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pérez et al_2019_on the turing completeness of modern neural network architectures.pdf;/home/trung/Zotero/storage/AB75HMED/1901.html},
  journal = {arXiv:1901.03429 [cs, stat]},
  keywords = {Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning,Statistics - Machine Learning,turing test},
  primaryClass = {cs, stat}
}

@article{perez19_TuringCompletenessModerna,
  title = {On the {{Turing Completeness}} of {{Modern Neural Network Architectures}}},
  author = {P{\'e}rez, Jorge and Marinkovi{\'c}, Javier and Barcel{\'o}, Pablo},
  year = {2019},
  month = jan,
  abstract = {Alternatives to recurrent neural networks, in particular, architectures based on attention or convolutions, have been gaining momentum for processing input sequences. In spite of their relevance, the computational properties of these alternatives have not yet been fully explored. We study the computational power of two of the most paradigmatic architectures exemplifying these mechanisms: the Transformer (Vaswani et al., 2017) and the Neural GPU (Kaiser \& Sutskever, 2016). We show both models to be Turing complete exclusively based on their capacity to compute and access internal dense representations of the data. In particular, neither the Transformer nor the Neural GPU requires access to an external memory to become Turing complete. Our study also reveals some minimal sets of elements needed to obtain these completeness results.},
  archivePrefix = {arXiv},
  eprint = {1901.03429},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pérez et al_2019_on the turing completeness of modern neural network architectures2.pdf},
  journal = {arXiv:1901.03429 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pergola19_TDAMTopicDependentAttention,
  title = {{{TDAM}}: A {{Topic}}-{{Dependent Attention Model}} for {{Sentiment Analysis}}},
  shorttitle = {{{TDAM}}},
  author = {Pergola, Gabriele and Gui, Lin and He, Yulan},
  year = {2019},
  month = nov,
  volume = {56},
  pages = {102084},
  issn = {03064573},
  doi = {10.1016/j.ipm.2019.102084},
  abstract = {We propose a topic-dependent attention model for sentiment classification and topic extraction. Our model assumes that a global topic embedding is shared across documents and employs an attention mechanism to derive local topic embedding for words and sentences. These are subsequently incorporated in a modified Gated Recurrent Unit (GRU) for sentiment classification and extraction of topics bearing different sentiment polarities. Those topics emerge from the words' local topic embeddings learned by the internal attention of the GRU cells in the context of a multi-task learning framework. In this paper, we present the hierarchical architecture, the new GRU unit and the experiments conducted on users' reviews which demonstrate classification performance on a par with the state-of-the-art methodologies for sentiment classification and topic coherence outperforming the current approaches for supervised topic extraction. In addition, our model is able to extract coherent aspect-sentiment clusters despite using no aspect-level annotations for training.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.06435},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pergola et al_2019_tdam.pdf;/home/trung/Zotero/storage/42JD5Z4T/1908.html},
  journal = {Information Processing \& Management},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,lda},
  number = {6}
}

@article{pergola20_DisentangledAdversarialNeural,
  title = {A {{Disentangled Adversarial Neural Topic Model}} for {{Separating Opinions}} from {{Plots}} in {{User Reviews}}},
  author = {Pergola, Gabriele and Gui, Lin and He, Yulan},
  year = {2020},
  month = oct,
  abstract = {The flexibility of the inference process in Variational Autoencoders (VAEs) has recently led to revising traditional probabilistic topic models giving rise to Neural Topic Models (NTM). Although these approaches have achieved significant results, surprisingly very little work has been done on how to disentangle the latent topics. Existing topic models when applied to reviews may extract topics associated with writers' subjective opinions mixed with those related to factual descriptions such as plot summaries in movie and book reviews. It is thus desirable to automatically separate opinion topics from plot/neutral ones enabling a better interpretability. In this paper, we propose a neural topic model combined with adversarial training to disentangle opinion topics from plot and neutral ones. We conduct an extensive experimental assessment introducing a new collection of movie and book reviews paired with their plots, namely MOBO dataset, showing an improved coherence and variety of topics, a consistent disentanglement rate, and sentiment classification performance superior to other supervised topic models.},
  archivePrefix = {arXiv},
  eprint = {2010.11384},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pergola et al_2020_a disentangled adversarial neural topic model for separating opinions from plots in user reviews.pdf},
  journal = {arXiv:2010.11384 [cs]},
  keywords = {_tablet},
  primaryClass = {cs}
}

@article{petegrosso20_Machinelearningstatistical,
  title = {Machine Learning and Statistical Methods for Clustering Single-Cell {{RNA}}-Sequencing Data},
  author = {Petegrosso, Raphael and Li, Zhuliu and Kuang, Rui},
  year = {2020},
  month = jul,
  volume = {21},
  pages = {1209--1223},
  issn = {1477-4054},
  doi = {10.1093/bib/bbz063},
  abstract = {Single-cell RNAsequencing (scRNA-seq) technologies have enabled the large-scale whole-transcriptome profiling of each individual single cell in a cell population. A core analysis of the scRNA-seq transcriptome profiles is to cluster the single cells to reveal cell subtypes and infer cell lineages based on the relations among the cells. This article reviews the machine learning and statistical methods for clustering scRNA-seq transcriptomes developed in the past few years. The review focuses on how conventional clustering techniques such as hierarchical clustering, graph-based clustering, mixture models, \$k\$-means, ensemble learning, neural networks and density-based clustering are modified or customized to tackle the unique challenges in scRNA-seq data analysis, such as the dropout of low-expression genes, low and uneven read coverage of transcripts, highly variable total mRNAs from single cells and ambiguous cell markers in the presence of technical biases and irrelevant confounding biological variations. We review how cell-specific normalization, the imputation of dropouts and dimension reduction methods can be applied with new statistical or optimization strategies to improve the clustering of single cells. We will also introduce those more advanced approaches to cluster scRNA-seq transcriptomes in time series data and multiple cell populations and to detect rare cell types. Several software packages developed to support the cluster analysis of scRNA-seq data are also reviewed and experimentally compared to evaluate their performance and efficiency. Finally, we conclude with useful observations and possible future directions in scRNA-seq data analytics.All the source code and data are available at https://github.com/kuanglab/single-cell-review.},
  file = {/home/trung/GoogleDrive/Zotero/petegrosso et al_2020_machine learning and statistical methods for clustering single-cell rna-sequencing data.pdf},
  journal = {Briefings in Bioinformatics},
  keywords = {_tablet},
  number = {4}
}

@article{petersen00_MatrixCookbook,
  title = {The {{Matrix Cookbook}}},
  author = {Petersen, Kaare Brandt and Pedersen, Michael Syskind},
  pages = {72},
  file = {/home/trung/Zotero/storage/GQV4VPTN/Petersen and Pedersen - [ httpmatrixcookbook.com ].pdf},
  keywords = {favorite},
  language = {en}
}

@article{petroni19_LanguageModelsKnowledge,
  title = {Language {{Models}} as {{Knowledge Bases}}?},
  author = {Petroni, Fabio and Rockt{\"a}schel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
  year = {2019},
  month = sep,
  abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
  archivePrefix = {arXiv},
  eprint = {1909.01066},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/petroni et al_2019_language models as knowledge bases.pdf;/home/trung/Zotero/storage/HF9S32XZ/1909.html},
  journal = {arXiv:1909.01066 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{petropoulos16_SingleCellRNASeqReveals,
  title = {Single-{{Cell RNA}}-{{Seq Reveals Lineage}} and {{X Chromosome Dynamics}} in {{Human Preimplantation Embryos}}},
  author = {Petropoulos, Sophie and Edsg{\"a}rd, Daniel and Reinius, Bj{\"o}rn and Deng, Qiaolin and Panula, Sarita Pauliina and Codeluppi, Simone and Plaza Reyes, Alvaro and Linnarsson, Sten and Sandberg, Rickard and Lanner, Fredrik},
  year = {2016},
  month = may,
  volume = {165},
  pages = {1012--1026},
  issn = {00928674},
  doi = {10.1016/j.cell.2016.03.023},
  abstract = {Mouse studies have been instrumental in forming our current understanding of early cell-lineage decisions; however, similar insights into the early human development are severely limited. Here, we present a comprehensive transcriptional map of human embryo development, including the sequenced transcriptomes of 1,529 individual cells from 88 human preimplantation embryos. These data show that cells undergo an intermediate state of co-expression of lineage-specific genes, followed by a concurrent establishment of the trophectoderm, epiblast, and primitive endoderm lineages, which coincide with blastocyst formation. Female cells of all three lineages achieve dosage compensation of X chromosome RNA levels prior to implantation. However, in contrast to the mouse, XIST is transcribed from both alleles throughout the progression of this expression dampening, and X chromosome genes maintain biallelic expression while dosage compensation proceeds. We envision broad utility of this transcriptional atlas in future studies on human development as well as in stem cell research.},
  file = {/home/trung/GoogleDrive/Zotero/petropoulos et al_2016_single-cell rna-seq reveals lineage and x chromosome dynamics in human preimplantation embryos.pdf},
  journal = {Cell},
  language = {en},
  number = {4}
}

@article{peychev17_QuantifyingEffectsEnforcing,
  title = {Quantifying the {{Effects}} of {{Enforcing Disentanglement}} on {{Variational Autoencoders}}},
  author = {Peychev, Momchil and Veli{\v c}kovi{\'c}, Petar and Li{\`o}, Pietro},
  year = {2017},
  month = nov,
  abstract = {The notion of disentangled autoencoders was proposed as an extension to the variational autoencoder by introducing a disentanglement parameter \$\textbackslash beta\$, controlling the learning pressure put on the possible underlying latent representations. For certain values of \$\textbackslash beta\$ this kind of autoencoders is capable of encoding independent input generative factors in separate elements of the code, leading to a more interpretable and predictable model behaviour. In this paper we quantify the effects of the parameter \$\textbackslash beta\$ on the model performance and disentanglement. After training multiple models with the same value of \$\textbackslash beta\$, we establish the existence of consistent variance in one of the disentanglement measures, proposed in literature. The negative consequences of the disentanglement to the autoencoder's discriminative ability are also asserted while varying the amount of examples available during training.},
  archivePrefix = {arXiv},
  eprint = {1711.09159},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/peychev et al_2017_quantifying the effects of enforcing disentanglement on variational autoencoders.pdf},
  journal = {arXiv:1711.09159 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{pezeshki16_DeconstructingLadderNetwork,
  title = {Deconstructing the {{Ladder Network Architecture}}},
  author = {Pezeshki, Mohammad and Fan, Linxi and Brakel, Philemon and Courville, Aaron and Bengio, Yoshua},
  year = {2016},
  month = may,
  abstract = {The Manual labeling of data is and will remain a costly endeavor. For this reason, semi-supervised learning remains a topic of practical importance. The recently proposed Ladder Network is one such approach that has proven to be very successful. In addition to the supervised objective, the Ladder Network also adds an unsupervised objective corresponding to the reconstruction costs of a stack of denoising autoencoders. Although the empirical results are impressive, the Ladder Network has many components intertwined, whose contributions are not obvious in such a complex architecture. In order to help elucidate and disentangle the different ingredients in the Ladder Network recipe, this paper presents an extensive experimental investigation of variants of the Ladder Network in which we replace or remove individual components to gain more insight into their relative importance. We find that all of the components are necessary for achieving optimal performance, but they do not contribute equally. For semi-supervised tasks, we conclude that the most important contribution is made by the lateral connection, followed by the application of noise, and finally the choice of what we refer to as the `combinator function' in the decoder path. We also find that as the number of labeled training examples increases, the lateral connections and reconstruction criterion become less important, with most of the improvement in generalization being due to the injection of noise in each layer. Furthermore, we present a new type of combinator function that outperforms the original design in both fully- and semi-supervised tasks, reducing record test error rates on Permutation-Invariant MNIST to 0.57\% for the supervised setting, and to 0.97\% and 1.0\% for semi-supervised settings with 1000 and 100 labeled examples respectively.},
  archivePrefix = {arXiv},
  eprint = {1511.06430},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pezeshki et al_2016_deconstructing the ladder network architecture.pdf;/home/trung/Zotero/storage/9QPJ2RUV/1511.html},
  journal = {arXiv:1511.06430 [cs]},
  keywords = {Computer Science - Machine Learning,ladder network},
  primaryClass = {cs}
}

@article{pfau20_DisentanglingSubspaceDiffusion,
  title = {Disentangling by {{Subspace Diffusion}}},
  author = {Pfau, David and Higgins, Irina and Botev, Aleksandar and Racani{\`e}re, S{\'e}bastien},
  year = {2020},
  month = jun,
  abstract = {We present a novel nonparametric algorithm for symmetry-based disentangling of data manifolds, the Geometric Manifold Component Estimator (GEOMANCER). GEOMANCER provides a partial answer to the question posed by Higgins et al. (2018): is it possible to learn how to factorize a Lie group solely from observations of the orbit of an object it acts on? We show that fully unsupervised factorization of a data manifold is possible if the true metric of the manifold is known and each factor manifold has nontrivial holonomy \textendash{} for example, rotation in 3D. Our algorithm works by estimating the subspaces that are invariant under random walk diffusion, giving an approximation to the de Rham decomposition from differential geometry. We demonstrate the efficacy of GEOMANCER on several complex synthetic manifolds.1 Our work reduces the question of whether unsupervised disentangling is possible to the question of whether unsupervised metric learning is possible, providing a unifying insight into the geometric nature of representation learning.},
  archivePrefix = {arXiv},
  eprint = {2006.12982},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pfau et al_2020_disentangling by subspace diffusion.pdf},
  journal = {arXiv:2006.12982 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pfister18_IdentifyingCausalStructure,
  title = {Identifying {{Causal Structure}} in {{Large}}-{{Scale Kinetic Systems}}},
  author = {Pfister, Niklas and Bauer, Stefan and Peters, Jonas},
  year = {2018},
  month = oct,
  abstract = {In the natural sciences, differential equations are widely used to describe dynamical systems. The discovery and verification of such models from data has become a fundamental challenge of science today. From a statistical point of view, we distinguish two problems: parameter estimation and structure search. In parameter estimation, we start from a given differential equation and estimate the parameters from noisy data that are observed at discrete time points. The estimate depends nonlinearly on the parameters. This poses both statistical and computational challenges and makes the task of structure search even more ambitious. Existing methods use either standard model selection techniques or various types of sparsity enforcing regularization, hence focusing on predictive performance. In this work, we develop novel methodology for structure search in ordinary differential equation models. Exploiting ideas from causal inference, we propose to rank models not only by their predictive performance, but also by taking into account stability, i.e., their ability to predict well in different experimental settings. Based on this model ranking we also construct a ranking of individual variables reflecting causal importance. It provides researchers with a list of promising candidate variables that may be investigated further in interventional experiments. Our ranking methodology (both for models and variables) comes with theoretical asymptotic guarantees and is shown to outperform current state-of-the art methods based on extensive experimental evaluation on simulated data. Practical applicability of the procedure is illustrated on a not yet published biological data set. Our methodology is fully implemented. Code will be provided online and will also be made available as an R package.},
  archivePrefix = {arXiv},
  eprint = {1810.11776},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pfister et al_2018_identifying causal structure in large-scale kinetic systems.pdf;/home/trung/Zotero/storage/DZCNUIKG/1810.html},
  journal = {arXiv:1810.11776 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{philippe16_physicalzeroknowledgeobjectcomparison,
  title = {A Physical Zero-Knowledge Object-Comparison System for Nuclear Warhead Verification},
  author = {Philippe, S{\'e}bastien and Goldston, Robert J. and Glaser, Alexander and {d'Errico}, Francesco},
  year = {2016},
  month = sep,
  volume = {7},
  pages = {1--7},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/ncomms12890},
  abstract = {Zero-knowledge proofs can be used to prove that a statement is true without revealing why it is. Here the authors demonstrate a non-electronic fast neutron radiography technique to confirm that two objects are identical without revealing any details about their design or composition.},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {2016 The Author(s)},
  file = {/home/trung/GoogleDrive/Zotero/philippe et al_2016_a physical zero-knowledge object-comparison system for nuclear warhead verification.pdf;/home/trung/Zotero/storage/G2ZSL49L/ncomms12890.html},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{phuong18_mutualautoencoderControlling,
  title = {The Mutual Autoencoder: {{Controlling}} Information in Latent Code Representations},
  author = {Phuong, Mary and Welling, Max and Kushman, Nate and Tomioka, Ryota and Nowozin, Sebastian},
  year = {2018},
  file = {/home/trung/GoogleDrive/Zotero/phuong et al_2018_the mutual autoencoder.pdf},
  keywords = {information}
}

@article{pierson15_ZIFADimensionalityreduction,
  title = {{{ZIFA}}: {{Dimensionality}} Reduction for Zero-Inflated Single-Cell Gene Expression Analysis},
  shorttitle = {{{ZIFA}}},
  author = {Pierson, Emma and Yau, Christopher},
  year = {2015},
  month = dec,
  volume = {16},
  issn = {1474-760X},
  doi = {10.1186/s13059-015-0805-z},
  abstract = {Single-cell RNA-seq data allows insight into normal cellular function and various disease states through molecular characterization of gene expression on the single cell level. Dimensionality reduction of such high-dimensional data sets is essential for visualization and analysis, but single-cell RNA-seq data are challenging for classical dimensionalityreduction methods because of the prevalence of dropout events, which lead to zero-inflated data. Here, we develop a dimensionality-reduction method, (Z)ero (I)nflated (F)actor (A)nalysis (ZIFA), which explicitly models the dropout characteristics, and show that it improves modeling accuracy on simulated and biological data sets.},
  file = {/home/trung/GoogleDrive/Zotero/pierson et al_2015_zifa.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{pineau18_InfoCatVAERepresentationLearning,
  title = {{{InfoCatVAE}}: {{Representation Learning}} with {{Categorical Variational Autoencoders}}},
  shorttitle = {{{InfoCatVAE}}},
  author = {Pineau, Edouard and Lelarge, Marc},
  year = {2018},
  month = jun,
  abstract = {This paper describes InfoCatVAE, an extension of the variational autoencoder that enables unsupervised disentangled representation learning. InfoCatVAE uses multimodal distributions for the prior and the inference network and then maximizes the evidence lower bound objective (ELBO). We connect the new ELBO derived for our model with a natural soft clustering objective which explains the robustness of our approach. We then adapt the InfoGANs method to our setting in order to maximize the mutual information between the categorical code and the generated inputs and obtain an improved model.},
  archivePrefix = {arXiv},
  eprint = {1806.08240},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pineau et al_2018_infocatvae.pdf},
  journal = {arXiv:1806.08240 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{pinu19_SystemsBiologyMultiOmics,
  title = {Systems {{Biology}} and {{Multi}}-{{Omics Integration}}: {{Viewpoints}} from the {{Metabolomics Research Community}}},
  shorttitle = {Systems {{Biology}} and {{Multi}}-{{Omics Integration}}},
  author = {Pinu, Farhana R. and Beale, David J. and Paten, Amy M. and Kouremenos, Konstantinos and Swarup, Sanjay and Schirra, Horst J. and Wishart, David},
  year = {2019},
  month = apr,
  volume = {9},
  pages = {76},
  issn = {2218-1989},
  doi = {10.3390/metabo9040076},
  abstract = {The use of multiple omics techniques (i.e., genomics, transcriptomics, proteomics, and metabolomics) is becoming increasingly popular in all facets of life science. Omics techniques provide a more holistic molecular perspective of studied biological systems compared to traditional approaches. However, due to their inherent data differences, integrating multiple omics platforms remains an ongoing challenge for many researchers. As metabolites represent the downstream products of multiple interactions between genes, transcripts, and proteins, metabolomics, the tools and approaches routinely used in this field could assist with the integration of these complex multi-omics data sets. The question is, how? Here we provide some answers (in terms of methods, software tools and databases) along with a variety of recommendations and a list of continuing challenges as identified during a peer session on multi-omics integration that was held at the recent `Australian and New Zealand Metabolomics Conference' (ANZMET 2018) in Auckland, New Zealand (Sept. 2018). We envisage that this document will serve as a guide to metabolomics researchers and other members of the community wishing to perform multi-omics studies. We also believe that these ideas may allow the full promise of integrated multi-omics research and, ultimately, of systems biology to be realized.},
  annotation = {ZSCC: 0000030},
  file = {/home/trung/Zotero/storage/FMREHFFU/Pinu et al. - 2019 - Systems Biology and Multi-Omics Integration Viewp.pdf},
  journal = {Metabolites},
  language = {en},
  number = {4}
}

@article{piponi20_JointDistributionsTensorFlow,
  title = {Joint {{Distributions}} for {{TensorFlow Probability}}},
  author = {Piponi, Dan and Moore, Dave and Dillon, Joshua V.},
  year = {2020},
  month = jan,
  abstract = {A central tenet of probabilistic programming is that a model is specified exactly once in a canonical representation which is usable by inference algorithms. We describe JointDistributions, a family of declarative representations of directed graphical models in TensorFlow Probability.},
  archivePrefix = {arXiv},
  eprint = {2001.11819},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/piponi et al_2020_joint distributions for tensorflow probability.pdf},
  journal = {arXiv:2001.11819 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{pitas19_limitationsnormbased,
  title = {Some Limitations of Norm Based Generalization Bounds in Deep Neural Networks},
  author = {Pitas, Konstantinos and Loukas, Andreas and Davies, Mike and Vandergheynst, Pierre},
  year = {2019},
  month = may,
  abstract = {Deep convolutional neural networks have been shown to be able to fit a labeling over random data while still being able to generalize well on normal datasets. Describing deep convolutional neural network capacity through the measure of spectral complexity has been recently proposed to tackle this apparent paradox. Spectral complexity correlates with GE and can distinguish networks trained on normal and random labels. We propose the first GE bound based on spectral complexity for deep convolutional neural networks and provide tighter bounds by orders of magnitude from the previous estimate. We then investigate theoretically and empirically the insensitivity of spectral complexity to invariances of modern deep convolutional neural networks, and show several limitations of spectral complexity that occur as a result.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1905.09677},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pitas et al_2019_some limitations of norm based generalization bounds in deep neural networks.pdf;/home/trung/Zotero/storage/4W7BFNJI/1905.html},
  journal = {arXiv:1905.09677 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{platt14_Disorderscholesterolmetabolism,
  title = {Disorders of Cholesterol Metabolism and Their Unanticipated Convergent Mechanisms of Disease},
  author = {Platt, Frances M. and Wassif, Christopher and Colaco, Alexandria and Dardis, Andrea and {Lloyd-Evans}, Emyr and Bembi, Bruno and Porter, D. Forbes},
  year = {2014},
  volume = {15},
  pages = {173--194},
  issn = {1527-8204},
  doi = {10.1146/annurev-genom-091212-153412},
  annotation = {ZSCC: 0000034},
  file = {/home/trung/GoogleDrive/Zotero/platt et al_2014_disorders of cholesterol metabolism and their unanticipated convergent mechanisms of disease.pdf},
  journal = {Annual review of genomics and human genetics},
  pmcid = {PMC6292211},
  pmid = {25184529}
}

@article{pliner19_Supervisedclassificationenables,
  title = {Supervised Classification Enables Rapid Annotation of Cell Atlases},
  author = {Pliner, Hannah A. and Shendure, Jay and Trapnell, Cole},
  year = {2019},
  month = oct,
  volume = {16},
  pages = {983--986},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0535-3},
  annotation = {ZSCC: 0000041},
  file = {/home/trung/Zotero/storage/7CED3KPH/Pliner et al. - 2019 - Supervised classification enables rapid annotation.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {10}
}

@article{plumerault20_Controllinggenerativemodels,
  title = {Controlling Generative Models with Continuous Factors of Variations},
  author = {Plumerault, Antoine and Borgne, Herv{\'e} Le and Hudelot, C{\'e}line},
  year = {2020},
  month = jan,
  abstract = {Recent deep generative models are able to provide photo-realistic images as well as visual or textual content embeddings useful to address various tasks of computer vision and natural language processing. Their usefulness is nevertheless often limited by the lack of control over the generative process or the poor understanding of the learned representation. To overcome these major issues, very recent work has shown the interest of studying the semantics of the latent space of generative models. In this paper, we propose to advance on the interpretability of the latent space of generative models by introducing a new method to find meaningful directions in the latent space of any generative model along which we can move to control precisely specific properties of the generated image like the position or scale of the object in the image. Our method does not require human annotations and is particularly well suited for the search of directions encoding simple transformations of the generated image, such as translation, zoom or color variations. We demonstrate the effectiveness of our method qualitatively and quantitatively, both for GANs and variational auto-encoders.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.10238},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/TAM96CHU/Plumerault et al. - 2020 - Controlling generative models with continuous fact.pdf},
  journal = {arXiv:2001.10238 [cs, eess, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@inproceedings{pmlr-v124-kugelgen20a,
  title = {Semi-Supervised Learning, Causality, and the Conditional Cluster Assumption},
  author = {{von K{\"u}gelgen}, Julius and Mey, Alexander and Loog, Marco and Sch{\"o}lkopf, Bernhard},
  editor = {Peters, Jonas and Sontag, David},
  year = {2020},
  month = aug,
  volume = {124},
  pages = {1--10},
  publisher = {{PMLR}},
  address = {{Virtual}},
  abstract = {While the success of semi-supervised learning (SSL) is still not fully understood, Sch\"olkopf et al. (2012) have established a link to the principle of independent causal mechanisms. They conclude that SSL should be impossible when predicting a target variable from its causes, but possible when predicting it from its effects. Since both these cases are restrictive, we extend their work by considering classification using cause and effect features at the same time, such as predicting a disease from both risk factors and symptoms. While standard SSL exploits information contained in the marginal distribution of all inputs (to improve the estimate of the conditional distribution of the target given in-puts), we argue that in our more general setting we should use information in the conditional distribution of effect features given causal features. We explore how this insight generalises the previous understanding, and how it relates to and can be exploited algorithmically for SSL.},
  file = {/home/trung/GoogleDrive/Zotero/von kügelgen et al_2020_semi-supervised learning, causality, and the conditional cluster assumption.pdf},
  keywords = {causal,disentanglement},
  pdf = {http://proceedings.mlr.press/v124/kugelgen20a/kugelgen20a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v97-ho19a,
  title = {Flow++: {{Improving}} Flow-Based Generative Models with Variational Dequantization and Architecture Design},
  author = {Ho, Jonathan and Chen, Xi and Srinivas, Aravind and Duan, Yan and Abbeel, Pieter},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {2722--2730},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  file = {/home/trung/GoogleDrive/Zotero/ho et al_2019_flow++.pdf;/home/trung/GoogleDrive/Zotero/ho et al_2019_flow++2.pdf},
  pdf = {http://proceedings.mlr.press/v97/ho19a/ho19a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@inproceedings{pmlr-v97-park19a,
  title = {Variational {{Laplace}} Autoencoders},
  author = {Park, Yookoon and Kim, Chris and Kim, Gunhee},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  volume = {97},
  pages = {5032--5041},
  publisher = {{PMLR}},
  address = {{Long Beach, California, USA}},
  abstract = {Variational autoencoders employ an amortized inference model to approximate the posterior of latent variables. However, such amortized variational inference faces two challenges: (1) the limited posterior expressiveness of fully-factorized Gaussian assumption and (2) the amortization error of the inference model. We present a novel approach that addresses both challenges. First, we focus on ReLU networks with Gaussian output and illustrate their connection to probabilistic PCA. Building on this observation, we derive an iterative algorithm that finds the mode of the posterior and apply fullcovariance Gaussian posterior approximation centered on the mode. Subsequently, we present a general framework named Variational Laplace Autoencoders (VLAEs) for training deep generative models. Based on the Laplace approximation of the latent variable posterior, VLAEs enhance the expressiveness of the posterior while reducing the amortization error. Empirical results on MNIST, Omniglot, Fashion-MNIST, SVHN and CIFAR10 show that the proposed approach significantly outperforms other recent amortized or iterative methods on the ReLU networks.},
  file = {/home/trung/GoogleDrive/Zotero/2019/false;/home/trung/GoogleDrive/Zotero/park et al_2019_variational laplace autoencoders.pdf},
  keywords = {vae_issues},
  pdf = {http://proceedings.mlr.press/v97/park19a/park19a.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{podareanu00_BestPracticeGuide,
  title = {Best {{Practice Guide}} - {{Deep Learning}}},
  author = {Podareanu, Damian},
  pages = {51},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/podareanu_best practice guide - deep learning.pdf},
  journal = {Deep Learning},
  language = {en}
}

@article{poggio18_TheoryIIIbGeneralization,
  title = {Theory {{IIIb}}: {{Generalization}} in {{Deep Networks}}},
  shorttitle = {Theory {{IIIb}}},
  author = {Poggio, Tomaso and Liao, Qianli and Miranda, Brando and Banburski, Andrzej and Boix, Xavier and Hidary, Jack},
  year = {2018},
  month = jun,
  abstract = {A main puzzle of deep neural networks (DNNs) revolves around the apparent absence of "overfitting", defined in this paper as follows: the expected error does not get worse when increasing the number of neurons or of iterations of gradient descent. This is surprising because of the large capacity demonstrated by DNNs to fit randomly labeled data and the absence of explicit regularization. Recent results by Srebro et al. provide a satisfying solution of the puzzle for linear networks used in binary classification. They prove that minimization of loss functions such as the logistic, the cross-entropy and the exp-loss yields asymptotic, "slow" convergence to the maximum margin solution for linearly separable datasets, independently of the initial conditions. Here we prove a similar result for nonlinear multilayer DNNs near zero minima of the empirical loss. The result holds for exponential-type losses but not for the square loss. In particular, we prove that the weight matrix at each layer of a deep network converges to a minimum norm solution up to a scale factor (in the separable case). Our analysis of the dynamical system corresponding to gradient descent of a multilayer network suggests a simple criterion for ranking the generalization performance of different zero minimizers of the empirical loss.},
  annotation = {ZSCC: 0000012},
  archivePrefix = {arXiv},
  eprint = {1806.11379},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/poggio et al_2018_theory iiib.pdf;/home/trung/Zotero/storage/GVHWXWP8/1806.html},
  journal = {arXiv:1806.11379 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{poldrack00_StatisticalThinking21st,
  title = {Statistical {{Thinking}} for the 21st {{Century}}},
  author = {Poldrack, Copyright 2019 Russell A.},
  abstract = {A book about statistics.}
}

@article{poltavski06_Effectstransdermalnicotine,
  title = {Effects of Transdermal Nicotine on Attention in Adult Non-Smokers with and without Attentional Deficits},
  author = {Poltavski, D. V. and Petros, T.},
  year = {2006},
  month = mar,
  volume = {87},
  pages = {614--624},
  issn = {0031-9384},
  doi = {10.1016/j.physbeh.2005.12.011},
  abstract = {Extant evidence suggests a possibility of self-medication to account for greater prevalence of cigarette smoking among adults with ADHD as they tend to show improvements on affective and cognitive measures, particularly on measures of sustained attention following nicotine administration. The present study was conducted to evaluate whether adult non-smokers with low attentiveness might exhibit greater improvements on measures of sustained attention than those with higher attentiveness using neuropsychological tests that had previously shown sensitivity to ADHD. On the basis of their scores on attention scales used in the diagnosis of adult ADHD, 62 male non-smokers were divided into 2 groups of either low or high attentiveness and treated with either a placebo or 7 mg nicotine patch. After 6 h of patch application each participant completed the Wisconsin Card Sorting Test (WCST), classic Stroop task, and Conners' Continuous Performance Test (CPT), which were administered in a counterbalanced order and a double-blind manner. No significant drug or group differences were observed on the Stroop task. On the Conners' CPT participants in the low attention group treated with nicotine committed significantly fewer errors of commission, showed improved stimulus detectability and fewer perseverations than those in the low attention placebo group. On the WCST nicotine significantly impaired the ability of participants in the high attention group to learn effective strategies to complete the test with fewer trials. The results showed nicotine-induced improvement on some measures of sustained attention in the low attention group and some decrement in working memory in the high attention group, which suggests that nicotine tends to optimize rather than improve performance on cognitive tasks.},
  journal = {Physiology \& Behavior},
  language = {eng},
  number = {3},
  pmid = {16466655}
}

@article{polu20_GenerativeLanguageModeling,
  title = {Generative {{Language Modeling}} for {{Automated Theorem Proving}}},
  author = {Polu, Stanislas and Sutskever, Ilya},
  year = {2020},
  month = sep,
  abstract = {We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.},
  archivePrefix = {arXiv},
  eprint = {2009.03393},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/polu et al_2020_generative language modeling for automated theorem proving.pdf},
  journal = {arXiv:2009.03393 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{polu20_GenerativeLanguageModelinga,
  title = {Generative {{Language Modeling}} for {{Automated Theorem Proving}}},
  author = {Polu, Stanislas and Sutskever, Ilya},
  year = {2020},
  month = sep,
  abstract = {We explore the application of transformer-based language models to automated theorem proving. This work is motivated by the possibility that a major limitation of automated theorem provers compared to humans -- the generation of original mathematical terms -- might be addressable via generation from language models. We present an automated prover and proof assistant, GPT-f, for the Metamath formalization language, and analyze its performance. GPT-f found new short proofs that were accepted into the main Metamath library, which is to our knowledge, the first time a deep-learning based system has contributed proofs that were adopted by a formal mathematics community.},
  archivePrefix = {arXiv},
  eprint = {2009.03393},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/polu et al_2020_generative language modeling for automated theorem proving2.pdf},
  journal = {arXiv:2009.03393 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{pons19_musicnnPretrainedconvolutional,
  title = {Musicnn: {{Pre}}-Trained Convolutional Neural Networks for Music Audio Tagging},
  shorttitle = {Musicnn},
  author = {Pons, Jordi and Serra, Xavier},
  year = {2019},
  month = sep,
  abstract = {Pronounced as "musician", the musicnn library contains a set of pre-trained musically motivated convolutional neural networks for music audio tagging: https://github.com/jordipons/musicnn. This repository also includes some pre-trained vgg-like baselines. These models can be used as out-of-the-box music audio taggers, as music feature extractors, or as pre-trained models for transfer learning. We also provide the code to train the aforementioned models: https://github.com/jordipons/musicnn-training. This framework also allows implementing novel models. For example, a musically motivated convolutional neural network with an attention-based output layer (instead of the temporal pooling layer) can achieve state-of-the-art results for music audio tagging: 90.77 ROC-AUC / 38.61 PR-AUC on the MagnaTagATune dataset --- and 88.81 ROC-AUC / 31.51 PR-AUC on the Million Song Dataset.},
  archivePrefix = {arXiv},
  eprint = {1909.06654},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pons et al_2019_musicnn.pdf;/home/trung/Zotero/storage/SSMLCAFR/1909.html},
  journal = {arXiv:1909.06654 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{ponti20_ParameterSpaceFactorization,
  title = {Parameter {{Space Factorization}} for {{Zero}}-{{Shot Learning}} across {{Tasks}} and {{Languages}}},
  author = {Ponti, Edoardo M. and Vuli{\'c}, Ivan and Cotterell, Ryan and Parovic, Marinela and Reichart, Roi and Korhonen, Anna},
  year = {2020},
  month = jan,
  abstract = {Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task\textendash language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task\textendash language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-theart, zero-shot cross-lingual transfer methods; it increases performance by 4.49 points for POS tagging and 7.73 points for NER on average compared to the strongest baseline.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.11453},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/39K6FQC7/Ponti et al. - 2020 - Parameter Space Factorization for Zero-Shot Learni.pdf},
  journal = {arXiv:2001.11453 [cs]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs}
}

@article{poole19_VariationalBoundsMutual,
  title = {On {{Variational Bounds}} of {{Mutual Information}}},
  author = {Poole, Ben and Ozair, Sherjil and van den Oord, Aaron and Alemi, Alexander A. and Tucker, George},
  year = {2019},
  month = may,
  abstract = {Estimating and optimizing Mutual Information (MI) is core to many problems in machine learning; however, bounding MI in high dimensions is challenging. To establish tractable and scalable objectives, recent work has turned to variational bounds parameterized by neural networks, but the relationships and tradeoffs between these bounds remains unclear. In this work, we unify these recent developments in a single framework. We find that the existing variational lower bounds degrade when the MI is large, exhibiting either high bias or high variance. To address this problem, we introduce a continuum of lower bounds that encompasses previous bounds and flexibly trades off bias and variance. On high-dimensional, controlled problems, we empirically characterize the bias and variance of the bounds and their gradients and demonstrate the effectiveness of our new bounds for estimation and representation learning.},
  annotation = {ZSCC: 0000019},
  archivePrefix = {arXiv},
  eprint = {1905.06922},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/poole et al_2019_on variational bounds of mutual information.pdf},
  journal = {arXiv:1905.06922 [cs, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, stat}
}

@article{popel18_TrainingTipsTransformer,
  title = {Training {{Tips}} for the {{Transformer Model}}},
  author = {Popel, Martin and Bojar, Ond{\v r}ej},
  year = {2018},
  month = apr,
  volume = {110},
  pages = {43--70},
  issn = {1804-0462},
  doi = {10.2478/pralin-2018-0002},
  abstract = {This article describes our experiments in neural machine translation using the recent Tensor2Tensor framework and the Transformer sequence-to-sequence model (Vaswani et al., 2017). We examine some of the critical parameters that affect the final translation quality, memory usage, training stability and training time, concluding each experiment with a set of recommendations for fellow researchers. In addition to confirming the general mantra "more data and larger models", we address scaling to multiple GPUs and provide practical tips for improved training regarding batch size, learning rate, warmup steps, maximum sentence length and checkpoint averaging. We hope that our observations will allow others to get better results given their particular hardware and data constraints.},
  annotation = {ZSCC: 0000037},
  archivePrefix = {arXiv},
  eprint = {1804.00247},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/popel et al_2018_training tips for the transformer model.pdf;/home/trung/Zotero/storage/CW3S6EWP/1804.html},
  journal = {The Prague Bulletin of Mathematical Linguistics},
  keywords = {Computer Science - Computation and Language},
  number = {1}
}

@article{postels00_SamplingFreeEpistemicUncertainty,
  title = {Sampling-{{Free Epistemic Uncertainty Estimation Using Approximated Variance Propagation}}},
  author = {Postels, Janis and Ferroni, Francesco and Coskun, Huseyin and Navab, Nassir and Tombari, Federico},
  pages = {10},
  abstract = {We present a sampling-free approach for computing the epistemic uncertainty of a neural network. Epistemic uncertainty is an important quantity for the deployment of deep neural networks in safety-critical applications, since it represents how much one can trust predictions on new data. Recently promising works were proposed using noise injection combined with Monte-Carlo (MC) sampling at inference time to estimate this quantity (e.g. MC dropout). Our main contribution is an approximation of the epistemic uncertainty estimated by these methods that does not require sampling, thus notably reducing the computational overhead. We apply our approach to large-scale visual tasks (i.e., semantic segmentation and depth regression) to demonstrate the advantages of our method compared to sampling-based approaches in terms of quality of the uncertainty estimates as well as of computational overhead.},
  annotation = {ZSCC: 0000003},
  file = {/home/trung/Zotero/storage/T9QVLE96/Postels et al. - Sampling-Free Epistemic Uncertainty Estimation Usi.pdf},
  language = {en}
}

@article{potapczynski00_InvertibleGaussianReparameterization,
  title = {Invertible {{Gaussian Reparameterization}}: {{Revisiting}} the {{Gumbel}}-{{Softmax}}},
  author = {Potapczynski, Andres and {Loaiza-Ganem}, Gabriel and Cunningham, John P},
  pages = {11},
  abstract = {The Gumbel-Softmax is a continuous distribution over the simplex that is often used as a relaxation of discrete distributions. Because it can be readily interpreted and easily reparameterized, it enjoys widespread use. We propose a modular and more flexible family of reparameterizable distributions where Gaussian noise is transformed into a one-hot approximation through an invertible function. This invertible function is composed of a modified softmax and can incorporate diverse transformations that serve different specific purposes. For example, the stickbreaking procedure allows us to extend the reparameterization trick to distributions with countably infinite support, thus enabling the use of our distribution along nonparametric models, or normalizing flows let us increase the flexibility of the distribution. Our construction enjoys theoretical advantages over the GumbelSoftmax, such as closed form KL, and significantly outperforms it in a variety of experiments. Our code is available at https://github.com/cunningham-lab/ igr.},
  file = {/home/trung/Zotero/storage/6RZDU2IS/Potapczynski et al. - Invertible Gaussian Reparameterization Revisiting.pdf},
  language = {en}
}

@article{potapczynski20_InvertibleGaussianReparameterization,
  title = {Invertible {{Gaussian Reparameterization}}: {{Revisiting}} the {{Gumbel}}-{{Softmax}}},
  shorttitle = {Invertible {{Gaussian Reparameterization}}},
  author = {Potapczynski, Andres and {Loaiza-Ganem}, Gabriel and Cunningham, John P.},
  year = {2020},
  month = jun,
  abstract = {The Gumbel-Softmax is a continuous distribution over the simplex that is often used as a relaxation of discrete distributions. Because it can be readily interpreted and easily reparameterized, it enjoys widespread use. We propose a conceptually simpler and more flexible alternative family of reparameterizable distributions where Gaussian noise is transformed into a one-hot approximation through an invertible function. This invertible function is composed of a modified softmax and can incorporate diverse transformations that serve different specific purposes. For example, the stick-breaking procedure allows us to extend the reparameterization trick to distributions with countably infinite support, or normalizing flows let us increase the flexibility of the distribution. Our construction enjoys theoretical advantages over the Gumbel-Softmax, such as closed form KL, and significantly outperforms it in a variety of experiments. Our code is available at https:// github.com/cunningham-lab/igr.},
  archivePrefix = {arXiv},
  eprint = {1912.09588},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/potapczynski et al_2020_invertible gaussian reparameterization.pdf},
  journal = {arXiv:1912.09588 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{pratapa20_Benchmarkingalgorithmsgene,
  title = {Benchmarking Algorithms for Gene Regulatory Network Inference from Single-Cell Transcriptomic Data},
  author = {Pratapa, Aditya and Jalihal, Amogh P. and Law, Jeffrey N. and Bharadwaj, Aditya and Murali, T. M.},
  year = {2020},
  month = jan,
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0690-6},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/5FITBI4D/Pratapa et al. - 2020 - Benchmarking algorithms for gene regulatory networ.pdf},
  journal = {Nature Methods},
  language = {en}
}

@article{prechelt00_EarlyStoppingwhen,
  title = {Early {{Stopping}} | but When?},
  author = {Prechelt, Lutz},
  pages = {15},
  abstract = {Validation can be used to detect when over tting starts during supervised training of a neural network; training is then stopped before convergence to avoid the over tting   early stopping" . The exact criterion used for validation-based early stopping, however, is usually chosen in an ad-hoc fashion or training is stopped interactively. This trick describes how to select a stopping criterion in a systematic fashion; it is a trick for either speeding learning procedures or improving generalization, whichever is more important in the particular situation. An empirical investigation on multi-layer perceptrons shows that there exists a tradeo between training time and generalization: From the given mix of 1296 training runs using di erent 12 problems and 24 di erent network architectures I conclude slower stopping criteria allow for small improvements in generalization  here: about 4  on average , but cost much more training time  here: about factor 4 longer on average .},
  annotation = {ZSCC: 0000642},
  file = {/home/trung/GoogleDrive/Zotero/prechelt_early stopping but when.pdf},
  keywords = {favorite},
  language = {en}
}

@article{press00_ImprovingTransformerModels,
  title = {Improving {{Transformer Models}} by {{Reordering}} Their {{Sublayers}}},
  author = {Press, Ofir and Smith, Noah A and Levy, Omer},
  pages = {8},
  abstract = {Multilayer transformer networks consist of interleaved self-attention and feedforward sublayers. Could ordering the sublayers in a different pattern achieve better performance? We generate randomly ordered transformers and train them with the language modeling objective. We observe that some of these models are able to achieve better performance than the interleaved baseline, and that those successful variants tend to have more selfattention at the bottom and more feedforward sublayers at the top. We propose a new transformer design pattern that adheres to this property, the sandwich transformer, and show that it improves perplexity on the WikiText-103 language modeling benchmark, at no cost in parameters, memory, or training time.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/press et al_improving transformer models by reordering their sublayers.pdf},
  keywords = {attention,transformer},
  language = {en}
}

@article{priol20_AnalysisAdaptationSpeed,
  title = {An {{Analysis}} of the {{Adaptation Speed}} of {{Causal Models}}},
  author = {Priol, R{\'e}mi Le and Harikandeh, Reza Babanezhad and Bengio, Yoshua and {Lacoste-Julien}, Simon},
  year = {2020},
  month = may,
  abstract = {We consider the problem of discovering the causal process that generated a collection of datasets. We assume that all these datasets were generated by unknown sparse interventions on a structural causal model (SCM) \$G\$, that we want to identify. Recently, Bengio et al. (2020) argued that among all SCMs, \$G\$ is the fastest to adapt from one dataset to another, and proposed a meta-learning criterion to identify the causal direction in a two-variable SCM. While the experiments were promising, the theoretical justification was incomplete. Our contribution is a theoretical investigation of the adaptation speed of simple two-variable SCMs. We use convergence rates from stochastic optimization to justify that a relevant proxy for adaptation speed is distance in parameter space after intervention. Using this proxy, we show that the SCM with the correct causal direction is advantaged for categorical and normal cause-effect datasets when the intervention is on the cause variable. When the intervention is on the effect variable, we provide a more nuanced picture which highlights that the fastest-to-adapt heuristic is not always valid. Code to reproduce experiments is available at https://github.com/remilepriol/causal-adaptation-speed},
  archivePrefix = {arXiv},
  eprint = {2005.09136},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/priol et al_2020_an analysis of the adaptation speed of causal models.pdf},
  journal = {arXiv:2005.09136 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{priol20_AnalysisAdaptationSpeeda,
  title = {An {{Analysis}} of the {{Adaptation Speed}} of {{Causal Models}}},
  author = {Priol, R{\'e}mi Le and Harikandeh, Reza Babanezhad and Bengio, Yoshua and {Lacoste-Julien}, Simon},
  year = {2020},
  month = may,
  abstract = {We consider the problem of discovering the causal process that generated a collection of datasets. We assume that all these datasets were generated by unknown sparse interventions on a structural causal model (SCM) \$G\$, that we want to identify. Recently, Bengio et al. (2020) argued that among all SCMs, \$G\$ is the fastest to adapt from one dataset to another, and proposed a meta-learning criterion to identify the causal direction in a two-variable SCM. While the experiments were promising, the theoretical justification was incomplete. Our contribution is a theoretical investigation of the adaptation speed of simple two-variable SCMs. We use convergence rates from stochastic optimization to justify that a relevant proxy for adaptation speed is distance in parameter space after intervention. Using this proxy, we show that the SCM with the correct causal direction is advantaged for categorical and normal cause-effect datasets when the intervention is on the cause variable. When the intervention is on the effect variable, we provide a more nuanced picture which highlights that the fastest-to-adapt heuristic is not always valid. Code to reproduce experiments is available at https://github.com/remilepriol/causal-adaptation-speed},
  archivePrefix = {arXiv},
  eprint = {2005.09136},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/priol et al_2020_an analysis of the adaptation speed of causal models2.pdf},
  journal = {arXiv:2005.09136 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{pu17_AdversarialSymmetricVariational,
  title = {Adversarial {{Symmetric Variational Autoencoder}}},
  author = {Pu, Yunchen and Wang, Weiyao and Henao, Ricardo and Chen, Liqun and Gan, Zhe and Li, Chunyuan and Carin, Lawrence},
  year = {2017},
  month = nov,
  abstract = {A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmark datasets.},
  archivePrefix = {arXiv},
  eprint = {1711.04915},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/pu et al_2017_adversarial symmetric variational autoencoder.pdf},
  journal = {arXiv:1711.04915 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{purwins19_DeepLearningAudio,
  title = {Deep {{Learning}} for {{Audio Signal Processing}}},
  author = {Purwins, Hendrik and Li, Bo and Virtanen, Tuomas and Schl{\"u}ter, Jan and Chang, Shuo-yiin and Sainath, Tara},
  year = {2019},
  month = may,
  volume = {13},
  pages = {206--219},
  issn = {1932-4553, 1941-0484},
  doi = {10.1109/JSTSP.2019.2908700},
  abstract = {Given the recent surge in developments of deep learning, this article provides a review of the state-of-the-art deep learning techniques for audio signal processing. Speech, music, and environmental sound processing are considered side-by-side, in order to point out similarities and differences between the domains, highlighting general methods, problems, key references, and potential for cross-fertilization between areas. The dominant feature representations (in particular, log-mel spectra and raw waveform) and deep learning models are reviewed, including convolutional neural networks, variants of the long short-term memory architecture, as well as more audio-specific neural network models. Subsequently, prominent deep learning application areas are covered, i.e. audio recognition (automatic speech recognition, music information retrieval, environmental sound detection, localization and tracking) and synthesis and transformation (source separation, audio enhancement, generative models for speech, sound, and music synthesis). Finally, key issues and future questions regarding deep learning applied to audio signal processing are identified.},
  archivePrefix = {arXiv},
  eprint = {1905.00078},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/purwins et al_2019_deep learning for audio signal processing.pdf;/home/trung/Zotero/storage/CIQ9WNC6/1905.html},
  journal = {IEEE Journal of Selected Topics in Signal Processing},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,favorite,H.5.1,I.2.6,SRE,Statistics - Machine Learning},
  number = {2}
}

@article{qi20_SinglecellRNA,
  title = {Single Cell {{RNA}} Sequencing of 13 Human Tissues Identify Cell Types and Receptors of Human Coronaviruses},
  author = {Qi, Furong and Qian, Shen and Zhang, Shuye and Zhang, Zheng},
  year = {2020},
  month = mar,
  pages = {S0006291X20305234},
  issn = {0006291X},
  doi = {10.1016/j.bbrc.2020.03.044},
  abstract = {The new coronavirus (SARS-CoV-2) outbreak from December 2019 in Wuhan, Hubei, China, has been declared a global public health emergency. Angiotensin I converting enzyme 2 (ACE2), is the host receptor by SARS-CoV-2 to infect human cells. Although ACE2 is reported to be expressed in lung, liver, stomach, ileum, kidney and colon, its expressing levels are rather low, especially in the lung. SARS-CoV-2 may use co-receptors/auxiliary proteins as ACE2 partner to facilitate the virus entry. To identify the potential candidates, we explored the single cell gene expression atlas including 119 cell types of 13 human tissues and analyzed the single cell co-expression spectrum of 51 reported RNA virus receptors and 400 other membrane proteins. Consistent with other recent reports, we confirmed that ACE2 was mainly expressed in lung AT2, liver cholangiocyte, colon colonocytes, esophagus keratinocytes, ileum ECs, rectum ECs, stomach epithelial cells, and kidney proximal tubules. Intriguingly, we found that the candidate co-receptors, manifesting the most similar expression patterns with ACE2 across 13 human tissues, are all peptidases, including ANPEP, DPP4 and ENPEP. Among them, ANPEP and DPP4 are the known receptors for human CoVs, suggesting ENPEP as another potential receptor for human CoVs. We also conducted ``CellPhoneDB'' analysis to understand the cell crosstalk between CoV-targets and their surrounding cells across different tissues. We found that macrophages frequently communicate with the CoVs targets through chemokine and phagocytosis signaling, highlighting the importance of tissue macrophages in immune defense and immune pathogenesis.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/qi et al_2020_single cell rna sequencing of 13 human tissues identify cell types and receptors of human coronaviruses.docx;/home/trung/GoogleDrive/Zotero/qi et al_2020_single cell rna sequencing of 13 human tissues identify cell types and receptors of human coronaviruses.pdf;/home/trung/Zotero/storage/4CLJENN4/1-s2.0-S0006291X20305234-mmc1.xlsx;/home/trung/Zotero/storage/6BMZUZWD/1-s2.0-S0006291X20305234-figs1_lrg.jpg},
  journal = {Biochemical and Biophysical Research Communications},
  language = {en}
}

@article{qian19_AUTOVCZeroShotVoice,
  title = {{{AUTOVC}}: {{Zero}}-{{Shot Voice Style Transfer}} with {{Only Autoencoder Loss}}},
  shorttitle = {{{AUTOVC}}},
  author = {Qian, Kaizhi and Zhang, Yang and Chang, Shiyu and Yang, Xuesong and {Hasegawa-Johnson}, Mark},
  year = {2019},
  month = jun,
  abstract = {Non-parallel many-to-many voice conversion, as well as zero-shot voice conversion, remain underexplored areas. Deep style transfer algorithms, such as generative adversarial networks (GAN) and conditional variational autoencoder (CVAE), are being applied as new solutions in this field. However, GAN training is sophisticated and difficult, and there is no strong evidence that its generated speech is of good perceptual quality. On the other hand, CVAE training is simple but does not come with the distribution-matching property of a GAN. In this paper, we propose a new style transfer scheme that involves only an autoencoder with a carefully designed bottleneck. We formally show that this scheme can achieve distributionmatching style transfer by training only on a selfreconstruction loss. Based on this scheme, we proposed AUTOVC, which achieves state-of-theart results in many-to-many voice conversion with non-parallel data, and which is the first to perform zero-shot voice conversion.},
  archivePrefix = {arXiv},
  eprint = {1905.05879},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/EGBDBYCL/Qian et al. - 2019 - AUTOVC Zero-Shot Voice Style Transfer with Only A.pdf},
  journal = {arXiv:1905.05879 [cs, eess, stat]},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@inproceedings{qian19_EnhancingVariationalAutoencoders,
  title = {Enhancing {{Variational Autoencoders}} with {{Mutual Information Neural Estimation}} for {{Text Generation}}},
  booktitle = {Proceedings of the 2019 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} and the 9th {{International Joint Conference}} on {{Natural Language Processing}} ({{EMNLP}}-{{IJCNLP}})},
  author = {Qian, Dong and Cheung, William K.},
  year = {2019},
  month = nov,
  pages = {4045--4055},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1416},
  abstract = {While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the latent variable fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the VAE. In this paper, we propose to introduce a mutual information (MI) term between the input and its latent variable to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ neural networks for the estimation of the MI and provide a training algorithm based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in language modeling and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/qian et al_2019_enhancing variational autoencoders with mutual information neural estimation for text generation.pdf}
}

@article{qian19_SoftTripleLossDeep,
  title = {{{SoftTriple Loss}}: {{Deep Metric Learning Without Triplet Sampling}}},
  shorttitle = {{{SoftTriple Loss}}},
  author = {Qian, Qi and Shang, Lei and Sun, Baigui and Hu, Juhua and Li, Hao and Jin, Rong},
  year = {2019},
  month = sep,
  abstract = {Distance metric learning (DML) is to learn the embeddings where examples from the same class are closer than examples from different classes. It can be cast as an optimization problem with triplet constraints. Due to the vast number of triplet constraints, a sampling strategy is essential for DML. With the tremendous success of deep learning in classifications, it has been applied for DML. When learning embeddings with deep neural networks (DNNs), only a mini-batch of data is available at each iteration. The set of triplet constraints has to be sampled within the mini-batch. Since a mini-batch cannot capture the neighbors in the original set well, it makes the learned embeddings sub-optimal. On the contrary, optimizing SoftMax loss, which is a classification loss, with DNN shows a superior performance in certain DML tasks. It inspires us to investigate the formulation of SoftMax. Our analysis shows that SoftMax loss is equivalent to a smoothed triplet loss where each class has a single center. In real-world data, one class can contain several local clusters rather than a single one, e.g., birds of different poses. Therefore, we propose the SoftTriple loss to extend the SoftMax loss with multiple centers for each class. Compared with conventional deep metric learning algorithms, optimizing SoftTriple loss can learn the embeddings without the sampling phase by mildly increasing the size of the last fully connected layer. Experiments on the benchmark fine-grained data sets demonstrate the effectiveness of the proposed loss function.},
  archivePrefix = {arXiv},
  eprint = {1909.05235},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/qian et al_2019_softtriple loss.pdf;/home/trung/Zotero/storage/RDHRC2ZM/1909.html},
  journal = {arXiv:1909.05235 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,loss},
  primaryClass = {cs}
}

@article{qiang18_STTMToolShort,
  title = {{{STTM}}: {{A Tool}} for {{Short Text Topic Modeling}}},
  shorttitle = {{{STTM}}},
  author = {Qiang, Jipeng and Li, Yun and Yuan, Yunhao and Liu, Wei and Wu, Xindong},
  year = {2018},
  month = aug,
  abstract = {Along with the emergence and popularity of social communications on the Internet, topic discovery from short texts becomes fundamental to many applications that require semantic understanding of textual content. As a rising research field, short text topic modeling presents a new and complementary algorithmic methodology to supplement regular text topic modeling, especially targets to limited word co-occurrence information in short texts. This paper presents the first comprehensive open-source package, called STTM, for use in Java that integrates the state-of-the-art models of short text topic modeling algorithms, benchmark datasets, and abundant functions for model inference and evaluation. The package is designed to facilitate the expansion of new methods in this research field and make evaluations between the new approaches and existing ones accessible. STTM is open-sourced at https://github.com/qiang2100/STTM.},
  archivePrefix = {arXiv},
  eprint = {1808.02215},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/qiang et al_2018_sttm.pdf},
  journal = {arXiv:1808.02215 [cs]},
  primaryClass = {cs}
}

@article{qiao19_DisentanglementChallengeRegularization,
  title = {Disentanglement {{Challenge}}: {{From Regularization}} to {{Reconstruction}}},
  shorttitle = {Disentanglement {{Challenge}}},
  author = {Qiao, Jie and Li, Zijian and Xu, Boyan and Cai, Ruichu and Zhang, Kun},
  year = {2019},
  month = nov,
  abstract = {The challenge of learning disentangled representation has recently attracted much attention and boils down to a competition1 using a new real world disentanglement dataset (Gondal et al., 2019). Various methods based on variational auto-encoder have been proposed to solve this problem, by enforcing the independence between the representation and modifying the regularization term in the variational lower bound. However recent work by Locatello et al. (2018) has demonstrated that the proposed methods are heavily influenced by randomness and the choice of the hyper-parameter. In this work, instead of designing a new regularization term, we adopt the FactorVAE but improve the reconstruction performance and increase the capacity of network and the training step. The strategy turns out to be very effective and achieve the 1st place in the challenge.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.00155},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/qiao et al_2019_disentanglement challenge.pdf;/home/trung/Zotero/storage/ZV42YRKQ/1912.html},
  journal = {arXiv:1912.00155 [cs, stat]},
  keywords = {beta vae,Computer Science - Machine Learning,disentanglement,factor vae,Statistics - Machine Learning,variational},
  language = {en},
  primaryClass = {cs, stat}
}

@article{qin19_CounterfactualStoryReasoning,
  title = {Counterfactual {{Story Reasoning}} and {{Generation}}},
  author = {Qin, Lianhui and Bosselut, Antoine and Holtzman, Ari and Bhagavatula, Chandra and Clark, Elizabeth and Choi, Yejin},
  year = {2019},
  month = sep,
  abstract = {Counterfactual reasoning requires predicting how alternative events, contrary to what actually happened, might have resulted in different outcomes. Despite being considered a necessary component of AI-complete systems, few resources have been developed for evaluating counterfactual reasoning in narratives. In this paper, we propose Counterfactual Story Rewriting: given an original story and an intervening counterfactual event, the task is to minimally revise the story to make it compatible with the given counterfactual event. Solving this task will require deep understanding of causal narrative chains and counterfactual invariance, and integration of such story reasoning capabilities into conditional language generation models. We present TimeTravel, a new dataset of 29,849 counterfactual rewritings, each with the original story, a counterfactual event, and human-generated revision of the original story compatible with the counterfactual event. Additionally, we include 80,115 counterfactual "branches" without a rewritten storyline to support future work on semi- or un-supervised approaches to counterfactual story rewriting. Finally, we evaluate the counterfactual rewriting capacities of several competitive baselines based on pretrained language models, and assess whether common overlap and model-based automatic metrics for text generation correlate well with human scores for counterfactual rewriting.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1909.04076},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/qin et al_2019_counterfactual story reasoning and generation.pdf;/home/trung/Zotero/storage/FBEE5EPV/1909.html},
  journal = {arXiv:1909.04076 [cs]},
  keywords = {causal,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,counterfactual,nlp,story generation},
  primaryClass = {cs}
}

@article{qiu00_DataIntensiveComputing,
  title = {Data {{Intensive Computing}} for {{Bioinformatics}}},
  author = {Qiu, Judy and Ekanayake, Jaliya and Gunarathne, Thilina and Choi, Jong Youl and Bae, Seung-Hee and Ekanayake, Saliya and Wu, Stephen and Beason, Scott and Fox, Geoffrey and Rho, Mina and Tang, Haixu},
  pages = {34},
  annotation = {ZSCC: 0000021},
  file = {/home/trung/GoogleDrive/Zotero/qiu et al_data intensive computing for bioinformatics.pdf},
  language = {en}
}

@article{qiu20_Mentalwellnesssystem,
  title = {Mental Wellness System for {{COVID}}-19},
  author = {Qiu, Jian-Yin and Zhou, Dong-Sheng and Liu, Jian and Yuan, Ti-Fei},
  year = {2020},
  month = apr,
  issn = {0889-1591},
  doi = {10.1016/j.bbi.2020.04.032},
  file = {/home/trung/GoogleDrive/Zotero/qiu et al_2020_mental wellness system for covid-19.pdf;/home/trung/Zotero/storage/5TIYL782/S0889159120304025.html},
  journal = {Brain, Behavior, and Immunity},
  language = {en}
}

@article{qu19_GMNNGraphMarkov,
  title = {{{GMNN}}: {{Graph Markov Neural Networks}}},
  shorttitle = {{{GMNN}}},
  author = {Qu, Meng and Bengio, Yoshua and Tang, Jian},
  year = {2019},
  month = may,
  abstract = {This paper studies semi-supervised object classification in relational data, which is a fundamental problem in relational data modeling. The problem has been extensively studied in the literature of both statistical relational learning (e.g. relational Markov networks) and graph neural networks (e.g. graph convolutional networks). Statistical relational learning methods can effectively model the dependency of object labels through conditional random fields for collective classification, whereas graph neural networks learn effective object representations for classification through end-to-end training. In this paper, we propose the Graph Markov Neural Network (GMNN) that combines the advantages of both worlds. A GMNN models the joint distribution of object labels with a conditional random field, which can be effectively trained with the variational EM algorithm. In the E-step, one graph neural network learns effective object representations for approximating the posterior distributions of object labels. In the M-step, another graph neural network is used to model the local label dependency. Experiments on object classification, link classification, and unsupervised node representation learning show that GMNN achieves state-of-the-art results.},
  archivePrefix = {arXiv},
  eprint = {1905.06214},
  eprinttype = {arxiv},
  journal = {arXiv:1905.06214 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Social and Information Networks,information,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{qu19_ProbabilisticLogicNeural,
  title = {Probabilistic {{Logic Neural Networks}} for {{Reasoning}}},
  author = {Qu, Meng and Tang, Jian},
  year = {2019},
  month = jun,
  abstract = {Knowledge graph reasoning, which aims at predicting the missing facts through reasoning with the observed facts, is critical to many applications. Such a problem has been widely explored by traditional logic rule-based approaches and recent knowledge graph embedding methods. A principled logic rule-based approach is the Markov Logic Network (MLN), which is able to leverage domain knowledge with first-order logic and meanwhile handle their uncertainty. However, the inference of MLNs is usually very difficult due to the complicated graph structures. Different from MLNs, knowledge graph embedding methods (e.g. TransE, DistMult) learn effective entity and relation embeddings for reasoning, which are much more effective and efficient. However, they are unable to leverage domain knowledge. In this paper, we propose the probabilistic Logic Neural Network (pLogicNet), which combines the advantages of both methods. A pLogicNet defines the joint distribution of all possible triplets by using a Markov logic network with first-order logic, which can be efficiently optimized with the variational EM algorithm. In the E-step, a knowledge graph embedding model is used for inferring the missing triplets, while in the M-step, the weights of logic rules are updated based on both the observed and predicted triplets. Experiments on multiple knowledge graphs prove the effectiveness of pLogicNet over many competitive baselines.},
  archivePrefix = {arXiv},
  eprint = {1906.08495},
  eprinttype = {arxiv},
  journal = {arXiv:1906.08495 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@book{quinonero-candela09_Datasetshiftmachine,
  title = {Dataset Shift in Machine Learning},
  editor = {{Qui{\~n}onero-Candela}, Joaquin},
  year = {2009},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  annotation = {OCLC: ocn227205909},
  file = {/home/trung/GoogleDrive/Zotero/quiñonero-candela_2009_dataset shift in machine learning.pdf},
  isbn = {978-0-262-17005-5},
  keywords = {Machine learning},
  language = {en},
  lccn = {Q325.5 .D37 2009},
  series = {Neural Information Processing Series}
}

@article{radford00_ImprovingLanguageUnderstanding,
  title = {Improving {{Language Understanding}} by {{Generative Pre}}-{{Training}}},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  pages = {12},
  abstract = {Natural language understanding comprises a wide range of diverse tasks such as textual entailment, question answering, semantic similarity assessment, and document classification. Although large unlabeled text corpora are abundant, labeled data for learning these specific tasks is scarce, making it challenging for discriminatively trained models to perform adequately. We demonstrate that large gains on these tasks can be realized by generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task. In contrast to previous approaches, we make use of task-aware input transformations during fine-tuning to achieve effective transfer while requiring minimal changes to the model architecture. We demonstrate the effectiveness of our approach on a wide range of benchmarks for natural language understanding. Our general task-agnostic model outperforms discriminatively trained models that use architectures specifically crafted for each task, significantly improving upon the state of the art in 9 out of the 12 tasks studied. For instance, we achieve absolute improvements of 8.9\% on commonsense reasoning (Stories Cloze Test), 5.7\% on question answering (RACE), and 1.5\% on textual entailment (MultiNLI).},
  annotation = {ZSCC: 0000479},
  file = {/home/trung/GoogleDrive/Zotero/radford et al_improving language understanding by generative pre-training.pdf},
  language = {en}
}

@article{radford16_UnsupervisedRepresentationLearning,
  title = {Unsupervised {{Representation Learning}} with {{Deep Convolutional Generative Adversarial Networks}}},
  author = {Radford, Alec and Metz, Luke and Chintala, Soumith},
  year = {2016},
  month = jan,
  abstract = {In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.},
  annotation = {ZSCC: 0000004},
  archivePrefix = {arXiv},
  eprint = {1511.06434},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/radford et al_2016_unsupervised representation learning with deep convolutional generative adversarial networks.pdf},
  journal = {arXiv:1511.06434 [cs]},
  keywords = {DCGAN},
  primaryClass = {cs}
}

@article{radhakrishnan00_MemorizationOverparameterizedAutoencoders,
  title = {Memorization in {{Overparameterized Autoencoders}}},
  author = {Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler, Caroline},
  pages = {14},
  abstract = {Interpolation of data in deep neural networks has become a subject of significant research interest. We prove that over-parameterized single layer fully connected autoencoders do not merely interpolate, but rather, memorize training data: they produce outputs in (a non-linear version of) the span of the training examples. In contrast to fully connected autoencoders, we prove that depth is necessary for memorization in convolutional autoencoders. Moreover, we observe that adding nonlinearity to deep convolutional autoencoders results in a stronger form of memorization: instead of outputting points in the span of the training images, deep convolutional autoencoders tend to output individual training images. Since convolutional autoencoder components are building blocks of deep convolutional networks, we envision that our findings will shed light on the important question of the inductive bias in over-parameterized deep networks.},
  file = {/home/trung/GoogleDrive/Zotero/radhakrishnan et al_memorization in overparameterized autoencoders.pdf},
  keywords = {overparameterization},
  language = {en}
}

@article{radhakrishnan19_OverparameterizedNeuralNetworks,
  title = {Overparameterized {{Neural Networks Can Implement Associative Memory}}},
  author = {Radhakrishnan, Adityanarayanan and Belkin, Mikhail and Uhler, Caroline},
  year = {2019},
  month = sep,
  abstract = {Identifying computational mechanisms for memorization and retrieval is a long-standing problem at the intersection of machine learning and neuroscience. In this work, we demonstrate empirically that overparameterized deep neural networks trained using standard optimization methods provide a mechanism for memorization and retrieval of real-valued data. In particular, we show that overparameterized autoencoders store training examples as attractors, and thus, can be viewed as implementations of associative memory with the retrieval mechanism given by iterating the map. We study this phenomenon under a variety of common architectures and optimization methods and construct a network that can recall 500 real-valued images without any apparent spurious attractor states. Lastly, we demonstrate how the same mechanism allows encoding sequences, including movies and audio, instead of individual examples. Interestingly, this appears to provide an even more efficient mechanism for storage and retrieval than autoencoding single instances.},
  archivePrefix = {arXiv},
  eprint = {1909.12362},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/radhakrishnan et al_2019_overparameterized neural networks can implement associative memory.pdf;/home/trung/Zotero/storage/72CJTR3Y/1909.html},
  journal = {arXiv:1909.12362 [cs, stat]},
  keywords = {Computer Science - Machine Learning,overparameterization,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{rae03_Oralcreatinemonohydrate,
  title = {Oral Creatine Monohydrate Supplementation Improves Brain Performance: A Double-Blind, Placebo-Controlled, Cross-over Trial.},
  shorttitle = {Oral Creatine Monohydrate Supplementation Improves Brain Performance},
  author = {Rae, Caroline and Digney, Alison L and McEwan, Sally R and Bates, Timothy C},
  year = {2003},
  month = oct,
  volume = {270},
  pages = {2147--2150},
  issn = {0962-8452},
  doi = {10.1098/rspb.2003.2492},
  abstract = {Creatine supplementation is in widespread use to enhance sports-fitness performance, and has been trialled successfully in the treatment of neurological, neuromuscular and atherosclerotic disease. Creatine plays a pivotal role in brain energy homeostasis, being a temporal and spatial buffer for cytosolic and mitochondrial pools of the cellular energy currency, adenosine triphosphate and its regulator, adenosine diphosphate. In this work, we tested the hypothesis that oral creatine supplementation (5 g d(-1) for six weeks) would enhance intelligence test scores and working memory performance in 45 young adult, vegetarian subjects in a double-blind, placebo-controlled, cross-over design. Creatine supplementation had a significant positive effect (p {$<$} 0.0001) on both working memory (backward digit span) and intelligence (Raven's Advanced Progressive Matrices), both tasks that require speed of processing. These findings underline a dynamic and significant role of brain energy capacity in influencing brain performance.},
  file = {/home/trung/GoogleDrive/Zotero/rae et al_2003_oral creatine monohydrate supplementation improves brain performance.pdf},
  journal = {Proceedings of the Royal Society B: Biological Sciences},
  number = {1529},
  pmcid = {PMC1691485},
  pmid = {14561278}
}

@article{rae18_FastParametricLearning,
  title = {Fast {{Parametric Learning}} with {{Activation Memorization}}},
  author = {Rae, Jack W. and Dyer, Chris and Dayan, Peter and Lillicrap, Timothy P.},
  year = {2018},
  month = mar,
  abstract = {Neural networks trained with backpropagation often struggle to identify classes that have been observed a small number of times. In applications where most class labels are rare, such as language modelling, this can become a performance bottleneck. One potential remedy is to augment the network with a fast-learning non-parametric model which stores recent activations and class labels into an external memory. We explore a simplified architecture where we treat a subset of the model parameters as fast memory stores. This can help retain information over longer time intervals than a traditional memory, and does not require additional space or compute. In the case of image classification, we display faster binding of novel classes on an Omniglot image curriculum task. We also show improved performance for word-based language models on news reports (GigaWord), books (Project Gutenberg) and Wikipedia articles (WikiText-103) --- the latter achieving a state-of-the-art perplexity of 29.2.},
  archivePrefix = {arXiv},
  eprint = {1803.10049},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rae et al_2018_fast parametric learning with activation memorization.pdf;/home/trung/Zotero/storage/44AD7C5H/1803.html},
  journal = {arXiv:1803.10049 [cs, stat]},
  keywords = {activation,Computer Science - Machine Learning,memory,recurrent,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{rae20_TransformersNeedDeep,
  title = {Do {{Transformers Need Deep Long}}-{{Range Memory}}?},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Rae, Jack and Razavi, Ali},
  year = {2020},
  pages = {7524--7529},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.672},
  abstract = {Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL \textemdash{} a Transformer augmented with a long-range memory of past activations \textemdash{} has been shown to be state-ofthe-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.},
  file = {/home/trung/GoogleDrive/Zotero/rae et al_2020_do transformers need deep long-range memory.pdf},
  language = {en}
}

@article{rafiee20_TestbedsReinforcementLearning,
  title = {Testbeds for {{Reinforcement Learning}}},
  author = {Rafiee, Banafsheh},
  year = {2020},
  month = nov,
  abstract = {We present three problems modeled after animal learning experiments designed to test online state construction or representation learning algorithms. Our test problems require the learner to construct compact summaries of their past interaction with the world in order to predict the future, updating online and incrementally on each time step without an explicit training-testing split. The majority of recent work in Deep Reinforcement Learning focuses on either fully observable tasks, or games where stacking a handful of recent frames is sufficient for good performance. Current benchmarks used for evaluating memory and recurrent learning make use of 3D visual environments (e.g., DeepMind Lab) which require billions of training samples, complex agent architectures, and cloud-scale compute. These domains are thus not well suited for rapid prototyping, hyper-parameter study, or extensive replication study. In this paper, we contribute a set of test problems and benchmark results to fill this gap. Our test problems are designed to be the simplest instantiation and test of learning capabilities which animals readily exhibit, including (1) trace conditioning (remembering a cue in order to predict another far in the future), (2) positive/negative patterning (a combination of cues predict another), (3) and combinations of both with additional non-relevant distracting signals. We provide baselines for the first problem including heuristics from the early days of neural network learning and simple ideas inspired by computational models of animal learning. Our results highlight the difficulty of our test problems for online recurrent learning systems and how the agent's performance often exhibits substantial sensitivity to the choice of key problem and agent parameters.},
  archivePrefix = {arXiv},
  eprint = {2011.04590},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rafiee_2020_testbeds for reinforcement learning.pdf},
  journal = {arXiv:2011.04590 [cs]},
  primaryClass = {cs}
}

@article{raghu20_SurveyDeepLearning,
  title = {A {{Survey}} of {{Deep Learning}} for {{Scientific Discovery}}},
  author = {Raghu, Maithra and Schmidt, Eric},
  year = {2020},
  month = mar,
  abstract = {Over the past few years, we have seen fundamental breakthroughs in core problems in machine learning, largely driven by advances in deep neural networks. At the same time, the amount of data collected in a wide array of scientific domains is dramatically increasing in both size and complexity. Taken together, this suggests many exciting opportunities for deep learning applications in scientific settings. But a significant challenge to this is simply knowing where to start. The sheer breadth and diversity of different deep learning techniques makes it difficult to determine what scientific problems might be most amenable to these methods, or which specific combination of methods might offer the most promising first approach. In this survey, we focus on addressing this central issue, providing an overview of many widely used deep learning models, spanning visual, sequential and graph structured data, associated tasks and different training methods, along with techniques to use deep learning with less data and better interpret these complex models \textemdash{} two central considerations for many scientific use cases. We also include overviews of the full design process, implementation tips, and links to a plethora of tutorials, research summaries and open-sourced deep learning pipelines and pretrained models, developed by the community. We hope that this survey will help accelerate the use of deep learning across different scientific domains.},
  archivePrefix = {arXiv},
  eprint = {2003.11755},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/raghu et al_2020_a survey of deep learning for scientific discovery.pdf},
  journal = {arXiv:2003.11755 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rahwan19_Machinebehaviour,
  title = {Machine Behaviour},
  author = {Rahwan, Iyad and Cebrian, Manuel and Obradovich, Nick and Bongard, Josh and Bonnefon, Jean-Fran{\c c}ois and Breazeal, Cynthia and Crandall, Jacob W. and Christakis, Nicholas A. and Couzin, Iain D. and Jackson, Matthew O. and Jennings, Nicholas R. and Kamar, Ece and Kloumann, Isabel M. and Larochelle, Hugo and Lazer, David and McElreath, Richard and Mislove, Alan and Parkes, David C. and Pentland, Alex `Sandy' and Roberts, Margaret E. and Shariff, Azim and Tenenbaum, Joshua B. and Wellman, Michael},
  year = {2019},
  month = apr,
  volume = {568},
  pages = {477--486},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1138-y},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/UEUXWUGP/Rahwan et al. - 2019 - Machine behaviour.pdf},
  journal = {Nature},
  language = {en},
  number = {7753}
}

@inproceedings{rainforth18_Tightervariationalbounds,
  title = {Tighter Variational Bounds Are Not Necessarily Better},
  author = {Rainforth, Tom and Kosiorek, Adam and Le, Tuan Anh and Maddison, Chris and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  editor = {Dy, Jennifer and Krause, Andreas},
  year = {2018},
  month = jul,
  volume = {80},
  pages = {4277--4285},
  publisher = {{PMLR}},
  address = {{Stockholmsm\"assan, Stockholm Sweden}},
  abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
  file = {/home/trung/GoogleDrive/Zotero/rainforth et al_2018_tighter variational bounds are not necessarily better.pdf},
  keywords = {vae_issues},
  pdf = {http://proceedings.mlr.press/v80/rainforth18b/rainforth18b.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{raj19_OrthogonalStructureSearch,
  title = {Orthogonal {{Structure Search}} for {{Efficient Causal Discovery}} from {{Observational Data}}},
  author = {Raj, Anant and Gresele, Luigi and Besserve, Michel and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  year = {2019},
  month = mar,
  abstract = {The problem of inferring the direct causal parents of a response variable among a large set of explanatory variables is of high practical importance in many disciplines. Recent work exploits stability of regression coefficients or invariance properties of models across different experimental conditions for reconstructing the full causal graph. These approaches generally do not scale well with the number of the explanatory variables and are difficult to extend to nonlinear relationships. Contrary to existing work, we propose an approach which even works for observational data alone, while still offering theoretical guarantees including the case of partially nonlinear relationships. Our algorithm requires only one estimation for each variable and in our experiments we apply our causal discovery algorithm even to large graphs, demonstrating significant improvements compared to well established approaches.},
  archivePrefix = {arXiv},
  eprint = {1903.02456},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/raj et al_2019_orthogonal structure search for efficient causal discovery from observational data.pdf;/home/trung/Zotero/storage/9GXWAKL3/1903.html},
  journal = {arXiv:1903.02456 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{raj19_ProbingInformationEncoded,
  title = {Probing the {{Information Encoded}} in {{X}}-Vectors},
  author = {Raj, Desh and Snyder, David and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2019},
  month = sep,
  abstract = {Deep neural network based speaker embeddings, such as x-vectors, have been shown to perform well in text-independent speaker recognition/verification tasks. In this paper, we use simple classifiers to investigate the contents encoded by x-vector embeddings. We probe these embeddings for information related to the speaker, channel, transcription (sentence, words, phones), and meta information about the utterance (duration and augmentation type), and compare these with the information encoded by i-vectors across a varying number of dimensions. We also study the effect of data augmentation during extractor training on the information captured by x-vectors. Experiments on the RedDots data set show that x-vectors capture spoken content and channel-related information, while performing well on speaker verification tasks.},
  archivePrefix = {arXiv},
  eprint = {1909.06351},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/raj et al_2019_probing the information encoded in x-vectors.pdf;/home/trung/Zotero/storage/ZRSHNZBQ/1909.html},
  journal = {arXiv:1909.06351 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@inproceedings{rallabandi19_VariationalAttentionUsing,
  title = {Variational {{Attention Using Articulatory Priors}} for {{Generating Code Mixed Speech Using Monolingual Corpora}}},
  booktitle = {Interspeech 2019},
  author = {Rallabandi, SaiKrishna and Black, Alan W.},
  year = {2019},
  month = sep,
  pages = {3735--3739},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1103},
  abstract = {Code Mixing - phenomenon where lexical items from one language are embedded in the utterance of another - is relatively frequent in multilingual communities and therefore speech systems should be able to process such content. However, building a voice capable of synthesizing such content typically requires bilingual recordings from the speaker which might not always be easy to obtain. In this work, we present an approach for building mixed lingual systems using only monolingual corpora. Specifically we present a way to train multi speaker text to speech system by incorporating stochastic latent variables into the attention mechanism with the objective of synthesizing code mixed content. We subject the prior distribution for such latent variables to match articulatory constraints. Subjective evaluation shows that our systems are capable of generating high quality synthesis in code mixed scenarios.},
  file = {/home/trung/Zotero/storage/35UV75AZ/Rallabandi and Black - 2019 - Variational Attention Using Articulatory Priors fo.pdf},
  language = {en}
}

@article{ramachandran17_SwishSelfGatedActivation,
  title = {Swish: A {{Self}}-{{Gated Activation Function}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = oct,
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose a new activation function, named Swish, which is simply f (x) = x {$\cdot$} sigmoid(x). Our experiments show that Swish tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\% for Mobile NASNetA and 0.6\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  annotation = {ZSCC: 0000437},
  archivePrefix = {arXiv},
  eprint = {1710.05941},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ramachandran et al_2017_swish.pdf},
  journal = {arXiv:1710.05941 [cs]},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{ramage09_LabeledLDAsupervised,
  title = {Labeled {{LDA}}: A Supervised Topic Model for Credit Attribution in Multi-Labeled Corpora},
  shorttitle = {Labeled {{LDA}}},
  booktitle = {Proceedings of the 2009 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing Volume}} 1 - {{EMNLP}} '09},
  author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.},
  year = {2009},
  volume = {1},
  pages = {248},
  publisher = {{Association for Computational Linguistics}},
  address = {{Singapore}},
  doi = {10.3115/1699510.1699543},
  abstract = {A significant portion of the world's text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal specificity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by defining a one-to-one correspondence between LDA's latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA's improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-specific document snippets. As a multi-label text classifier, our model is competitive with a discriminative baseline on a variety of datasets.},
  file = {/home/trung/GoogleDrive/Zotero/ramage et al_2009_labeled lda.pdf},
  isbn = {978-1-932432-59-6},
  language = {en}
}

@book{ramalho15_FluentPython,
  title = {Fluent {{Python}}},
  author = {Ramalho, Luciano},
  year = {2015},
  edition = {First edition},
  publisher = {{O'Reilly}},
  address = {{Sebastopol, CA}},
  abstract = {Learn how to write idiomatic, effective Python code by leveraging its best features. Python's simplicity quickly lets you become productive with it, but this often means you aren't using everything the language has to offer. By taking you through Python's key language features and libraries, this practical book shows you how to make your code shorter, faster, and more readable all at the same time--what experts consider Pythonic},
  annotation = {OCLC: ocn884808025},
  file = {/home/trung/GoogleDrive/Zotero/ramalho_2015_fluent python.pdf},
  isbn = {978-1-4919-4600-8},
  keywords = {Object-oriented programming languages,Objektorienterad programmering,Programming Languages,Python,Python (Computer program language)},
  language = {en},
  lccn = {QA76.73.P98 R36 2015}
}

@article{ramanujan19_WhatHiddenRandomly,
  title = {What's {{Hidden}} in a {{Randomly Weighted Neural Network}}?},
  author = {Ramanujan, Vivek and Wortsman, Mitchell and Kembhavi, Aniruddha and Farhadi, Ali and Rastegari, Mohammad},
  year = {2019},
  month = nov,
  abstract = {Training a neural network is synonymous with learning the values of the weights. In contrast, we demonstrate that randomly weighted neural networks contain subnetworks which achieve impressive performance without ever training the weight values. Hidden in a randomly weighted Wide ResNet-50 we show that there is a subnetwork (with random weights) that is smaller than, but matches the performance of a ResNet-34 trained on ImageNet. Not only do these "untrained subnetworks" exist, but we provide an algorithm to effectively find them. We empirically show that as randomly weighted neural networks with fixed weights grow wider and deeper, an "untrained subnetwork" approaches a network with learned weights in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1911.13299},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ramanujan et al_2019_what's hidden in a randomly weighted neural network.pdf;/home/trung/Zotero/storage/G92G5QNN/1911.html},
  journal = {arXiv:1911.13299 [cs]},
  primaryClass = {cs}
}

@article{ramasesh20_AnatomyCatastrophicForgetting,
  title = {Anatomy of {{Catastrophic Forgetting}}: {{Hidden Representations}} and {{Task Semantics}}},
  shorttitle = {Anatomy of {{Catastrophic Forgetting}}},
  author = {Ramasesh, Vinay V. and Dyer, Ethan and Raghu, Maithra},
  year = {2020},
  month = jul,
  abstract = {A central challenge in developing versatile machine learning systems is catastrophic forgetting \textemdash{} a model trained on tasks in sequence will suffer significant performance drops on earlier tasks. Despite the ubiquity of catastrophic forgetting, there is limited understanding of the underlying process and its causes. In this paper, we address this important knowledge gap, investigating how forgetting affects representations in neural network models. Through representational analysis techniques, we find that deeper layers are disproportionately the source of forgetting. Supporting this, a study of methods to mitigate forgetting illustrates that they act to stabilize deeper layers. These insights enable the development of an analytic argument and empirical picture relating the degree of forgetting to representational similarity between tasks. Consistent with this picture, we observe maximal forgetting occurs for task sequences with intermediate similarity. We perform empirical studies on the standard split CIFAR-10 setup and also introduce a novel CIFAR-100 based task approximating realistic input distribution shift.},
  archivePrefix = {arXiv},
  eprint = {2007.07400},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ramasesh et al_2020_anatomy of catastrophic forgetting.pdf},
  journal = {arXiv:2007.07400 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rampasek17_DrVAEDrug,
  title = {Dr.{{VAE}}: {{Drug Response Variational Autoencoder}}},
  shorttitle = {Dr.{{VAE}}},
  author = {Rampasek, Ladislav and Hidru, Daniel and Smirnov, Petr and {Haibe-Kains}, Benjamin and Goldenberg, Anna},
  year = {2017},
  month = jun,
  abstract = {We present two deep generative models based on Variational Autoencoders to improve the accuracy of drug response prediction. Our models, Perturbation Variational Autoencoder and its semi-supervised extension, Drug Response Variational Autoencoder (Dr.VAE), learn latent representation of the underlying gene states before and after drug application that depend on: (i) drug-induced biological change of each gene and (ii) overall treatment response outcome. Our VAE-based models outperform the current published benchmarks in the field by anywhere from 3 to 11\% AUROC and 2 to 30\% AUPR. In addition, we found that better reconstruction accuracy does not necessarily lead to improvement in classification accuracy and that jointly trained models perform better than models that minimize reconstruction error independently.},
  archivePrefix = {arXiv},
  eprint = {1706.08203},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rampasek et al_2017_dr.pdf},
  journal = {arXiv:1706.08203 [stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@article{ramsauer20_HopfieldNetworksAll,
  title = {Hopfield {{Networks}} Is {{All You Need}}},
  author = {Ramsauer, Hubert and Sch{\"a}fl, Bernhard and Lehner, Johannes and Seidl, Philipp and Widrich, Michael and Gruber, Lukas and Holzleitner, Markus and Pavlovi{\'c}, Milena and Sandve, Geir Kjetil and Greiff, Victor and Kreil, David and Kopp, Michael and Klambauer, G{\"u}nter and Brandstetter, Johannes and Hochreiter, Sepp},
  year = {2020},
  month = jul,
  abstract = {We show that the transformer attention mechanism is the update rule of a modern Hopfield network with continuous states. This new Hopfield network can store exponentially (with the dimension) many patterns, converges with one update, and has exponentially small retrieval errors. The number of stored patterns is traded off against convergence speed and retrieval error. The new Hopfield network has three types of energy minima (fixed points of the update): (1) global fixed point averaging over all patterns, (2) metastable states averaging over a subset of patterns, and (3) fixed points which store a single pattern. Transformer and BERT models operate in their first layers preferably in the global averaging regime, while they operate in higher layers in metastable states. The gradient in transformers is maximal for metastable states, is uniformly distributed for global averaging, and vanishes for a fixed point near a stored pattern. Using the Hopfield network interpretation, we analyzed learning of transformer and BERT models. Learning starts with attention heads that average and then most of them switch to metastable states. However, the majority of heads in the first layers still averages and can be replaced by averaging, e.g. our proposed Gaussian weighting. In contrast, heads in the last layers steadily learn and seem to use metastable states to collect information created in lower layers. These heads seem to be a promising target for improving transformers. Neural networks with Hopfield networks outperform other methods on immune repertoire classification, where the Hopfield net stores several hundreds of thousands of patterns. We provide a new PyTorch layer called "Hopfield", which allows to equip deep learning architectures with modern Hopfield networks as a new powerful concept comprising pooling, memory, and attention. GitHub: https://github.com/ml-jku/hopfield-layers},
  archivePrefix = {arXiv},
  eprint = {2008.02217},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ramsauer et al_2020_hopfield networks is all you need.pdf},
  journal = {arXiv:2008.02217 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ran20_BigeminalPriorsVariational,
  title = {Bigeminal {{Priors Variational}} Auto-Encoder},
  author = {Ran, Xuming and Xu, Mingkun and Xu, Qi and Zhou, Huihui and Liu, Quanying},
  year = {2020},
  month = oct,
  abstract = {Variational auto-encoders (VAEs) are an influential and generally-used class of likelihood-based generative models in unsupervised learning. The likelihood-based generative models have been reported to be highly robust to the out-of-distribution (OOD) inputs and can be a detector by assuming that the model assigns higher likelihoods to the samples from the in-distribution (ID) dataset than an OOD dataset. However, recent works reported a phenomenon that VAE recognizes some OOD samples as ID by assigning a higher likelihood to the OOD inputs compared to the one from ID. In this work, we introduce a new model, namely Bigeminal Priors Variational auto-encoder (BPVAE), to address this phenomenon. The BPVAE aims to enhance the robustness of the VAEs by combing the power of VAE with the two independent priors that belong to the training dataset and simple dataset, which complexity is lower than the training dataset, respectively. BPVAE learns two datasets'features, assigning a higher likelihood for the training dataset than the simple dataset. In this way, we can use BPVAE's density estimate for detecting the OOD samples. Quantitative experimental results suggest that our model has better generalization capability and stronger robustness than the standard VAEs, proving the effectiveness of the proposed approach of hybrid learning by collaborative priors. Overall, this work paves a new avenue to potentially overcome the OOD problem via multiple latent priors modeling.},
  archivePrefix = {arXiv},
  eprint = {2010.01819},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ran et al_2020_bigeminal priors variational auto-encoder.pdf},
  journal = {arXiv:2010.01819 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ran20_DetectingOutofdistributionSamples,
  title = {Detecting {{Out}}-of-Distribution {{Samples}} via {{Variational Auto}}-Encoder with {{Reliable Uncertainty Estimation}}},
  author = {Ran, Xuming and Xu, Mingkun and Mei, Lingrui and Xu, Qi and Liu, Quanying},
  year = {2020},
  month = jul,
  abstract = {In unsupervised learning, variational auto-encoders (VAEs) are an influential class of deep generative models with rich representational power of neural networks and Bayesian methods. However, VAEs suffer from assigning higher likelihood to out-of-distribution (OOD) inputs than in-distribution (ID) inputs. Recent studies advise that the deep generative models with reliable uncertainty estimation is critical to a deep understanding of OOD inputs. Meanwhile, noise contrastive prior (NCP) is an emerging promising method for obtaining uncertainty, with the advantages of easy to scale, being trainable, and compatibility with extensive models. Inspired by these ideas, We propose an improved noise contrastive prior (INCP) to acquire reliable uncertainty estimate for standard VAEs. By combining INCP with the encoder of VAE, patterns between OOD and ID inputs can be well captured and distinguished. Our method outperforms standard VAEs on the FashionMNIST and CIFAR10 datasets. We also demonstrate the preferred robustness of our model by the extensive experiments on anomaly detection tasks.},
  archivePrefix = {arXiv},
  eprint = {2007.08128},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ran et al_2020_detecting out-of-distribution samples via variational auto-encoder with reliable uncertainty estimation.pdf},
  journal = {arXiv:2007.08128 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ranganath18_OperatorVariationalInference,
  title = {Operator {{Variational Inference}}},
  author = {Ranganath, Rajesh and Altosaar, Jaan and Tran, Dustin and Blei, David M.},
  year = {2018},
  month = mar,
  abstract = {Variational inference is an umbrella term for algorithms which cast Bayesian inference as optimization. Classically, variational inference uses the Kullback-Leibler divergence to define the optimization. Though this divergence has been widely used, the resultant posterior approximation can suffer from undesirable statistical properties. To address this, we reexamine variational inference from its roots as an optimization problem. We use operators, or functions of functions, to design variational objectives. As one example, we design a variational objective with a Langevin-Stein operator. We develop a black box algorithm, operator variational inference (OPVI), for optimizing any operator objective. Importantly, operators enable us to make explicit the statistical and computational tradeoffs for variational inference. We can characterize different properties of variational objectives, such as objectives that admit data subsampling---allowing inference to scale to massive data---as well as objectives that admit variational programs---a rich class of posterior approximations that does not require a tractable density. We illustrate the benefits of OPVI on a mixture model and a generative model of images.},
  archivePrefix = {arXiv},
  eprint = {1610.09033},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ranganath et al_2018_operator variational inference.pdf;/home/trung/Zotero/storage/YVC6HUDE/1610.html},
  journal = {arXiv:1610.09033 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning,Statistics - Methodology,variational},
  primaryClass = {cs, stat}
}

@article{ranzato00_UnifiedEnergyBasedFramework,
  title = {A {{Unified Energy}}-{{Based Framework}} for {{Unsupervised Learning}}},
  author = {Ranzato, Marc'Aurelio and Boureau, Y-Lan and Chopra, Sumit and LeCun, Yann},
  pages = {9},
  abstract = {We introduce a view of unsupervised learning that integrates probabilistic and nonprobabilistic methods for clustering, dimensionality reduction, and feature extraction in a unified framework. In this framework, an energy function associates low energies to input points that are similar to training samples, and high energies to unobserved points. Learning consists in minimizing the energies of training samples while ensuring that the energies of unobserved ones are higher. Some traditional methods construct the architecture so that only a small number of points can have low energy, while other methods explicitly ``pull up'' on the energies of unobserved points. In probabilistic methods the energy of unobserved points is pulled by minimizing the log partition function, an expensive, and sometimes intractable process. We explore different and more efficient methods using an energy-based approach. In particular, we show that a simple solution is to restrict the amount of information contained in codes that represent the data. We demonstrate such a method by training it on natural image patches and by applying to image denoising.},
  annotation = {ZSCC: 0000083},
  file = {/home/trung/GoogleDrive/Zotero/ranzato et al_a uniﬁed energy-based framework for unsupervised learning.pdf},
  language = {en}
}

@article{rao19_ContinualUnsupervisedRepresentation,
  title = {Continual {{Unsupervised Representation Learning}}},
  author = {Rao, Dushyant and Visin, Francesco and Rusu, Andrei A. and Teh, Yee Whye and Pascanu, Razvan and Hadsell, Raia},
  year = {2019},
  month = oct,
  abstract = {Continual learning aims to improve the ability of modern learning systems to deal with non-stationary distributions, typically by attempting to learn a series of tasks sequentially. Prior art in the field has largely considered supervised or reinforcement learning tasks, and often assumes full knowledge of task labels and boundaries. In this work, we propose an approach (CURL) to tackle a more general problem that we will refer to as unsupervised continual learning. The focus is on learning representations without any knowledge about task identity, and we explore scenarios when there are abrupt changes between tasks, smooth transitions from one task to another, or even when the data is shuffled. The proposed approach performs task inference directly within the model, is able to dynamically expand to capture new concepts over its lifetime, and incorporates additional rehearsal-based techniques to deal with catastrophic forgetting. We demonstrate the efficacy of CURL in an unsupervised learning setting with MNIST and Omniglot, where the lack of labels ensures no information is leaked about the task. Further, we demonstrate strong performance compared to prior art in an i.i.d setting, or when adapting the technique to supervised tasks such as incremental class learning.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.14481},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rao et al_2019_continual unsupervised representation learning.pdf;/home/trung/Zotero/storage/QH7T8GTK/1910.html},
  journal = {arXiv:1910.14481 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,continual learning,forget,mixture,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@techreport{rao20_ImputingSinglecellRNAseq,
  title = {Imputing {{Single}}-Cell {{RNA}}-Seq Data by Combining {{Graph Convolution}} and {{Autoencoder Neural Networks}}},
  author = {Rao, Jiahua and Zhou, Xiang and Lu, Yutong and Zhao, Huiying and Yang, Yuedong},
  year = {2020},
  month = feb,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.02.05.935296},
  abstract = {Abstract           Single-cell RNA sequencing technology promotes the profiling of single-cell transcriptomes at an unprecedented throughput and resolution. However, in scRNA-seq studies, only a low amount of sequenced mRNA in each cell leads to missing detection for a portion of mRNA molecules, i.e. the dropout problem. The dropout event hinders various downstream analysis, such as clustering analysis, differential expression analysis, and inference of gene-to-gene relationships. Therefore, it is necessary to develop robust and effective imputation methods for the increasing scRNA-seq data. In this study, we have developed an imputation method (GraphSCI) to impute the dropout events in scRNA-seq data based on the graph convolution networks. The method takes advantage of low-dimensional representations of similar cells and gene-gene interactions to impute the dropouts. Extensive experiments demonstrated that GraphSCI outperforms other state-of-the-art methods for imputation on both simulated and real scRNA-seq data. Meanwhile, GraphSCI is able to accurately infer gene-to-gene relationships by utilizing the imputed matrix that are concealed by dropout events in raw data.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/FKCD49AK/Rao et al. - 2020 - Imputing Single-cell RNA-seq data by combining Gra.pdf},
  language = {en},
  type = {Preprint}
}

@book{raschka19_PythonMachineLearning,
  title = {Python {{Machine Learning}}, {{Third Edition}}},
  author = {Raschka, Sebastian and Mirjalili, Vahid},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/raschka et al_2019_python machine learning, third edition.pdf}
}

@misc{raschka20_rasbtdeeplearningmodels,
  title = {Rasbt/Deeplearning-Models},
  author = {Raschka, Sebastian},
  year = {2020},
  month = jan,
  abstract = {A collection of various deep learning architectures, models, and tips},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {MIT}
}

@book{rasmussen06_Gaussianprocessesmachine,
  title = {Gaussian Processes for Machine Learning},
  author = {Rasmussen, Carl Edward and Williams, Christopher K. I.},
  year = {2006},
  publisher = {{MIT Press}},
  address = {{Cambridge, Mass}},
  annotation = {ZSCC: 0000695  OCLC: ocm61285753},
  file = {/home/trung/GoogleDrive/Zotero/rasmussen et al_2006_gaussian processes for machine learning.pdf},
  isbn = {978-0-262-18253-9},
  keywords = {Data processing,Gaussian processes,Machine learning,Mathematical models},
  language = {en},
  lccn = {QA274.4 .R37 2006},
  series = {Adaptive Computation and Machine Learning}
}

@phdthesis{rastgoufard18_MultiLabelLatentSpaces,
  title = {Multi-{{Label Latent Spaces}} with {{Semi}}-{{Supervised Deep Generative Models}}},
  author = {Rastgoufard, Rastin},
  year = {2018},
  file = {/home/trung/GoogleDrive/Zotero/rastgoufard_2018_multi-label latent spaces with semi-supervised deep generative models.pdf},
  school = {University of New Orleans}
}

@article{ravanelli00_DeepLearningDistant,
  title = {Deep {{Learning}} for {{Distant Speech Recognition}}},
  author = {Ravanelli, Mirco},
  pages = {250},
  annotation = {ZSCC: 0000009},
  file = {/home/trung/GoogleDrive/Zotero/ravanelli_deep learning for distant speech recognition.pdf},
  language = {en}
}

@article{ravanelli00_SPEAKERRECOGNITIONRAW,
  title = {{{SPEAKER RECOGNITION FROM RAW WAVEFORM WITH SINCNET}}},
  author = {Ravanelli, Mirco and Bengio, Yoshua},
  pages = {8},
  abstract = {Deep learning is progressively gaining popularity as a viable alternative to i-vectors for speaker recognition. Promising results have been recently obtained with Convolutional Neural Networks (CNNs) when fed by raw speech samples directly. Rather than employing standard hand-crafted features, the latter CNNs learn low-level speech representations from waveforms, potentially allowing the network to better capture important narrow-band speaker characteristics such as pitch and formants. Proper design of the neural network is crucial to achieve this goal.},
  file = {/home/trung/GoogleDrive/Zotero/ravanelli et al_speaker recognition from raw waveform with sincnet.pdf;/home/trung/GoogleDrive/Zotero/ravanelli et al_speaker recognition from raw waveform with sincnet2.pdf;/home/trung/Zotero/storage/SCTTETAI/1808.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Electrical Engineering and Systems Science - Signal Processing},
  language = {en}
}

@article{ravanelli20_Multitaskselfsupervisedlearning,
  title = {Multi-Task Self-Supervised Learning for {{Robust Speech Recognition}}},
  author = {Ravanelli, Mirco and Zhong, Jianyuan and Pascual, Santiago and Swietojanski, Pawel and Monteiro, Joao and Trmal, Jan and Bengio, Yoshua},
  year = {2020},
  month = apr,
  abstract = {Despite the growing interest in unsupervised learning, extracting meaningful knowledge from unlabelled audio remains an open challenge. To take a step in this direction, we recently proposed a problem-agnostic speech encoder (PASE), that combines a convolutional encoder followed by multiple neural networks, called workers, tasked to solve self-supervised problems (i.e., ones that do not require manual annotations as ground truth). PASE was shown to capture relevant speech information, including speaker voice-print and phonemes. This paper proposes PASE+, an improved version of PASE for robust speech recognition in noisy and reverberant environments. To this end, we employ an online speech distortion module, that contaminates the input signals with a variety of random disturbances. We then propose a revised encoder that better learns short- and long-term speech dynamics with an efficient combination of recurrent and convolutional networks. Finally, we refine the set of workers used in self-supervision to encourage better cooperation. Results on TIMIT, DIRHA and CHiME-5 show that PASE+ significantly outperforms both the previous version of PASE as well as common acoustic features. Interestingly, PASE+ learns transferable representations suitable for highly mismatched acoustic conditions.},
  archivePrefix = {arXiv},
  eprint = {2001.09239},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ravanelli et al_2020_multi-task self-supervised learning for robust speech recognition.pdf},
  journal = {arXiv:2001.09239 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{ravi19_AMORTIZEDBAYESIANMETALEARNING,
  title = {{{AMORTIZED BAYESIAN META}}-{{LEARNING}}},
  author = {Ravi, Sachin and Beatson, Alex},
  year = {2019},
  pages = {14},
  abstract = {Meta-learning, or learning-to-learn, has proven to be a successful strategy in attacking problems in supervised learning and reinforcement learning that involve small amounts of data. State-of-the-art solutions involve learning an initialization and/or optimization algorithm using a set of training episodes so that the metalearner can generalize to an evaluation episode quickly. These methods perform well but often lack good quantification of uncertainty, which can be vital to realworld applications when data is lacking. We propose a meta-learning method which efficiently amortizes hierarchical variational inference across tasks, learning a prior distribution over neural network weights so that a few steps of Bayes by Backprop will produce a good task-specific approximate posterior. We show that our method produces good uncertainty estimates on contextual bandit and few-shot learning benchmarks.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/63IDKC7S/Ravi and Beatson - 2019 - AMORTIZED BAYESIAN META-LEARNING.pdf},
  language = {en}
}

@article{ravuri19_ClassificationAccuracyScore,
  title = {Classification {{Accuracy Score}} for {{Conditional Generative Models}}},
  author = {Ravuri, Suman and Vinyals, Oriol},
  year = {2019},
  month = oct,
  abstract = {Deep generative models (DGMs) of images are now sufficiently mature that they produce nearly photorealistic samples and obtain scores similar to the data distribution on heuristics such as Frechet Inception Distance (FID). These results, especially on large-scale datasets such as ImageNet, suggest that DGMs are learning the data distribution in a perceptually meaningful space and can be used in downstream tasks. To test this latter hypothesis, we use class-conditional generative models from a number of model classes---variational autoencoders, autoregressive models, and generative adversarial networks (GANs)---to infer the class labels of real data. We perform this inference by training an image classifier using only synthetic data and using the classifier to predict labels on real data. The performance on this task, which we call Classification Accuracy Score (CAS), reveals some surprising results not identified by traditional metrics and constitute our contributions. First, when using a state-of-the-art GAN (BigGAN-deep), Top-1 and Top-5 accuracy decrease by 27.9\textbackslash\% and 41.6\textbackslash\%, respectively, compared to the original data; and conditional generative models from other model classes, such as Vector-Quantized Variational Autoencoder-2 (VQ-VAE-2) and Hierarchical Autoregressive Models (HAMs), substantially outperform GANs on this benchmark. Second, CAS automatically surfaces particular classes for which generative models failed to capture the data distribution, and were previously unknown in the literature. Third, we find traditional GAN metrics such as Inception Score (IS) and FID neither predictive of CAS nor useful when evaluating non-GAN models. Furthermore, in order to facilitate better diagnoses of generative models, we open-source the proposed metric.},
  archivePrefix = {arXiv},
  eprint = {1905.10887},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ravuri et al_2019_classification accuracy score for conditional generative models.pdf;/home/trung/Zotero/storage/43PU26RK/1905.html},
  journal = {arXiv:1905.10887 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{rawlinson18_SparseUnsupervisedCapsules,
  title = {Sparse {{Unsupervised Capsules Generalize Better}}},
  author = {Rawlinson, David and Ahmed, Abdelrahman and Kowadlo, Gideon},
  year = {2018},
  month = apr,
  abstract = {We show that unsupervised training of latent capsule layers using only the reconstruction loss, without masking to select the correct output class, causes a loss of equivariances and other desirable capsule qualities. This implies that supervised capsules networks can't be very deep. Unsupervised sparsening of latent capsule layer activity both restores these qualities and appears to generalize better than supervised masking, while potentially enabling deeper capsules networks. We train a sparse, unsupervised capsules network of similar geometry to Sabour et al (2017) on MNIST, and then test classification accuracy on affNIST using an SVM layer. Accuracy is improved from benchmark 79\% to 90\%.},
  archivePrefix = {arXiv},
  eprint = {1804.06094},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rawlinson et al_2018_sparse unsupervised capsules generalize better.pdf;/home/trung/Zotero/storage/NA6CCR56/1804.html},
  journal = {arXiv:1804.06094 [cs]},
  keywords = {capsule,Computer Science - Computer Vision and Pattern Recognition,sparse},
  primaryClass = {cs}
}

@article{raza20_surveynewsrecommender,
  title = {A Survey on News Recommender System -- {{Dealing}} with Timeliness, Dynamic User Interest and Content Quality, and Effects of Recommendation on News Readers},
  author = {Raza, Shaina and Ding, Chen},
  year = {2020},
  month = sep,
  abstract = {Nowadays, more and more news readers tend to read news online where they have access to millions of news articles from multiple sources. In order to help users to find the right and relevant content, news recommender systems (NRS) are developed to relieve the information overload problem and suggest news items that users might be in-terested in. In this paper, we highlight the major challenges faced by the news recommen-dation domain and identify the possible solutions from the state-of-the-art. Due to the rapid growth of building recommender systems using deep learning models, we divide our dis-cussion in two parts. In the first part, we present an overview of the conventional recom-mendation solutions, datasets, evaluation criteria beyond accuracy and recommendation platforms being used in NRS. In the second part, we explain deep learning-based recom-mendation solutions applied in NRS. Different from previous surveys, we also study the effects of news recommendations on user behavior and try to suggest the possible reme-dies to mitigate these effects. By providing the state-of-the-art knowledge, this survey can help researchers and practical professionals in their understanding of developments in news recommendation algorithms. It also sheds light on potential new directions.},
  archivePrefix = {arXiv},
  eprint = {2009.04964},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/raza et al_2020_a survey on news recommender system -- dealing with timeliness, dynamic user interest and content quality, and effects of recommendation on news readers.pdf},
  journal = {arXiv:2009.04964 [cs]},
  primaryClass = {cs}
}

@article{razavi19_PreventingPosteriorCollapse,
  title = {Preventing {{Posterior Collapse}} with Delta-{{VAEs}}},
  author = {Razavi, Ali and van den Oord, A{\"a}ron and Poole, Ben and Vinyals, Oriol},
  year = {2019},
  month = jan,
  abstract = {Due to the phenomenon of "posterior collapse," current latent variable generative models pose a challenging design choice that either weakens the capacity of the decoder or requires augmenting the objective so it does not only maximize the likelihood of the data. In this paper, we propose an alternative that utilizes the most powerful generative models as decoders, whilst optimising the variational lower bound all while ensuring that the latent variables preserve and encode useful information. Our proposed \$\textbackslash delta\$-VAEs achieve this by constraining the variational family for the posterior to have a minimum distance to the prior. For sequential latent variable models, our approach resembles the classic representation learning approach of slow feature analysis. We demonstrate the efficacy of our approach at modeling text on LM1B and modeling images: learning representations, improving sample quality, and achieving state of the art log-likelihood on CIFAR-10 and ImageNet \$32\textbackslash times 32\$.},
  annotation = {ZSCC: 0000015},
  archivePrefix = {arXiv},
  eprint = {1901.03416},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/razavi et al_2019_preventing posterior collapse with delta-vaes.pdf;/home/trung/Zotero/storage/4ZKD5G6C/1901.html},
  journal = {arXiv:1901.03416 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,vae_issues},
  primaryClass = {cs, stat}
}

@article{razavian20_validatedrealtimeprediction,
  title = {A Validated, Real-Time Prediction Model for Favorable Outcomes in Hospitalized {{COVID}}-19 Patients},
  author = {Razavian, Narges and Major, Vincent J. and Sudarshan, Mukund and {Burk-Rafel}, Jesse and Stella, Peter and Randhawa, Hardev and Bilaloglu, Seda and Chen, Ji and Nguy, Vuthy and Wang, Walter and Zhang, Hao and Reinstein, Ilan and Kudlowitz, David and Zenger, Cameron and Cao, Meng and Zhang, Ruina and Dogra, Siddhant and Harish, Keerthi B. and Bosworth, Brian and Francois, Fritz and Horwitz, Leora I. and Ranganath, Rajesh and Austrian, Jonathan and Aphinyanaphongs, Yindalon},
  year = {2020},
  month = oct,
  volume = {3},
  pages = {130},
  issn = {2398-6352},
  doi = {10.1038/s41746-020-00343-x},
  abstract = {The COVID-19 pandemic has challenged front-line clinical decision-making, leading to numerous published prognostic tools. However, few models have been prospectively validated and none report implementation in practice. Here, we use 3345 retrospective and 474 prospective hospitalizations to develop and validate a parsimonious model to identify patients with favorable outcomes within 96\,h of a prediction, based on real-time lab values, vital signs, and oxygen support variables. In retrospective and prospective validation, the model achieves high average precision (88.6\% 95\% CI: [88.4\textendash 88.7] and 90.8\% [90.8\textendash 90.8]) and discrimination (95.1\% [95.1\textendash 95.2] and 86.8\% [86.8\textendash 86.9]) respectively. We implemented and integrated the model into the EHR, achieving a positive predictive value of 93.3\% with 41\% sensitivity. Preliminary results suggest clinicians are adopting these scores into their clinical workflows.},
  file = {/home/trung/GoogleDrive/Zotero/razavian et al_2020_a validated, real-time prediction model for favorable outcomes in hospitalized covid-19 patients.pdf},
  journal = {npj Digital Medicine},
  number = {1}
}

@article{reardon00_StepasideCRISPR,
  title = {Step aside {{CRISPR}}, {{RNA}} Editing Is Taking Off},
  author = {Reardon, Sara},
  pages = {15},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/EN8PCAM9/Reardon - Step aside CRISPR, RNA editing is taking off.pdf},
  language = {en}
}

@article{redko20_surveydomainadaptation,
  title = {A Survey on Domain Adaptation Theory: Learning Bounds and Theoretical Guarantees},
  shorttitle = {A Survey on Domain Adaptation Theory},
  author = {Redko, Ievgen and Morvant, Emilie and Habrard, Amaury and Sebban, Marc and Bennani, Youn{\`e}s},
  year = {2020},
  month = aug,
  abstract = {All famous machine learning algorithms that comprise both supervised and semi-supervised learning work well only under a common assumption: the training and test data follow the same distribution. When the distribution changes, most statistical models must be reconstructed from new collected data, which for some applications can be costly or impossible to obtain. Therefore, it has become necessary to develop approaches that reduce the need and the effort to obtain new labeled samples by exploiting data that are available in related areas, and using these further across similar fields. This has given rise to a new machine learning framework known as transfer learning: a learning setting inspired by the capability of a human being to extrapolate knowledge across tasks to learn more efficiently. Despite a large amount of different transfer learning scenarios, the main objective of this survey is to provide an overview of the state-of-the-art theoretical results in a specific, and arguably the most popular, sub-field of transfer learning, called domain adaptation. In this sub-field, the data distribution is assumed to change across the training and the test data, while the learning task remains the same. We provide a first up-to-date description of existing results related to domain adaptation problem that cover learning bounds based on different statistical learning frameworks.},
  archivePrefix = {arXiv},
  eprint = {2004.11829},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/redko et al_2020_a survey on domain adaptation theory.pdf},
  journal = {arXiv:2004.11829 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{redmon18_YOLOv3IncrementalImprovement,
  title = {{{YOLOv3}}: {{An Incremental Improvement}}},
  shorttitle = {{{YOLOv3}}},
  author = {Redmon, Joseph and Farhadi, Ali},
  year = {2018},
  month = apr,
  abstract = {We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at https://pjreddie.com/yolo/},
  archivePrefix = {arXiv},
  eprint = {1804.02767},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/redmon et al_2018_yolov3.pdf},
  journal = {arXiv:1804.02767 [cs]},
  keywords = {favorite},
  primaryClass = {cs}
}

@article{reed00_LearningDisentangleFactors,
  title = {Learning to {{Disentangle Factors}} of {{Variation}} with {{Manifold Interaction}}},
  author = {Reed, Scott and Sohn, Kihyuk and Zhang, Yuting and Lee, Honglak},
  pages = {9},
  abstract = {Many latent factors of variation interact to generate sensory data; for example, pose, morphology and expression in face images. In this work, we propose to learn manifold coordinates for the relevant factors of variation and to model their joint interaction. Many existing feature learning algorithms focus on a single task and extract features that are sensitive to the task-relevant factors and invariant to all others. However, models that just extract a single set of invariant features do not exploit the relationships among the latent factors. To address this, we propose a higher-order Boltzmann machine that incorporates multiplicative interactions among groups of hidden units that each learn to encode a distinct factor of variation. Furthermore, we propose correspondencebased training strategies that allow effective disentangling. Our model achieves state-of-the-art emotion recognition and face verification performance on the Toronto Face Database. We also demonstrate disentangled features learned on the CMU Multi-PIE dataset.},
  annotation = {ZSCC: 0000163},
  file = {/home/trung/GoogleDrive/Zotero/reed et al_learning to disentangle factors of variation with manifold interaction.pdf},
  language = {en}
}

@misc{rees00_Whytechcompanies,
  title = {Why Tech Companies Need Philosophers\textemdash and How {{I}} Convinced {{Google}} to Hire Them},
  author = {Rees, Tobias},
  abstract = {A collective called Transformations of the Human are helping big tech and AI labs to hire philosophers and artists.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/VPNXZ3I5/why-tech-companies-need-to-hire-philosophers.html},
  howpublished = {https://qz.com/1734381/why-tech-companies-need-to-hire-philosophers/},
  journal = {Quartz},
  language = {en}
}

@article{ren00_LikelihoodRatiosOutofDistribution,
  title = {Likelihood {{Ratios}} for {{Out}}-of-{{Distribution Detection}}},
  author = {Ren, Jie and Liu, Peter J and Fertig, Emily and Snoek, Jasper and Poplin, Ryan and Depristo, Mark and Dillon, Joshua and Lakshminarayanan, Balaji},
  pages = {12},
  abstract = {Discriminative neural networks offer little or no performance guarantees when deployed on data not generated by the same process as the training distribution. On such out-of-distribution (OOD) inputs, the prediction may not only be erroneous, but confidently so, limiting the safe deployment of classifiers in real-world applications. One such challenging application is bacteria identification based on genomic sequences, which holds the promise of early detection of diseases, but requires a model that can output low confidence predictions on OOD genomic sequences from new bacteria that were not present in the training data. We introduce a genomics dataset for OOD detection that allows other researchers to benchmark progress on this important problem. We investigate deep generative model based approaches for OOD detection and observe that the likelihood score is heavily affected by population level background statistics. We propose a likelihood ratio method for deep generative models which effectively corrects for these confounding background statistics. We benchmark the OOD detection performance of the proposed method against existing approaches on the genomics dataset and show that our method achieves state-of-the-art performance. We demonstrate the generality of the proposed method by showing that it significantly improves OOD detection when applied to deep generative models of images.},
  file = {/home/trung/Zotero/storage/RV4JUQDR/Ren et al. - Likelihood Ratios for Out-of-Distribution Detectio.pdf},
  language = {en}
}

@article{ren18_IncrementalFewShotLearning,
  title = {Incremental {{Few}}-{{Shot Learning}} with {{Attention Attractor Networks}}},
  author = {Ren, Mengye and Liao, Renjie and Fetaya, Ethan and Zemel, Richard S.},
  year = {2018},
  month = oct,
  abstract = {Machine learning classifiers are often trained to recognize a set of pre-defined classes. However, in many real applications, it is often desirable to have the flexibility of learning additional concepts, without re-training on the full training set. This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes; and several extra novel classes are being considered, each with only a few labeled examples. After learning the novel classes, the model is then evaluated on the overall performance of both base and novel classes. To this end, we propose a meta-learning model, the Attention Attractor Network, which regularizes the learning of novel classes. In each episode, we train a set of new weights to recognize novel classes until they converge, and we show that the technique of recurrent back-propagation can back-propagate through the optimization process and facilitate the learning of the attractor network regularizer. We demonstrate that the learned attractor network can recognize novel classes while remembering old classes without the need to review the original training set, outperforming baselines that do not rely on an iterative optimization process.},
  annotation = {ZSCC: 0000004},
  archivePrefix = {arXiv},
  eprint = {1810.07218},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ren et al_2018_incremental few-shot learning with attention attractor networks.pdf;/home/trung/Zotero/storage/XA4FM52K/1810.html},
  journal = {arXiv:1810.07218 [cs, stat]},
  keywords = {attetion,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ren19_FastSpeechFastRobust,
  title = {{{FastSpeech}}: {{Fast}}, {{Robust}} and {{Controllable Text}} to {{Speech}}},
  shorttitle = {{{FastSpeech}}},
  author = {Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  year = {2019},
  month = may,
  abstract = {Neural network based end-to-end text to speech (TTS) has significantly improved the quality of synthesized speech. Prominent methods (e.g., Tacotron 2) usually first generate mel-spectrogram from text, and then synthesize speech from the mel-spectrogram using vocoder such as WaveNet. Compared with traditional concatenative and statistical parametric approaches, neural network based end-to-end models suffer from slow inference speed, and the synthesized speech is usually not robust (i.e., some words are skipped or repeated) and lack of controllability (voice speed or prosody control). In this work, we propose a novel feed-forward network based on Transformer to generate mel-spectrogram in parallel for TTS. Specifically, we extract attention alignments from an encoder-decoder based teacher model for phoneme duration prediction, which is used by a length regulator to expand the source phoneme sequence to match the length of the target mel-spectrogram sequence for parallel mel-spectrogram generation. Experiments on the LJSpeech dataset show that our parallel model matches autoregressive models in terms of speech quality, nearly eliminates the problem of word skipping and repeating in particularly hard cases, and can adjust voice speed smoothly. Most importantly, compared with autoregressive Transformer TTS, our model speeds up mel-spectrogram generation by 270x and the end-to-end speech synthesis by 38x. Therefore, we call our model FastSpeech.},
  archivePrefix = {arXiv},
  eprint = {1905.09263},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ren et al_2019_fastspeech.pdf},
  journal = {arXiv:1905.09263 [cs, eess]},
  primaryClass = {cs, eess}
}

@misc{ren20_jason718awesomeselfsupervisedlearning,
  title = {Jason718/Awesome-Self-Supervised-Learning},
  author = {Ren, Jason},
  year = {2020},
  month = may,
  abstract = {A curated list of awesome self-supervised methods. Contribute to jason718/awesome-self-supervised-learning development by creating an account on GitHub.},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{renz20_failuremodesmolecule,
  title = {On Failure Modes in Molecule Generation and Optimization},
  author = {Renz, Philipp and Van Rompaey, Dries and Wegner, J{\"o}rg Kurt and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  year = {2020},
  month = oct,
  issn = {1740-6749},
  doi = {10.1016/j.ddtec.2020.09.003},
  abstract = {There has been a wave of generative models for molecules triggered by advances in the field of Deep Learning. These generative models are often used to optimize chemical compounds towards particular properties or a desired biological activity. The evaluation of generative models remains challenging and suggested performance metrics or scoring functions often do not cover all relevant aspects of drug design projects. In this work, we highlight some unintended failure modes in molecular generation and optimization and how these evade detection by current performance metrics.},
  file = {/home/trung/GoogleDrive/Zotero/renz et al_2020_on failure modes in molecule generation and optimization.pdf},
  journal = {Drug Discovery Today: Technologies},
  keywords = {_tablet,favorite}
}

@misc{researcher19_MostImportantSupreme,
  title = {The {{Most Important Supreme Court Decision For Data Science}} and {{Machine Learning}}},
  author = {Researcher, PhD, Matthew Stewart},
  year = {2019},
  month = nov,
  abstract = {Training algorithms on copyrighted data is not illegal, according to the United States Supreme Court.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/NGETBYFP/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf.html},
  howpublished = {https://towardsdatascience.com/the-most-important-supreme-court-decision-for-data-science-and-machine-learning-44cfc1c1bcaf},
  journal = {Medium},
  language = {en}
}

@article{resnick19_CapacityBandwidthCompositionality,
  title = {Capacity, {{Bandwidth}}, and {{Compositionality}} in {{Emergent Language Learning}}},
  author = {Resnick, Cinjon and Gupta, Abhinav and Foerster, Jakob and Dai, Andrew M. and Cho, Kyunghyun},
  year = {2019},
  month = oct,
  abstract = {Many recent works have discussed the propensity, or lack thereof, for emergent languages to exhibit properties of natural languages. A favorite in the literature is learning compositionality. We note that most of those works have focused on communicative bandwidth as being of primary importance. While important, it is not the only contributing factor. In this paper, we investigate the learning biases that affect the efficacy and compositionality of emergent languages. Our foremost contribution is to explore how capacity of a neural network impacts its ability to learn a compositional language. We additionally introduce a set of evaluation metrics with which we analyze the learned languages. Our hypothesis is that there should be a specific range of model capacity and channel bandwidth that induces compositional structure in the resulting language and consequently encourages systematic generalization. While we empirically see evidence for the bottom of this range, we curiously do not find evidence for the top part of the range and believe that this is an open question for the community.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.11424},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/resnick et al_2019_capacity, bandwidth, and compositionality in emergent language learning.pdf;/home/trung/Zotero/storage/BXSW93DX/1910.html},
  journal = {arXiv:1910.11424 [cs]},
  keywords = {compositional,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,discrete,semi-supervised,sequence,variational},
  primaryClass = {cs}
}

@article{rethage00_WavenetSpeechDenoising,
  title = {A {{Wavenet}} for {{Speech Denoising}}},
  author = {Rethage, Dario and Pons, Jordi and Serra, Xavier},
  pages = {11},
  abstract = {Currently, most speech processing techniques use magnitude spectrograms as frontend and are therefore by default discarding part of the signal: the phase. In order to overcome this limitation, we propose an end-to-end learning method for speech denoising based on Wavenet. The proposed model adaptation retains Wavenet's powerful acoustic modeling capabilities, while significantly reducing its timecomplexity by eliminating its autoregressive nature. Specifically, the model makes use of non-causal, dilated convolutions and predicts target fields instead of a single target sample. The discriminative adaptation of the model we propose, learns in a supervised fashion via minimizing a regression loss. These modifications make the model highly parallelizable during both training and inference. Both computational and perceptual evaluations indicate that the proposed method is preferred to Wiener filtering, a common method based on processing the magnitude spectrogram.},
  file = {/home/trung/GoogleDrive/Zotero/rethage et al_a wavenet for speech denoising.pdf},
  language = {en}
}

@article{rey00_CanVAEscapture,
  title = {Can {{VAEs}} Capture Topological Properties?},
  author = {Rey, Luis A P{\'e}rez and Menkovski, Vlado and Portegies, Jacobus W},
  pages = {12},
  abstract = {To what extent can Variational Autoencoders (VAEs) identify semantically meaningful latent variables? Can they at least capture the correct topology if ground-truth latent variables are known? To investigate these questions, we introduce the Diffusion VAE, which allows for arbitrary (closed) manifolds in latent space. A Diffusion VAE uses transition kernels of Brownian motion on the manifold. In particular, it uses properties of the Brownian motion to implement the reparametrization trick and fast approximations to the KL divergence. We show that the Diffusion Variational Autoencoder is indeed capable of capturing topological properties.},
  file = {/home/trung/GoogleDrive/Zotero/rey et al_can vaes capture topological properties.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{rey19_DiffusionVariationalAutoencoders,
  title = {Diffusion {{Variational Autoencoders}}},
  author = {Rey, Luis A. P{\'e}rez and Menkovski, Vlado and Portegies, Jacobus W.},
  year = {2019},
  month = mar,
  abstract = {A standard Variational Autoencoder, with a Euclidean latent space, is structurally incapable of capturing topological properties of certain datasets. To remove topological obstructions, we introduce Diffusion Variational Autoencoders with arbitrary manifolds as a latent space. A Diffusion Variational Autoencoder uses transition kernels of Brownian motion on the manifold. In particular, it uses properties of the Brownian motion to implement the reparametrization trick and fast approximations to the KL divergence. We show that the Diffusion Variational Autoencoder is capable of capturing topological properties of synthetic datasets. Additionally, we train MNIST on spheres, tori, projective spaces, SO(3), and a torus embedded in R3. Although a natural dataset like MNIST does not have latent variables with a clear-cut topological structure, training it on a manifold can still highlight topological and geometrical properties.},
  archivePrefix = {arXiv},
  eprint = {1901.08991},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rey et al_2019_diffusion variational autoencoders.pdf},
  journal = {arXiv:1901.08991 [cs, stat]},
  keywords = {_tablet},
  primaryClass = {cs, stat}
}

@article{rey20_DisentanglementHypersphericalLatent,
  title = {Disentanglement with {{Hyperspherical Latent Spaces}} Using {{Diffusion Variational Autoencoders}}},
  author = {Rey, Luis A. P{\'e}rez},
  year = {2020},
  month = mar,
  abstract = {A disentangled representation of a data set should be capable of recovering the underlying factors that generated it. One question that arises is whether using Euclidean space for latent variable models can produce a disentangled representation when the underlying generating factors have a certain geometrical structure. Take for example the images of a car seen from different angles. The angle has a periodic structure but a 1-dimensional representation would fail to capture this topology. How can we address this problem? The submissions presented for the first stage of the NeurIPS2019 Disentanglement Challenge consist of a Diffusion Variational Autoencoder (\$\textbackslash Delta\$VAE) with a hyperspherical latent space which can, for example, recover periodic true factors. The training of the \$\textbackslash Delta\$VAE is enhanced by incorporating a modified version of the Evidence Lower Bound (ELBO) for tailoring the encoding capacity of the posterior approximate.},
  archivePrefix = {arXiv},
  eprint = {2003.08996},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rey_2020_disentanglement with hyperspherical latent spaces using diffusion variational autoencoders.pdf},
  journal = {arXiv:2003.08996 [cs, stat]},
  keywords = {_tablet},
  primaryClass = {cs, stat}
}

@article{rezaabad20_LearningRepresentationsMaximizing,
  title = {Learning {{Representations}} by {{Maximizing Mutual Information}} in {{Variational Autoencoders}}},
  author = {Rezaabad, Ali Lotfi and Vishwanath, Sriram},
  year = {2020},
  month = jan,
  abstract = {Variational autoencoders (VAEs) have ushered in a new era of unsupervised learning methods for complex distributions. Although these techniques are elegant in their approach, they are typically not useful for representation learning. In this work, we propose a simple yet powerful class of VAEs that simultaneously result in meaningful learned representations. Our solution is to combine traditional VAEs with mutual information maximization, with the goal to enhance amortized inference in VAEs using Information Theoretic techniques. We call this approach InfoMax-VAE, and such an approach can significantly boost the quality of learned high-level representations. We realize this through the explicit maximization of information measures associated with the representation. Using extensive experiments on varied datasets and setups, we show that InfoMax-VAE outperforms contemporary popular approaches, including Info-VAE and \$\textbackslash beta\$-VAE.},
  archivePrefix = {arXiv},
  eprint = {1912.13361},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rezaabad et al_2020_learning representations by maximizing mutual information in variational autoencoders.pdf},
  journal = {arXiv:1912.13361 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{rezende00_propertieshighcapacityVAEs,
  title = {On the Properties of High-Capacity {{VAEs}}},
  author = {Rezende, Danilo J and Viola, Fabio},
  pages = {10},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/rezende et al_on the properties of high-capacity vaes.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{rezende14_StochasticBackpropagationApproximate,
  title = {Stochastic {{Backpropagation}} and {{Approximate Inference}} in {{Deep Generative Models}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir and Wierstra, Daan},
  year = {2014},
  month = may,
  abstract = {We marry ideas from deep neural networks and approximate Bayesian inference to derive a generalised class of deep, directed generative models, endowed with a new algorithm for scalable inference and learning. Our algorithm introduces a recognition model to represent an approximate posterior distribution and uses this for optimisation of a variational lower bound. We develop stochastic backpropagation \textendash{} rules for gradient backpropagation through stochastic variables \textendash{} and derive an algorithm that allows for joint optimisation of the parameters of both the generative and recognition models. We demonstrate on several real-world data sets that by using stochastic backpropagation and variational inference, we obtain models that are able to generate realistic samples of data, allow for accurate imputations of missing data, and provide a useful tool for high-dimensional data visualisation.},
  archivePrefix = {arXiv},
  eprint = {1401.4082},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rezende et al_2014_stochastic backpropagation and approximate inference in deep generative models.pdf},
  journal = {arXiv:1401.4082 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rezende16_VariationalInferenceNormalizing,
  title = {Variational {{Inference}} with {{Normalizing Flows}}},
  author = {Rezende, Danilo Jimenez and Mohamed, Shakir},
  year = {2016},
  month = jun,
  abstract = {The choice of approximate posterior distribution is one of the core problems in variational inference. Most applications of variational inference employ simple families of posterior approximations in order to allow for efficient inference, focusing on mean-field or other simple structured approximations. This restriction has a significant impact on the quality of inferences made using variational methods. We introduce a new approach for specifying flexible, arbitrarily complex and scalable approximate posterior distributions. Our approximations are distributions constructed through a normalizing flow, whereby a simple initial density is transformed into a more complex one by applying a sequence of invertible transformations until a desired level of complexity is attained. We use this view of normalizing flows to develop categories of finite and infinitesimal flows and provide a unified view of approaches for constructing rich posterior approximations. We demonstrate that the theoretical advantages of having posteriors that better match the true posterior, combined with the scalability of amortized variational approaches, provides a clear improvement in performance and applicability of variational inference.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1505.05770},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rezende et al_2016_variational inference with normalizing flows.pdf},
  journal = {arXiv:1505.05770 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rezende18_TamingVAEs,
  title = {Taming {{VAEs}}},
  author = {Rezende, Danilo Jimenez and Viola, Fabio},
  year = {2018},
  month = oct,
  abstract = {In spite of remarkable progress in deep latent variable generative modeling, training still remains a challenge due to a combination of optimization and generalization issues. In practice, a combination of heuristic algorithms (such as hand-crafted annealing of KL-terms) is often used in order to achieve the desired results, but such solutions are not robust to changes in model architecture or dataset. The best settings can often vary dramatically from one problem to another, which requires doing expensive parameter sweeps for each new case. Here we develop on the idea of training VAEs with additional constraints as a way to control their behaviour. We first present a detailed theoretical analysis of constrained VAEs, expanding our understanding of how these models work. We then introduce and analyze a practical algorithm termed Generalized ELBO with Constrained Optimization, GECO. The main advantage of GECO for the machine learning practitioner is a more intuitive, yet principled, process of tuning the loss. This involves defining of a set of constraints, which typically have an explicit relation to the desired model performance, in contrast to tweaking abstract hyper-parameters which implicitly affect the model behavior. Encouraging experimental results in several standard datasets indicate that GECO is a very robust and effective tool to balance reconstruction and compression constraints.},
  annotation = {ZSCC: 0000026},
  archivePrefix = {arXiv},
  eprint = {1810.00597},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rezende et al_2018_taming vaes.pdf;/home/trung/Zotero/storage/N4C2TAM9/1810.html},
  journal = {arXiv:1810.00597 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{rezende20_CausallyCorrectPartial,
  title = {Causally {{Correct Partial Models}} for {{Reinforcement Learning}}},
  author = {Rezende, Danilo J. and Danihelka, Ivo and Papamakarios, George and Ke, Nan Rosemary and Jiang, Ray and Weber, Theophane and Gregor, Karol and Merzic, Hamza and Viola, Fabio and Wang, Jane and Mitrovic, Jovana and Besse, Frederic and Antonoglou, Ioannis and Buesing, Lars},
  year = {2020},
  month = feb,
  abstract = {In reinforcement learning, we can learn a model of future observations and rewards, and use it to plan the agent's next actions. However, jointly modeling future observations can be computationally expensive or even intractable if the observations are high-dimensional (e.g. images). For this reason, previous works have considered partial models, which model only part of the observation. In this paper, we show that partial models can be causally incorrect: they are confounded by the observations they don't model, and can therefore lead to incorrect planning. To address this, we introduce a general family of partial models that are provably causally correct, yet remain fast because they do not need to fully model future observations.},
  archivePrefix = {arXiv},
  eprint = {2002.02836},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rezende et al_2020_causally correct partial models for reinforcement learning.pdf},
  journal = {arXiv:2002.02836 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{rezende20_NormalizingFlowsTori,
  title = {Normalizing {{Flows}} on {{Tori}} and {{Spheres}}},
  author = {Rezende, Danilo Jimenez and Papamakarios, George and Racani{\`e}re, S{\'e}bastien and Albergo, Michael S. and Kanwar, Gurtej and Shanahan, Phiala E. and Cranmer, Kyle},
  year = {2020},
  month = feb,
  abstract = {Normalizing flows are a powerful tool for building expressive distributions in high dimensions. So far, most of the literature has concentrated on learning flows on Euclidean spaces. Some problems however, such as those involving angles, are defined on spaces with more complex geometries, such as tori or spheres. In this paper, we propose and compare expressive and numerically stable flows on such spaces. Our flows are built recursively on the dimension of the space, starting from flows on circles, closed intervals or spheres.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2002.02428},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/4MNWD7RT/Rezende et al. - 2020 - Normalizing Flows on Tori and Spheres.pdf},
  journal = {arXiv:2002.02428 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rhodes18_VariationalNoiseContrastiveEstimation,
  title = {Variational {{Noise}}-{{Contrastive Estimation}}},
  author = {Rhodes, Benjamin and Gutmann, Michael},
  year = {2018},
  month = oct,
  abstract = {Unnormalised latent variable models are a broad and flexible class of statistical models. However, learning their parameters from data is intractable, and few estimation techniques are currently available for such models. To increase the number of techniques in our arsenal, we propose variational noise-contrastive estimation (VNCE), building on NCE which is a method that only applies to unnormalised models. The core idea is to use a variational lower bound to the NCE objective function, which can be optimised in the same fashion as the evidence lower bound (ELBO) in standard variational inference (VI). We prove that VNCE can be used for both parameter estimation of unnormalised models and posterior inference of latent variables. The developed theory shows that VNCE has the same level of generality as standard VI, meaning that advances made there can be directly imported to the unnormalised setting. We validate VNCE on toy models and apply it to a realistic problem of estimating an undirected graphical model from incomplete data.},
  archivePrefix = {arXiv},
  eprint = {1810.08010},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rhodes et al_2018_variational noise-contrastive estimation.pdf},
  journal = {arXiv:1810.08010 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{richards19_deeplearningframework,
  title = {A Deep Learning Framework for Neuroscience},
  author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and {de Berker}, Archy and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, Jo{\~a}o and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
  year = {2019},
  month = nov,
  volume = {22},
  pages = {1761--1770},
  issn = {1097-6256, 1546-1726},
  doi = {10.1038/s41593-019-0520-2},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/richards et al_2019_a deep learning framework for neuroscience.pdf},
  journal = {Nature Neuroscience},
  language = {en},
  number = {11}
}

@article{richardson20_PresentingCharacteristicsComorbidities,
  title = {Presenting {{Characteristics}}, {{Comorbidities}}, and {{Outcomes Among}} 5700 {{Patients Hospitalized With COVID}}-19 in the {{New York City Area}}},
  author = {Richardson, Safiya and Hirsch, Jamie S. and Narasimhan, Mangala and Crawford, James M. and McGinn, Thomas and Davidson, Karina W. and Barnaby, Douglas P. and Becker, Lance B. and Chelico, John D. and Cohen, Stuart L. and Cookingham, Jennifer and Coppa, Kevin and Diefenbach, Michael A. and Dominello, Andrew J. and {Duer-Hefele}, Joan and Falzon, Louise and Gitlin, Jordan and Hajizadeh, Negin and Harvin, Tiffany G. and Hirschwerk, David A. and Kim, Eun Ji and Kozel, Zachary M. and Marrast, Lyndonna M. and Mogavero, Jazmin N. and Osorio, Gabrielle A. and Qiu, Michael and Zanos, Theodoros P.},
  year = {2020},
  month = apr,
  doi = {10.1001/jama.2020.6775},
  abstract = {{$<$}h3{$>$}Importance{$<$}/h3{$><$}p{$>$}There is limited information describing the presenting characteristics and outcomes of US patients requiring hospitalization for coronavirus disease 2019 (COVID-19).{$<$}/p{$><$}h3{$>$}Objective{$<$}/h3{$><$}p{$>$}To describe the clinical characteristics and outcomes of patients with COVID-19 hospitalized in a US health care system.{$<$}/p{$><$}h3{$>$}Design, Setting, and Participants{$<$}/h3{$><$}p{$>$}Case series of patients with COVID-19 admitted to 12 hospitals in New York City, Long Island, and Westchester County, New York, within the Northwell Health system. The study included all sequentially hospitalized patients between March 1, 2020, and April 4, 2020, inclusive of these dates.{$<$}/p{$><$}h3{$>$}Exposures{$<$}/h3{$><$}p{$>$}Confirmed severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection by positive result on polymerase chain reaction testing of a nasopharyngeal sample among patients requiring admission.{$<$}/p{$><$}h3{$>$}Main Outcomes and Measures{$<$}/h3{$><$}p{$>$}Clinical outcomes during hospitalization, such as invasive mechanical ventilation, kidney replacement therapy, and death. Demographics, baseline comorbidities, presenting vital signs, and test results were also collected.{$<$}/p{$><$}h3{$>$}Results{$<$}/h3{$><$}p{$>$}A total of 5700 patients were included (median age, 63 years [interquartile range \{IQR\}, 52-75; range, 0-107 years]; 39.7\% female). The most common comorbidities were hypertension (3026; 56.6\%), obesity (1737; 41.7\%), and diabetes (1808; 33.8\%). At triage, 30.7\% of patients were febrile, 17.3\% had a respiratory rate greater than 24 breaths/min, and 27.8\% received supplemental oxygen. The rate of respiratory virus co-infection was 2.1\%. Outcomes were assessed for 2634 patients who were discharged or had died at the study end point. During hospitalization, 373 patients (14.2\%) (median age, 68 years [IQR, 56-78]; 33.5\% female) were treated in the intensive care unit care, 320 (12.2\%) received invasive mechanical ventilation, 81 (3.2\%) were treated with kidney replacement therapy, and 553 (21\%) died. As of April 4, 2020, for patients requiring mechanical ventilation (n = 1151, 20.2\%), 38 (3.3\%) were discharged alive, 282 (24.5\%) died, and 831 (72.2\%) remained in hospital. The median postdischarge follow-up time was 4.4 days (IQR, 2.2-9.3). A total of 45 patients (2.2\%) were readmitted during the study period. The median time to readmission was 3 days (IQR, 1.0-4.5) for readmitted patients. Among the 3066 patients who remained hospitalized at the final study follow-up date (median age, 65 years [IQR, 54-75]), the median follow-up at time of censoring was 4.5 days (IQR, 2.4-8.1).{$<$}/p{$><$}h3{$>$}Conclusions and Relevance{$<$}/h3{$><$}p{$>$}This case series provides characteristics and early outcomes of sequentially hospitalized patients with confirmed COVID-19 in the New York City area.{$<$}/p{$>$}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/richardson et al_2020_presenting characteristics, comorbidities, and outcomes among 5700 patients hospitalized with covid-19 in the new york city area.pdf;/home/trung/Zotero/storage/QQ3HK9N5/2765184.html},
  journal = {JAMA},
  language = {en}
}

@article{ridgeway16_SurveyInductiveBiases,
  title = {A {{Survey}} of {{Inductive Biases}} for {{Factorial Representation}}-{{Learning}}},
  author = {Ridgeway, Karl},
  year = {2016},
  month = dec,
  abstract = {With the resurgence of interest in neural networks, representation learning has re-emerged as a central focus in artificial intelligence. Representation learning refers to the discovery of useful encodings of data that make domain-relevant information explicit. Factorial representations identify underlying independent causal factors of variation in data. A factorial representation is compact and faithful, makes the causal factors explicit, and facilitates human interpretation of data. Factorial representations support a variety of applications, including the generation of novel examples, indexing and search, novelty detection, and transfer learning.},
  annotation = {ZSCC: 0000018},
  archivePrefix = {arXiv},
  eprint = {1612.05299},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ridgeway_2016_a survey of inductive biases for factorial representation-learning.pdf},
  journal = {arXiv:1612.05299 [cs]},
  keywords = {_tablet,disentanglement,favorite},
  language = {en},
  primaryClass = {cs}
}

@article{ridgeway18_LearningDeepDisentangled,
  title = {Learning {{Deep Disentangled Embeddings}} with the {{F}}-{{Statistic Loss}}},
  author = {Ridgeway, Karl and Mozer, Michael C.},
  year = {2018},
  month = may,
  abstract = {Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the \$F\$ statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@\$k\$ and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.},
  archivePrefix = {arXiv},
  eprint = {1802.05312},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ridgeway et al_2018_learning deep disentangled embeddings with the f-statistic loss.pdf},
  journal = {arXiv:1802.05312 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@misc{riebesell20_janoshawesomenormalizingflows,
  title = {Janosh/Awesome-Normalizing-Flows},
  author = {Riebesell, Janosh},
  year = {2020},
  month = nov,
  abstract = {A list of awesome resources on normalizing flows. Contribute to janosh/awesome-normalizing-flows development by creating an account on GitHub.},
  copyright = {MIT License         ,                 MIT License}
}

@misc{rinder20_LukasRindernormalizingflows,
  title = {{{LukasRinder}}/Normalizing-Flows},
  author = {Rinder, Lukas},
  year = {2020},
  month = apr,
  abstract = {Implementation of normalizing flows in TensorFlow 2 including a small tutorial.},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{risi19_ImprovingDeepNeuroevolution,
  title = {Improving {{Deep Neuroevolution}} via {{Deep Innovation Protection}}},
  author = {Risi, Sebastian and Stanley, Kenneth O.},
  year = {2019},
  month = dec,
  abstract = {Evolutionary-based optimization approaches have recently shown promising results in domains such as Atari and robot locomotion but less so in solving 3D tasks directly from pixels. This paper presents a method called Deep Innovation Protection (DIP) that allows training complex world models end-to-end for such 3D environments. The main idea behind the approach is to employ multiobjective optimization to temporally reduce the selection pressure on specific components in a world model, allowing other components to adapt. We investigate the emergent representations of these evolved networks, which learn a model of the world without the need for a specific forward-prediction loss.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2001.01683},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/risi et al_2019_improving deep neuroevolution via deep innovation protection.pdf;/home/trung/Zotero/storage/YNPF5QVF/2001.html},
  journal = {arXiv:2001.01683 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{risso18_generalflexiblemethod,
  title = {A General and Flexible Method for Signal Extraction from Single-Cell {{RNA}}-Seq Data},
  author = {Risso, Davide and Perraudeau, Fanny and Gribkova, Svetlana and Dudoit, Sandrine and Vert, Jean-Philippe},
  year = {2018},
  month = dec,
  volume = {9},
  issn = {2041-1723},
  doi = {10.1038/s41467-017-02554-5},
  file = {/home/trung/GoogleDrive/Zotero/risso et al_2018_a general and flexible method for signal extraction from single-cell rna-seq data.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{ritchie00_DeepAmortizedInference,
  title = {Deep {{Amortized Inference}} for {{Probabilistic Programs}}},
  author = {Ritchie, Daniel and Horsfall, Paul and Goodman, Noah D},
  pages = {31},
  abstract = {Probabilistic programming languages (PPLs) are a powerful modeling tool, able to represent any computable probability distribution. Unfortunately, probabilistic program inference is often intractable, and existing PPLs mostly rely on expensive, approximate sampling-based methods. To alleviate this problem, one could try to learn from past inferences, so that future inferences run faster. This strategy is known as amortized inference; it has recently been applied to Bayesian networks [28, 22] and deep generative models [20, 15, 24]. This paper proposes a system for amortized inference in PPLs. In our system, amortization comes in the form of a parameterized guide program. Guide programs have similar structure to the original program, but can have richer data flow, including neural network components. These networks can be optimized so that the guide approximately samples from the posterior distribution defined by the original program. We present a flexible interface for defining guide programs and a stochastic gradient-based scheme for optimizing guide parameters, as well as some preliminary results on automatically deriving guide programs. We explore in detail the common machine learning pattern in which a `local' model is specified by `global' random values and used to generate independent observed data points; this gives rise to amortized local inference supporting global model learning.},
  file = {/home/trung/GoogleDrive/Zotero/ritchie et al_deep amortized inference for probabilistic programs.pdf},
  language = {en}
}

@techreport{rives19_Biologicalstructurefunction,
  title = {Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences},
  author = {Rives, Alexander and Meier, Joshua and Sercu, Tom and Goyal, Siddharth and Lin, Zeming and Guo, Demi and Ott, Myle and Zitnick, C. Lawrence and Ma, Jerry and Fergus, Rob},
  year = {2019},
  month = apr,
  institution = {{Synthetic Biology}},
  doi = {10.1101/622803},
  abstract = {In the field of artificial intelligence, a combination of scale in data and model capacity enabled by unsupervised learning has led to major advances in representation learning and statistical generation. In the life sciences, the anticipated growth of sequencing promises unprecedented data on natural sequence diversity. Evolutionary-scale language modeling is a logical step toward predictive and generative artificial intelligence for biology. To this end we use unsupervised learning to train a deep contextual language model on 86 billion amino acids across 250 million protein sequences spanning evolutionary diversity. The resulting model contains information about biological properties in its representations. The representations are learned from sequence data alone; no information about the properties of the sequences is given through supervision or domain-specific features. The learned representation space has a multi-scale organization reflecting structure from the level of biochemical properties of amino acids to remote homology of proteins. Information about secondary and tertiary structure is encoded in the representations and can be identified by linear projections. Representation learning produces features that generalize across a range of applications, enabling state-of-the-art supervised prediction of mutational effect and secondary structure, and improving state-of-the-art features for long-range contact prediction.},
  file = {/home/trung/GoogleDrive/Zotero/rives et al_2019_biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences.pdf},
  language = {en},
  type = {Preprint}
}

@article{roberts00_HierarchicalVariationalAutoencoders,
  title = {Hierarchical {{Variational Autoencoders}} for {{Music}}},
  author = {Roberts, Adam and Engel, Jesse and Eck, Douglas},
  pages = {6},
  abstract = {In this work we develop recurrent variational autoencoders (VAEs) trained to reproduce short musical sequences and demonstrate their use as a creative device both via random sampling and data interpolation. Furthermore, by using a novel hierarchical decoder, we show that we are able to model long sequences with musical structure for both individual instruments and a three-piece band (lead, bass, and drums). Finally, we demonstrate the effectiveness of scheduled sampling in significantly improving our reconstruction accuracy.},
  annotation = {ZSCC: 0000034},
  file = {/home/trung/GoogleDrive/Zotero/roberts et al_hierarchical variational autoencoders for music.pdf},
  keywords = {hierarchical,variational},
  language = {en}
}

@article{roberts19_HierarchicalLatentVector,
  title = {A {{Hierarchical Latent Vector Model}} for {{Learning Long}}-{{Term Structure}} in {{Music}}},
  author = {Roberts, Adam and Engel, Jesse and Raffel, Colin and Hawthorne, Curtis and Eck, Douglas},
  year = {2019},
  month = nov,
  abstract = {The Variational Autoencoder (VAE) has proven to be an effective model for producing semantically meaningful latent representations for natural data. However, it has thus far seen limited application to sequential data, and, as we demonstrate, existing recurrent VAE models have difficulty modeling sequences with long-term structure. To address this issue, we propose the use of a hierarchical decoder, which first outputs embeddings for subsequences of the input and then uses these embeddings to generate each subsequence independently. This structure encourages the model to utilize its latent code, thereby avoiding the "posterior collapse" problem, which remains an issue for recurrent VAEs. We apply this architecture to modeling sequences of musical notes and find that it exhibits dramatically better sampling, interpolation, and reconstruction performance than a "flat" baseline model. An implementation of our "MusicVAE" is available online at http://g.co/magenta/musicvae-code.},
  archivePrefix = {arXiv},
  eprint = {1803.05428},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/roberts et al_2019_a hierarchical latent vector model for learning long-term structure in music.pdf},
  journal = {arXiv:1803.05428 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{robinson00_PriorsBayesianNeural,
  title = {Priors for {{Bayesian Neural Networks}}},
  author = {Robinson, Mark},
  pages = {75},
  file = {/home/trung/GoogleDrive/Zotero/robinson_priors for bayesian neural networks.pdf},
  keywords = {prior,variational},
  language = {en}
}

@misc{rodriguez19_NashEquilibriumDeepMind,
  title = {Beyond the {{Nash Equilibrium}}: {{DeepMind}}'s {{Clever Strategy}} to {{Solve Asymmetric Games}}},
  shorttitle = {Beyond the {{Nash Equilibrium}}},
  author = {Rodriguez, Jesus},
  year = {2019},
  month = sep,
  abstract = {DeepMind found an innovative strategy to break an asymmetric game into smaller symmetric games while preserving the Nash equilibrium.},
  file = {/home/trung/Zotero/storage/SUSVQFTA/beyond-the-nash-equilibrium-deepminds-clever-strategy-to-solve-asymmetric-games-d571c998dac4.html},
  howpublished = {https://towardsdatascience.com/beyond-the-nash-equilibrium-deepminds-clever-strategy-to-solve-asymmetric-games-d571c998dac4},
  journal = {Medium},
  language = {en}
}

@article{roeder19_EfficientAmortisedBayesian,
  title = {Efficient {{Amortised Bayesian Inference}} for {{Hierarchical}} and {{Nonlinear Dynamical Systems}}},
  author = {Roeder, Geoffrey and Grant, Paul K. and Phillips, Andrew and Dalchau, Neil and Meeds, Edward},
  year = {2019},
  month = may,
  abstract = {We introduce a flexible, scalable Bayesian inference framework for nonlinear dynamical systems characterised by distinct and hierarchical variability at the individual, group, and population levels. Our model class is a generalisation of nonlinear mixed-effects (NLME) dynamical systems, the statistical workhorse for many experimental sciences. We cast parameter inference as stochastic optimisation of an end-to-end differentiable, block-conditional variational autoencoder. We specify the dynamics of the data-generating process as an ordinary differential equation (ODE) such that both the ODE and its solver are fully differentiable. This model class is highly flexible: the ODE right-hand sides can be a mixture of user-prescribed or "white-box" sub-components and neural network or "black-box" sub-components. Using stochastic optimisation, our amortised inference algorithm could seamlessly scale up to massive data collection pipelines (common in labs with robotic automation). Finally, our framework supports interpretability with respect to the underlying dynamics, as well as predictive generalization to unseen combinations of group components (also called "zero-shot" learning). We empirically validate our method by predicting the dynamic behaviour of bacteria that were genetically engineered to function as biosensors.},
  archivePrefix = {arXiv},
  eprint = {1905.12090},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/roeder et al_2019_efficient amortised bayesian inference for hierarchical and nonlinear dynamical systems.pdf;/home/trung/Zotero/storage/5BSMGEZZ/1905.html},
  journal = {arXiv:1905.12090 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{roeder20_LinearIdentifiabilityLearneda,
  title = {On {{Linear Identifiability}} of {{Learned Representations}}},
  author = {Roeder, Geoffrey and Metz, Luke and Kingma, Diederik P.},
  year = {2020},
  month = jul,
  abstract = {Identifiability is a desirable property of a statistical model: it implies that the true model parameters may be estimated to any desired precision, given sufficient computational resources and data. We study identifiability in the context of representation learning: discovering nonlinear data representations that are optimal with respect to some downstream task. When parameterized as deep neural networks, such representation functions typically lack identifiability in parameter space, because they are overparameterized by design. In this paper, building on recent advances in nonlinear ICA, we aim to rehabilitate identifiability by showing that a large family of discriminative models are in fact identifiable in function space, up to a linear indeterminacy. Many models for representation learning in a wide variety of domains have been identifiable in this sense, including text, images and audio, state-of-the-art at time of publication. We derive sufficient conditions for linear identifiability and provide empirical support for the result on both simulated and real-world data.},
  archivePrefix = {arXiv},
  eprint = {2007.00810},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2020/false},
  journal = {arXiv:2007.00810 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{rohekar00_ConstructingDeepNeural,
  title = {Constructing {{Deep Neural Networks}} by {{Bayesian Network Structure Learning}}},
  author = {Rohekar, Raanan Y and Nisimov, Shami and Gurwicz, Yaniv and Koren, Guy and Novik, Gal},
  pages = {12},
  abstract = {We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus, the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then, instead of directly learning the discriminative structure, it learns a generative graph, constructs its stochastic inverse, and then constructs a discriminative graph. We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) of common networks can be replaced by significantly smaller learned structures, while maintaining classification accuracy\textemdash state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.},
  file = {/home/trung/Zotero/storage/NRSHJXJQ/Rohekar et al. - Constructing Deep Neural Networks by Bayesian Netw.pdf},
  language = {en}
}

@article{rohekar19_ModelingUncertaintyLearning,
  title = {Modeling {{Uncertainty}} by {{Learning}} a {{Hierarchy}} of {{Deep Neural Connections}}},
  author = {Rohekar, Raanan Y. and Gurwicz, Yaniv and Nisimov, Shami and Novik, Gal},
  year = {2019},
  month = oct,
  abstract = {Modeling uncertainty in deep neural networks, despite recent important advances, is still an open problem. Bayesian neural networks are a powerful solution, where the prior over network weights is a design choice, often a normal distribution or other distribution encouraging sparsity. However, this prior is agnostic to the generative process of the input data, which might lead to unwarranted generalization for outof-distribution tested data. We suggest the presence of a confounder for the relation between the input data and the discriminative function given the target label. We propose an approach for modeling this confounder by sharing neural connectivity patterns between the generative and discriminative networks. This approach leads to a new deep architecture, where networks are sampled from the posterior of local causal structures, and coupled into a compact hierarchy. We demonstrate that sampling networks from this hierarchy, proportionally to their posterior, is efficient and enables estimating various types of uncertainties. Empirical evaluations of our method demonstrate significant improvement compared to state-of-the-art calibration and out-of-distribution detection methods.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1905.13195},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rohekar et al_2019_modeling uncertainty by learning a hierarchy of deep neural connections.pdf},
  journal = {arXiv:1905.13195 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{rojas-carulla00_LearningTransferableRepresentations,
  title = {Learning {{Transferable Representations}}},
  author = {{Rojas-Carulla}, Mateo},
  pages = {164},
  file = {/home/trung/GoogleDrive/Zotero/rojas-carulla_learning transferable representations.pdf},
  language = {en}
}

@article{rojas-carulla18_InvariantModelsCausal,
  title = {Invariant {{Models}} for {{Causal Transfer Learning}}},
  author = {{Rojas-Carulla}, Mateo and Sch{\"o}lkopf, Bernhard and Turner, Richard and Peters, Jonas},
  year = {2018},
  month = sep,
  abstract = {Methods of transfer learning try to combine knowledge from several related tasks (or domains) to improve performance on a test task. Inspired by causal methodology, we relax the usual covariate shift assumption and assume that it holds true for a subset of predictor variables: the conditional distribution of the target variable given this subset of predictors is invariant over all tasks. We show how this assumption can be motivated from ideas in the field of causality. We focus on the problem of Domain Generalization, in which no examples from the test task are observed. We prove that in an adversarial setting using this subset for prediction is optimal in Domain Generalization; we further provide examples, in which the tasks are sufficiently diverse and the estimator therefore outperforms pooling the data, even on average. If examples from the test task are available, we also provide a method to transfer knowledge from the training tasks and exploit all available features for prediction. However, we provide no guarantees for this method. We introduce a practical method which allows for automatic inference of the above subset and provide corresponding code. We present results on synthetic data sets and a gene deletion data set.},
  archivePrefix = {arXiv},
  eprint = {1507.05333},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rojas-carulla et al_2018_invariant models for causal transfer learning.pdf},
  journal = {arXiv:1507.05333 [stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {stat}
}

@article{rolfe17_DiscreteVariationalAutoencoders,
  title = {Discrete {{Variational Autoencoders}}},
  author = {Rolfe, Jason Tyler},
  year = {2017},
  month = apr,
  abstract = {Probabilistic models with discrete latent variables naturally capture datasets composed of discrete classes. However, they are difficult to train efficiently, since backpropagation through discrete variables is generally not possible. We present a novel method to train a class of probabilistic models with discrete latent variables using the variational autoencoder framework, including backpropagation through the discrete latent variables. The associated class of probabilistic models comprises an undirected discrete component and a directed hierarchical continuous component. The discrete component captures the distribution over the disconnected smooth manifolds induced by the continuous component. As a result, this class of models efficiently learns both the class of objects in an image, and their specific realization in pixels, from unsupervised data, and outperforms state-of-the-art methods on the permutation-invariant MNIST, Omniglot, and Caltech-101 Silhouettes datasets.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1609.02200},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rolfe_2017_discrete variational autoencoders.pdf;/home/trung/Zotero/storage/FDHLA7JM/1609.html},
  journal = {arXiv:1609.02200 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{rolinek19_VariationalAutoencodersPursue,
  title = {Variational {{Autoencoders Pursue PCA Directions}} (by {{Accident}})},
  author = {Rolinek, Michal and Zietlow, Dominik and Martius, Georg},
  year = {2019},
  month = apr,
  abstract = {The Variational Autoencoder (VAE) is a powerful architecture capable of representation learning and generative modeling. When it comes to learning interpretable (disentangled) representations, VAE and its variants show unparalleled performance. However, the reasons for this are unclear, since a very particular alignment of the latent embedding is needed but the design of the VAE does not encourage it in any explicit way. We address this matter and offer the following explanation: the diagonal approximation in the encoder together with the inherent stochasticity force local orthogonality of the decoder. The local behavior of promoting both reconstruction and orthogonality matches closely how the PCA embedding is chosen. Alongside providing an intuitive understanding, we justify the statement with full theoretical analysis as well as with experiments.},
  annotation = {ZSCC: 0000007},
  archivePrefix = {arXiv},
  eprint = {1812.06775},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rolinek et al_2019_variational autoencoders pursue pca directions (by accident).pdf;/home/trung/Zotero/storage/KTL8TK3G/1812.html},
  journal = {arXiv:1812.06775 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,disentanglement,explain,favorite,pca,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{rolnick19_IdentifyingWeightsArchitectures,
  title = {Identifying {{Weights}} and {{Architectures}} of {{Unknown ReLU Networks}}},
  author = {Rolnick, David and Kording, Konrad P.},
  year = {2019},
  month = oct,
  abstract = {The output of a neural network depends on its parameters in a highly nonlinear way, and it is widely assumed that a network's parameters cannot be identified from its outputs. Here, we show that in many cases it is possible to reconstruct the architecture, weights, and biases of a deep ReLU network given the ability to query the network. ReLU networks are piecewise linear and the boundaries between pieces correspond to inputs for which one of the ReLUs switches between inactive and active states. Thus, first-layer ReLUs can be identified (up to sign and scaling) based on the orientation of their associated hyperplanes. Later-layer ReLU boundaries bend when they cross earlier-layer boundaries and the extent of bending reveals the weights between them. Our algorithm uses this to identify the units in the network and weights connecting them (up to isomorphism). The fact that considerable parts of deep networks can be identified from their outputs has implications for security, neuroscience, and our understanding of neural networks.},
  archivePrefix = {arXiv},
  eprint = {1910.00744},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rolnick et al_2019_identifying weights and architectures of unknown relu networks.pdf;/home/trung/Zotero/storage/CS25RK58/1910.html},
  journal = {arXiv:1910.00744 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{rolnick19_TacklingClimateChange,
  title = {Tackling {{Climate Change}} with {{Machine Learning}}},
  author = {Rolnick, David and Donti, Priya L. and Kaack, Lynn H. and Kochanski, Kelly and Lacoste, Alexandre and Sankaran, Kris and Ross, Andrew Slavin and {Milojevic-Dupont}, Nikola and Jaques, Natasha and {Waldman-Brown}, Anna and Luccioni, Alexandra and Maharaj, Tegan and Sherwin, Evan D. and Mukkavilli, S. Karthik and Kording, Konrad P. and Gomes, Carla and Ng, Andrew Y. and Hassabis, Demis and Platt, John C. and Creutzig, Felix and Chayes, Jennifer and Bengio, Yoshua},
  year = {2019},
  month = jun,
  abstract = {Climate change is one of the greatest challenges facing humanity, and we, as machine learning experts, may wonder how we can help. Here we describe how machine learning can be a powerful tool in reducing greenhouse gas emissions and helping society adapt to a changing climate. From smart grids to disaster management, we identify high impact problems where existing gaps can be filled by machine learning, in collaboration with other fields. Our recommendations encompass exciting research questions as well as promising business opportunities. We call on the machine learning community to join the global effort against climate change.},
  archivePrefix = {arXiv},
  eprint = {1906.05433},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rolnick et al_2019_tackling climate change with machine learning.pdf;/home/trung/GoogleDrive/Zotero/rolnick et al_2019_tackling climate change with machine learning2.pdf;/home/trung/Zotero/storage/DJ2XMSVR/1906.html;/home/trung/Zotero/storage/XQNK4C4I/1906.html},
  journal = {arXiv:1906.05433 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{rombach20_MakingSenseCNNs,
  title = {Making {{Sense}} of {{CNNs}}: {{Interpreting Deep Representations}} \& {{Their Invariances}} with {{INNs}}},
  shorttitle = {Making {{Sense}} of {{CNNs}}},
  author = {Rombach, Robin and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2020},
  month = aug,
  abstract = {To tackle increasingly complex tasks, it has become an essential ability of neural networks to learn abstract representations. These task-specific representations and, particularly, the invariances they capture turn neural networks into black box models that lack interpretability. To open such a black box, it is, therefore, crucial to uncover the different semantic concepts a model has learned as well as those that it has learned to be invariant to. We present an approach based on INNs that (i) recovers the task-specific, learned invariances by disentangling the remaining factor of variation in the data and that (ii) invertibly transforms these recovered invariances combined with the model representation into an equally expressive one with accessible semantic concepts. As a consequence, neural network representations become understandable by providing the means to (i) expose their semantic meaning, (ii) semantically modify a representation, and (iii) visualize individual learned semantic concepts and invariances. Our invertible approach significantly extends the abilities to understand black box models by enabling post-hoc interpretations of state-of-the-art networks without compromising their performance. Our implementation is available at https://compvis.github.io/invariances/ .},
  archivePrefix = {arXiv},
  eprint = {2008.01777},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rombach et al_2020_making sense of cnns.pdf},
  journal = {arXiv:2008.01777 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{rombach20_NetworkFusionContent,
  title = {Network {{Fusion}} for {{Content Creation}} with {{Conditional INNs}}},
  author = {Rombach, Robin and Esser, Patrick and Ommer, Bj{\"o}rn},
  year = {2020},
  month = may,
  abstract = {Artificial Intelligence for Content Creation has the potential to reduce the amount of manual content creation work significantly. While automation of laborious work is welcome, it is only useful if it allows users to control aspects of the creative process when desired. Furthermore, widespread adoption of semi-automatic content creation depends on low barriers regarding the expertise, computational budget and time required to obtain results and experiment with new techniques. With state-of-the-art approaches relying on task-specific models, multi-GPU setups and weeks of training time, we must find ways to reuse and recombine them to meet these requirements. Instead of designing and training methods for controllable content creation from scratch, we thus present a method to repurpose powerful, existing models for new tasks, even though they have never been designed for them. We formulate this problem as a translation between expert models, which includes common content creation scenarios, such as text-to-image and image-to-image translation, as a special case. As this translation is ambiguous, we learn a generative model of hidden representations of one expert conditioned on hidden representations of the other expert. Working on the level of hidden representations makes optimal use of the computational effort that went into the training of the expert model to produce these efficient, low-dimensional representations. Experiments demonstrate that our approach can translate from BERT, a state-of-the-art expert for text, to BigGAN, a state-of-the-art expert for images, to enable text-to-image generation, which neither of the experts can perform on its own. Additional experiments show the wide applicability of our approach across different conditional image synthesis tasks and improvements over existing methods for image modifications.},
  archivePrefix = {arXiv},
  eprint = {2005.13580},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rombach et al_2020_network fusion for content creation with conditional inns.pdf},
  journal = {arXiv:2005.13580 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{rosca19_DistributionMatchingVariational,
  title = {Distribution {{Matching}} in {{Variational Inference}}},
  author = {Rosca, Mihaela and Lakshminarayanan, Balaji and Mohamed, Shakir},
  year = {2019},
  month = jun,
  abstract = {With the increasingly widespread deployment of generative models, there is a mounting need for a deeper understanding of their behaviors and limitations. In this paper, we expose the limitations of Variational Autoencoders (VAEs), which consistently fail to learn marginal distributions in both latent and visible spaces. We show this to be a consequence of learning by matching conditional distributions, and the limitations of explicit model and posterior distributions. It is popular to consider Generative Adversarial Networks (GANs) as a means of overcoming these limitations, leading to hybrids of VAEs and GANs. We perform a large-scale evaluation of several VAE-GAN hybrids and analyze the implications of class probability estimation for learning distributions. While promising, we conclude that at present, VAE-GAN hybrids have limited applicability: they are harder to scale, evaluate, and use for inference compared to VAEs; and they do not improve over the generation quality of GANs.},
  archivePrefix = {arXiv},
  eprint = {1802.06847},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rosca et al_2019_distribution matching in variational inference.pdf},
  journal = {arXiv:1802.06847 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{roscher20_ExplainableMachineLearning,
  title = {Explainable {{Machine Learning}} for {{Scientific Insights}} and {{Discoveries}}},
  author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
  year = {2020},
  volume = {8},
  pages = {42200--42216},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2976199},
  abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.},
  file = {/home/trung/GoogleDrive/Zotero/roscher et al_2020_explainable machine learning for scientific insights and discoveries.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{roscher20_ExplainableMachineLearninga,
  title = {Explainable {{Machine Learning}} for {{Scientific Insights}} and {{Discoveries}}},
  author = {Roscher, Ribana and Bohn, Bastian and Duarte, Marco F. and Garcke, Jochen},
  year = {2020},
  volume = {8},
  pages = {42200--42216},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2976199},
  abstract = {Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.},
  file = {/home/trung/GoogleDrive/Zotero/roscher et al_2020_explainable machine learning for scientific insights and discoveries2.pdf},
  journal = {IEEE Access},
  language = {en}
}

@article{ross00_BayesianApproachLearning,
  title = {A {{Bayesian Approach}} for {{Learning}} and {{Planning}} in {{Partially Observable Markov Decision Processes}}},
  author = {Ross, Stephane and Pineau, Joelle and {Chaib-draa}, Brahim and Kreitmann, Pierre},
  pages = {45},
  abstract = {Bayesian learning methods have recently been shown to provide an elegant solution to the exploration-exploitation trade-off in reinforcement learning. However most investigations of Bayesian reinforcement learning to date focus on the standard Markov Decision Processes (MDPs). The primary focus of this paper is to extend these ideas to the case of partially observable domains, by introducing the Bayes-Adaptive Partially Observable Markov Decision Processes. This new framework can be used to simultaneously (1) learn a model of the POMDP domain through interaction with the environment, (2) track the state of the system under partial observability, and (3) plan (near-)optimal sequences of actions. An important contribution of this paper is to provide theoretical results showing how the model can be finitely approximated while preserving good learning performance. We present approximate algorithms for belief tracking and planning in this model, as well as empirical results that illustrate how the model estimate and agent's return improve as a function of experience.},
  file = {/home/trung/GoogleDrive/Zotero/ross et al_a bayesian approach for learning and planning in partially observable markov decision processes.pdf},
  keywords = {favorite},
  language = {en}
}

@article{ross14_MutualInformationDiscrete,
  title = {Mutual {{Information}} between {{Discrete}} and {{Continuous Data Sets}}},
  author = {Ross, Brian C.},
  year = {2014},
  month = feb,
  volume = {9},
  pages = {e87357},
  publisher = {{Public Library of Science}},
  doi = {10.1371/journal.pone.0087357},
  abstract = {Mutual information (MI) is a powerful method for detecting relationships between data sets. There are accurate methods for estimating MI that avoid problems with ``binning'' when both data sets are discrete or when both data sets are continuous. We present an accurate, non-binning MI estimator for the case of one discrete data set and one continuous data set. This case applies when measuring, for example, the relationship between base sequence and gene expression level, or the effect of a cancer drug on patient survival time. We also show how our method can be adapted to calculate the Jensen\textendash Shannon divergence of two or more data sets.},
  file = {/home/trung/GoogleDrive/Zotero/ross_2014_mutual information between discrete and continuous data sets.PDF},
  journal = {PLOS ONE},
  keywords = {information},
  number = {2}
}

@article{rouvier00_Reviewdifferentrobust,
  title = {Review of Different Robust X-Vector Extractors for Speaker Verification},
  author = {Rouvier, Mickael and Dufour, Richard and Bousquet, Pierre-Michel},
  pages = {5},
  abstract = {Recently, the x-vector framework, extracted with deep neural network architectures, became the state-of-theart method for speaker verification. Although another level of performance has been overcome with this approach, fine-tuning and optimizing the hyper-parameters of a deep neural network to obtain a robust x-vector extractor is cost- and time-consuming. Several approaches have been proposed to train robust x-vector extractors. In this paper, we propose to review and analyse the impact of the most significant x-vector related approaches, including variations in terms of data augmentation, number of epochs, size of mini-batch, acoustic features and frames per iteration. By applying these approaches to the default recipe provided in the Kaldi toolkit, we observed a significant relative gain of more than 50\% in terms of EER on Speaker in the Wild and Voxceleb1-E datasets.},
  file = {/home/trung/GoogleDrive/Zotero/rouvier et al_review of different robust x-vector extractors for speaker veriﬁcation.pdf},
  language = {en}
}

@misc{rowan20_PrimerGenerativeVoice,
  title = {A {{Primer}} for {{Generative Voice Models}}},
  author = {Rowan, Ian},
  year = {2020},
  month = jul,
  abstract = {Audio generation is now possible with Machine Learning. Learn the basics to get started.},
  howpublished = {https://towardsdatascience.com/a-primer-for-generative-voice-models-b41bec0003d2},
  journal = {Medium},
  language = {en}
}

@article{roy18_TheoryExperimentsVector,
  title = {Theory and {{Experiments}} on {{Vector Quantized Autoencoders}}},
  author = {Roy, Aurko and Vaswani, Ashish and Neelakantan, Arvind and Parmar, Niki},
  year = {2018},
  month = jul,
  abstract = {Deep neural networks with discrete latent variables offer the promise of better symbolic reasoning, and learning abstractions that are more useful to new tasks. There has been a surge in interest in discrete latent variable models, however, despite several recent improvements, the training of discrete latent variable models has remained challenging and their performance has mostly failed to match their continuous counterparts. Recent work on vector quantized autoencoders (VQVAE) has made substantial progress in this direction, with its perplexity almost matching that of a VAE on datasets such as CIFAR-10. In this work, we investigate an alternate training technique for VQ-VAE, inspired by its connection to the Expectation Maximization (EM) algorithm. Training the discrete bottleneck with EM helps us achieve better image generation results on CIFAR-10, and together with knowledge distillation, allows us to develop a non-autoregressive machine translation model whose accuracy almost matches a strong greedy autoregressive baseline Transformer, while being 3.3 times faster at inference.},
  annotation = {ZSCC: 0000015},
  archivePrefix = {arXiv},
  eprint = {1805.11063},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/roy et al_2018_theory and experiments on vector quantized autoencoders.pdf},
  journal = {arXiv:1805.11063 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{rozemberczki20_benedekrozemberczkiawesomedecisiontreepapers,
  title = {Benedekrozemberczki/Awesome-Decision-Tree-Papers},
  author = {Rozemberczki, Benedek},
  year = {2020},
  month = feb,
  abstract = {A collection of research papers on decision, classification and regression trees with implementations.},
  annotation = {ZSCC: NoCitationData[s0]},
  copyright = {CC0-1.0}
}

@article{rubanova19_LatentODEsIrregularlySampled,
  title = {Latent {{ODEs}} for {{Irregularly}}-{{Sampled Time Series}}},
  author = {Rubanova, Yulia and Chen, Ricky T. Q. and Duvenaud, David},
  year = {2019},
  month = jul,
  abstract = {Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.},
  archivePrefix = {arXiv},
  eprint = {1907.03907},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rubanova et al_2019_latent odes for irregularly-sampled time series.pdf;/home/trung/Zotero/storage/Y8LCM4SH/1907.html},
  journal = {arXiv:1907.03907 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{ruder16_Artisticstyletransfer,
  title = {Artistic Style Transfer for Videos},
  author = {Ruder, Manuel and Dosovitskiy, Alexey and Brox, Thomas},
  year = {2016},
  volume = {9796},
  pages = {26--36},
  doi = {10.1007/978-3-319-45886-1_3},
  abstract = {In the past, manually re-drawing an image in a certain artistic style required a professional artist and a long time. Doing this for a video sequence single-handed was beyond imagination. Nowadays computers provide new possibilities. We present an approach that transfers the style from one image (for example, a painting) to a whole video sequence. We make use of recent advances in style transfer in still images and propose new initializations and loss functions applicable to videos. This allows us to generate consistent and stable stylized video sequences, even in cases with large motion and strong occlusion. We show that the proposed method clearly outperforms simpler baselines both qualitatively and quantitatively.},
  archivePrefix = {arXiv},
  eprint = {1604.08610},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ruder et al_2016_artistic style transfer for videos.pdf},
  journal = {arXiv:1604.08610 [cs]},
  primaryClass = {cs}
}

@misc{ruder20_sebastianruderNLPprogress,
  title = {Sebastianruder/{{NLP}}-Progress},
  author = {Ruder, Sebastian},
  year = {2020},
  month = nov,
  abstract = {Repository to track the progress in Natural Language Processing (NLP), including the datasets and the current state-of-the-art for the most common NLP tasks.},
  copyright = {MIT License         ,                 MIT License}
}

@article{rudolph19_StructuringAutoencoders,
  title = {Structuring {{Autoencoders}}},
  author = {Rudolph, Marco and Wandt, Bastian and Rosenhahn, Bodo},
  year = {2019},
  month = aug,
  abstract = {In this paper we propose Structuring AutoEncoders (SAE). SAEs are neural networks which learn a low dimensional representation of data which are additionally enriched with a desired structure in this low dimensional space. While traditional Autoencoders have proven to structure data naturally they fail to discover semantic structure that is hard to recognize in the raw data. The SAE solves the problem by enhancing a traditional Autoencoder using weak supervision to form a structured latent space. In the experiments we demonstrate, that the structured latent space allows for a much more efficient data representation for further tasks such as classification for sparsely labeled data, an efficient choice of data to label, and morphing between classes. To demonstrate the general applicability of our method, we show experiments on the benchmark image datasets MNIST, Fashion-MNIST, DeepFashion2 and on a dataset of 3D human shapes.},
  archivePrefix = {arXiv},
  eprint = {1908.02626},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rudolph et al_2019_structuring autoencoders.pdf},
  journal = {arXiv:1908.02626 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ruff20_UnifyingReviewDeep,
  title = {A {{Unifying Review}} of {{Deep}} and {{Shallow Anomaly Detection}}},
  author = {Ruff, Lukas and Kauffmann, Jacob R. and Vandermeulen, Robert A. and Montavon, Gr{\'e}goire and Samek, Wojciech and Kloft, Marius and Dietterich, Thomas G. and M{\"u}ller, Klaus-Robert},
  year = {2020},
  month = sep,
  abstract = {Deep learning approaches to anomaly detection have recently improved the state of the art in detection performance on complex datasets such as large collections of images or text. These results have sparked a renewed interest in the anomaly detection problem and led to the introduction of a great variety of new methods. With the emergence of numerous such methods, including approaches based on generative models, one-class classification, and reconstruction, there is a growing need to bring methods of this field into a systematic and unified perspective. In this review we aim to identify the common underlying principles as well as the assumptions that are often made implicitly by various methods. In particular, we draw connections between classic 'shallow' and novel deep approaches and show how this relation might cross-fertilize or extend both directions. We further provide an empirical assessment of major existing methods that is enriched by the use of recent explainability techniques, and present specific worked-through examples together with practical advice. Finally, we outline critical open challenges and identify specific paths for future research in anomaly detection.},
  archivePrefix = {arXiv},
  eprint = {2009.11732},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ruff et al_2020_a unifying review of deep and shallow anomaly detection.pdf},
  journal = {arXiv:2009.11732 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ruiz19_ContrastiveDivergenceCombining,
  title = {A {{Contrastive Divergence}} for {{Combining Variational Inference}} and {{MCMC}}},
  author = {Ruiz, Francisco J. R. and Titsias, Michalis K.},
  year = {2019},
  month = may,
  abstract = {We develop a method to combine Markov chain Monte Carlo (MCMC) and variational inference (VI), leveraging the advantages of both inference approaches. Specifically, we improve the variational distribution by running a few MCMC steps. To make inference tractable, we introduce the variational contrastive divergence (VCD), a new divergence that replaces the standard Kullback-Leibler (KL) divergence used in VI. The VCD captures a notion of discrepancy between the initial variational distribution and its improved version (obtained after running the MCMC steps), and it converges asymptotically to the symmetrized KL divergence between the variational distribution and the posterior of interest. The VCD objective can be optimized efficiently with respect to the variational parameters via stochastic optimization. We show experimentally that optimizing the VCD leads to better predictive performance on two latent variable models: logistic matrix factorization and variational autoencoders (VAEs).},
  archivePrefix = {arXiv},
  eprint = {1905.04062},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ruiz et al_2019_a contrastive divergence for combining variational inference and mcmc.pdf;/home/trung/Zotero/storage/6ZXG85AC/1905.html},
  journal = {arXiv:1905.04062 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{ruiz19_LearningDisentangledRepresentations,
  title = {Learning {{Disentangled Representations}} with {{Reference}}-{{Based Variational Autoencoders}}},
  author = {Ruiz, Adria and Martinez, Oriol and Binefa, Xavier and Verbeek, Jakob},
  year = {2019},
  month = jan,
  abstract = {Learning disentangled representations from visual data, where different high-level generative factors are independently encoded, is of importance for many computer vision tasks. Solving this problem, however, typically requires to explicitly label all the factors of interest in training images. To alleviate the annotation cost, we introduce a learning setting which we refer to as "reference-based disentangling". Given a pool of unlabeled images, the goal is to learn a representation where a set of target factors are disentangled from others. The only supervision comes from an auxiliary "reference set" containing images where the factors of interest are constant. In order to address this problem, we propose reference-based variational autoencoders, a novel deep generative model designed to exploit the weak-supervision provided by the reference set. By addressing tasks such as feature learning, conditional image generation or attribute transfer, we validate the ability of the proposed model to learn disentangled representations from this minimal form of supervision.},
  archivePrefix = {arXiv},
  eprint = {1901.08534},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ruiz et al_2019_learning disentangled representations with reference-based variational autoencoders.pdf},
  journal = {arXiv:1901.08534 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{rumetshofer19_HUMANLEVELPROTEINLOCALIZATION,
  title = {{{HUMAN}}-{{LEVEL PROTEIN LOCALIZATION WITH CON}}- {{VOLUTIONAL NEURAL NETWORKS}}},
  author = {Rumetshofer, Elisabeth and Hofmarcher, Markus and R{\"o}hrl, Clemens and Hochreiter, Sepp and Klambauer, G{\"u}nter},
  year = {2019},
  pages = {18},
  file = {/home/trung/Zotero/storage/GW6WFPUU/Rumetshofer et al. - 2019 - HUMAN-LEVEL PROTEIN LOCALIZATION WITH CON- VOLUTIO.pdf},
  language = {en}
}

@article{ryabinin20_EmbeddingWordsNonVector,
  title = {Embedding {{Words}} in {{Non}}-{{Vector Space}} with {{Unsupervised Graph Learning}}},
  author = {Ryabinin, Max and Popov, Sergei and Prokhorenkova, Liudmila and Voita, Elena},
  year = {2020},
  month = oct,
  abstract = {It has become a de-facto standard to represent words as elements of a vector space (word2vec, GloVe). While this approach is convenient, it is unnatural for language: words form a graph with a latent hierarchical structure, and this structure has to be revealed and encoded by word embeddings. We introduce GraphGlove: unsupervised graph word representations which are learned end-to-end. In our setting, each word is a node in a weighted graph and the distance between words is the shortest path distance between the corresponding nodes. We adopt a recent method learning a representation of data in the form of a differentiable weighted graph and use it to modify the GloVe training algorithm. We show that our graph-based representations substantially outperform vector-based methods on word similarity and analogy tasks. Our analysis reveals that the structure of the learned graphs is hierarchical and similar to that of WordNet, the geometry is highly non-trivial and contains subgraphs with different local topology.},
  archivePrefix = {arXiv},
  eprint = {2010.02598},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ryabinin et al_2020_embedding words in non-vector space with unsupervised graph learning.pdf},
  journal = {arXiv:2010.02598 [cs]},
  primaryClass = {cs}
}

@article{rybkin20_SimpleEffectiveVAE,
  title = {Simple and {{Effective VAE Training}} with {{Calibrated Decoders}}},
  author = {Rybkin, Oleh and Daniilidis, Kostas and Levine, Sergey},
  year = {2020},
  month = aug,
  abstract = {Variational autoencoders (VAEs) provide an effective and simple method for modeling complex distributions. However, training VAEs often requires considerable hyperparameter tuning, and often utilizes a heuristic weight on the prior KL-divergence term. In this work, we study how the performance of VAEs can be improved while not requiring the use of this heuristic hyperparameter, by learning calibrated decoders that accurately model the decoding distribution. While in some sense it may seem obvious that calibrated decoders should perform better than uncalibrated decoders, much of the recent literature that employs VAEs uses uncalibrated Gaussian decoders with constant variance. We observe empirically that the na\textbackslash "\{i\}ve way of learning variance in Gaussian decoders does not lead to good results. However, other calibrated decoders, such as discrete decoders or learning shared variance can substantially improve performance. To further improve results, we propose a simple but novel modification to the commonly used Gaussian decoder, which represents the prediction variance non-parametrically. We observe empirically that using the heuristic weight hyperparameter is not necessary with our method. We analyze the performance of various discrete and continuous decoders on a range of datasets and several single-image and sequential VAE models. Project website: https://orybkin.github.io/sigma-vae/},
  archivePrefix = {arXiv},
  eprint = {2006.13202},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/rybkin et al_2020_simple and effective vae training with calibrated decoders.pdf},
  journal = {arXiv:2006.13202 [cs, eess, stat]},
  keywords = {vae_issues},
  language = {en},
  primaryClass = {cs, eess, stat}
}

@article{ryzhikov19_Normalizingflowsdeep,
  title = {Normalizing Flows for Deep Anomaly Detection},
  author = {Ryzhikov, Artem and Borisyak, Maxim and Ustyuzhanin, Andrey and Derkach, Denis},
  year = {2019},
  month = dec,
  abstract = {Anomaly detection for complex data is a challenging task from the perspective of machine learning. In this work, weconsider cases with missing certain kinds of anomalies in the training dataset, while significant statistics for the normal class isavailable. For such scenarios, conventional supervised methods might suffer from the class imbalance, while unsupervised methodstend to ignore difficult anomalous examples. We extend the idea of the supervised classification approach for class-imbalanceddatasets by exploiting normalizing flows for proper Bayesian inference of the posterior probabilities.},
  archivePrefix = {arXiv},
  eprint = {1912.09323},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ryzhikov et al_2019_normalizing flows for deep anomaly detection.pdf;/home/trung/GoogleDrive/Zotero/ryzhikov et al_2019_normalizing flows for deep anomaly detection2.pdf},
  journal = {arXiv:1912.09323 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{saatchi17_BayesianGAN,
  title = {Bayesian {{GAN}}},
  author = {Saatchi, Yunus and Wilson, Andrew Gordon},
  year = {2017},
  month = nov,
  abstract = {Generative adversarial networks (GANs) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood. We present a practical Bayesian formulation for unsupervised and semi-supervised learning with GANs. Within this framework, we use stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of the generator and discriminator networks. The resulting approach is straightforward and obtains good performance without any standard interventions such as feature matching or mini-batch discrimination. By exploring an expressive posterior over the parameters of the generator, the Bayesian GAN avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN, Wasserstein GANs, and DCGAN ensembles.},
  archivePrefix = {arXiv},
  eprint = {1705.09558},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/BWUL24SM/Saatchi and Wilson - 2017 - Bayesian GAN.pdf},
  journal = {arXiv:1705.09558 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{sabour17_DynamicRoutingCapsules,
  title = {Dynamic {{Routing Between Capsules}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Sabour, Sara and Frosst, Nicholas and Hinton, Geoffrey E},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {3856--3866},
  publisher = {{Curran Associates, Inc.}},
  file = {/home/trung/GoogleDrive/Zotero/sabour et al_2017_dynamic routing between capsules.pdf;/home/trung/Zotero/storage/STDKSGJM/6975-dynamic-routing-between-capsules.html}
}

@article{sadeghi19_PixelVAEImprovedPixelVAE,
  title = {{{PixelVAE}}++: {{Improved PixelVAE}} with {{Discrete Prior}}},
  shorttitle = {{{PixelVAE}}++},
  author = {Sadeghi, Hossein and Andriyash, Evgeny and Vinci, Walter and Buffoni, Lorenzo and Amin, Mohammad H.},
  year = {2019},
  month = aug,
  abstract = {Constructing powerful generative models for natural images is a challenging task. PixelCNN models capture details and local information in images very well but have limited receptive field. Variational autoencoders with a factorial decoder can capture global information easily, but they often fail to reconstruct details faithfully. PixelVAE combines the best features of the two models and constructs a generative model that is able to learn local and global structures. Here we introduce PixelVAE++, a VAE with three types of latent variables and a PixelCNN++ for the decoder. We introduce a novel architecture that reuses a part of the decoder as an encoder. We achieve the state of the art performance on binary data sets such as MNIST and Omniglot and achieve the state of the art performance on CIFAR-10 among latent variable models while keeping the latent variables informative.},
  archivePrefix = {arXiv},
  eprint = {1908.09948},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/false;/home/trung/Zotero/storage/Q5JDS9X8/1908.html},
  journal = {arXiv:1908.09948 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,discrete,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{saeed20_ContrastiveLearningGeneralPurpose,
  title = {Contrastive {{Learning}} of {{General}}-{{Purpose Audio Representations}}},
  author = {Saeed, Aaqib and Grangier, David and Zeghidour, Neil},
  year = {2020},
  month = oct,
  abstract = {We introduce COLA, a self-supervised pre-training approach for learning a general-purpose representation of audio. Our approach is based on contrastive learning: it learns a representation which assigns high similarity to audio segments extracted from the same recording while assigning lower similarity to segments from different recordings. We build on top of recent advances in contrastive learning for computer vision and reinforcement learning to design a lightweight, easy-to-implement self-supervised model of audio. We pre-train embeddings on the large-scale Audioset database and transfer these representations to 9 diverse classification tasks, including speech, music, animal sounds, and acoustic scenes. We show that despite its simplicity, our method significantly outperforms previous self-supervised systems. We furthermore conduct ablation studies to identify key design choices and release a library to pre-train and fine-tune COLA models.},
  archivePrefix = {arXiv},
  eprint = {2010.10915},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/saeed et al_2020_contrastive learning of general-purpose audio representations.pdf},
  journal = {arXiv:2010.10915 [cs, eess]},
  primaryClass = {cs, eess}
}

@techreport{saelens18_comparisonsinglecelltrajectory,
  title = {A Comparison of Single-Cell Trajectory Inference Methods: Towards More Accurate and Robust Tools},
  shorttitle = {A Comparison of Single-Cell Trajectory Inference Methods},
  author = {Saelens, Wouter and Cannoodt, Robrecht and Todorov, Helena and Saeys, Yvan},
  year = {2018},
  month = mar,
  institution = {{Bioinformatics}},
  doi = {10.1101/276907},
  abstract = {Using single-cell -omics data, it is now possible to computationally order cells along trajectories, allowing the unbiased study of cellular dynamic processes. Since 2014, more than 50 trajectory inference methods have been developed, each with its own set of methodological characteristics. As a result, choosing a method to infer trajectories is often challenging, since a comprehensive assessment of the performance and robustness of each method is still lacking. In order to facilitate the comparison of the results of these methods to each other and to a gold standard, we developed a global framework to benchmark trajectory inference tools. Using this framework, we compared the trajectories from a total of 29 trajectory inference methods, on a large collection of real and synthetic datasets. We evaluate methods using several metrics, including accuracy of the inferred ordering, correctness of the network topology, code quality and user friendliness. We found that some methods, including Slingshot, TSCAN and Monocle DDRTree, clearly outperform other methods, although their performance depended on the type of trajectory present in the data. Based on our benchmarking results, we therefore developed a set of guidelines for method users. However, our analysis also indicated that there is still a lot of room for improvement, especially for methods detecting complex trajectory topologies. Our evaluation pipeline can therefore be used to spearhead the development of new scalable and more accurate methods, and is available at github.com/dynverse/dynverse.},
  file = {/home/trung/GoogleDrive/Zotero/saelens et al_2018_a comparison of single-cell trajectory inference methods.pdf},
  language = {en},
  type = {Preprint}
}

@article{saemundsson19_VariationalIntegratorNetworks,
  title = {Variational {{Integrator Networks}} for {{Physically Meaningful Embeddings}}},
  author = {Saemundsson, Steindor and Terenin, Alexander and Hofmann, Katja and Deisenroth, Marc Peter},
  year = {2019},
  month = oct,
  abstract = {Learning workable representations of dynamical systems is becoming an increasingly important problem in a number of application areas. By leveraging recent work connecting deep neural networks to systems of differential equations, we propose variational integrator networks, a class of neural network architectures designed to ensure faithful representations of the dynamics under study. This class of network architectures facilitates accurate long-term prediction, interpretability, and data-efficient learning, while still remaining highly flexible and capable of modeling complex behavior. We demonstrate that they can accurately learn dynamical systems from both noisy observations in phase space and from image pixels within which the unknown dynamics are embedded.},
  archivePrefix = {arXiv},
  eprint = {1910.09349},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/44C73PIM/Saemundsson et al. - 2019 - Variational Integrator Networks for Physically Mea.pdf},
  journal = {arXiv:1910.09349 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{sagar19_StateoftheArtsProspectiveNeural,
  title = {A {{State}}-of-the-{{Arts}} and {{Prospective}} in {{Neural Style Transfer}}},
  booktitle = {2019 6th {{International Conference}} on {{Signal Processing}} and {{Integrated Networks}} ({{SPIN}})},
  author = {Sagar and Vishwakarma, Dinesh Kumar},
  year = {2019},
  month = mar,
  pages = {244--247},
  issn = {null},
  doi = {10.1109/SPIN.2019.8711612},
  abstract = {Artistic style transfer is one the enormous research fields in computer vision. It is a technique to synthesize a content image and the style together to form an stylized image. Various popular applications like Prisma, Deepart, Pikazoapp provide a platform to synthesis images in order to produce very exciting artistic style. These applications use Convolutional Neural Networks to transform the images in an artistic style. Prisma photo editor has more than 250 modern art filters and 110 million users. Hence, the amount of images being synthesized is huge and computation requires to perform the task efficiently is large. Abundant CNN architectures like VGG16, VGG19, GoogleNet etc are introduced to solve this problem of heavy computations. The system helps users to transform their images into artworks using style of famous artists for stirring image filters, and it might be useful for artists to have an indication about the art. This paper presents the fundamental concepts, classification of Style Transfer and major progress towards Neural Style Transfer.}
}

@article{saha19_HumanComprehensionFairness,
  title = {Human {{Comprehension}} of {{Fairness}} in {{Machine Learning}}},
  author = {Saha, Debjani and Schumann, Candice and McElfresh, Duncan C. and Dickerson, John P. and Mazurek, Michelle L. and Tschantz, Michael Carl},
  year = {2019},
  month = dec,
  abstract = {Bias in machine learning has manifested injustice in several areas, such as medicine, hiring, and criminal justice. In response, computer scientists have developed myriad definitions of fairness to correct this bias in fielded algorithms. While some definitions are based on established legal and ethical norms, others are largely mathematical. It is unclear whether the general public agrees with these fairness definitions, and perhaps more importantly, whether they understand these definitions. We take initial steps toward bridging this gap between ML researchers and the public, by addressing the question: does a non-technical audience understand a basic definition of ML fairness? We develop a metric to measure comprehension of one such definition--demographic parity. We validate this metric using online surveys, and study the relationship between comprehension and sentiment, demographics, and the application at hand.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2001.00089},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/saha et al_2019_human comprehension of fairness in machine learning.pdf;/home/trung/Zotero/storage/LGA2YXTC/2001.html},
  journal = {arXiv:2001.00089 [cs]},
  primaryClass = {cs}
}

@article{saha20_LearningJointArticulatoryAcoustic,
  title = {Learning {{Joint Articulatory}}-{{Acoustic Representations}} with {{Normalizing Flows}}},
  author = {Saha, Pramit and Fels, Sidney},
  year = {2020},
  month = may,
  abstract = {The articulatory geometric configurations of the vocal tract and the acoustic properties of the resultant speech sound are considered to have a strong causal relationship. This paper aims at finding a joint latent representation between the articulatory and acoustic domain for vowel sounds via invertible neural network models, while simultaneously preserving the respective domain-specific features. Our model utilizes a convolutional autoencoder architecture and normalizing flow-based models to allow both forward and inverse mappings in a semi-supervised manner, between the mid-sagittal vocal tract geometry of a two degrees-of-freedom articulatory synthesizer with 1D acoustic wave model and the Mel-spectrogram representation of the synthesized speech sounds. Our approach achieves satisfactory performance in achieving both articulatory-to-acoustic as well as acoustic-to-articulatory mapping, thereby demonstrating our success in achieving a joint encoding of both the domains.},
  archivePrefix = {arXiv},
  eprint = {2005.09463},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/saha et al_2020_learning joint articulatory-acoustic representations with normalizing flows.pdf},
  journal = {arXiv:2005.09463 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{sahoo20_UnsupervisedDomainAdaptation,
  title = {Unsupervised {{Domain Adaptation}} in the {{Absence}} of {{Source Data}}},
  author = {Sahoo, Roshni and Shanmugam, Divya and Guttag, John},
  year = {2020},
  month = jul,
  abstract = {Current unsupervised domain adaptation methods can address many types of distribution shift, but they assume data from the source domain is freely available. As the use of pre-trained models becomes more prevalent, it is reasonable to assume that source data is unavailable. We propose an unsupervised method for adapting a source classifier to a target domain that varies from the source domain along natural axes, such as brightness and contrast. Our method only requires access to unlabeled target instances and the source classifier. We validate our method in scenarios where the distribution shift involves brightness, contrast, and rotation and show that it outperforms fine-tuning baselines in scenarios with limited labeled data.},
  archivePrefix = {arXiv},
  eprint = {2007.10233},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sahoo et al_2020_unsupervised domain adaptation in the absence of source data.pdf},
  journal = {arXiv:2007.10233 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{sainburg20_ParametricUMAPlearning,
  title = {Parametric {{UMAP}}: Learning Embeddings with Deep Neural Networks for Representation and Semi-Supervised Learning},
  shorttitle = {Parametric {{UMAP}}},
  author = {Sainburg, Tim and McInnes, Leland and Gentner, Timothy Q.},
  year = {2020},
  month = sep,
  abstract = {We propose Parametric UMAP, a parametric variation of the UMAP (Uniform Manifold Approximation and Projection) algorithm. UMAP is a non-parametric graph-based dimensionality reduction algorithm using applied Riemannian geometry and algebraic topology to find low-dimensional embeddings of structured data. The UMAP algorithm consists of two steps: (1) Compute a graphical representation of a dataset (fuzzy simplicial complex), and (2) Through stochastic gradient descent, optimize a low-dimensional embedding of the graph. Here, we replace the second step of UMAP with a deep neural network that learns a parametric relationship between data and embedding. We demonstrate that our method performs similarly to its non-parametric counterpart while conferring the benefit of a learned parametric mapping (e.g. fast online embeddings for new data). We then show that UMAP loss can be extended to arbitrary deep learning applications, for example constraining the latent distribution of autoencoders, and improving classifier accuracy for semi-supervised learning by capturing structure in unlabeled data. Our code is available at https://github.com/timsainb/ParametricUMAP\_paper.},
  archivePrefix = {arXiv},
  eprint = {2009.12981},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sainburg et al_2020_parametric umap.pdf},
  journal = {arXiv:2009.12981 [cs, q-bio, stat]},
  primaryClass = {cs, q-bio, stat}
}

@article{salakhutdinov00_ReplicatedSoftmaxUndirected,
  title = {Replicated {{Softmax}}: An {{Undirected Topic Model}}},
  author = {Salakhutdinov, Ruslan and Hinton, Geoffrey},
  pages = {8},
  abstract = {We introduce a two-layer undirected graphical model, called a ``Replicated Softmax'', that can be used to model and automatically extract low-dimensional latent semantic representations from a large unstructured collection of documents. We present efficient learning and inference algorithms for this model, and show how a Monte-Carlo based method, Annealed Importance Sampling, can be used to produce an accurate estimate of the log-probability the model assigns to test data. This allows us to demonstrate that the proposed model is able to generalize much better compared to Latent Dirichlet Allocation in terms of both the log-probability of held-out documents and the retrieval accuracy.},
  file = {/home/trung/GoogleDrive/Zotero/salakhutdinov et al_replicated softmax.pdf},
  language = {en}
}

@article{salimans00_ImprovedTechniquesTraining,
  title = {Improved {{Techniques}} for {{Training GANs}}},
  author = {Salimans, Tim and Goodfellow, Ian and Zaremba, Wojciech and Cheung, Vicki and Radford, Alec and Chen, Xi and Chen, Xi},
  pages = {9},
  abstract = {We present a variety of new architectural features and training procedures that we apply to the generative adversarial networks (GANs) framework. Using our new techniques, we achieve state-of-the-art results in semi-supervised classification on MNIST, CIFAR-10 and SVHN. The generated images are of high quality as confirmed by a visual Turing test: our model generates MNIST samples that humans cannot distinguish from real data, and CIFAR-10 samples that yield a human error rate of 21.3\%. We also present ImageNet samples with unprecedented resolution and show that our methods enable the model to learn recognizable features of ImageNet classes.},
  file = {/home/trung/GoogleDrive/Zotero/salimans et al_improved techniques for training gans.pdf},
  language = {en}
}

@article{salimans16_WeightNormalizationSimple,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  author = {Salimans, Tim and Kingma, Diederik P.},
  year = {2016},
  month = feb,
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  archivePrefix = {arXiv},
  eprint = {1602.07868},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/salimans et al_2016_weight normalization.pdf;/home/trung/Zotero/storage/25P8RDLW/1602.html},
  journal = {arXiv:1602.07868 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,normalization},
  primaryClass = {cs}
}

@article{salimans17_PixelCNNImprovingPixelCNN,
  title = {{{PixelCNN}}++: {{Improving}} the {{PixelCNN}} with {{Discretized Logistic Mixture Likelihood}} and {{Other Modifications}}},
  shorttitle = {{{PixelCNN}}++},
  author = {Salimans, Tim and Karpathy, Andrej and Chen, Xi and Kingma, Diederik P.},
  year = {2017},
  month = jan,
  abstract = {PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.},
  archivePrefix = {arXiv},
  eprint = {1701.05517},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/salimans et al_2017_pixelcnn++.pdf},
  journal = {arXiv:1701.05517 [cs, stat]},
  primaryClass = {cs, stat}
}

@book{salvaris18_DeepLearningAzure,
  title = {Deep {{Learning}} with {{Azure}}: {{Building}} and {{Deploying Artificial Intelligence Solutions}} on the {{Microsoft AI Platform}}},
  shorttitle = {Deep {{Learning}} with {{Azure}}},
  author = {Salvaris, Mathew and Dean, Danielle and Tok, Wee Hyong},
  year = {2018},
  publisher = {{Apress}},
  address = {{Berkeley, CA}},
  doi = {10.1007/978-1-4842-3679-6},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/GoogleDrive/Zotero/salvaris et al_2018_deep learning with azure.pdf},
  isbn = {978-1-4842-3678-9 978-1-4842-3679-6},
  language = {en}
}

@article{samek00_TutorialInterpretableMachine,
  title = {Tutorial on {{Interpretable}}   {{Machine Learning}}},
  author = {Samek, Wojciech and Binder, Alexander},
  pages = {239},
  file = {/home/trung/GoogleDrive/Zotero/samek et al_tutorial on interpretable machine learning.pdf},
  language = {en}
}

@article{sanchez-gonzalez19_HamiltonianGraphNetworks,
  title = {Hamiltonian {{Graph Networks}} with {{ODE Integrators}}},
  author = {{Sanchez-Gonzalez}, Alvaro and Bapst, Victor and Cranmer, Kyle and Battaglia, Peter},
  year = {2019},
  month = sep,
  abstract = {We introduce an approach for imposing physically informed inductive biases in learned simulation models. We combine graph networks with a differentiable ordinary differential equation integrator as a mechanism for predicting future states, and a Hamiltonian as an internal representation. We find that our approach outperforms baselines without these biases in terms of predictive accuracy, energy accuracy, and zero-shot generalization to time-step sizes and integrator orders not experienced during training. This advances the state-of-the-art of learned simulation, and in principle is applicable beyond physical domains.},
  archivePrefix = {arXiv},
  eprint = {1909.12790},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sanchez-gonzalez et al_2019_hamiltonian graph networks with ode integrators.pdf;/home/trung/Zotero/storage/36IZDLYQ/1909.html},
  journal = {arXiv:1909.12790 [physics]},
  keywords = {Computer Science - Machine Learning,graph,ode,Physics - Computational Physics},
  primaryClass = {physics}
}

@article{sanchez-lengeling19_MachineLearningScent,
  title = {Machine {{Learning}} for {{Scent}}: {{Learning Generalizable Perceptual Representations}} of {{Small Molecules}}},
  shorttitle = {Machine {{Learning}} for {{Scent}}},
  author = {{Sanchez-Lengeling}, Benjamin and Wei, Jennifer N. and Lee, Brian K. and Gerkin, Richard C. and {Aspuru-Guzik}, Al{\'a}n and Wiltschko, Alexander B.},
  year = {2019},
  month = oct,
  abstract = {Predicting the relationship between a molecule's structure and its odor remains a difficult, decades-old task. This problem, termed quantitative structure-odor relationship (QSOR) modeling, is an important challenge in chemistry, impacting human nutrition, manufacture of synthetic fragrance, the environment, and sensory neuroscience. We propose the use of graph neural networks for QSOR, and show they significantly out-perform prior methods on a novel data set labeled by olfactory experts. Additional analysis shows that the learned embeddings from graph neural networks capture a meaningful odor space representation of the underlying relationship between structure and odor, as demonstrated by strong performance on two challenging transfer learning tasks. Machine learning has already had a large impact on the senses of sight and sound. Based on these early results with graph neural networks for molecular properties, we hope machine learning can eventually do for olfaction what it has already done for vision and hearing.},
  archivePrefix = {arXiv},
  eprint = {1910.10685},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sanchez-lengeling et al_2019_machine learning for scent.pdf;/home/trung/Zotero/storage/N675ETZA/1910.html},
  journal = {arXiv:1910.10685 [physics, stat]},
  keywords = {Computer Science - Machine Learning,graph,Physics - Chemical Physics,Statistics - Machine Learning},
  primaryClass = {physics, stat}
}

@article{sanh19_DistilBERTdistilledversion,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  shorttitle = {{{DistilBERT}}, a Distilled Version of {{BERT}}},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2019},
  month = oct,
  abstract = {As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40\%, while retaining 97\% of its language understanding capabilities and being 60\% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.},
  archivePrefix = {arXiv},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sanh et al_2019_distilbert, a distilled version of bert.pdf;/home/trung/Zotero/storage/TZRZQKCB/1910.html},
  journal = {arXiv:1910.01108 [cs]},
  keywords = {bert,Computer Science - Computation and Language,distil,transfer},
  primaryClass = {cs}
}

@article{santos16_AttentivePoolingNetworks,
  title = {Attentive {{Pooling Networks}}},
  author = {dos Santos, Cicero and Tan, Ming and Xiang, Bing and Zhou, Bowen},
  year = {2016},
  month = feb,
  abstract = {In this work, we propose Attentive Pooling (AP), a two-way attention mechanism for discriminative model training. In the context of pair-wise ranking or classification with neural networks, AP enables the pooling layer to be aware of the current input pair, in a way that information from the two input items can directly influence the computation of each other's representations. Along with such representations of the paired inputs, AP jointly learns a similarity measure over projected segments (e.g. trigrams) of the pair, and subsequently, derives the corresponding attention vector for each input to guide the pooling. Our two-way attention mechanism is a general framework independent of the underlying representation learning, and it has been applied to both convolutional neural networks (CNNs) and recurrent neural networks (RNNs) in our studies. The empirical results, from three very different benchmark tasks of question answering/answer selection, demonstrate that our proposed models outperform a variety of strong baselines and achieve state-of-the-art performance in all the benchmarks.},
  annotation = {ZSCC: 0000189},
  archivePrefix = {arXiv},
  eprint = {1602.03609},
  eprinttype = {arxiv},
  journal = {arXiv:1602.03609 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{santurkar18_HowDoesBatch,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Ma, Aleksander},
  year = {2018},
  pages = {11},
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called ``internal covariate shift''. In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  file = {/home/trung/GoogleDrive/Zotero/santurkar et al_2018_how does batch normalization help optimization.pdf},
  keywords = {_tablet,favorite},
  language = {en}
}

@article{sanyal19_NeuralNetworkTraining,
  title = {Neural {{Network Training}} with {{Approximate Logarithmic Computations}}},
  author = {Sanyal, Arnab and Beerel, Peter A. and Chugg, Keith M.},
  year = {2019},
  month = oct,
  abstract = {The high computational complexity associated with training deep neural networks limits online and real-time training on edge devices. This paper proposed an end-to-end training and inference scheme that eliminates multiplications by approximate operations in the log-domain which has the potential to significantly reduce implementation complexity. We implement the entire training procedure in the log-domain, with fixed-point data representations. This training procedure is inspired by hardware-friendly approximations of logdomain addition which are based on look-up tables and bitshifts. We show that our 16-bit log-based training can achieve classification accuracy within approximately 1\% of the equivalent floating-point baselines for a number of commonly used datasets.},
  archivePrefix = {arXiv},
  eprint = {1910.09876},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sanyal et al_2019_neural network training with approximate logarithmic computations.pdf},
  journal = {arXiv:1910.09876 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sarawgi20_WhyhaveUnified,
  title = {Why Have a {{Unified Predictive Uncertainty}}? {{Disentangling}} It Using {{Deep Split Ensembles}}},
  shorttitle = {Why Have a {{Unified Predictive Uncertainty}}?},
  author = {Sarawgi, Utkarsh and Zulfikar, Wazeer and Khincha, Rishab and Maes, Pattie},
  year = {2020},
  month = sep,
  abstract = {Understanding and quantifying uncertainty in black box Neural Networks (NNs) is critical when deployed in real-world settings such as healthcare. Recent works using Bayesian and non-Bayesian methods have shown how a unified predictive uncertainty can be modelled for NNs. Decomposing this uncertainty to disentangle the granular sources of heteroscedasticity in data provides rich information about its underlying causes. We propose a conceptually simple non-Bayesian approach, deep split ensemble, to disentangle the predictive uncertainties using a multivariate Gaussian mixture model. The NNs are trained with clusters of input features, for uncertainty estimates per cluster. We evaluate our approach on a series of benchmark regression datasets, while also comparing with unified uncertainty methods. Extensive analyses using dataset shits and empirical rule highlight our inherently well-calibrated models. Our work further demonstrates its applicability in a multi-modal setting using a benchmark Alzheimer's dataset and also shows how deep split ensembles can highlight hidden modality-specific biases. The minimal changes required to NNs and the training procedure, and the high flexibility to group features into clusters makes it readily deployable and useful. The source code is available at https://github.com/wazeerzulfikar/deep-split-ensembles},
  archivePrefix = {arXiv},
  eprint = {2009.12406},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sarawgi et al_2020_why have a unified predictive uncertainty.pdf},
  journal = {arXiv:2009.12406 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{sarvari19_UnsupervisedBoostingbasedAutoencoder,
  title = {Unsupervised {{Boosting}}-Based {{Autoencoder Ensembles}} for {{Outlier Detection}}},
  author = {Sarvari, Hamed and Domeniconi, Carlotta and Prenkaj, Bardh and Stilo, Giovanni},
  year = {2019},
  month = oct,
  abstract = {Autoencoders, as a dimensionality reduction technique, have been recently applied to outlier detection. However, neural networks are known to be vulnerable to overfitting, and therefore have limited potential in the unsupervised outlier detection setting. Current approaches to ensemble-based autoencoders do not generate a sufficient level of diversity to avoid the overfitting issue. To overcome the aforementioned limitations we develop a Boosting-based Autoencoder Ensemble approach (in short, BAE). BAE is an unsupervised ensemble method that, similarly to the boosting approach, builds an adaptive cascade of autoencoders to achieve improved and robust results. BAE trains the autoencoder components sequentially by performing a weighted sampling of the data, aimed at reducing the amount of outliers used during training, and at injecting diversity in the ensemble. We perform extensive experiments and show that the proposed methodology outperforms state-of-the-art approaches under a variety of conditions.},
  archivePrefix = {arXiv},
  eprint = {1910.09754},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sarvari et al_2019_unsupervised boosting-based autoencoder ensembles for outlier detection.pdf;/home/trung/Zotero/storage/RGPFVTDS/1910.html},
  journal = {arXiv:1910.09754 [cs, stat]},
  keywords = {Computer Science - Machine Learning,ensemble,outlier detection,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sasaki19_Robustcontrastivelearning,
  title = {Robust Contrastive Learning and Nonlinear {{ICA}} in the Presence of Outliers},
  author = {Sasaki, Hiroaki and Takenouchi, Takashi and Monti, Ricardo and Hyv{\"a}rinen, Aapo},
  year = {2019},
  month = nov,
  abstract = {Nonlinear independent component analysis (ICA) is a general framework for unsupervised representation learning, and aimed at recovering the latent variables in data. Recent practical methods perform nonlinear ICA by solving a series of classification problems based on logistic regression. However, it is well-known that logistic regression is vulnerable to outliers, and thus the performance can be strongly weakened by outliers. In this paper, we first theoretically analyze nonlinear ICA models in the presence of outliers. Our analysis implies that estimation in nonlinear ICA can be seriously hampered when outliers exist on the tails of the (noncontaminated) target density, which happens in a typical case of contamination by outliers. We develop two robust nonlinear ICA methods based on the \{\textbackslash gamma\}-divergence, which is a robust alternative to the KL-divergence in logistic regression. The proposed methods are shown to have desired robustness properties in the context of nonlinear ICA. We also experimentally demonstrate that the proposed methods are very robust and outperform existing methods in the presence of outliers. Finally, the proposed method is applied to ICA-based causal discovery and shown to find a plausible causal relationship on fMRI data.},
  archivePrefix = {arXiv},
  eprint = {1911.00265},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sasaki et al_2019_robust contrastive learning and nonlinear ica in the presence of outliers.pdf},
  journal = {arXiv:1911.00265 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{sato20_RandomFeaturesStrengthen,
  title = {Random {{Features Strengthen Graph Neural Networks}}},
  author = {Sato, Ryoma and Yamada, Makoto and Kashima, Hisashi},
  year = {2020},
  month = mar,
  abstract = {Graph neural networks (GNNs) are powerful machine learning models for various graph learning tasks. Recently, the limitations of the expressive power of various GNN models have been revealed. For example, GNNs cannot distinguish some non-isomorphic graphs (Xu et al., 2019) and they cannot learn efficient graph algorithms (Sato et al., 2019), and several GNN models have been proposed to overcome these limitations. In this paper, we demonstrate that GNNs become powerful just by adding a random feature to each node. We prove that the random features enable GNNs to learn almost optimal polynomialtime approximation algorithms for the minimum dominating set problem and maximum matching problem in terms of the approximation ratio. The main advantage of our method is that it can be combined with off-the-shelf GNN models with slight modifications. Through experiments, we show that the addition of random features enables GNNs to solve various problems that normal GNNs, including GCNs and GINs, cannot solve.},
  archivePrefix = {arXiv},
  eprint = {2002.03155},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sato et al_2020_random features strengthen graph neural networks.pdf},
  journal = {arXiv:2002.03155 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{satpathy19_Massivelyparallelsinglecell,
  title = {Massively Parallel Single-Cell Chromatin Landscapes of Human Immune Cell Development and Intratumoral {{T}} Cell Exhaustion},
  author = {Satpathy, Ansuman T. and Granja, Jeffrey M. and Yost, Kathryn E. and Qi, Yanyan and Meschi, Francesca and McDermott, Geoffrey P. and Olsen, Brett N. and Mumbach, Maxwell R. and Pierce, Sarah E. and Corces, M. Ryan and Shah, Preyas and Bell, Jason C. and Jhutty, Darisha and Nemec, Corey M. and Wang, Jean and Wang, Li and Yin, Yifeng and Giresi, Paul G. and Chang, Anne Lynn S. and Zheng, Grace X. Y. and Greenleaf, William J. and Chang, Howard Y.},
  year = {2019},
  month = aug,
  volume = {37},
  pages = {925--936},
  publisher = {{Nature Publishing Group}},
  issn = {1546-1696},
  doi = {10.1038/s41587-019-0206-z},
  abstract = {A droplet-based single-cell ATAC-seq method enables rapid mapping of chromatin accessibility in tens of thousands of cells.},
  annotation = {ZSCC: 0000040},
  copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
  file = {/home/trung/GoogleDrive/Zotero/satpathy et al_2019_massively parallel single-cell chromatin landscapes of human immune cell development and intratumoral t cell exhaustion.pdf;/home/trung/Zotero/storage/Q3ETDNVZ/s41587-019-0206-z.html},
  journal = {Nature Biotechnology},
  language = {en},
  number = {8}
}

@article{sattigeri00_ProbabilisticMixtureModelAgnostic,
  title = {Probabilistic {{Mixture}} of {{Model}}-{{Agnostic Meta}}-{{Learners}}},
  author = {Sattigeri, Prasanna and Ghosh, Soumya and Kumar, Abhishek and Ramamurthy, Karthikeyan Natesan and Hoffman, Samuel},
  pages = {6},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/HVLPS2NH/Sattigeri et al. - Probabilistic Mixture of Model-Agnostic Meta-Learn.pdf},
  language = {en}
}

@article{savage19_NovelistCormacMcCarthy,
  title = {Novelist {{Cormac McCarthy}}'s Tips on How to Write a Great Science Paper},
  author = {Savage, Van and Yeh, Pamela},
  year = {2019},
  month = sep,
  volume = {574},
  pages = {441--442},
  doi = {10.1038/d41586-019-02918-5},
  abstract = {The Pulitzer prizewinner shares his advice for pleasing readers, editors and yourself.},
  copyright = {2019 Nature},
  file = {/home/trung/Zotero/storage/VQ98QBFD/d41586-019-02918-5.html},
  journal = {Nature},
  language = {en}
}

@article{saxe19_informationbottlenecktheory,
  title = {On the Information Bottleneck Theory of Deep Learning},
  author = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  year = {2019},
  month = dec,
  volume = {2019},
  pages = {124020},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ab3985},
  abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
  annotation = {ZSCC: 0000106},
  file = {/home/trung/GoogleDrive/Zotero/saxe et al_2019_on the information bottleneck theory of deep learning.pdf},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  keywords = {information},
  language = {en},
  number = {12}
}

@article{saxe19_informationbottlenecktheorya,
  title = {On the Information Bottleneck Theory of Deep Learning},
  author = {Saxe, Andrew M and Bansal, Yamini and Dapello, Joel and Advani, Madhu and Kolchinsky, Artemy and Tracey, Brendan D and Cox, David D},
  year = {2019},
  month = dec,
  volume = {2019},
  pages = {124020},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ab3985},
  abstract = {The practical successes of deep neural networks have not been matched by theoretical progress that satisfyingly explains their behavior. In this work, we study the information bottleneck (IB) theory of deep learning, which makes three specific claims: first, that deep networks undergo two distinct phases consisting of an initial fitting phase and a subsequent compression phase; second, that the compression phase is causally related to the excellent generalization performance of deep networks; and third, that the compression phase occurs due to the diffusion-like behavior of stochastic gradient descent. Here we show that none of these claims hold true in the general case. Through a combination of analytical results and simulation, we demonstrate that the information plane trajectory is predominantly a function of the neural nonlinearity employed: double-sided saturating nonlinearities like tanh yield a compression phase as neural activations enter the saturation regime, but linear activation functions and single-sided saturating nonlinearities like the widely used ReLU in fact do not. Moreover, we find that there is no evident causal connection between compression and generalization: networks that do not compress are still capable of generalization, and vice versa. Next, we show that the compression phase, when it exists, does not arise from stochasticity in training by demonstrating that we can replicate the IB findings using full batch gradient descent rather than stochastic gradient descent. Finally, we show that when an input domain consists of a subset of task-relevant and task-irrelevant information, hidden representations do compress the task-irrelevant information, although the overall information about the input may monotonically increase with training time, and that this compression happens concurrently with the fitting process rather than during a subsequent compression period.},
  file = {/home/trung/Zotero/storage/BCCBMMWL/Saxe et al. - 2019 - On the information bottleneck theory of deep learn.pdf},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  keywords = {information},
  language = {en},
  number = {12}
}

@article{saxena00_GenerativeAdversarialNetworks,
  title = {Generative {{Adversarial Networks}} ({{GANs}}): {{Challenges}}, {{Solutions}}, and {{Future Directions}}},
  author = {Saxena, Divya and Cao, Jiannong},
  pages = {41},
  abstract = {Generative Adversarial Networks (GANs) is a novel class of deep generative models which has recently gained significant attention. GANs learns complex and high-dimensional distributions implicitly over images, audio, and data. However, there exists major challenges in training of GANs, i.e., mode collapse, nonconvergence and instability, due to inappropriate design of network architecture, use of objective function and selection of optimization algorithm. Recently, to address these challenges, several solutions for better design and optimization of GANs have been investigated based on techniques of re-engineered network architectures, new objective functions and alternative optimization algorithms. To the best of our knowledge, there is no existing survey that has particularly focused on broad and systematic developments of these solutions. In this study, we perform a comprehensive survey of the advancements in GANs design and optimization solutions proposed to handle GANs challenges. We first identify key research issues within each design and optimization technique and then propose a new taxonomy to structure solutions by key research issues. In accordance with the taxonomy, we provide a detailed discussion on different GANs variants proposed within each solution and their relationships. Finally, based on the insights gained, we present the promising research directions in this rapidly growing field.},
  file = {/home/trung/GoogleDrive/Zotero/saxena et al_generative adversarial networks (gans).pdf},
  language = {en}
}

@article{schaarschmidt00_Endtoenddeepreinforcement,
  title = {End-to-End Deep Reinforcement Learning in Computer Systems},
  author = {Schaarschmidt, Michael},
  pages = {166},
  abstract = {The growing complexity of data processing systems has long led systems designers to imagine systems (e.g. databases, schedulers) which can self-configure and adapt based on environmental cues. In this context, reinforcement learning (RL) methods have since their inception appealed to systems developers. They promise to acquire complex decision policies from raw feedback signals. Despite their conceptual popularity, RL methods are scarcely found in real-world data processing systems. Recently, RL has seen explosive growth in interest due to high profile successes when utilising large neural networks (deep reinforcement learning). Newly emerging machine learning frameworks and powerful hardware accelerators have given rise to a plethora of new potential applications.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/NBE2XUVP/Schaarschmidt - End-to-end deep reinforcement learning in computer.pdf},
  language = {en}
}

@article{schick20_ItNotJust,
  title = {It's {{Not Just Size That Matters}}: {{Small Language Models Are Also Few}}-{{Shot Learners}}},
  shorttitle = {It's {{Not Just Size That Matters}}},
  author = {Schick, Timo and Sch{\"u}tze, Hinrich},
  year = {2020},
  month = sep,
  abstract = {When scaled to hundreds of billions of parameters, pretrained language models such as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance on challenging natural language understanding benchmarks. In this work, we show that performance similar to GPT-3 can be obtained with language models whose parameter count is several orders of magnitude smaller. This is achieved by converting textual inputs into cloze questions that contain some form of task description, combined with gradient-based optimization; additionally exploiting unlabeled data gives further improvements. Based on our findings, we identify several key factors required for successful natural language understanding with small language models.},
  archivePrefix = {arXiv},
  eprint = {2009.07118},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schick et al_2020_it's not just size that matters.pdf},
  journal = {arXiv:2009.07118 [cs]},
  primaryClass = {cs}
}

@article{schlag19_EnhancingTransformerExplicit,
  title = {Enhancing the {{Transformer}} with {{Explicit Relational Encoding}} for {{Math Problem Solving}}},
  author = {Schlag, Imanol and Smolensky, Paul and Fernandez, Roland and Jojic, Nebojsa and Schmidhuber, J{\"u}rgen and Gao, Jianfeng},
  year = {2019},
  month = oct,
  abstract = {We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.},
  archivePrefix = {arXiv},
  eprint = {1910.06611},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schlag et al_2019_enhancing the transformer with explicit relational encoding for math problem solving.pdf;/home/trung/GoogleDrive/Zotero/schlag et al_2019_enhancing the transformer with explicit relational encoding for math problem solving2.pdf;/home/trung/Zotero/storage/QTBLE86I/1910.html},
  journal = {arXiv:1910.06611 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,graph,reasoning,relational,Statistics - Machine Learning,transformer},
  primaryClass = {cs, stat}
}

@article{schlag20_LearningAssociativeInference,
  title = {Learning {{Associative Inference Using Fast Weight Memory}}},
  author = {Schlag, Imanol and Munkhdalai, Tsendsuren and Schmidhuber, J{\"u}rgen},
  year = {2020},
  month = nov,
  abstract = {Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.},
  archivePrefix = {arXiv},
  eprint = {2011.07831},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schlag et al_2020_learning associative inference using fast weight memory.pdf},
  journal = {arXiv:2011.07831 [cs]},
  primaryClass = {cs}
}

@misc{schmidhuber00_ActiveExplorationArtificial,
  title = {Active {{Exploration}}, {{Artificial Curiosity}} \& {{What}}'s {{Interesting}}},
  author = {Schmidhuber, J{\"u}rgen}
}

@misc{schmidhuber00_ComputableUniversesAlgorithmic,
  title = {Computable {{Universes}} \& {{Algorithmic Theory}} of {{Everything}}: {{The Computational Multiverse}}},
  author = {Schmidhuber, J{\"u}rgen}
}

@misc{schmidhuber00_TheoryUniversalLearning,
  title = {Theory of {{Universal Learning Machines}} \& {{Universal AI}}},
  author = {Schmidhuber, J{\"u}rgen}
}

@article{schmidhuber04_Turingwarwork,
  title = {Turing's War Work Counts for More than Computers},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2004},
  month = jun,
  volume = {429},
  pages = {501--501},
  issn = {1476-4687},
  doi = {10.1038/429501c},
  file = {/home/trung/GoogleDrive/Zotero/schmidhuber_2004_turing's war work counts for more than computers.pdf},
  journal = {Nature},
  number = {6991}
}

@misc{schmidhuber10_FormalTheoryCreativity,
  title = {Formal {{Theory}} of {{Creativity}} \& {{Fun}} \& {{Intrinsic Motivation}} (1990-2010)},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2010}
}

@article{schmidhuber12_TuringKeephis,
  title = {Turing: {{Keep}} His Work in Perspective},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2012},
  month = mar,
  volume = {483},
  pages = {541--541},
  issn = {1476-4687},
  doi = {10.1038/483541b},
  file = {/home/trung/GoogleDrive/Zotero/schmidhuber_2012_turing.pdf},
  journal = {Nature},
  number = {7391}
}

@article{schmidhuber15_Deeplearningneural,
  title = {Deep Learning in Neural Networks: {{An}} Overview},
  author = {Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = jan,
  volume = {61},
  pages = {85--117},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2014.09.003},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarizes relevant work, much of it from the previous millennium. Shallow and Deep Learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  file = {/home/trung/GoogleDrive/Zotero/schmidhuber_2015_deep learning in neural networks.pdf},
  journal = {Neural Networks},
  keywords = {Deep learning,Evolutionary computation,favorite,Reinforcement learning,Supervised learning,Unsupervised learning}
}

@article{schmidhuber20_GenerativeAdversarialNetworks,
  title = {Generative {{Adversarial Networks}} Are {{Special Cases}} of {{Artificial Curiosity}} (1990) and Also {{Closely Related}} to {{Predictability Minimization}} (1991)},
  author = {Schmidhuber, Juergen},
  year = {2020},
  month = apr,
  abstract = {I review unsupervised or self-supervised neural networks playing minimax games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based on two such networks. One network learns to generate a probability distribution over outputs, the other learns to predict effects of the outputs. Each network minimizes the objective function maximized by the other. (ii) Generative Adversarial Networks (GANs, 2010-2014) are an application of AC where the effect of an output is 1 if the output is in a given set, and 0 otherwise. (iii) Predictability Minimization (PM, 1990s) models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components. I correct a previously published claim that PM is not based on a minimax game.},
  archivePrefix = {arXiv},
  eprint = {1906.04493},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schmidhuber_2020_generative adversarial networks are special cases of artificial curiosity (1990) and also closely related to predictability minimization (1991).pdf},
  journal = {arXiv:1906.04493 [cs]},
  primaryClass = {cs}
}

@article{schmidt19_GeneralizationGenerationcloser,
  title = {Generalization in {{Generation}}: {{A}} Closer Look at {{Exposure Bias}}},
  shorttitle = {Generalization in {{Generation}}},
  author = {Schmidt, Florian},
  year = {2019},
  month = nov,
  abstract = {Exposure bias refers to the train-test discrepancy that seemingly arises when an autoregressive generative model uses only ground-truth contexts at training time but generated ones at test time. We separate the contributions of the model and the learning framework to clarify the debate on consequences and review proposed counter-measures.},
  archivePrefix = {arXiv},
  eprint = {1910.00292},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/VHE8Z4UQ/Schmidt - 2019 - Generalization in Generation A closer look at Exp.pdf},
  journal = {arXiv:1910.00292 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{schmidt19_RecurrentNeuralNetworks,
  title = {Recurrent {{Neural Networks}} ({{RNNs}}): {{A}} Gentle {{Introduction}} and {{Overview}}},
  shorttitle = {Recurrent {{Neural Networks}} ({{RNNs}})},
  author = {Schmidt, Robin M.},
  year = {2019},
  month = nov,
  abstract = {State-of-the-art solutions in the areas of "Language Modelling \& Generating Text", "Speech Recognition", "Generating Image Descriptions" or "Video Tagging" have been using Recurrent Neural Networks as the foundation for their approaches. Understanding the underlying concepts is therefore of tremendous importance if we want to keep up with recent or upcoming publications in those areas. In this work we give a short overview over some of the most important concepts in the realm of Recurrent Neural Networks which enables readers to easily understand the fundamentals such as but not limited to "Backpropagation through Time" or "Long Short-Term Memory Units" as well as some of the more recent advances like the "Attention Mechanism" or "Pointer Networks". We also give recommendations for further reading regarding more complex topics where it is necessary.},
  archivePrefix = {arXiv},
  eprint = {1912.05911},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schmidt_2019_recurrent neural networks (rnns).pdf;/home/trung/Zotero/storage/UEQJMLXM/1912.html},
  journal = {arXiv:1912.05911 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{schmidt20_DescendingCrowdedValley,
  title = {Descending through a {{Crowded Valley}} -- {{Benchmarking Deep Learning Optimizers}}},
  author = {Schmidt, Robin M. and Schneider, Frank and Hennig, Philipp},
  year = {2020},
  month = oct,
  abstract = {Choosing the optimizer is considered to be among the most crucial design decisions in deep learning, and it is not an easy one. The growing literature now lists hundreds of optimization methods. In the absence of clear theoretical guidance and conclusive empirical evidence, the decision is often made based on anecdotes. In this work, we aim to replace these anecdotes, if not with a conclusive ranking, then at least with evidence-backed heuristics. To do so, we perform an extensive, standardized benchmark of more than a dozen particularly popular deep learning optimizers while giving a concise overview of the wide range of possible choices. Analyzing almost 35,000 individual runs, we contribute the following three points: (i) Optimizer performance varies greatly across tasks. (ii) We observe that evaluating multiple optimizers with default parameters works approximately as well as tuning the hyperparameters of a single, fixed optimizer. (iii) While we can not discern an optimization method clearly dominating across all tested tasks, we identify a significantly reduced subset of specific algorithms and parameter choices that generally lead to competitive results in our experiments. This subset includes popular favorites and some lesser-known contenders. We have open-sourced all our experimental results, making them directly available as challenging and well-tuned baselines. This allows for more meaningful comparisons when evaluating novel optimization methods without requiring any further computational efforts.},
  archivePrefix = {arXiv},
  eprint = {2007.01547},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schmidt et al_2020_descending through a crowded valley -- benchmarking deep learning optimizers.pdf},
  journal = {arXiv:2007.01547 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{schneider19_wav2vecUnsupervisedPreTraining,
  title = {Wav2vec: {{Unsupervised Pre}}-{{Training}} for {{Speech Recognition}}},
  shorttitle = {Wav2vec},
  booktitle = {Interspeech 2019},
  author = {Schneider, Steffen and Baevski, Alexei and Collobert, Ronan and Auli, Michael},
  year = {2019},
  month = sep,
  pages = {3465--3469},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1873},
  file = {/home/trung/GoogleDrive/Zotero/schneider et al_2019_wav2vec.pdf},
  keywords = {embedding},
  language = {en}
}

@article{schoffl00_CLIMBERSCOMPENSATIONTRAINING,
  title = {{{CLIMBERS}}' {{COMPENSATION TRAINING WITH A MEDICAL FOUNDATION}}},
  author = {Sch{\"o}ffl, Volker and Matros, Patrick},
  pages = {49},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/schöffl et al_climbers’ compensation training with a medical foundation.pdf},
  language = {en}
}

@article{scholkopf00_CausalAnticausalLearning,
  title = {On {{Causal}} and {{Anticausal Learning}}},
  author = {Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  pages = {8},
  abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
  annotation = {ZSCC: 0000161},
  file = {/home/trung/GoogleDrive/Zotero/schölkopf et al_on causal and anticausal learning.pdf},
  keywords = {causal},
  language = {en}
}

@article{scholkopf00_CausalAnticausalLearninga,
  title = {On {{Causal}} and {{Anticausal Learning}}},
  author = {Sch{\"o}lkopf, Bernhard and Janzing, Dominik and Peters, Jonas and Sgouritsa, Eleni and Zhang, Kun and Mooij, Joris},
  pages = {8},
  abstract = {We consider the problem of function estimation in the case where an underlying causal model can be inferred. This has implications for popular scenarios such as covariate shift, concept drift, transfer learning and semi-supervised learning. We argue that causal knowledge may facilitate some approaches for a given problem, and rule out others. In particular, we formulate a hypothesis for when semi-supervised learning can help, and corroborate it with empirical results.},
  file = {/home/trung/GoogleDrive/Zotero/false},
  keywords = {causal,favorite},
  language = {en}
}

@article{scholkopf19_CausalityMachineLearning,
  title = {Causality for {{Machine Learning}}},
  author = {Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = nov,
  abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.10500},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/false;/home/trung/Zotero/storage/DSTASMEY/1911.html},
  journal = {arXiv:1911.10500 [cs, stat]},
  keywords = {causal,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2,I.5,K.4,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{schonfeld19_CrossLinkedVariationalAutoencoders,
  title = {Cross-{{Linked Variational Autoencoders}} for {{Generalized Zero}}-{{Shot Learning}}},
  author = {Schonfeld, Edgar and Ebrahimi, Sayna and Darrell, Trevor and Sinha, Samarth and Akata, Zeynep},
  year = {2019},
  pages = {6},
  file = {/home/trung/Zotero/storage/PIU4NWJR/Schonfeld et al. - 2019 - CROSS-LINKED VARIATIONAL AUTOENCODERS FOR GENERALI.pdf},
  language = {en}
}

@article{schwab19_CXPlainCausalExplanations,
  title = {{{CXPlain}}: {{Causal Explanations}} for {{Model Interpretation}} under {{Uncertainty}}},
  shorttitle = {{{CXPlain}}},
  author = {Schwab, Patrick and Karlen, Walter},
  year = {2019},
  month = oct,
  abstract = {Feature importance estimates that inform users about the degree to which given inputs influence the output of a predictive model are crucial for understanding, validating, and interpreting machine-learning models. However, providing fast and accurate estimates of feature importance for high-dimensional data, and quantifying the uncertainty of such estimates remain open challenges. Here, we frame the task of providing explanations for the decisions of machine-learning models as a causal learning task, and train causal explanation (CXPlain) models that learn to estimate to what degree certain inputs cause outputs in another machine-learning model. CXPlain can, once trained, be used to explain the target model in little time, and enables the quantification of the uncertainty associated with its feature importance estimates via bootstrap ensembling. We present experiments that demonstrate that CXPlain is significantly more accurate and faster than existing model-agnostic methods for estimating feature importance. In addition, we confirm that the uncertainty estimates provided by CXPlain ensembles are strongly correlated with their ability to accurately estimate feature importance on held-out data.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1910.12336},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schwab et al_2019_cxplain.pdf;/home/trung/Zotero/storage/YKNUDDYT/1910.html},
  journal = {arXiv:1910.12336 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,explain,interpretation,Statistics - Machine Learning,uncertainty},
  primaryClass = {cs, stat}
}

@article{schwab19_LearningCounterfactualRepresentations,
  title = {Learning {{Counterfactual Representations}} for {{Estimating Individual Dose}}-{{Response Curves}}},
  author = {Schwab, Patrick and Linhardt, Lorenz and Bauer, Stefan and Buhmann, Joachim M. and Karlen, Walter},
  year = {2019},
  month = feb,
  abstract = {Estimating what would be an individual's potential response to varying levels of exposure to a treatment is of high practical relevance for several important fields, such as healthcare, economics and public policy. However, existing methods for learning to estimate counterfactual outcomes from observational data are either focused on estimating average dose-response curves, or limited to settings with only two treatments that do not have an associated dosage parameter. Here, we present a novel machine-learning approach towards learning counterfactual representations for estimating individual dose-response curves for any number of treatments with continuous dosage parameters with neural networks. Building on the established potential outcomes framework, we introduce performance metrics, model selection criteria, model architectures, and open benchmarks for estimating individual dose-response curves. Our experiments show that the methods developed in this work set a new state-of-the-art in estimating individual dose-response.},
  archivePrefix = {arXiv},
  eprint = {1902.00981},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schwab et al_2019_learning counterfactual representations for estimating individual dose-response curves.pdf;/home/trung/Zotero/storage/6XGPUK7T/1902.html},
  journal = {arXiv:1902.00981 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{schwab20_RealtimePredictionCOVID19,
  title = {Real-Time {{Prediction}} of {{COVID}}-19 Related {{Mortality}} Using {{Electronic Health Records}}},
  author = {Schwab, Patrick and Mehrjou, Arash and Parbhoo, Sonali and Celi, Leo Anthony and Hetzel, J{\"u}rgen and Hofer, Markus and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  year = {2020},
  month = aug,
  abstract = {Coronavirus Disease 2019 (COVID-19) is an emerging respiratory disease caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) with rapid human-to-human transmission and a high case fatality rate particularly in older patients. Due to the exponential growth of infections, many healthcare systems across the world are under pressure to care for increasing amounts of at-risk patients. Given the high number of infected patients, identifying patients with the highest mortality risk early is critical to enable effective intervention and optimal prioritisation of care. Here, we present the COVID-19 Early Warning System (CovEWS), a clinical risk scoring system for assessing COVID-19 related mortality risk. CovEWS provides continuous real-time risk scores for individual patients with clinically meaningful predictive performance up to 192 hours (8 days) in advance, and is automatically derived from patients' electronic health records (EHRs) using machine learning. We trained and evaluated CovEWS using de-identified data from a cohort of 66430 COVID-19 positive patients seen at over 69 healthcare institutions in the United States (US), Australia, Malaysia and India amounting to an aggregated total of over 2863 years of patient observation time. On an external test cohort of 5005 patients, CovEWS predicts COVID-19 related mortality from \$78.8\textbackslash\%\$ (\$95\textbackslash\%\$ confidence interval [CI]: \$76.0\$, \$84.7\textbackslash\%\$) to \$69.4\textbackslash\%\$ (\$95\textbackslash\%\$ CI: \$57.6, 75.2\textbackslash\%\$) specificity at a sensitivity greater than \$95\textbackslash\%\$ between respectively 1 and 192 hours prior to observed mortality events - significantly outperforming existing generic and COVID-19 specific clinical risk scores. CovEWS could enable clinicians to intervene at an earlier stage, and may therefore help in preventing or mitigating COVID-19 related mortality.},
  archivePrefix = {arXiv},
  eprint = {2008.13412},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/schwab et al_2020_real-time prediction of covid-19 related mortality using electronic health records.pdf},
  journal = {arXiv:2008.13412 [cs, q-bio, stat]},
  primaryClass = {cs, q-bio, stat}
}

@article{seeger00_BayesianModellingMachine,
  title = {Bayesian {{Modelling}} in {{Machine Learning}}: {{A Tutorial Review}}},
  author = {Seeger, Matthias},
  pages = {38},
  abstract = {Many facets of Bayesian Modelling are firmly established in Machine Learning and give rise to state-of-the-art solutions to application problems. The sheer number of techniques, ideas and models which have been proposed, and the terminology, can be bewildering. With this tutorial review, we aim to give a wide high-level overview over this important field, concentrating on central ideas and methods, and on their interconnections. The reader will gain a basic understanding of the topics and their relationships, armed with which she can branch to details of her interest using the references to more specialized textbooks and reviews we provide here.},
  annotation = {ZSCC: 0000012},
  file = {/home/trung/GoogleDrive/Zotero/seeger_bayesian modelling in machine learning.pdf},
  language = {en}
}

@article{sejnowski20_UnreasonableEffectivenessDeep,
  title = {The {{Unreasonable Effectiveness}} of {{Deep Learning}} in {{Artificial Intelligence}}},
  author = {Sejnowski, Terrence J.},
  year = {2020},
  month = jan,
  pages = {201907373},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1907373117},
  abstract = {Deep learning networks have been trained to recognize speech, caption photographs and translate text between languages at high levels of performance. Although applications of deep learning networks to real world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and non-convex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.},
  archivePrefix = {arXiv},
  eprint = {2002.04806},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sejnowski_2020_the unreasonable effectiveness of deep learning in artificial intelligence.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {_tablet,favorite}
}

@article{senior20_Improvedproteinstructure,
  title = {Improved Protein Structure Prediction Using Potentials from Deep Learning},
  author = {Senior, Andrew W. and Evans, Richard and Jumper, John and Kirkpatrick, James and Sifre, Laurent and Green, Tim and Qin, Chongli and {\v Z}{\'i}dek, Augustin and Nelson, Alexander W. R. and Bridgland, Alex and Penedones, Hugo and Petersen, Stig and Simonyan, Karen and Crossan, Steve and Kohli, Pushmeet and Jones, David T. and Silver, David and Kavukcuoglu, Koray and Hassabis, Demis},
  year = {2020},
  month = jan,
  volume = {577},
  pages = {706--710},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1923-7},
  file = {/home/trung/Zotero/storage/IPH48KMW/Senior et al. - 2020 - Improved protein structure prediction using potent.pdf},
  journal = {Nature},
  language = {en},
  number = {7792}
}

@article{sepliarskaia19_EvaluatingDisentangledRepresentations,
  title = {Evaluating {{Disentangled Representations}}},
  author = {Sepliarskaia, Anna and Kiseleva, Julia and {de Rijke}, Maarten},
  year = {2019},
  month = oct,
  abstract = {There is no generally agreed upon definition of disentangled representation. Intuitively, the data is generated by a few factors of variation, which are captured and separated in a disentangled representation. Disentangled representations are useful for many tasks such as reinforcement learning, transfer learning, and zero-shot learning. However, the absence of a formally accepted definition makes it difficult to evaluate algorithms for learning disentangled representations. Recently, important steps have been taken towards evaluating disentangled representations: the existing metrics of disentanglement were compared through an experimental study and a framework for the quantitative evaluation of disentangled representations was proposed. However, theoretical guarantees for existing metrics of disentanglement are still missing. In this paper, we analyze metrics of disentanglement and their properties. Specifically, we analyze if the metrics satisfy two desirable properties: (1)\textasciitilde give a high score to representations that are disentangled according to the definition; and (2)\textasciitilde give a low score to representations that are entangled according to the definition. We show that most of the current metrics do not satisfy at least one of these properties. Consequently, we propose a new definition for a metric of disentanglement that satisfies both of the properties.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.05587},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sepliarskaia et al_2019_evaluating disentangled representations.pdf;/home/trung/Zotero/storage/ZC3PZQIN/1910.html},
  journal = {arXiv:1910.05587 [cs, stat]},
  keywords = {beta vae,Computer Science - Machine Learning,disentanglement,factor vae,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{serra18_Overcomingcatastrophicforgetting,
  title = {Overcoming Catastrophic Forgetting with Hard Attention to the Task},
  author = {Serr{\`a}, Joan and Sur{\'i}s, D{\'i}dac and Miron, Marius and Karatzoglou, Alexandros},
  year = {2018},
  month = jan,
  abstract = {Catastrophic forgetting occurs when a neural network loses the information learned in a previous task after training on subsequent tasks. This problem remains a hurdle for artificial intelligence systems with sequential learning capabilities. In this paper, we propose a task-based hard attention mechanism that preserves previous tasks' information without affecting the current task's learning. A hard attention mask is learned concurrently to every task, through stochastic gradient descent, and previous masks are exploited to condition such learning. We show that the proposed mechanism is effective for reducing catastrophic forgetting, cutting current rates by 45 to 80\%. We also show that it is robust to different hyperparameter choices, and that it offers a number of monitoring capabilities. The approach features the possibility to control both the stability and compactness of the learned knowledge, which we believe makes it also attractive for online learning or network compression applications.},
  annotation = {ZSCC: 0000058},
  archivePrefix = {arXiv},
  eprint = {1801.01423},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/serrà et al_2018_overcoming catastrophic forgetting with hard attention to the task.pdf;/home/trung/Zotero/storage/6XGJZQIE/1801.html},
  journal = {arXiv:1801.01423 [cs, stat]},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,forgeting,hard attention,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{serra20_LosslessCompressionDeep,
  title = {Lossless {{Compression}} of {{Deep Neural Networks}}},
  author = {Serra, Thiago and Kumar, Abhinav and Ramalingam, Srikumar},
  year = {2020},
  month = jan,
  abstract = {Deep neural networks have been successful in many predictive modeling tasks, such as image and language recognition, where large neural networks are often used to obtain good accuracy. Consequently, it is challenging to deploy these networks under limited computational resources, such as in mobile devices. In this work, we introduce an algorithm that removes units and layers of a neural network while not changing the output that is produced, which thus implies a lossless compression. This algorithm, which we denote as LEO (Lossless Expressiveness Optimization), relies on Mixed-Integer Linear Programming (MILP) to identify Rectifier Linear Units (ReLUs) with linear behavior over the input domain. By using L1 regularization to induce such behavior, we can benefit from training over a larger architecture than we would later use in the environment where the trained neural network is deployed.},
  archivePrefix = {arXiv},
  eprint = {2001.00218},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/serra et al_2020_lossless compression of deep neural networks.pdf;/home/trung/Zotero/storage/BZFCYEIT/2001.html},
  journal = {arXiv:2001.00218 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@inproceedings{Serrà2020Input,
  title = {Input Complexity and Out-of-Distribution Detection with Likelihood-Based Generative Models},
  booktitle = {International Conference on Learning Representations},
  author = {Serr{\`a}, Joan and {\'A}lvarez, David and G{\'o}mez, Vicen{\c c} and Slizovskaia, Olga and N{\'u}{\~n}ez, Jos{\'e} F. and Luque, Jordi},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/serrà et al_2020_input complexity and out-of-distribution detection with likelihood-based generative models.pdf}
}

@article{setlur20_NonlinearISAAuxiliary,
  title = {Nonlinear {{ISA}} with {{Auxiliary Variables}} for {{Learning Speech Representations}}},
  author = {Setlur, Amrith and Poczos, Barnabas and Black, Alan W.},
  year = {2020},
  month = jul,
  abstract = {This paper extends recent work on nonlinear Independent Component Analysis (ICA) by introducing a theoretical framework for nonlinear Independent Subspace Analysis (ISA) in the presence of auxiliary variables. Observed high dimensional acoustic features like log Mel spectrograms can be considered as surface level manifestations of nonlinear transformations over individual multivariate sources of information like speaker characteristics, phonological content etc. Under assumptions of energy based models we use the theory of nonlinear ISA to propose an algorithm that learns unsupervised speech representations whose subspaces are independent and potentially highly correlated with the original non-stationary multivariate sources. We show how nonlinear ICA with auxiliary variables can be extended to a generic identifiable model for subspaces as well while also providing sufficient conditions for the identifiability of these high dimensional subspaces. Our proposed methodology is generic and can be integrated with standard unsupervised approaches to learn speech representations with subspaces that can theoretically capture independent higher order speech signals. We evaluate the gains of our algorithm when integrated with the Autoregressive Predictive Decoding (APC) model by showing empirical results on the speaker verification and phoneme recognition tasks.},
  archivePrefix = {arXiv},
  eprint = {2007.12948},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/setlur et al_2020_nonlinear isa with auxiliary variables for learning speech representations.pdf},
  journal = {arXiv:2007.12948 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{seybold19_DuelingDecodersRegularizing,
  title = {Dueling {{Decoders}}: {{Regularizing Variational Autoencoder Latent Spaces}}},
  shorttitle = {Dueling {{Decoders}}},
  author = {Seybold, Bryan and Fertig, Emily and Alemi, Alex and Fischer, Ian},
  year = {2019},
  month = may,
  abstract = {Variational autoencoders learn unsupervised data representations, but these models frequently converge to minima that fail to preserve meaningful semantic information. For example, variational autoencoders with autoregressive decoders often collapse into autodecoders, where they learn to ignore the encoder input. In this work, we demonstrate that adding an auxiliary decoder to regularize the latent space can prevent this collapse, but successful auxiliary decoding tasks are domain dependent. Auxiliary decoders can increase the amount of semantic information encoded in the latent space and visible in the reconstructions. The semantic information in the variational autoencoder's representation is only weakly correlated with its rate, distortion, or evidence lower bound. Compared to other popular strategies that modify the training objective, our regularization of the latent space generally increased the semantic information content.},
  archivePrefix = {arXiv},
  eprint = {1905.07478},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/seybold et al_2019_dueling decoders.pdf},
  journal = {arXiv:1905.07478 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@article{seybold19_DuelingDecodersRegularizinga,
  title = {Dueling {{Decoders}}: {{Regularizing Variational Autoencoder Latent Spaces}}},
  shorttitle = {Dueling {{Decoders}}},
  author = {Seybold, Bryan and Fertig, Emily and Alemi, Alex and Fischer, Ian},
  year = {2019},
  month = may,
  abstract = {Variational autoencoders learn unsupervised data representations, but these models frequently converge to minima that fail to preserve meaningful semantic information. For example, variational autoencoders with autoregressive decoders often collapse into autodecoders, where they learn to ignore the encoder input. In this work, we demonstrate that adding an auxiliary decoder to regularize the latent space can prevent this collapse, but successful auxiliary decoding tasks are domain dependent. Auxiliary decoders can increase the amount of semantic information encoded in the latent space and visible in the reconstructions. The semantic information in the variational autoencoder's representation is only weakly correlated with its rate, distortion, or evidence lower bound. Compared to other popular strategies that modify the training objective, our regularization of the latent space generally increased the semantic information content.},
  archivePrefix = {arXiv},
  eprint = {1905.07478},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/false},
  journal = {arXiv:1905.07478 [cs, stat]},
  keywords = {favorite,vae_issues},
  primaryClass = {cs, stat}
}

@article{shah19_FeasibilityLearningRather,
  title = {On the {{Feasibility}} of {{Learning}}, {{Rather}} than {{Assuming}}, {{Human Biases}} for {{Reward Inference}}},
  author = {Shah, Rohin and Gundotra, Noah and Abbeel, Pieter and Dragan, Anca D.},
  year = {2019},
  month = jun,
  abstract = {Our goal is for agents to optimize the right reward function, despite how difficult it is for us to specify what that is. Inverse Reinforcement Learning (IRL) enables us to infer reward functions from demonstrations, but it usually assumes that the expert is noisily optimal. Real people, on the other hand, often have systematic biases: risk-aversion, myopia, etc. One option is to try to characterize these biases and account for them explicitly during learning. But in the era of deep learning, a natural suggestion researchers make is to avoid mathematical models of human behavior that are fraught with specific assumptions, and instead use a purely data-driven approach. We decided to put this to the test \textendash{} rather than relying on assumptions about which specific bias the demonstrator has when planning, we instead learn the demonstrator's planning algorithm that they use to generate demonstrations, as a differentiable planner. Our exploration yielded mixed findings: on the one hand, learning the planner can lead to better reward inference than relying on the wrong assumption; on the other hand, this benefit is dwarfed by the loss we incur by going from an exact to a differentiable planner. This suggest that at least for the foreseeable future, agents need a middle ground between the flexibility of data-driven methods and the useful bias of known human biases. Code is available at https: //tinyurl.com/learningbiases.},
  archivePrefix = {arXiv},
  eprint = {1906.09624},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/Z4796A8A/Shah et al. - 2019 - On the Feasibility of Learning, Rather than Assumi.pdf},
  journal = {arXiv:1906.09624 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shaham00_SinGANLearningGenerative,
  title = {{{SinGAN}}: {{Learning}} a {{Generative Model From}} a {{Single Natural Image}}},
  author = {Shaham, Tamar Rott and Dekel, Tali and Michaeli, Tomer},
  pages = {11},
  abstract = {We introduce SinGAN, an unconditional generative model that can be learned from a single natural image. Our model is trained to capture the internal distribution of patches within the image, and is then able to generate high quality, diverse samples that carry the same visual content as the image. SinGAN contains a pyramid of fully convolutional GANs, each responsible for learning the patch distribution at a different scale of the image. This allows generating new samples of arbitrary size and aspect ratio, that have significant variability, yet maintain both the global structure and the fine textures of the training image. In contrast to previous single image GAN schemes, our approach is not limited to texture images, and is not conditional (i.e. it generates samples from noise). User studies confirm that the generated samples are commonly confused to be real images. We illustrate the utility of SinGAN in a wide range of image manipulation tasks.},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/GoogleDrive/Zotero/shaham et al_singan.pdf},
  keywords = {adversarial,art,gan,generative,style},
  language = {en}
}

@article{shamout20_artificialintelligencesystem,
  title = {An Artificial Intelligence System for Predicting the Deterioration of {{COVID}}-19 Patients in the Emergency Department},
  author = {Shamout, Farah E. and Shen, Yiqiu and Wu, Nan and Kaku, Aakash and Park, Jungkyu and Makino, Taro and Jastrz{\k{e}}bski, Stanis{\l}aw and Wang, Duo and Zhang, Ben and Dogra, Siddhant and Cao, Meng and Razavian, Narges and Kudlowitz, David and Azour, Lea and Moore, William and Lui, Yvonne W. and Aphinyanaphongs, Yindalon and {Fernandez-Granda}, Carlos and Geras, Krzysztof J.},
  year = {2020},
  month = aug,
  abstract = {During the COVID-19 pandemic, rapid and accurate triage of patients at the emergency department is critical to inform decision-making. We propose a datadriven approach for automatic prediction of deterioration risk using a deep neural network that learns from chest X-ray images, and a gradient boosting model that learns from routine clinical variables. Our AI prognosis system, trained using data from 3,661 patients, achieves an AUC of 0.786 (95\% CI: 0.742-0.827) when predicting deterioration within 96 hours. The deep neural network extracts informative areas of chest X-ray images to assist clinicians in interpreting the predictions, and performs comparably to two radiologists in a reader study. In order to verify performance in a real clinical setting, we silently deployed a preliminary version of the deep neural network at NYU Langone Health during the first wave of the pandemic, which produced accurate predictions in real-time. In summary, our findings demonstrate the potential of the proposed system for assisting front-line physicians in the triage of COVID-19 patients.},
  archivePrefix = {arXiv},
  eprint = {2008.01774},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shamout et al_2020_an artificial intelligence system for predicting the deterioration of covid-19 patients in the emergency department.pdf},
  journal = {arXiv:2008.01774 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@inproceedings{shankar18_SurprisinglyEasyHardAttention,
  title = {Surprisingly {{Easy Hard}}-{{Attention}} for {{Sequence}} to {{Sequence Learning}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Shankar, Shiv and Garg, Siddhant and Sarawagi, Sunita},
  year = {2018},
  month = oct,
  pages = {640--645},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1065},
  abstract = {In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning. The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention. On five translation tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.},
  annotation = {ZSCC: 0000011},
  file = {/home/trung/GoogleDrive/Zotero/shankar et al_2018_surprisingly easy hard-attention for sequence to sequence learning.pdf}
}

@article{shanmugam18_Elementscausalinferencea,
  title = {Elements of Causal Inference: Foundations and Learning Algorithms},
  shorttitle = {Elements of Causal Inference},
  author = {Shanmugam, Ramalingam},
  year = {2018},
  month = nov,
  volume = {88},
  pages = {3248--3248},
  issn = {0094-9655, 1563-5163},
  doi = {10.1080/00949655.2018.1505197},
  file = {/home/trung/GoogleDrive/Zotero/2018/false},
  journal = {Journal of Statistical Computation and Simulation},
  language = {en},
  number = {16}
}

@article{shannon48_MathematicalTheoryCommunication,
  title = {A {{Mathematical Theory}} of {{Communication}}},
  author = {Shannon, C E},
  year = {1948},
  pages = {55},
  file = {/home/trung/GoogleDrive/Zotero/shannon_a mathematical theory of communication.pdf},
  keywords = {favorite},
  language = {en}
}

@article{shao20_ControlVAEControllableVariational,
  title = {{{ControlVAE}}: {{Controllable Variational Autoencoder}}},
  shorttitle = {{{ControlVAE}}},
  author = {Shao, Huajie and Yao, Shuochao and Sun, Dachun and Zhang, Aston and Liu, Shengzhong and Liu, Dongxin and Wang, Jun and Abdelzaher, Tarek},
  year = {2020},
  month = jun,
  abstract = {Variational Autoencoders (VAE) and their variants have been widely used in a variety of applications, such as dialog generation, image generation and disentangled representation learning. However, the existing VAE models have some limitations in different applications. For example, a VAE easily suffers from KL vanishing in language modeling and low reconstruction quality for disentangling. To address these issues, we propose a novel controllable variational autoencoder framework, ControlVAE, that combines a controller, inspired by automatic control theory, with the basic VAE to improve the performance of resulting generative models. Specifically, we design a new non-linear PI controller, a variant of the proportional-integral-derivative (PID) control, to automatically tune the hyperparameter (weight) added in the VAE objective using the output KL-divergence as feedback during model training. The framework is evaluated using three applications; namely, language modeling, disentangled representation learning, and image generation. The results show that ControlVAE can achieve better disentangling and reconstruction quality than the existing methods. For language modelling, it not only averts the KL-vanishing, but also improves the diversity of generated text. Finally, we also demonstrate that ControlVAE improves the reconstruction quality of generated images compared to the original VAE.},
  archivePrefix = {arXiv},
  eprint = {2004.05988},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shao et al_2020_controlvae.pdf},
  journal = {arXiv:2004.05988 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{shao20_UnderstandingClassifierMistakes,
  title = {Understanding {{Classifier Mistakes}} with {{Generative Models}}},
  author = {Shao, La{\"e}titia and Song, Yang and Ermon, Stefano},
  year = {2020},
  month = oct,
  abstract = {Although deep neural networks are effective on supervised learning tasks, they have been shown to be brittle. They are prone to overfitting on their training distribution and are easily fooled by small adversarial perturbations. In this paper, we leverage generative models to identify and characterize instances where classifiers fail to generalize. We propose a generative model of the features extracted by a classifier, and show using rigorous hypothesis testing that errors tend to occur when features are assigned low-probability by our model. From this observation, we develop a detection criteria for samples on which a classifier is likely to fail at test time. In particular, we test against three different sources of classification failures: mistakes made on the test set due to poor model generalization, adversarial samples and out-of-distribution samples. Our approach is agnostic to class labels from the training set which makes it applicable to models trained in a semi-supervised way.},
  archivePrefix = {arXiv},
  eprint = {2010.02364},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shao et al_2020_understanding classifier mistakes with generative models.pdf},
  journal = {arXiv:2010.02364 [cs]},
  primaryClass = {cs}
}

@article{shaw17_Vitaminenrichedgelatin,
  title = {Vitamin {{C}}\textendash Enriched Gelatin Supplementation before Intermittent Activity Augments Collagen Synthesis12},
  author = {Shaw, Gregory and {Lee-Barthel}, Ann and Ross, Megan LR and Wang, Bing and Baar, Keith},
  year = {2017},
  month = jan,
  volume = {105},
  pages = {136--143},
  issn = {0002-9165},
  doi = {10.3945/ajcn.116.138594},
  abstract = {Background: Musculoskeletal injuries are the most common complaint in active populations. More than 50\% of all injuries in sports can be classified as sprains, strains, ruptures, or breaks of musculoskeletal tissues. Nutritional and/or exercise interventions that increase collagen synthesis and strengthen these tissues could have an important effect on injury rates., Objective: This study was designed to determine whether gelatin supplementation could increase collagen synthesis., Design: Eight healthy male subjects completed a randomized, double-blinded, crossover-design study in which they consumed either 5 or 15 g of vitamin C\textendash enriched gelatin or a placebo control. After the initial drink, blood was taken every 30 min to determine amino acid content in the blood. A larger blood sample was taken before and 1 h after consumption of gelatin for treatment of engineered ligaments. One hour after the initial supplement, the subjects completed 6 min of rope-skipping to stimulate collagen synthesis. This pattern of supplementation was repeated 3 times/d with {$\geq$}6 h between exercise bouts for 3 d. Blood was drawn before and 4, 24, 48, and 72 h after the first exercise bout for determination of amino-terminal propeptide of collagen I content., Results: Supplementation with increasing amounts of gelatin increased circulating glycine, proline, hydroxyproline, and hydroxylysine, peaking 1 h after the supplement was given. Engineered ligaments treated for 6 d with serum from samples collected before or 1 h after subjects consumed a placebo or 5 or 15 g gelatin showed increased collagen content and improved mechanics. Subjects who took 15 g gelatin 1 h before exercise showed double the amino-terminal propeptide of collagen I in their blood, indicating increased collagen synthesis., Conclusion: These data suggest that adding gelatin to an intermittent exercise program improves collagen synthesis and could play a beneficial role in injury prevention and tissue repair. This trial was registered at the Australian New Zealand Clinical Trials Registry as ACTRN12616001092482.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/shaw et al_2017_vitamin c–enriched gelatin supplementation before intermittent activity augments collagen synthesis12.pdf},
  journal = {The American Journal of Clinical Nutrition},
  number = {1},
  pmcid = {PMC5183725},
  pmid = {27852613}
}

@article{shaw18_SelfAttentionRelativePosition,
  title = {Self-{{Attention}} with {{Relative Position Representations}}},
  author = {Shaw, Peter and Uszkoreit, Jakob and Vaswani, Ashish},
  year = {2018},
  month = apr,
  abstract = {Relying entirely on an attention mechanism, the Transformer introduced by Vaswani et al. (2017) achieves state-of-the-art results for machine translation. In contrast to recurrent and convolutional neural networks, it does not explicitly model relative or absolute position information in its structure. Instead, it requires adding representations of absolute positions to its inputs. In this work we present an alternative approach, extending the self-attention mechanism to efficiently consider representations of the relative positions, or distances between sequence elements. On the WMT 2014 English-to-German and English-to-French translation tasks, this approach yields improvements of 1.3 BLEU and 0.3 BLEU over absolute position representations, respectively. Notably, we observe that combining relative and absolute position representations yields no further improvement in translation quality. We describe an efficient implementation of our method and cast it as an instance of relation-aware self-attention mechanisms that can generalize to arbitrary graph-labeled inputs.},
  annotation = {ZSCC: 0000097},
  archivePrefix = {arXiv},
  eprint = {1803.02155},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shaw et al_2018_self-attention with relative position representations.pdf;/home/trung/Zotero/storage/XSSY9384/1803.html},
  journal = {arXiv:1803.02155 [cs]},
  keywords = {attention,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{shazeer17_OutrageouslyLargeNeural,
  title = {Outrageously {{Large Neural Networks}}: {{The Sparsely}}-{{Gated Mixture}}-of-{{Experts Layer}}},
  shorttitle = {Outrageously {{Large Neural Networks}}},
  author = {Shazeer, Noam and Mirhoseini, Azalia and Maziarz, Krzysztof and Davis, Andy and Le, Quoc and Hinton, Geoffrey and Dean, Jeff},
  year = {2017},
  month = jan,
  abstract = {The capacity of a neural network to absorb information is limited by its number of parameters. Conditional computation, where parts of the network are active on a per-example basis, has been proposed in theory as a way of dramatically increasing model capacity without a proportional increase in computation. In practice, however, there are significant algorithmic and performance challenges. In this work, we address these challenges and finally realize the promise of conditional computation, achieving greater than 1000x improvements in model capacity with only minor losses in computational efficiency on modern GPU clusters. We introduce a Sparsely-Gated Mixture-of-Experts layer (MoE), consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. We apply the MoE to the tasks of language modeling and machine translation, where model capacity is critical for absorbing the vast quantities of knowledge available in the training corpora. We present model architectures in which a MoE with up to 137 billion parameters is applied convolutionally between stacked LSTM layers. On large language modeling and machine translation benchmarks, these models achieve significantly better results than state-of-the-art at lower computational cost.},
  archivePrefix = {arXiv},
  eprint = {1701.06538},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shazeer et al_2017_outrageously large neural networks.pdf},
  journal = {arXiv:1701.06538 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shazeer19_FastTransformerDecoding,
  title = {Fast {{Transformer Decoding}}: {{One Write}}-{{Head}} Is {{All You Need}}},
  shorttitle = {Fast {{Transformer Decoding}}},
  author = {Shazeer, Noam},
  year = {2019},
  month = nov,
  abstract = {Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large "keys" and "values" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention "heads", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.02150},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shazeer_2019_fast transformer decoding.pdf;/home/trung/Zotero/storage/J58XIUL6/1911.html},
  journal = {arXiv:1911.02150 [cs]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,fast,transformer},
  primaryClass = {cs}
}

@article{shekhovtsov18_StochasticNormalizationsBayesian,
  title = {Stochastic {{Normalizations}} as {{Bayesian Learning}}},
  author = {Shekhovtsov, Alexander and Flach, Boris},
  year = {2018},
  month = nov,
  abstract = {In this work we investigate the reasons why Batch Normalization (BN) improves the generalization performance of deep networks. We argue that one major reason, distinguishing it from data-independent normalization methods, is randomness of batch statistics. This randomness appears in the parameters rather than in activations and admits an interpretation as a practical Bayesian learning. We apply this idea to other (deterministic) normalization techniques that are oblivious to the batch size. We show that their generalization performance can be improved significantly by Bayesian learning of the same form. We obtain test performance comparable to BN and, at the same time, better validation losses suitable for subsequent output uncertainty estimation through approximate Bayesian posterior.},
  archivePrefix = {arXiv},
  eprint = {1811.00639},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shekhovtsov et al_2018_stochastic normalizations as bayesian learning.pdf;/home/trung/Zotero/storage/3DBDXLWY/1811.html},
  journal = {arXiv:1811.00639 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,normalization,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{shen00_AutomaticChemicalDesign,
  title = {Automatic {{Chemical Design}} with {{Molecular Graph Variational Autoencoders}}},
  author = {Shen, Richard Devin},
  pages = {58},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/shen_automatic chemical design with molecular graph variational autoencoders.pdf},
  language = {en}
}

@article{shen18_ImprovingVariationalEncoderDecoders,
  title = {Improving {{Variational Encoder}}-{{Decoders}} in {{Dialogue Generation}}},
  author = {Shen, Xiaoyu and Su, Hui and Niu, Shuzi and Demberg, Vera},
  year = {2018},
  month = feb,
  abstract = {Variational encoder-decoders (VEDs) have shown promising results in dialogue generation. However, the latent variable distributions are usually approximated by a much simpler model than the powerful RNN structure used for encoding and decoding, yielding the KL-vanishing problem and inconsistent training objective. In this paper, we separate the training step into two phases: The first phase learns to autoencode discrete texts into continuous embeddings, from which the second phase learns to generalize latent representations by reconstructing the encoded embedding. In this case, latent variables are sampled by transforming Gaussian noise through multi-layer perceptrons and are trained with a separate VED model, which has the potential of realizing a much more flexible distribution. We compare our model with current popular models and the experiment demonstrates substantial improvement in both metric-based and human evaluations.},
  archivePrefix = {arXiv},
  eprint = {1802.02032},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/6TJUVC59/Shen et al. - 2018 - Improving Variational Encoder-Decoders in Dialogue.pdf},
  journal = {arXiv:1802.02032 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{shen18_NaturalTTSSynthesis,
  title = {Natural {{TTS Synthesis}} by {{Conditioning WaveNet}} on {{Mel Spectrogram Predictions}}},
  author = {Shen, Jonathan and Pang, Ruoming and Weiss, Ron J. and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and {Skerry-Ryan}, R. J. and Saurous, Rif A. and Agiomyrgiannakis, Yannis and Wu, Yonghui},
  year = {2018},
  month = feb,
  abstract = {This paper describes Tacotron 2, a neural network architecture for speech synthesis directly from text. The system is composed of a recurrent sequence-to-sequence feature prediction network that maps character embeddings to mel-scale spectrograms, followed by a modified WaveNet model acting as a vocoder to synthesize timedomain waveforms from those spectrograms. Our model achieves a mean opinion score (MOS) of \$4.53\$ comparable to a MOS of \$4.58\$ for professionally recorded speech. To validate our design choices, we present ablation studies of key components of our system and evaluate the impact of using mel spectrograms as the input to WaveNet instead of linguistic, duration, and \$F\_0\$ features. We further demonstrate that using a compact acoustic intermediate representation enables significant simplification of the WaveNet architecture.},
  archivePrefix = {arXiv},
  eprint = {1712.05884},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shen et al_2018_natural tts synthesis by conditioning wavenet on mel spectrogram predictions.pdf},
  journal = {arXiv:1712.05884 [cs]},
  primaryClass = {cs}
}

@article{shen18_SharpAttentionNetwork,
  title = {Sharp {{Attention Network}} via {{Adaptive Sampling}} for {{Person Re}}-Identification},
  author = {Shen, Chen and Qi, Guo-Jun and Jiang, Rongxin and Jin, Zhongming and Yong, Hongwei and Chen, Yaowu and Hua, Xian-Sheng},
  year = {2018},
  month = sep,
  abstract = {In this paper, we present novel sharp attention networks by adaptively sampling feature maps from convolutional neural networks (CNNs) for person re-identification (reID) problem. Due to the introduction of sampling-based attention models, the proposed approach can adaptively generate sharper attention-aware feature masks. This greatly differs from the gating-based attention mechanism that relies soft gating functions to select the relevant features for person re-ID. In contrast, the proposed sampling-based attention mechanism allows us to effectively trim irrelevant features by enforcing the resultant feature masks to focus on the most discriminative features. It can produce sharper attentions that are more assertive in localizing subtle features relevant to re-identifying people across cameras. For this purpose, a differentiable Gumbel-Softmax sampler is employed to approximate the Bernoulli sampling to train the sharp attention networks. Extensive experimental evaluations demonstrate the superiority of this new sharp attention model for person re-ID over other related existing published state-of-theart works on three challenging benchmarks including CUHK03, Market-1501, and DukeMTMC-reID.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1805.02336},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shen et al_2018_sharp attention network via adaptive sampling for person re-identification.pdf;/home/trung/Zotero/storage/HWAFXLXY/1805.html},
  journal = {arXiv:1805.02336 [cs]},
  keywords = {attention,Computer Science - Computer Vision and Pattern Recognition,hard attention,sharp attention},
  language = {en},
  primaryClass = {cs}
}

@article{shen19_BayesianNashEquilibrium,
  title = {Bayesian {{Nash Equilibrium}} in {{First}}-{{Price Auction}} with {{Discrete Value Distributions}}},
  author = {Shen, Weiran and Wang, Zihe and Zuo, Song},
  year = {2019},
  month = jul,
  abstract = {First price auctions are widely used in government contracts and industrial auctions. In this paper, we consider the Bayesian Nash Equilibrium (BNE) in first price auctions with discrete value distributions. We study the characterization of the BNE in the first price auction and provide an algorithm to compute the BNE at the same time. Moreover, we prove the existence and the uniqueness of the BNE. Some of the previous results in the case of continuous value distributions do not apply to the case of discrete value distributions. In the meanwhile, the uniqueness result in discrete case cannot be implied by the uniqueness property in the continuous case. Unlike in the continuous case, we do not need to solve ordinary differential equations and thus do not suffer from the solution errors therein. Compared to the method of using continuous distributions to approximate discrete ones, our experiments show that our algorithm is both faster and more accurate. The results in this paper are derived in the asymmetric independent private values model, which assumes that the buyers' value distributions are common knowledge.},
  archivePrefix = {arXiv},
  eprint = {1906.09403},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shen et al_2019_bayesian nash equilibrium in first-price auction with discrete value distributions.pdf},
  journal = {arXiv:1906.09403 [cs]},
  primaryClass = {cs}
}

@article{shen19_SelectAttendControllable,
  title = {Select and {{Attend}}: {{Towards Controllable Content Selection}} in {{Text Generation}}},
  shorttitle = {Select and {{Attend}}},
  author = {Shen, Xiaoyu and Suzuki, Jun and Inui, Kentaro and Su, Hui and Klakow, Dietrich and Sekine, Satoshi},
  year = {2019},
  month = sep,
  abstract = {Many text generation tasks naturally contain two steps: content selection and surface realization. Current neural encoder-decoder models conflate both steps into a black-box architecture. As a result, the content to be described in the text cannot be explicitly controlled. This paper tackles this problem by decoupling content selection from the decoder. The decoupled content selection is human interpretable, whose value can be manually manipulated to control the content of generated text. The model can be trained end-to-end without human annotations by maximizing a lower bound of the marginal likelihood. We further propose an effective way to trade-off between performance and controllability with a single adjustable hyperparameter. In both data-to-text and headline generation tasks, our model achieves promising results, paving the way for controllable content selection in text generation.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1909.04453},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shen et al_2019_select and attend.pdf;/home/trung/Zotero/storage/SGDRICWZ/1909.html},
  journal = {arXiv:1909.04453 [cs]},
  keywords = {attention,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{shen20_NonAttentiveTacotronRobust,
  title = {Non-{{Attentive Tacotron}}: {{Robust}} and {{Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling}}},
  shorttitle = {Non-{{Attentive Tacotron}}},
  author = {Shen, Jonathan and Jia, Ye and Chrzanowski, Mike and Zhang, Yu and Elias, Isaac and Zen, Heiga and Wu, Yonghui},
  year = {2020},
  month = oct,
  abstract = {This paper presents Non-Attentive Tacotron based on the Tacotron 2 text-to-speech model, replacing the attention mechanism with an explicit duration predictor. This improves robustness significantly as measured by unaligned duration ratio and word deletion rate, two metrics introduced in this paper for large-scale robustness evaluation using a pre-trained speech recognition model. With the use of Gaussian upsampling, Non-Attentive Tacotron achieves a 5-scale mean opinion score for naturalness of 4.41, slightly outperforming Tacotron 2. The duration predictor enables both utterance-wide and per-phoneme control of duration at inference time. When accurate target durations are scarce or unavailable in the training data, we propose a method using a fine-grained variational auto-encoder to train the duration predictor in a semi-supervised or unsupervised manner, with results almost as good as supervised training.},
  archivePrefix = {arXiv},
  eprint = {2010.04301},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shen et al_2020_non-attentive tacotron.pdf},
  journal = {arXiv:2010.04301 [cs]},
  primaryClass = {cs}
}

@article{shevtsov03_randomizedtrialtwo,
  title = {A Randomized Trial of Two Different Doses of a {{SHR}}-5 {{Rhodiola}} Rosea Extract versus Placebo and Control of Capacity for Mental Work},
  author = {Shevtsov, V.A. and Zholus, B.I. and Shervarly, V.I. and Vol'skij, V.B. and Korovin, Y.P. and Khristich, M.P. and Roslyakova, N.A. and Wikman, G.},
  year = {2003},
  month = jan,
  volume = {10},
  pages = {95--105},
  issn = {09447113},
  doi = {10.1078/094471103321659780},
  abstract = {A randomized, double-blind, placebo-controlled, parallel-group clinical study with an extra nontreatment group was performed to measure the effect of a single dose of standardized SHR-5 Rhodiola rosea extract on capacity for mental work against a background of fatigue and stress. An additional objective was to investigate a possible difference between two doses, one dose being chosen as the standard mean dose in accordance with well-established medicinal use as a psychostimulant/adaptogen, the other dose being 50\% higher. Some physiological parameters, e.g. pulse rate, systolic and diastolic blood pressure, were also measured. The study was carried out on a highly uniform population comprising 161 cadets aged from 19 to 21 years. All groups were found to have very similar initial data, with no significant difference with regard to any parameter. The study showed a pronounced antifatigue effect reflected in an antifatigue index defined as a ratio called AFI. The verum groups had AFI mean values of 1.0385 and 1.0195, 2 and 3 capsules respectively, whilst the figure for the placebo group was 0.9046. This was statistically highly significant (p {$<$} 0.001) for both doses (verum groups), whilst no significant difference between the two dosage groups was observed. There was a possible trend in favour of the lower dose in the psychometric tests. No such trend was found in the physiological tests.},
  file = {/home/trung/GoogleDrive/Zotero/shevtsov et al_2003_a randomized trial of two different doses of a shr-5 rhodiola rosea extract versus placebo and control of capacity for mental work.pdf},
  journal = {Phytomedicine},
  language = {en},
  number = {2-3}
}

@article{shi17_KernelImplicitVariational,
  title = {Kernel {{Implicit Variational Inference}}},
  author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
  year = {2017},
  month = may,
  abstract = {Recent progress in variational inference has paid much attention to the flexibility of variational posteriors. One promising direction is to use implicit distributions, i.e., distributions without tractable densities as the variational posterior. However, existing methods on implicit posteriors still face challenges of noisy estimation and computational infeasibility when applied to models with high-dimensional latent variables. In this paper, we present a new approach named Kernel Implicit Variational Inference that addresses these challenges. As far as we know, for the first time implicit variational inference is successfully applied to Bayesian neural networks, which shows promising results on both regression and classification tasks.},
  archivePrefix = {arXiv},
  eprint = {1705.10119},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shi et al_2017_kernel implicit variational inference.pdf;/home/trung/Zotero/storage/8I4QHYS7/1705.html},
  journal = {arXiv:1705.10119 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,kernel,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{shi18_SpectralApproachGradient,
  title = {A {{Spectral Approach}} to {{Gradient Estimation}} for {{Implicit Distributions}}},
  author = {Shi, Jiaxin and Sun, Shengyang and Zhu, Jun},
  year = {2018},
  month = jun,
  abstract = {Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nystr\textbackslash "om method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-of-sample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nystr\textbackslash "om method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.},
  archivePrefix = {arXiv},
  eprint = {1806.02925},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shi et al_2018_a spectral approach to gradient estimation for implicit distributions.pdf},
  journal = {arXiv:1806.02925 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{shi19_AdaptingNeuralNetworks,
  title = {Adapting {{Neural Networks}} for the {{Estimation}} of {{Treatment Effects}}},
  author = {Shi, Claudia and Blei, David M. and Veitch, Victor},
  year = {2019},
  month = jun,
  abstract = {This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we fit models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these fitted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the first step. The question we address is: how can we adapt the design and training of the neural networks used in the first step in order to improve the quality of the final estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The first is a new architecture, the Dragonnet, that exploits the sufficiency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties `out-of-the-box`. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at github.com/claudiashi57/dragonnet},
  archivePrefix = {arXiv},
  eprint = {1906.02120},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shi et al_2019_adapting neural networks for the estimation of treatment effects.pdf;/home/trung/Zotero/storage/P4BD62LY/1906.html},
  journal = {arXiv:1906.02120 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{shi19_AdaptingNeuralNetworksa,
  title = {Adapting {{Neural Networks}} for the {{Estimation}} of {{Treatment Effects}}},
  author = {Shi, Claudia and Blei, David M. and Veitch, Victor},
  year = {2019},
  month = oct,
  abstract = {This paper addresses the use of neural networks for the estimation of treatment effects from observational data. Generally, estimation proceeds in two stages. First, we fit models for the expected outcome and the probability of treatment (propensity score) for each unit. Second, we plug these fitted models into a downstream estimator of the effect. Neural networks are a natural choice for the models in the first step. The question we address is: how can we adapt the design and training of the neural networks used in the first step in order to improve the quality of the final estimate of the treatment effect? We propose two adaptations based on insights from the statistical literature on the estimation of treatment effects. The first is a new architecture, the Dragonnet, that exploits the sufficiency of the propensity score for estimation adjustment. The second is a regularization procedure, targeted regularization, that induces a bias towards models that have non-parametrically optimal asymptotic properties `out-of-the-box`. Studies on benchmark datasets for causal inference show these adaptations outperform existing methods. Code is available at github.com/claudiashi57/dragonnet.},
  archivePrefix = {arXiv},
  eprint = {1906.02120},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/false},
  journal = {arXiv:1906.02120 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{shi19_FixingGaussianMixture,
  title = {Fixing {{Gaussian Mixture VAEs}} for {{Interpretable Text Generation}}},
  author = {Shi, Wenxian and Zhou, Hao and Miao, Ning and Zhao, Shenjian and Li, Lei},
  year = {2019},
  month = jun,
  abstract = {Variational auto-encoder (VAE) with Gaussian priors is effective in text generation. To improve the controllability and interpretability, we propose to use Gaussian mixture distribution as the prior for VAE (GMVAE), since it includes an extra discrete latent variable in addition to the continuous one. Unfortunately, training GMVAE using standard variational approximation often leads to the mode-collapse problem. We theoretically analyze the root cause \textemdash{} maximizing the evidence lower bound of GMVAE implicitly aggregates the means of multiple Gaussian priors. We propose Dispersed-GMVAE (DGMVAE), an improved model for text generation. It introduces two extra terms to alleviate mode-collapse and to induce a better structured latent space. Experimental results show that DGMVAE outperforms strong baselines in several language modeling and text generation benchmarks.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1906.06719},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/MZUTV5IF/Shi et al. - 2019 - Fixing Gaussian Mixture VAEs for Interpretable Tex.pdf},
  journal = {arXiv:1906.06719 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shi19_ScalableTrainingInference,
  title = {Scalable {{Training}} of {{Inference Networks}} for {{Gaussian}}-{{Process Models}}},
  author = {Shi, Jiaxin and Khan, Mohammad Emtiyaz and Zhu, Jun},
  year = {2019},
  month = may,
  abstract = {Inference in Gaussian process (GP) models is computationally challenging for large data, and often difficult to approximate with a small number of inducing points. We explore an alternative approximation that employs stochastic inference networks for a flexible inference. Unfortunately, for such networks, minibatch training is difficult to be able to learn meaningful correlations over function outputs for a large dataset. We propose an algorithm that enables such training by tracking a stochastic, functional mirror-descent algorithm. At each iteration, this only requires considering a finite number of input locations, resulting in a scalable and easy-to-implement algorithm. Empirical results show comparable and, sometimes, superior performance to existing sparse variational GP methods.},
  archivePrefix = {arXiv},
  eprint = {1905.10969},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shi et al_2019_scalable training of inference networks for gaussian-process models.pdf;/home/trung/Zotero/storage/58FAL5PW/1905.html},
  journal = {arXiv:1905.10969 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,varational},
  primaryClass = {cs, stat}
}

@article{shi19_VariationalMixtureofExpertsAutoencoders,
  title = {Variational {{Mixture}}-of-{{Experts Autoencoders}} for {{Multi}}-{{Modal Deep Generative Models}}},
  author = {Shi, Yuge and Siddharth, N and Paige, Brooks and H.S. Torr, Philip},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/shi et al_2019_variational mixture-of-experts autoencoders for multi-modal deep generative models.pdf}
}

@article{shi20_DispersedExponentialFamily,
  title = {Dispersed {{Exponential Family Mixture VAEs}} for {{Interpretable Text Generation}}},
  author = {Shi, Wenxian and Zhou, Hao and Miao, Ning and Li, Lei},
  year = {2020},
  month = aug,
  abstract = {Deep generative models are commonly used for generating images and text. Interpretability of these models is one important pursuit, other than the generation quality. Variational auto-encoder (VAE) with Gaussian distribution as prior has been successfully applied in text generation, but it is hard to interpret the meaning of the latent variable. To enhance the controllability and interpretability, one can replace the Gaussian prior with a mixture of Gaussian distributions (GMVAE), whose mixture components could be related to hidden semantic aspects of data. In this paper, we generalize the practice and introduce DEM-VAE, a class of models for text generation using VAEs with a mixture distribution of exponential family. Unfortunately, a standard variational training algorithm fails due to the mode-collapse problem. We theoretically identify the root cause of the problem and propose an effective algorithm to train DEM-VAE. Our method penalizes the training with an extra dispersion term to induce a well-structured latent space. Experimental results show that our approach does obtain a meaningful space, and it outperforms strong baselines in text generation benchmarks. The code is available at https: //github.com/wenxianxian/demvae.},
  archivePrefix = {arXiv},
  eprint = {1906.06719},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shi et al_2020_dispersed exponential family mixture vaes for interpretable text generation.pdf},
  journal = {arXiv:1906.06719 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shi20_MultiviewAlignmentGeneration,
  title = {Multi-View {{Alignment}} and {{Generation}} in {{CCA}} via {{Consistent Latent Encoding}}},
  author = {Shi, Yaxin and Pan, Yuangang and Xu, Donna and Tsang, Ivor W.},
  year = {2020},
  month = may,
  abstract = {Multi-view alignment, achieving one-to-one correspondence of multi-view inputs, is critical in many real-world multi-view applications, especially for cross-view data analysis problems. Recently, an increasing number of works study this alignment problem with Canonical Correlation Analysis (CCA). However, existing CCA models are prone to misalign the multiple views due to either the neglect of uncertainty or the inconsistent encoding of the multiple views. To tackle these two issues, this paper studies multi-view alignment from the Bayesian perspective. Delving into the impairments of inconsistent encodings, we propose to recover correspondence of the multi-view inputs by matching the marginalization of the joint distribution of multi-view random variables under different forms of factorization. To realize our design, we present Adversarial CCA (ACCA) which achieves consistent latent encodings by matching the marginalized latent encodings through the adversarial training paradigm. Our analysis based on conditional mutual information reveals that ACCA is flexible for handling implicit distributions. Extensive experiments on correlation analysis and cross-view generation under noisy input settings demonstrate the superiority of our model.},
  archivePrefix = {arXiv},
  eprint = {2005.11716},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shi et al_2020_multi-view alignment and generation in cca via consistent latent encoding.pdf},
  journal = {arXiv:2005.11716 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{shih20_ProbabilisticCircuitsVariational,
  title = {Probabilistic {{Circuits}} for {{Variational Inference}} in {{Discrete Graphical Models}}},
  author = {Shih, Andy and Ermon, Stefano},
  year = {2020},
  month = oct,
  abstract = {Inference in discrete graphical models with variational methods is difficult because of the inability to re-parameterize gradients of the Evidence Lower Bound (ELBO). Many sampling-based methods have been proposed for estimating these gradients, but they suffer from high bias or variance. In this paper, we propose a new approach that leverages the tractability of probabilistic circuit models, such as Sum Product Networks (SPN), to compute ELBO gradients exactly (without sampling) for a certain class of densities. In particular, we show that selective-SPNs are suitable as an expressive variational distribution, and prove that when the log-density of the target model is a polynomial the corresponding ELBO can be computed analytically. To scale to graphical models with thousands of variables, we develop an efficient and effective construction of selective-SPNs with size \$O(kn)\$, where \$n\$ is the number of variables and \$k\$ is an adjustable hyperparameter. We demonstrate our approach on three types of graphical models -- Ising models, Latent Dirichlet Allocation, and factor graphs from the UAI Inference Competition. Selective-SPNs give a better lower bound than mean-field and structured mean-field, and is competitive with approximations that do not provide a lower bound, such as Loopy Belief Propagation and Tree-Reweighted Belief Propagation. Our results show that probabilistic circuits are promising tools for variational inference in discrete graphical models as they combine tractability and expressivity.},
  archivePrefix = {arXiv},
  eprint = {2010.11446},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shih et al_2020_probabilistic circuits for variational inference in discrete graphical models.pdf},
  journal = {arXiv:2010.11446 [cs]},
  primaryClass = {cs}
}

@article{shim20_PseudoConditionalRegularization,
  title = {Pseudo {{Conditional Regularization}} for {{Inverse Mapping}} of {{GANs}}},
  author = {Shim, Sang-Heon and Heo, Jae-Pil},
  year = {2020},
  volume = {8},
  pages = {86972--86983},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.2992850},
  abstract = {Inverse mapping of the Generative Adversarial Networks (GANs) which projects data to latent space have been recently introduced, and it is shown that the inverse mapping models trained by the bidirectional adversarial learning can enable novel and practical operations including interpolation between real data. However, existing techniques still do not ensure the consistent mapping between the data and their latent representation so that the models are hardly converged throughout training steps. Our discussion begins with empirical investigations on the inconsistency issue of the prior techniques, and we further propose a novel adversarial learning method, Pseudo Conditional Bidirectional GAN (PC-BiGAN), for training the inverse mapping of GANs with a high degree of consistency and similarity-awareness. Our models are specifically guided by the pseudo conditions defined by the proximity relationship among data in unsupervised learned feature space. We demonstrate that our novel bidirectional adversarial learning frameworks improve the performance in sample reconstruction, generation, and interpolation.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/shim et al_2020_pseudo conditional regularization for inverse mapping of gans.pdf;/home/trung/Zotero/storage/IZIDMGM6/9088150.html},
  journal = {IEEE Access}
}

@article{shimizu20_HyperbolicNeuralNetworks,
  title = {Hyperbolic {{Neural Networks}}++},
  author = {Shimizu, Ryohei and Mukuta, Yusuke and Harada, Tatsuya},
  year = {2020},
  month = jun,
  abstract = {Hyperbolic spaces, which have the capacity to embed tree structures without distortion owing to their exponential volume growth, have recently been applied to machine learning to better capture the hierarchical nature of data. In this study, we reconsider a way to generalize the fundamental components of neural networks in a single hyperbolic geometry model, and propose novel methodologies to construct a multinomial logistic regression, fully-connected layers, convolutional layers, and attention mechanisms under a unified mathematical interpretation, without increasing the parameters. A series of experiments show the parameter efficiency of our methods compared to a conventional hyperbolic component, and stability and outperformance over their Euclidean counterparts.},
  archivePrefix = {arXiv},
  eprint = {2006.08210},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shimizu et al_2020_hyperbolic neural networks++.pdf},
  journal = {arXiv:2006.08210 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{shin17_TreeStructuredVariationalAutoencoder,
  title = {Tree-{{Structured Variational Autoencoder}}},
  author = {Shin, Richard and Alemi, Alexander A and Irving, Geoffrey and Vinyals, Oriol},
  year = {2017},
  pages = {12},
  abstract = {Many kinds of variable-sized data we would like to model contain an internal hierarchical structure in the form of a tree, including source code, formal logical statements, and natural language sentences with parse trees. For such data it is natural to consider a model with matching computational structure. In this work, we introduce a variational autoencoder-based generative model for tree-structured data. We evaluate our model on a synthetic dataset, and a dataset with applications to automated theorem proving. By learning a latent representation over trees, our model can achieve similar test log likelihood to a standard autoregressive decoder, but with the number of sequentially dependent computations proportional to the depth of the tree instead of the number of nodes in the tree.},
  file = {/home/trung/GoogleDrive/Zotero/shin et al_2017_tree-structured variational autoencoder.pdf},
  language = {en}
}

@article{shin19_HierarchicallyClusteredRepresentation,
  title = {Hierarchically {{Clustered Representation Learning}}},
  author = {Shin, Su-Jin and Song, Kyungwoo and Moon, Il-Chul},
  year = {2019},
  month = mar,
  abstract = {The joint optimization of representation learning and clustering in the embedding space has experienced a breakthrough in recent years. In spite of the advance, clustering with representation learning has been limited to flat-level categories, which often involves cohesive clustering with a focus on instance relations. To overcome the limitations of flat clustering, we introduce hierarchically-clustered representation learning (HCRL), which simultaneously optimizes representation learning and hierarchical clustering in the embedding space. Compared with a few prior works, HCRL firstly attempts to consider a generation of deep embeddings from every component of the hierarchy, not just leaf components. In addition to obtaining hierarchically clustered embeddings, we can reconstruct data by the various abstraction levels, infer the intrinsic hierarchical structure, and learn the level-proportion features. We conducted evaluations with image and text domains, and our quantitative analyses showed competent likelihoods and the best accuracies compared with the baselines.},
  archivePrefix = {arXiv},
  eprint = {1901.09906},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shin et al_2019_hierarchically clustered representation learning.pdf},
  journal = {arXiv:1901.09906 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{shleifer20_PretrainedSummarizationDistillation,
  title = {Pre-Trained {{Summarization Distillation}}},
  author = {Shleifer, Sam and Rush, Alexander M.},
  year = {2020},
  month = oct,
  abstract = {Recent state-of-the-art approaches to summarization utilize large pre-trained Transformer models. Distilling these models to smaller student models has become critically important for practical use; however there are many different distillation methods proposed by the NLP literature. Recent work on distilling BERT for classification and regression tasks shows strong performance using direct knowledge distillation. Alternatively, machine translation practitioners distill using pseudo-labeling, where a small model is trained on the translations of a larger model. A third, simpler approach is to 'shrink and fine-tune' (SFT), which avoids any explicit distillation by copying parameters to a smaller student model and then fine-tuning. We compare these three approaches for distillation of Pegasus and BART, the current and former state of the art, pre-trained summarization models, and find that SFT outperforms knowledge distillation and pseudo-labeling on the CNN/DailyMail dataset, but under-performs pseudo-labeling on the more abstractive XSUM dataset. PyTorch Code and checkpoints of different sizes are available through Hugging Face transformers here http://tiny.cc/4iy0tz.},
  archivePrefix = {arXiv},
  eprint = {2010.13002},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shleifer et al_2020_pre-trained summarization distillation.pdf},
  journal = {arXiv:2010.13002 [cs]},
  keywords = {favorite},
  primaryClass = {cs}
}

@article{shlens14_TutorialIndependentComponent,
  title = {A {{Tutorial}} on {{Independent Component Analysis}}},
  author = {Shlens, Jonathon},
  year = {2014},
  month = apr,
  abstract = {Independent component analysis (ICA) has become a standard data analysis technique applied to an array of problems in signal processing and machine learning. This tutorial provides an introduction to ICA based on linear algebra formulating an intuition for ICA from first principles. The goal of this tutorial is to provide a solid foundation on this advanced topic so that one might learn the motivation behind ICA, learn why and when to apply this technique and in the process gain an introduction to this exciting field of active research.},
  archivePrefix = {arXiv},
  eprint = {1404.2986},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shlens_2014_a tutorial on independent component analysis.pdf},
  journal = {arXiv:1404.2986 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{shorten19_surveyImageData,
  title = {A Survey on {{Image Data Augmentation}} for {{Deep Learning}}},
  author = {Shorten, Connor and Khoshgoftaar, Taghi M.},
  year = {2019},
  month = dec,
  volume = {6},
  pages = {60},
  issn = {2196-1115},
  doi = {10.1186/s40537-019-0197-0},
  abstract = {Deep convolutional neural networks have performed remarkably well on many Computer Vision tasks. However, these networks are heavily reliant on big data to avoid overfitting. Overfitting refers to the phenomenon when a network learns a function with very high variance such as to perfectly model the training data. Unfortunately, many application domains do not have access to big data, such as medical image analysis. This survey focuses on Data Augmentation, a data-space solution to the problem of limited data. Data Augmentation encompasses a suite of techniques that enhance the size and quality of training datasets such that better Deep Learning models can be built using them. The image augmentation algorithms discussed in this survey include geometric transformations, color space augmentations, kernel filters, mixing images, random erasing, feature space augmentation, adversarial training, generative adversarial networks, neural style transfer, and meta-learning. The application of augmentation methods based on GANs are heavily covered in this survey. In addition to augmentation techniques, this paper will briefly discuss other characteristics of Data Augmentation such as test-time augmentation, resolution impact, final dataset size, and curriculum learning. This survey will present existing methods for Data Augmentation, promising developments, and meta-level decisions for implementing Data Augmentation. Readers will understand how Data Augmentation can improve the performance of their models and expand limited datasets to take advantage of the capabilities of big data.},
  file = {/home/trung/GoogleDrive/Zotero/shorten et al_2019_a survey on image data augmentation for deep learning.pdf},
  journal = {Journal of Big Data},
  language = {en},
  number = {1}
}

@article{shridhar19_ComprehensiveguideBayesian,
  title = {A {{Comprehensive}} Guide to {{Bayesian Convolutional Neural Network}} with {{Variational Inference}}},
  author = {Shridhar, Kumar and Laumann, Felix and Liwicki, Marcus},
  year = {2019},
  month = jan,
  abstract = {Artificial Neural Networks are connectionist systems that perform a given task by learning on examples without having prior knowledge about the task. This is done by finding an optimal point estimate for the weights in every node. Generally, the network using point estimates as weights perform well with large datasets, but they fail to express uncertainty in regions with little or no data, leading to overconfident decisions. In this paper, Bayesian Convolutional Neural Network (BayesCNN) using Variational Inference is proposed, that introduces probability distribution over the weights. Furthermore, the proposed BayesCNN architecture is applied to tasks like Image Classification, Image Super-Resolution and Generative Adversarial Networks. The results are compared to point-estimates based architectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image CLassification task, on BSD300 dataset for Image Super Resolution task and on CIFAR10 dataset again for Generative Adversarial Network task. BayesCNN is based on Bayes by Backprop which derives a variational approximation to the true posterior. We, therefore, introduce the idea of applying two convolutional operations, one for the mean and one for the variance. Our proposed method not only achieves performances equivalent to frequentist inference in identical architectures but also incorporate a measurement for uncertainties and regularisation. It further eliminates the use of dropout in the model. Moreover, we predict how certain the model prediction is based on the epistemic and aleatoric uncertainties and empirically show how the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the training accuracy increases. Finally, we propose ways to prune the Bayesian architecture and to make it more computational and time effective.},
  archivePrefix = {arXiv},
  eprint = {1901.02731},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shridhar et al_2019_a comprehensive guide to bayesian convolutional neural network with variational inference.pdf},
  journal = {arXiv:1901.02731 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{shridhar19_ComprehensiveguideBayesiana,
  title = {A {{Comprehensive}} Guide to {{Bayesian Convolutional Neural Network}} with {{Variational Inference}}},
  author = {Shridhar, Kumar and Laumann, Felix and Liwicki, Marcus},
  year = {2019},
  month = jan,
  abstract = {Artificial Neural Networks are connectionist systems that perform a given task by learning on examples without having prior knowledge about the task. This is done by finding an optimal point estimate for the weights in every node. Generally, the network using point estimates as weights perform well with large datasets, but they fail to express uncertainty in regions with little or no data, leading to overconfident decisions. In this paper, Bayesian Convolutional Neural Network (BayesCNN) using Variational Inference is proposed, that introduces probability distribution over the weights. Furthermore, the proposed BayesCNN architecture is applied to tasks like Image Classification, Image Super-Resolution and Generative Adversarial Networks. The results are compared to point-estimates based architectures on MNIST, CIFAR-10 and CIFAR-100 datasets for Image CLassification task, on BSD300 dataset for Image Super Resolution task and on CIFAR10 dataset again for Generative Adversarial Network task. BayesCNN is based on Bayes by Backprop which derives a variational approximation to the true posterior. We, therefore, introduce the idea of applying two convolutional operations, one for the mean and one for the variance. Our proposed method not only achieves performances equivalent to frequentist inference in identical architectures but also incorporate a measurement for uncertainties and regularisation. It further eliminates the use of dropout in the model. Moreover, we predict how certain the model prediction is based on the epistemic and aleatoric uncertainties and empirically show how the uncertainty can decrease, allowing the decisions made by the network to become more deterministic as the training accuracy increases. Finally, we propose ways to prune the Bayesian architecture and to make it more computational and time effective.},
  archivePrefix = {arXiv},
  eprint = {1901.02731},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/false},
  journal = {arXiv:1901.02731 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{shu16_BottleneckConditionalDensity,
  title = {Bottleneck {{Conditional Density Estimation}}},
  author = {Shu, Rui and Bui, Hung H. and Ghavamzadeh, Mohammad},
  year = {2016},
  month = nov,
  abstract = {We introduce a new framework for training deep generative models for high-dimensional conditional density estimation. The Bottleneck Conditional Density Estimator (BCDE) is a variant of the conditional variational autoencoder (CVAE) that employs layer(s) of stochastic variables as the bottleneck between the input x and target y, where both are high-dimensional. Crucially, we propose a new hybrid training method that blends the conditional generative model with a joint generative model. Hybrid blending is the key to effective training of the BCDE, which avoids overfitting and provides a novel mechanism for leveraging unlabeled data. We show that our hybrid training procedure enables models to achieve competitive results in the MNIST quadrant prediction task in the fullysupervised setting, and sets new benchmarks in the semi-supervised regime for MNIST, SVHN, and CelebA.},
  archivePrefix = {arXiv},
  eprint = {1611.08568},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shu et al_2016_bottleneck conditional density estimation.pdf},
  journal = {arXiv:1611.08568 [cs, stat]},
  keywords = {Computer Science - Machine Learning,information,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shu18_RethinkingStyleContent,
  title = {Rethinking {{Style}} and {{Content Disentanglement}} in {{Variational Autoencoders}}},
  author = {Shu, Rui and Zhao, Shengjia and Kochenderfer, Mykel J},
  year = {2018},
  pages = {6},
  abstract = {A common test for whether a generative model learns disentangled representations is its ability to learn style and content as independent factors of variation on digit datasets. To achieve such disentanglement with variational autoencoders, the label information is often provided in either a fully-supervised or semi-supervised fashion. We show, however, that the variational objective is insufficient in explaining the observed style and content disentanglement. Furthermore, we present an empirical framework to systematically evaluate the disentanglement behavior of our models. We show that the encoder and decoder independently favor disentangled representations and that this tendency depends on the implicit regularization by stochastic gradient descent.},
  file = {/home/trung/GoogleDrive/Zotero/shu et al_2018_rethinking style and content disentanglement in variational autoencoders.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{shu19_AmortizedInferenceRegularization,
  title = {Amortized {{Inference Regularization}}},
  author = {Shu, Rui and Bui, Hung H. and Zhao, Shengjia and Kochenderfer, Mykel J. and Ermon, Stefano},
  year = {2019},
  month = jan,
  abstract = {The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.},
  archivePrefix = {arXiv},
  eprint = {1805.08913},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shu et al_2019_amortized inference regularization.pdf},
  journal = {arXiv:1805.08913 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shu19_WeaklySupervisedDisentanglement,
  title = {Weakly {{Supervised Disentanglement}} with {{Guarantees}}},
  author = {Shu, Rui and Chen, Yining and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2019},
  month = oct,
  abstract = {Learning disentangled representations that correspond to factors of variation in real-world data is critical to interpretable and human-controllable machine learning. Recently, concerns about the viability of learning disentangled representations in a purely unsupervised manner has spurred a shift toward the incorporation of weak supervision. However, there is currently no formalism that identifies when and how weak supervision will guarantee disentanglement. To address this issue, we provide a theoretical framework to assist in analyzing the disentanglement guarantees (or lack thereof) conferred by weak supervision when coupled with learning algorithms based on distribution matching. We empirically verify the guarantees and limitations of several weak supervision methods (restricted labeling, match-pairing, and rank-pairing), demonstrating the predictive power and usefulness of our theoretical framework.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.09772},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shu et al_2019_weakly supervised disentanglement with guarantees.pdf},
  journal = {arXiv:1910.09772 [cs, stat]},
  keywords = {_tablet,disentanglement,favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shui20_mathcalHDivergenceDomain,
  title = {Beyond \$\textbackslash mathcal\{\vphantom\}{{H}}\vphantom\{\}\$-{{Divergence}}: {{Domain Adaptation Theory With Jensen}}-{{Shannon Divergence}}},
  shorttitle = {Beyond \$\textbackslash mathcal\{\vphantom\}{{H}}\vphantom\{\}\$-{{Divergence}}},
  author = {Shui, Changjian and Chen, Qi and Wen, Jun and Zhou, Fan and Gagn{\'e}, Christian and Wang, Boyu},
  year = {2020},
  month = jul,
  abstract = {We reveal the incoherence between the widely-adopted empirical domain adversarial training and its generally-assumed theoretical counterpart based on H-divergence. Concretely, we find that Hdivergence is not equivalent to Jensen-Shannon divergence, the optimization objective in domain adversarial training. To this end, we establish a new theoretical framework by directly proving the upper and lower target risk bounds based on joint distributional Jensen-Shannon divergence. We further derive bi-directional upper bounds for marginal and conditional shifts. Our framework exhibits inherent flexibilities for different transfer learning problems, which is usable for various scenarios where H-divergence-based theory fails to adapt. From an algorithmic perspective, our theory enables a generic guideline unifying principles of semantic conditional matching, feature marginal matching, and label marginal shift correction. We employ algorithms for each principle and empirically validate the benefits of our framework on real datasets.},
  archivePrefix = {arXiv},
  eprint = {2007.15567},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shui et al_2020_beyond $-mathcal h $-divergence.pdf},
  journal = {arXiv:2007.15567 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{shukla19_GeometryDeepGenerative,
  title = {Geometry of {{Deep Generative Models}} for {{Disentangled Representations}}},
  author = {Shukla, Ankita and Uppal, Shagun and Bhagat, Sarthak and Anand, Saket and Turaga, Pavan},
  year = {2019},
  month = feb,
  abstract = {Deep generative models like variational autoencoders approximate the intrinsic geometry of high dimensional data manifolds by learning low-dimensional latent-space variables and an embedding function. The geometric properties of these latent spaces has been studied under the lens of Riemannian geometry; via analysis of the non-linearity of the generator function. In new developments, deep generative models have been used for learning semantically meaningful `disentangled' representations; that capture task relevant attributes while being invariant to other attributes. In this work, we explore the geometry of popular generative models for disentangled representation learning. We use several metrics to compare the properties of latent spaces of disentangled representation models in terms of class separability and curvature of the latent-space. The results we obtain establish that the class distinguishable features in the disentangled latent space exhibits higher curvature as opposed to a variational autoencoder. We evaluate and compare the geometry of three such models with variational autoencoder on two different datasets. Further, our results show that distances and interpolation in the latent space are significantly improved with Riemannian metrics derived from the curvature of the space. We expect these results will have implications on understanding how deep-networks can be made more robust, generalizable, as well as interpretable.},
  archivePrefix = {arXiv},
  eprint = {1902.06964},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/shukla et al_2019_geometry of deep generative models for disentangled representations.pdf},
  journal = {arXiv:1902.06964 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{sicks20_GeneralisedLinearModel,
  title = {A {{Generalised Linear Model Framework}} for {{Variational Autoencoders}} Based on {{Exponential Dispersion Families}}},
  author = {Sicks, Robert and Korn, Ralf and Schwaar, Stefanie},
  year = {2020},
  month = jun,
  abstract = {Although variational autoencoders (VAE) are successfully used to obtain meaningful low-dimensional representations for high-dimensional data, aspects of their loss function are not yet fully understood. We introduce a theoretical framework that is based on a connection between VAE and generalized linear models (GLM). The equality between the activation function of a VAE and the inverse of the link function of a GLM enables us to provide a systematic generalization of the loss analysis for VAE based on the assumption that the distribution of the decoder belongs to an exponential dispersion family (EDF). As a further result, we can initialize VAE nets by maximum likelihood estimates (MLE) that enhance the training performance on both synthetic and real world data sets.},
  archivePrefix = {arXiv},
  eprint = {2006.06267},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sicks et al_2020_a generalised linear model framework for variational autoencoders based on exponential dispersion families.pdf},
  journal = {arXiv:2006.06267 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{siddharth17_LearningDisentangledRepresentations,
  title = {Learning {{Disentangled Representations}} with {{Semi}}-{{Supervised Deep Generative Models}}},
  author = {Siddharth, N. and Paige, Brooks and {van de Meent}, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H. S.},
  year = {2017},
  month = nov,
  abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
  annotation = {ZSCC: 0000129},
  archivePrefix = {arXiv},
  eprint = {1706.00400},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/siddharth et al_2017_learning disentangled representations with semi-supervised deep generative models.pdf;/home/trung/Zotero/storage/W8RT9EJL/1706.html},
  journal = {arXiv:1706.00400 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@inproceedings{siddharth17_Learningdisentangledrepresentations,
  title = {Learning Disentangled Representations with Semi-Supervised Deep Generative Models},
  booktitle = {Proceedings of the 31st International Conference on Neural Information Processing Systems},
  author = {Siddharth, N. and Paige, Brooks and {van de Meent}, Jan-Willem and Desmaison, Alban and Goodman, Noah D. and Kohli, Pushmeet and Wood, Frank and Torr, Philip H.S.},
  year = {2017},
  pages = {5927--5937},
  publisher = {{Curran Associates Inc.}},
  address = {{Red Hook, NY, USA}},
  abstract = {Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.},
  file = {/home/trung/GoogleDrive/Zotero/siddharth et al_2017_learning disentangled representations with semi-supervised deep generative models2.pdf},
  isbn = {978-1-5108-6096-4},
  series = {{{NIPS}}'17}
}

@article{sikka20_DeeperLookUnsupervised,
  title = {A {{Deeper Look}} at the {{Unsupervised Learning}} of {{Disentangled Representations}} in \$\textbackslash beta\$-{{VAE}} from the {{Perspective}} of {{Core Object Recognition}}},
  author = {Sikka, Harshvardhan},
  year = {2020},
  month = apr,
  abstract = {The ability to recognize objects despite there being differences in appearance, known as Core Object Recognition, forms a critical part of human perception. While it is understood that the brain accomplishes Core Object Recognition through feedforward, hierarchical computations through the visual stream, the underlying algorithms that allow for invariant representations to form downstream is still not well understood. (DiCarlo et al., 2012) Various computational perceptual models have been built to attempt and tackle the object identification task in an artificial perceptual setting. Artificial Neural Networks, computational graphs consisting of weighted edges and mathematical operations at vertices, are loosely inspired by neural networks in the brain and have proven effective at various visual perceptual tasks, including object characterization and identification. (Pinto et al., 2008) (DiCarlo et al., 2012) For many data analysis tasks, learning representations where each dimension is statistically independent and thus disentangled from the others is useful. If the underlying generative factors of the data are also statistically independent, Bayesian inference of latent variables can form disentangled representations. This thesis constitutes a research project exploring a generalization of the Variational Autoencoder (VAE), \$\textbackslash beta\$-VAE, that aims to learn disentangled representations using variational inference. \$\textbackslash beta\$-VAE incorporates the hyperparameter \$\textbackslash beta\$, and enforces conditional independence of its bottleneck neurons, which is in general not compatible with the statistical independence of latent variables. This text examines this architecture, and provides analytical and numerical arguments, with the goal of demonstrating that this incompatibility leads to a non-monotonic inference performance in \$\textbackslash beta\$-VAE with a finite optimal \$\textbackslash beta\$.},
  archivePrefix = {arXiv},
  eprint = {2005.07114},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sikka_2020_a deeper look at the unsupervised learning of disentangled representations in $-beta$-vae from the perspective of core object recognition.pdf},
  journal = {arXiv:2005.07114 [cs, stat]},
  keywords = {_tablet,disentanglement},
  primaryClass = {cs, stat}
}

@article{silnova18_FastvariationalBayes,
  title = {Fast Variational {{Bayes}} for Heavy-Tailed {{PLDA}} Applied to i-Vectors and x-Vectors},
  author = {Silnova, Anna and Brummer, Niko and {Garcia-Romero}, Daniel and Snyder, David and Burget, Lukas},
  year = {2018},
  month = mar,
  abstract = {The standard state-of-the-art backend for text-independent speaker recognizers that use i-vectors or x-vectors, is Gaussian PLDA (G-PLDA), assisted by a Gaussianization step involving length normalization. G-PLDA can be trained with both generative or discriminative methods. It has long been known that heavy-tailed PLDA (HT-PLDA), applied without length normalization, gives similar accuracy, but at considerable extra computational cost. We have recently introduced a fast scoring algorithm for a discriminatively trained HT-PLDA backend. This paper extends that work by introducing a fast, variational Bayes, generative training algorithm. We compare old and new backends, with and without length-normalization, with i-vectors and x-vectors, on SRE'10, SRE'16 and SITW.},
  archivePrefix = {arXiv},
  eprint = {1803.09153},
  eprinttype = {arxiv},
  journal = {arXiv:1803.09153 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{silver00_generalreinforcementlearning,
  title = {A General Reinforcement Learning Algorithm That Masters Chess, Shogi and {{Go}} through Self-Play},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and Lillicrap, Timothy and Simonyan, Karen and Hassabis, Demis},
  pages = {32},
  abstract = {The game of chess is the longest-studied domain in the history of artificial intelligence. The strongest programs are based on a combination of sophisticated search techniques, domain-specific adaptations, and handcrafted evaluation functions that have been refined by human experts over several decades. By contrast, the AlphaGo Zero program recently achieved superhuman performance in the game of Go by reinforcement learning from selfplay. In this paper, we generalize this approach into a single AlphaZero algorithm that can achieve superhuman performance in many challenging games. Starting from random play and given no domain knowledge except the game rules, AlphaZero convincingly defeated a world champion program in the games of chess and shogi (Japanese chess) as well as Go.},
  file = {/home/trung/GoogleDrive/Zotero/silver et al_a general reinforcement learning algorithm that masters chess, shogi and go through self-play.pdf},
  language = {en}
}

@article{simeone18_BriefIntroductionMachine,
  title = {A {{Brief Introduction}} to {{Machine Learning}} for {{Engineers}}},
  author = {Simeone, Osvaldo},
  year = {2018},
  month = may,
  abstract = {This monograph aims at providing an introduction to key concepts, algorithms, and theoretical results in machine learning. The treatment concentrates on probabilistic models for supervised and unsupervised learning problems. It introduces fundamental concepts and algorithms by building on first principles, while also exposing the reader to more advanced topics with extensive pointers to the literature, within a unified notation and mathematical framework. The material is organized according to clearly defined categories, such as discriminative and generative models, frequentist and Bayesian approaches, exact and approximate inference, as well as directed and undirected models. This monograph is meant as an entry point for researchers with a background in probability and linear algebra.},
  archivePrefix = {arXiv},
  eprint = {1709.02840},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/simeone_2018_a brief introduction to machine learning for engineers.pdf},
  journal = {arXiv:1709.02840 [cs, math, stat]},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{simo-serra16_Learningsimplifyfully,
  title = {Learning to Simplify: Fully Convolutional Networks for Rough Sketch Cleanup},
  shorttitle = {Learning to Simplify},
  author = {{Simo-Serra}, Edgar and Iizuka, Satoshi and Sasaki, Kazuma and Ishikawa, Hiroshi},
  year = {2016},
  month = jul,
  volume = {35},
  pages = {1--11},
  issn = {07300301},
  doi = {10.1145/2897824.2925972},
  file = {/home/trung/GoogleDrive/Zotero/simo-serra et al_2016_learning to simplify.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {4}
}

@techreport{simon19_DrivAERIdentificationdriving,
  title = {{{DrivAER}}: {{Identification}} of Driving Transcriptional Programs in Single-Cell {{RNA}} Sequencing Data},
  shorttitle = {{{DrivAER}}},
  author = {Simon, Lukas M. and Yan, Fangfang and Zhao, Zhongming},
  year = {2019},
  month = dec,
  institution = {{Bioinformatics}},
  doi = {10.1101/864165},
  abstract = {Abstract           Single cell RNA sequencing (scRNA-seq) unfolds complex transcriptomic data sets into detailed cellular maps. Despite recent success, there is a pressing need for specialized methods tailored towards the functional interpretation of these cellular maps. Here, we present DrivAER, a machine learning approach that scores annotated gene sets based on their relevance to user-specified outcomes such as pseudotemporal ordering or disease status. We demonstrate that DrivAER extracts the key driving pathways and transcription factors that regulate complex biological processes from scRNA-seq data.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/simon et al_2019_drivaer.pdf},
  language = {en},
  type = {Preprint}
}

@article{simonovsky18_GraphVAEGenerationSmall,
  title = {{{GraphVAE}}: {{Towards Generation}} of {{Small Graphs Using Variational Autoencoders}}},
  shorttitle = {{{GraphVAE}}},
  author = {Simonovsky, Martin and Komodakis, Nikos},
  year = {2018},
  month = feb,
  abstract = {Deep learning on graphs has become a popular research topic with many applications. However, past work has concentrated on learning graph embedding tasks, which is in contrast with advances in generative models for images and text. Is it possible to transfer this progress to the domain of graphs? We propose to sidestep hurdles associated with linearization of such discrete structures by having a decoder output a probabilistic fullyconnected graph of a predefined maximum size directly at once. Our method is formulated as a variational autoencoder. We evaluate on the challenging task of molecule generation.},
  annotation = {ZSCC: 0000130},
  archivePrefix = {arXiv},
  eprint = {1802.03480},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/6K9KKBRV/Simonovsky and Komodakis - 2018 - GraphVAE Towards Generation of Small Graphs Using.pdf},
  journal = {arXiv:1802.03480 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{singh04_Chemicalantioxidantantifungal,
  title = {Chemical, Antioxidant and Antifungal Activities of Volatile Oil of Black Pepper and Its Acetone Extract},
  author = {Singh, Gurdip and Marimuthu, P and Catalan, C and {deLampasona}, Mp},
  year = {2004},
  month = nov,
  volume = {84},
  pages = {1878--1884},
  issn = {0022-5142, 1097-0010},
  doi = {10.1002/jsfa.1863},
  abstract = {GC and GC-MS analysis of volatile oil obtained from Piper nigrum L resulted in the identification of 49 components accounting for 99.39\% of the total amount, and the major components were {$\beta$}caryophyllene (24.24\%), limonene (16.88\%), sabinene (13.01\%), {$\beta$}-bisabolene (7.69\%) and {$\alpha$}-copaene (6.3\%). The acetone extract of pepper showed the presence of 18 components accounting for 75.59\% of the total amount. Piperine (33.53\%), piperolein B (13.73\%), piperamide (3.43\%) and guineensine (3.23\%) were the major components. The oil was found to be 100\% effective in controlling the mycelial growth of Fusarium graminearum in inverted petriplate technique. The acetone extract retarded 100\% mycelial growth of Penicillium viridcatum and Aspergillus ochraceus in food-poisoning technique. Volatile oil and acetone extract were identified as a better antioxidant for linseed oil, in comparison with butylated hydroxyanisole (BHA) and butylated hydroxytoluene (BHT).},
  file = {/home/trung/GoogleDrive/Zotero/singh et al_2004_chemical, antioxidant and antifungal activities of volatile oil of black pepper and its acetone extract.pdf},
  journal = {Journal of the Science of Food and Agriculture},
  language = {en},
  number = {14}
}

@article{singh19_SequentialNeuralProcesses,
  title = {Sequential {{Neural Processes}}},
  author = {Singh, Gautam and Yoon, Jaesik and Son, Youngsung and Ahn, Sungjin},
  year = {2019},
  month = jun,
  abstract = {Neural processes combine the strengths of neural networks and Gaussian processes to achieve both flexible learning and fast prediction of stochastic processes. However, neural processes do not consider the temporal dependency structure of underlying processes and thus are limited in modeling a large class of problems with temporal structure. In this paper, we propose Sequential Neural Processes (SNP). By incorporating temporal state-transition model into neural processes, the proposed model extends the potential of neural processes to modeling dynamic stochastic processes. In applying SNP to dynamic 3D scene modeling, we also introduce the Temporal Generative Query Networks. To our knowledge, this is the first 4D model that can deal with temporal dynamics of 3D scenes. In experiments, we evaluate the proposed methods in dynamic (non-stationary) regression and 4D scene inference and rendering.},
  archivePrefix = {arXiv},
  eprint = {1906.10264},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/singh et al_2019_sequential neural processes.pdf;/home/trung/Zotero/storage/WJKGHI45/1906.html},
  journal = {arXiv:1906.10264 [cs, stat]},
  keywords = {Computer Science - Machine Learning,gaussian process,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@inproceedings{sinha18_HierarchicalNeuralAttentionbased,
  title = {A {{Hierarchical Neural Attention}}-Based {{Text Classifier}}},
  booktitle = {{{EMNLP}}},
  author = {Sinha, Koustuv and Dong, Yue and Cheung, Jackie Chi Kit and Ruths, Derek},
  year = {2018},
  doi = {10.18653/v1/d18-1094},
  abstract = {Deep neural networks have been displaying superior performance over traditional supervised classifiers in text classification. They learn to extract useful features automatically when sufficient amount of data is presented. However, along with the growth in the number of documents comes the increase in the number of categories, which often results in poor performance of the multiclass classifiers. In this work, we use external knowledge in the form of topic category taxonomies to aide the classification by introducing a deep hierarchical neural attention-based classifier. Our model performs better than or comparable to state-of-the-art hierarchical models at significantly lower computational cost while maintaining high interpretability.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/GoogleDrive/Zotero/sinha et al_2018_a hierarchical neural attention-based text classifier.pdf},
  keywords = {Algorithmic efficiency,Artificial neural network,attention,Bayesian network,Computation,Document classification,Graphics processing unit,Hierarchical classifier,Hierarchical database model,Linear classifier,Living document,Supervised learning}
}

@article{sinha19_SmallGANSpeedingGAN,
  title = {Small-{{GAN}}: {{Speeding Up GAN Training Using Core}}-Sets},
  shorttitle = {Small-{{GAN}}},
  author = {Sinha, Samarth and Zhang, Han and Goyal, Anirudh and Bengio, Yoshua and Larochelle, Hugo and Odena, Augustus},
  year = {2019},
  month = oct,
  abstract = {Recent work by Brock et al. (2018) suggests that Generative Adversarial Networks (GANs) benefit disproportionately from large mini-batch sizes. Unfortunately, using large batches is slow and expensive on conventional hardware. Thus, it would be nice if we could generate batches that were effectively large though actually small. In this work, we propose a method to do this, inspired by the use of Coreset-selection in active learning. When training a GAN, we draw a large batch of samples from the prior and then compress that batch using Coreset-selection. To create effectively large batches of 'real' images, we create a cached dataset of Inception activations of each training image, randomly project them down to a smaller dimension, and then use Coreset-selection on those projected activations at training time. We conduct experiments showing that this technique substantially reduces training time and memory usage for modern GAN variants, that it reduces the fraction of dropped modes in a synthetic dataset, and that it allows GANs to reach a new state of the art in anomaly detection.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.13540},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sinha et al_2019_small-gan.pdf},
  journal = {arXiv:1910.13540 [cs, stat]},
  keywords = {adversarial,computational,Computer Science - Machine Learning,gan,small,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sinha19_VariationalAdversarialActive,
  title = {Variational {{Adversarial Active Learning}}},
  author = {Sinha, Samarth and Ebrahimi, Sayna and Darrell, Trevor},
  year = {2019},
  month = oct,
  abstract = {Active learning aims to develop label-efficient algorithms by sampling the most representative queries to be labeled by an oracle. We describe a pool-based semi-supervised active learning algorithm that implicitly learns this sampling mechanism in an adversarial manner. Unlike conventional active learning algorithms, our approach is task agnostic, i.e., it does not depend on the performance of the task for which we are trying to acquire labeled data. Our method learns a latent space using a variational autoencoder (VAE) and an adversarial network trained to discriminate between unlabeled and labeled data. The mini-max game between the VAE and the adversarial network is played such that while the VAE tries to trick the adversarial network into predicting that all data points are from the labeled pool, the adversarial network learns how to discriminate between dissimilarities in the latent space. We extensively evaluate our method on various image classification and semantic segmentation benchmark datasets and establish a new state of the art on \$\textbackslash text\{CIFAR10/100\}\$, \$\textbackslash text\{Caltech-256\}\$, \$\textbackslash text\{ImageNet\}\$, \$\textbackslash text\{Cityscapes\}\$, and \$\textbackslash text\{BDD100K\}\$. Our results demonstrate that our adversarial approach learns an effective low dimensional latent space in large-scale settings and provides for a computationally efficient sampling method. Our code is available at https://github.com/sinhasam/vaal.},
  archivePrefix = {arXiv},
  eprint = {1904.00370},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sinha et al_2019_variational adversarial active learning.pdf;/home/trung/GoogleDrive/Zotero/sinha et al_2019_variational adversarial active learning2.pdf;/home/trung/Zotero/storage/AXAAWYV5/1904.html;/home/trung/Zotero/storage/VGKBIAXW/1904.html},
  journal = {arXiv:1904.00370 [cs, stat]},
  keywords = {adversarial,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{sinha20_ConsistencyRegularizationVariational,
  title = {Consistency {{Regularization}} for {{Variational Auto}}-{{Encoders}}},
  author = {Sinha, Samarth and Odena, Augustus and Dieng, Adji B},
  year = {2020},
  pages = {6},
  abstract = {Variational auto-encoders (VAEs) are a powerful approach to unsupervised learning. They enable scalable approximate posterior inference in latent-variable models using variational inference (VI). A VAE posits a variational family parameterized by a deep neural network\textemdash called an encoder\textemdash that takes data as input. This encoder is shared across all the observations, which amortizes the cost of inference. However the encoder of a VAE has the undesirable property that it maps a given observation and a semanticpreserving transformation of it to different latent representations. This ``inconsistency" in the representations induced by the encoder negatively affects generalization. In this paper, we propose a regularization method to enforce consistency in VAEs. The idea is to minimize the KullbackLeibler (KL) divergence between the variational distribution when conditioning on the observation and the variational distribution when conditioning on a random semantic-preserving transformation of this observation. This regularization is applicable to any VAE. In our experiments we apply it to three different VAE variants on several benchmark datasets and found it always improves generalization but also yields more interpretable latent variables as measured by mutual information.},
  file = {/home/trung/GoogleDrive/Zotero/sinha et al_2020_consistency regularization for variational auto-encoders.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{sinha20_DIBSDiversityinducing,
  title = {{{DIBS}}: {{Diversity}} Inducing {{Information Bottleneck}} in {{Model Ensembles}}},
  shorttitle = {{{DIBS}}},
  author = {Sinha, Samarth and Bharadhwaj, Homanga and Goyal, Anirudh and Larochelle, Hugo and Garg, Animesh and Shkurti, Florian},
  year = {2020},
  month = mar,
  abstract = {Although deep learning models have achieved state-of-the-art performance on a number of vision tasks, generalization over high dimensional multi-modal data, and reliable predictive uncertainty estimation are still active areas of research. Bayesian approaches including Bayesian Neural Nets (BNNs) do not scale well to modern computer vision tasks, as they are difficult to train, and have poor generalization under dataset-shift. This motivates the need for effective ensembles which can generalize and give reliable uncertainty estimates. In this paper, we target the problem of generating effective ensembles of neural networks by encouraging diversity in prediction. We explicitly optimize a diversity inducing adversarial loss for learning the stochastic latent variables and thereby obtain diversity in the output predictions necessary for modeling multi-modal data. We evaluate our method on benchmark datasets: MNIST, CIFAR100, TinyImageNet and MIT Places 2, and compared to the most competitive baselines show significant improvements in classification accuracy, under a shift in the data distribution and in out-of-distribution detection. Code will be released in this url https://github.com/rvl-lab-utoronto/dibs},
  archivePrefix = {arXiv},
  eprint = {2003.04514},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sinha et al_2020_dibs.pdf},
  journal = {arXiv:2003.04514 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{sinha20_MaximumEntropyModels,
  title = {Maximum {{Entropy Models}} for {{Fast Adaptation}}},
  author = {Sinha, Samarth and Goyal, Anirudh and Garg, Animesh},
  year = {2020},
  month = jun,
  abstract = {Deep Neural Networks have shown great promise on a variety of downstream tasks; but their ability to adapt to new data and tasks remains a challenging problem. The ability of a model to perform few-shot adaptation to a novel task is important for the scalability and deployment of machine learning models. Recent work has shown that the learned features in a neural network follow a normal distribution [41], which thereby results in a strong prior on the downstream task. This implicit overfitting to data from training tasks limits the ability to generalize and adapt to unseen tasks at test time. This also highlights the importance of learning task-agnostic representations from data. In this paper, we propose a regularization scheme using a max-entropy prior on the learned features of a neural network; such that the extracted features make minimal assumptions about the training data. We evaluate our method on adaptation to unseen tasks by performing experiments in 4 distinct settings. We find that our method compares favourably against multiple strong baselines across all of these experiments.},
  archivePrefix = {arXiv},
  eprint = {2006.16524},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sinha et al_2020_maximum entropy models for fast adaptation.pdf},
  journal = {arXiv:2006.16524 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@inproceedings{sivic08_Unsuperviseddiscoveryvisual,
  title = {Unsupervised Discovery of Visual Object Class Hierarchies},
  booktitle = {2008 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Sivic, Josef and Russell, Bryan C. and Zisserman, Andrew and Freeman, William T. and Efros, Alexei A.},
  year = {2008},
  month = jun,
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1109/CVPR.2008.4587622},
  abstract = {Objects in the world can be arranged into a hierarchy based on their semantic meaning (e.g. organism \textendash{} animal \textendash{} feline \textendash{} cat). What about defining a hierarchy based on the visual appearance of objects? This paper investigates ways to automatically discover a hierarchical structure for the visual world from a collection of unlabeled images. Previous approaches for unsupervised object and scene discovery focused on partitioning the visual data into a set of nonoverlapping classes of equal granularity. In this work, we propose to group visual objects using a multi-layer hierarchy tree that is based on common visual elements. This is achieved by adapting to the visual domain the generative Hierarchical Latent Dirichlet Allocation (hLDA) model previously used for unsupervised discovery of topic hierarchies in text. Images are modeled using quantized local image regions as analogues to words in text. Employing the multiple segmentation framework of Russell et al. [22], we show that meaningful object hierarchies, together with object segmentations, can be automatically learned from unlabeled and unsegmented image collections without supervision. We demonstrate improved object classification and localization performance using hLDA over the previous non-hierarchical method on the MSRC dataset [33].},
  annotation = {ZSCC: 0000242},
  file = {/home/trung/GoogleDrive/Zotero/sivic et al_2008_unsupervised discovery of visual object class hierarchies.pdf},
  isbn = {978-1-4244-2242-5},
  language = {en}
}

@incollection{sizov14_UnifyingProbabilisticLinear,
  title = {Unifying {{Probabilistic Linear Discriminant Analysis Variants}} in {{Biometric Authentication}}},
  booktitle = {Structural, {{Syntactic}}, and {{Statistical Pattern Recognition}}},
  author = {Sizov, Aleksandr and Lee, Kong Aik and Kinnunen, Tomi},
  editor = {Fr{\"a}nti, Pasi and Brown, Gavin and Loog, Marco and Escolano, Francisco and Pelillo, Marcello},
  year = {2014},
  volume = {8621},
  pages = {464--475},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-662-44415-3_47},
  abstract = {Probabilistic linear discriminant analysis (PLDA) is commonly used in biometric authentication. We review three PLDA variants \textemdash{} standard, simplified and two-covariance \textemdash{} and show how they are related. These clarifications are important because the variants were introduced in literature without argumenting their benefits. We analyse their predictive power, covariance structure and provide scalable algorithms for straightforward implementation of all the three variants. Experiments involve state-of-the-art speaker verification with i-vector features.},
  file = {/home/trung/GoogleDrive/Zotero/sizov et al_2014_unifying probabilistic linear discriminant analysis variants in biometric authentication.pdf},
  isbn = {978-3-662-44414-6 978-3-662-44415-3},
  language = {en}
}

@inproceedings{Skopek2020Mixed-curvature,
  title = {Mixed-Curvature Variational Autoencoders},
  booktitle = {International Conference on Learning Representations},
  author = {Skopek, Ondrej and Ganea, Octavian-Eugen and B{\'e}cigneul, Gary},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/skopek et al_2020_mixed-curvature variational autoencoders.pdf}
}

@article{slack20_HowMuchShould,
  title = {How {{Much Should I Trust You}}? {{Modeling Uncertainty}} of {{Black Box Explanations}}},
  shorttitle = {How {{Much Should I Trust You}}?},
  author = {Slack, Dylan and Hilgard, Sophie and Singh, Sameer and Lakkaraju, Himabindu},
  year = {2020},
  month = aug,
  abstract = {As local explanations of black box models are increasingly being employed to establish model credibility in high stakes settings, it is important to ensure that these explanations are accurate and reliable. However, local explanations generated by existing techniques are often prone to high variance. Further, these techniques are computationally inefficient, require significant hyper-parameter tuning, and provide little insight into the quality of the resulting explanations. By identifying lack of uncertainty modeling as the main cause of these challenges, we propose a novel Bayesian framework that produces explanations that go beyond point-wise estimates of feature importance. We instantiate this framework to generate Bayesian versions of LIME and KernelSHAP. In particular, we estimate credible intervals (CIs) that capture the uncertainty associated with each feature importance in local explanations. These credible intervals are tight when we have high confidence in the feature importances of a local explanation. The CIs are also informative both for estimating how many perturbations we need to sample \textemdash{} sampling can proceed until the CIs are sufficiently narrow \textemdash{} and where to sample \textemdash{} sampling in regions with high predictive uncertainty leads to faster convergence. Experimental evaluation with multiple real world datasets and user studies demonstrate the efficacy of our framework and the resulting explanations.},
  archivePrefix = {arXiv},
  eprint = {2008.05030},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/slack et al_2020_how much should i trust you.pdf},
  journal = {arXiv:2008.05030 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{slonim05_Informationbasedclustering,
  title = {Information-Based Clustering},
  author = {Slonim, Noam and Atwal, Gurinder Singh and Tka{\v c}ik, Ga{\v s}per and Bialek, William},
  year = {2005},
  volume = {102},
  pages = {18297--18302},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  doi = {10.1073/pnas.0507432102},
  abstract = {In an age of increasingly large data sets, investigators in many different disciplines have turned to clustering as a tool for data analysis and exploration. Existing clustering methods, however, typically depend on several nontrivial assumptions about the structure of data. Here, we reformulate the clustering problem from an information theoretic perspective that avoids many of these assumptions. In particular, our formulation obviates the need for defining a cluster prototype, does not require an a priori similarity metric, is invariant to changes in the representation of the data, and naturally captures nonlinear relations. We apply this approach to different domains and find that it consistently produces clusters that are more coherent than those extracted by existing algorithms. Finally, our approach provides a way of clustering based on collective notions of similarity rather than the traditional pairwise measures.},
  eprint = {https://www.pnas.org/content/102/51/18297.full.pdf},
  file = {/home/trung/GoogleDrive/Zotero/slonim et al_2005_information-based clustering.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {information},
  number = {51}
}

@article{smirnov14_ComparisonRegularizationMethodsa,
  title = {Comparison of {{Regularization Methods}} for {{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  author = {Smirnov, Evgeny A. and Timoshenko, Denis M. and Andrianov, Serge N.},
  year = {2014},
  volume = {6},
  pages = {89--94},
  issn = {22126716},
  doi = {10.1016/j.aasri.2014.05.013},
  abstract = {Large and Deep Convolutional Neural Networks achieve good results in image classification tasks, but they need methods to prevent overfitting. In this paper we compare performance of different regularization techniques on ImageNet Large Scale Visual Recognition Challenge 2013. We show empirically that Dropout works better than DropConnect on ImageNet dataset.},
  file = {/home/trung/Zotero/storage/RL9DVNCX/Smirnov et al. - 2014 - Comparison of Regularization Methods for ImageNet},
  journal = {AASRI Procedia},
  language = {en}
}

@article{smith00_ExplorationStyleTransfer,
  title = {An {{Exploration}} of {{Style Transfer Using Deep Neural Networks}}},
  author = {Smith, Cameron Y},
  pages = {65},
  abstract = {Convolutional Neural Networks and Graphics Processing Units have been at the core of a paradigm shift in computer vision research that some researchers have called ``the algorithmic perception revolution.'' This thesis presents the implementation and analysis of several techniques for performing artistic style transfer using a Convolutional Neural Network architecture trained for large-scale image recognition tasks. We present an implementation of an existing algorithm for artistic style transfer in images and video. The neural algorithm separates and recombines the style and content of arbitrary images. Additionally, we present an extension of the algorithm to perform weighted artistic style transfer.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/smith_an exploration of style transfer using deep neural networks.pdf},
  language = {en}
}

@article{smith18_BayesianPerspectiveGeneralization,
  title = {A {{Bayesian Perspective}} on {{Generalization}} and {{Stochastic Gradient Descent}}},
  author = {Smith, Samuel L. and Le, Quoc V.},
  year = {2018},
  month = feb,
  abstract = {We consider two questions at the heart of machine learning; how can we predict if a minimum will generalize to the test set, and why does stochastic gradient descent find minima that generalize well? Our work responds to Zhang et al. (2016), who showed deep neural networks can easily memorize randomly labeled training data, despite generalizing well on real labels of the same inputs. We show that the same phenomenon occurs in small linear models. These observations are explained by the Bayesian evidence, which penalizes sharp minima but is invariant to model parameterization. We also demonstrate that, when one holds the learning rate fixed, there is an optimum batch size which maximizes the test set accuracy. We propose that the noise introduced by small mini-batches drives the parameters towards minima whose evidence is large. Interpreting stochastic gradient descent as a stochastic differential equation, we identify the "noise scale" \$g = \textbackslash epsilon (\textbackslash frac\{N\}\{B\} - 1) \textbackslash approx \textbackslash epsilon N/B\$, where \$\textbackslash epsilon\$ is the learning rate, \$N\$ the training set size and \$B\$ the batch size. Consequently the optimum batch size is proportional to both the learning rate and the size of the training set, \$B\_\{opt\} \textbackslash propto \textbackslash epsilon N\$. We verify these predictions empirically.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1710.06451},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/smith et al_2018_a bayesian perspective on generalization and stochastic gradient descent.pdf;/home/trung/Zotero/storage/CZHQ2CTR/1710.html},
  journal = {arXiv:1710.06451 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{smith18_UnderstandingMeasuresUncertainty,
  title = {Understanding {{Measures}} of {{Uncertainty}} for {{Adversarial Example Detection}}},
  author = {Smith, Lewis and Gal, Yarin},
  year = {2018},
  month = mar,
  abstract = {Measuring uncertainty is a promising technique for detecting adversarial examples, crafted inputs on which the model predicts an incorrect class with high confidence. But many measures of uncertainty exist, including predictive en- tropy and mutual information, each capturing different types of uncertainty. We study these measures, and shed light on why mutual information seems to be effective at the task of adversarial example detection. We highlight failure modes for MC dropout, a widely used approach for estimating uncertainty in deep models. This leads to an improved understanding of the drawbacks of current methods, and a proposal to improve the quality of uncertainty estimates using probabilistic model ensembles. We give illustrative experiments using MNIST to demonstrate the intuition underlying the different measures of uncertainty, as well as experiments on a real world Kaggle dogs vs cats classification dataset.},
  archivePrefix = {arXiv},
  eprint = {1803.08533},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/smith et al_2018_understanding measures of uncertainty for adversarial example detection.pdf},
  journal = {arXiv:1803.08533 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{smith19_OneInventorRace,
  title = {One {{Inventor}}'s {{Race}} to {{Manage His Parkinson}}'s {{Disease With}} an {{App}}},
  author = {Smith, Peter Andrey},
  year = {2019},
  month = may,
  abstract = {Ray Finucane was dubbed an engineering `wizard.' Managing his own brain is his toughest challenge yet.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/XKP9MWQJ/one-inventors-race-to-treat-parkinson-s-with-an-app-f2bf197ee70.html},
  howpublished = {https://onezero.medium.com/one-inventors-race-to-treat-parkinson-s-with-an-app-f2bf197ee70},
  journal = {Medium},
  language = {en}
}

@inproceedings{snyder19_SpeakerRecognitionMultispeaker,
  title = {Speaker {{Recognition}} for {{Multi}}-Speaker {{Conversations Using X}}-Vectors},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Snyder, David and {Garcia-Romero}, Daniel and Sell, Gregory and McCree, Alan and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2019},
  month = may,
  pages = {5796--5800},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683760},
  abstract = {Recently, deep neural networks that map utterances to fixeddimensional embeddings have emerged as the state-of-the-art in speaker recognition. Our prior work introduced x-vectors, an embedding that is very effective for both speaker recognition and diarization. This paper combines our previous work and applies it to the problem of speaker recognition on multi-speaker conversations. We measure performance on Speakers in the Wild and report what we believe are the best published error rates on this dataset. Moreover, we find that diarization substantially reduces error rate when there are multiple speakers, while maintaining excellent performance on single-speaker recordings. Finally, we introduce an easily implemented method to remove the domain-sensitive threshold typically used in the clustering stage of a diarization system. The proposed method is more robust to domain shifts, and achieves similar results to those obtained using a well-tuned threshold.},
  file = {/home/trung/GoogleDrive/Zotero/snyder et al_2019_speaker recognition for multi-speaker conversations using x-vectors.pdf},
  isbn = {978-1-4799-8131-1},
  language = {en}
}

@article{so19_EvolvedTransformer,
  title = {The {{Evolved Transformer}}},
  author = {So, David R. and Liang, Chen and Le, Quoc V.},
  year = {2019},
  month = may,
  abstract = {Recent works have highlighted the strength of the Transformer architecture on sequence tasks while, at the same time, neural architecture search (NAS) has begun to outperform human-designed models. Our goal is to apply NAS to search for a better alternative to the Transformer. We first construct a large search space inspired by the recent advances in feed-forward sequence models and then run evolutionary architecture search with warm starting by seeding our initial population with the Transformer. To directly search on the computationally expensive WMT 2014 EnglishGerman translation task, we develop the Progressive Dynamic Hurdles method, which allows us to dynamically allocate more resources to more promising candidate models. The architecture found in our experiments \textendash{} the Evolved Transformer \textendash{} demonstrates consistent improvement over the Transformer on four well-established language tasks: WMT 2014 English-German, WMT 2014 English-French, WMT 2014 EnglishCzech and LM1B. At a big model size, the Evolved Transformer establishes a new state-ofthe-art BLEU score of 29.8 on WMT'14 EnglishGerman; at smaller sizes, it achieves the same quality as the original ''big'' Transformer with 37.6\% less parameters and outperforms the Transformer by 0.7 BLEU at a mobile-friendly model size of {$\sim$}7M parameters.},
  archivePrefix = {arXiv},
  eprint = {1901.11117},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/C9PH8J7R/So et al. - 2019 - The Evolved Transformer.pdf},
  journal = {arXiv:1901.11117 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sobolev19_ImportanceWeightedHierarchical,
  title = {Importance {{Weighted Hierarchical Variational Inference}}},
  author = {Sobolev, Artem and Vetrov, Dmitry},
  year = {2019},
  month = may,
  abstract = {Variational Inference is a powerful tool in the Bayesian modeling toolkit, however, its effectiveness is determined by the expressivity of the utilized variational distributions in terms of their ability to match the true posterior distribution. In turn, the expressivity of the variational family is largely limited by the requirement of having a tractable density function. To overcome this roadblock, we introduce a new family of variational upper bounds on a marginal log density in the case of hierarchical models (also known as latent variable models). We then give an upper bound on the Kullback-Leibler divergence and derive a family of increasingly tighter variational lower bounds on the otherwise intractable standard evidence lower bound for hierarchical variational distributions, enabling the use of more expressive approximate posteriors. We show that previously known methods, such as Hierarchical Variational Models, Semi-Implicit Variational Inference and Doubly Semi-Implicit Variational Inference can be seen as special cases of the proposed approach, and empirically demonstrate superior performance of the proposed method in a set of experiments.},
  archivePrefix = {arXiv},
  eprint = {1905.03290},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sobolev et al_2019_importance weighted hierarchical variational inference.pdf;/home/trung/Zotero/storage/V7LWQRIG/1905.html},
  journal = {arXiv:1905.03290 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{soeken03_Herbalmedicinestreatment,
  title = {Herbal Medicines for the Treatment of Rheumatoid Arthritis: A Systematic Review},
  shorttitle = {Herbal Medicines for the Treatment of Rheumatoid Arthritis},
  author = {Soeken, K. L.},
  year = {2003},
  month = may,
  volume = {42},
  pages = {652--659},
  issn = {14602172},
  doi = {10.1093/rheumatology/keg183},
  abstract = {Objective. With the growing interest in herbal therapies among persons with rheumatoid arthritis, there exists a need for investigation into their safety and efficacy. The purpose of this study was to conduct a systematic review to examine the evidence for the use of herbal medicines for RA based on randomized clinical trials (RCTs). Methods. A computerized search of eight electronic databases and the bibliographies of identified articles resulted in 14 studies meeting the inclusion criteria. Two raters independently extracted data and rated the trials for quality. Results. There is moderate support for c-linolenic acid (GLA), which is found in some herbal medicines, for reducing pain, tender joint count and stiffness. For other herbal medicines there was only a single RCT available, resulting in weak evidence. In general, herbal preparations were relatively safe to use. Conclusions. Given the number of herbal medicines promoted for RA, further research is needed to examine their efficacy, safety and potential drug interactions.},
  file = {/home/trung/GoogleDrive/Zotero/soeken_2003_herbal medicines for the treatment of rheumatoid arthritis.pdf},
  journal = {Rheumatology},
  language = {en},
  number = {5}
}

@article{sohl-dickstein16_NoteEquivalenceRecurrent,
  title = {Note on {{Equivalence Between Recurrent Neural Network Time Series Models}} and {{Variational Bayesian Models}}},
  author = {{Sohl-Dickstein}, Jascha and Kingma, Diederik P.},
  year = {2016},
  month = jun,
  abstract = {We observe that the standard log likelihood training objective for a Recurrent Neural Network (RNN) model of time series data is equivalent to a variational Bayesian training objective, given the proper choice of generative and inference models. This perspective may motivate extensions to both RNNs and variational Bayesian models. We propose one such extension, where multiple particles are used for the hidden state of an RNN, allowing a natural representation of uncertainty or multimodality.},
  archivePrefix = {arXiv},
  eprint = {1504.08025},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sohl-dickstein et al_2016_note on equivalence between recurrent neural network time series models and variational bayesian models.pdf},
  journal = {arXiv:1504.08025 [cs]},
  primaryClass = {cs}
}

@article{sohn20_FixMatchSimplifyingSemiSupervised,
  title = {{{FixMatch}}: {{Simplifying Semi}}-{{Supervised Learning}} with {{Consistency}} and {{Confidence}}},
  shorttitle = {{{FixMatch}}},
  author = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
  year = {2020},
  month = jan,
  abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2001.07685},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sohn et al_2020_fixmatch.pdf;/home/trung/Zotero/storage/ZYYVMY6T/2001.html},
  journal = {arXiv:2001.07685 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{sohn20_FixMatchSimplifyingSemiSuperviseda,
  title = {{{FixMatch}}: {{Simplifying Semi}}-{{Supervised Learning}} with {{Consistency}} and {{Confidence}}},
  shorttitle = {{{FixMatch}}},
  author = {Sohn, Kihyuk and Berthelot, David and Li, Chun-Liang and Zhang, Zizhao and Carlini, Nicholas and Cubuk, Ekin D. and Kurakin, Alex and Zhang, Han and Raffel, Colin},
  year = {2020},
  month = jan,
  abstract = {Semi-supervised learning (SSL) provides an effective means of leveraging unlabeled data to improve a model's performance. In this paper, we demonstrate the power of a simple combination of two common SSL methods: consistency regularization and pseudo-labeling. Our algorithm, FixMatch, first generates pseudo-labels using the model's predictions on weakly-augmented unlabeled images. For a given image, the pseudo-label is only retained if the model produces a high-confidence prediction. The model is then trained to predict the pseudo-label when fed a strongly-augmented version of the same image. Despite its simplicity, we show that FixMatch achieves state-of-the-art performance across a variety of standard semi-supervised learning benchmarks, including 94.93\% accuracy on CIFAR-10 with 250 labels and 88.61\% accuracy with 40 -- just 4 labels per class. Since FixMatch bears many similarities to existing SSL methods that achieve worse performance, we carry out an extensive ablation study to tease apart the experimental factors that are most important to FixMatch's success. We make our code available at https://github.com/google-research/fixmatch.},
  archivePrefix = {arXiv},
  eprint = {2001.07685},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sohn et al_2020_fixmatch2.pdf},
  journal = {arXiv:2001.07685 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{soleimani16_SemisupervisedMultiLabelTopic,
  title = {Semi-Supervised {{Multi}}-{{Label Topic Models}} for {{Document Classification}} and {{Sentence Labeling}}},
  booktitle = {Proceedings of the 25th {{ACM International}} on {{Conference}} on {{Information}} and {{Knowledge Management}}},
  author = {Soleimani, Hossein and Miller, David J.},
  year = {2016},
  month = oct,
  pages = {105--114},
  publisher = {{ACM}},
  address = {{Indianapolis Indiana USA}},
  doi = {10.1145/2983323.2983752},
  abstract = {Extracting parts of a text document relevant to a class label is a critical information retrieval task. We propose a semi-supervised multi-label topic model for jointly achieving document and sentence-level class inferences. Under our model, each sentence is associated with only a subset of the document's labels (including possibly none of them), with the label set of the document the union of the labels of all of its sentences. For training, we use both labeled documents, and, typically, a larger set of unlabeled documents. Our model, in a semisupervised fashion, discovers the topics present, learns associations between topics and class labels, predicts labels for new (or unlabeled) documents, and determines label associations for each sentence in every document. For learning, our model does not require any ground-truth labels on sentences. We develop a Hamiltonian Monte Carlo based algorithm for efficiently sampling from the joint label distribution over all sentences, a very high-dimensional discrete space. Our experiments show that our approach outperforms several benchmark methods with respect to both document and sentence-level classification, as well as test set log-likelihood. All code for replicating our experiments is available from https://github.com/hsoleimani/ MLTM.},
  file = {/home/trung/GoogleDrive/Zotero/soleimani et al_2016_semi-supervised multi-label topic models for document classification and sentence labeling.pdf},
  isbn = {978-1-4503-4073-1},
  language = {en}
}

@article{solon-biet19_Branchedchainamino,
  title = {Branched Chain Amino Acids Impact Health and Lifespan Indirectly via Amino Acid Balance and Appetite Control},
  author = {{Solon-Biet}, Samantha M and Cogger, Victoria C and Pulpitel, Tamara and Wahl, Devin and Clark, Ximonie and Bagley, Elena and Gregoriou, Gabrielle C and Senior, Alistair M and Wang, Qiao-Ping and Brandon, Amanda E and Perks, Ruth and O'Sullivan, John and Koay, Yen Chin and {Bell-Anderson}, Kim and Kebede, Melkam and Yau, Belinda and Atkinson, Clare and Svineng, Gunbjorg and Dodgson, Timothy and Wali, Jibran A and Piper, Matthew D W and Juricic, Paula and Partridge, Linda and Rose, Adam J and Raubenheimer, David and Cooney, Gregory J and Le Couteur, David G and Simpson, Stephen J},
  year = {2019},
  month = may,
  volume = {1},
  pages = {532--545},
  issn = {2522-5812},
  doi = {10.1038/s42255-019-0059-2},
  abstract = {Elevated branched chain amino acids (BCAAs) are associated with obesity and insulin resistance. How long-term dietary BCAAs impact late-life health and lifespan is unknown. Here, we show that when dietary BCAAs are varied against a fixed, isocaloric macronutrient background, long-term exposure to high BCAA diets leads to hyperphagia, obesity and reduced lifespan. These effects are not due to elevated BCAA per se or hepatic mTOR activation, but rather due to a shift in the relative quantity of dietary BCAAs and other AAs, notably tryptophan and threonine. Increasing the ratio of BCAAs to these AAs resulted in hyperphagia and is associated with central serotonin depletion. Preventing hyperphagia by calorie restriction or pair-feeding averts the health costs of a high BCAA diet. Our data highlight a role for amino acid quality in energy balance and show that health costs of chronic high BCAA intakes need not be due to intrinsic toxicity but, rather, a consequence of hyperphagia driven by AA imbalance.},
  file = {/home/trung/GoogleDrive/Zotero/solon-biet et al_2019_branched chain amino acids impact health and lifespan indirectly via amino acid balance and appetite control.pdf},
  journal = {Nature metabolism},
  number = {5},
  pmcid = {PMC6814438},
  pmid = {31656947}
}

@article{song19_SpeechXLNetUnsupervisedAcoustic,
  title = {Speech-{{XLNet}}: {{Unsupervised Acoustic Model Pretraining For Self}}-{{Attention Networks}}},
  shorttitle = {Speech-{{XLNet}}},
  author = {Song, Xingchen and Wang, Guangsen and Wu, Zhiyong and Huang, Yiheng and Su, Dan and Yu, Dong and Meng, Helen},
  year = {2019},
  month = oct,
  abstract = {Self-attention network (SAN) can benefit significantly from the bi-directional representation learning through unsupervised pretraining paradigms such as BERT and XLNet. In this paper, we present an XLNet-like pretraining scheme "Speech-XLNet" for unsupervised acoustic model pretraining to learn speech representations with SAN. The pretrained SAN is finetuned under the hybrid SAN/HMM framework. We conjecture that by shuffling the speech frame orders, the permutation in Speech-XLNet serves as a strong regularizer to encourage the SAN to make inferences by focusing on global structures through its attention weights. In addition, Speech-XLNet also allows the model to explore the bi-directional contexts for effective speech representation learning. Experiments on TIMIT and WSJ demonstrate that Speech-XLNet greatly improves the SAN/HMM performance in terms of both convergence speed and recognition accuracy compared to the one trained from randomly initialized weights. Our best systems achieve a relative improvement of 11.9\% and 8.3\% on the TIMIT and WSJ tasks respectively. In particular, the best system achieves a phone error rate (PER) of 13.3\% on the TIMIT test set, which to our best knowledge, is the lowest PER obtained from a single system.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.10387},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/song et al_2019_speech-xlnet.pdf;/home/trung/Zotero/storage/SD6263E3/1910.html},
  journal = {arXiv:1910.10387 [cs, eess]},
  keywords = {attention,Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing,pretraining,speech recognition,sre,xlnet},
  primaryClass = {cs, eess}
}

@article{song19_UnderstandingLimitationsVariational,
  title = {Understanding the {{Limitations}} of {{Variational Mutual Information Estimators}}},
  author = {Song, Jiaming and Ermon, Stefano},
  year = {2019},
  month = oct,
  abstract = {Variational approaches based on neural networks are showing promise for estimating mutual information (MI) between high dimensional variables. However, they can be difficult to use in practice due to poorly understood bias/variance tradeoffs. We theoretically show that, under some conditions, estimators such as MINE exhibit variance that could grow exponentially with the true amount of underlying MI. We also empirically demonstrate that existing estimators fail to satisfy basic self-consistency properties of MI, such as data processing and additivity under independence. Based on a unified perspective of variational approaches, we develop a new estimator that focuses on variance reduction. Empirical results on standard benchmark tasks demonstrate that our proposed estimator exhibits improved biasvariance trade-offs on standard benchmark tasks.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1910.06222},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/false},
  journal = {arXiv:1910.06222 [cs, math, stat]},
  keywords = {favorite,information},
  language = {en},
  primaryClass = {cs, math, stat}
}

@article{song20_DenoisingDiffusionImplicit,
  title = {Denoising {{Diffusion Implicit Models}}},
  author = {Song, Jiaming and Meng, Chenlin and Ermon, Stefano},
  year = {2020},
  month = oct,
  abstract = {Denoising diffusion probabilistic models (DDPMs) have achieved high quality image generation without adversarial training, yet they require simulating a Markov chain for many steps to produce a sample. To accelerate sampling, we present denoising diffusion implicit models (DDIMs), a more efficient class of iterative implicit probabilistic models with the same training procedure as DDPMs. In DDPMs, the generative process is defined as the reverse of a Markovian diffusion process. We construct a class of non-Markovian diffusion processes that lead to the same training objective, but whose reverse process can be much faster to sample from. We empirically demonstrate that DDIMs can produce high quality samples \$10 \textbackslash times\$ to \$50 \textbackslash times\$ faster in terms of wall-clock time compared to DDPMs, allow us to trade off computation for sample quality, and can perform semantically meaningful image interpolation directly in the latent space.},
  archivePrefix = {arXiv},
  eprint = {2010.02502},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/song et al_2020_denoising diffusion implicit models.pdf},
  journal = {arXiv:2010.02502 [cs]},
  keywords = {_tablet},
  primaryClass = {cs}
}

@article{song20_DiscriminatorContrastiveDivergence,
  title = {Discriminator {{Contrastive Divergence}}: {{Semi}}-{{Amortized Generative Modeling}} by {{Exploring Energy}} of the {{Discriminator}}},
  shorttitle = {Discriminator {{Contrastive Divergence}}},
  author = {Song, Yuxuan and Ye, Qiwei and Xu, Minkai and Liu, Tie-Yan},
  year = {2020},
  month = apr,
  abstract = {Generative Adversarial Networks (GANs) have shown great promise in modeling high dimensional data. The learning objective of GANs usually minimizes some measure discrepancy, \textbackslash textit\{e.g.\}, \$f\$-divergence\textasciitilde (\$f\$-GANs) or Integral Probability Metric\textasciitilde (Wasserstein GANs). With \$f\$-divergence as the objective function, the discriminator essentially estimates the density ratio, and the estimated ratio proves useful in further improving the sample quality of the generator. However, how to leverage the information contained in the discriminator of Wasserstein GANs (WGAN) is less explored. In this paper, we introduce the Discriminator Contrastive Divergence, which is well motivated by the property of WGAN's discriminator and the relationship between WGAN and energy-based model. Compared to standard GANs, where the generator is directly utilized to obtain new samples, our method proposes a semi-amortized generation procedure where the samples are produced with the generator's output as an initial state. Then several steps of Langevin dynamics are conducted using the gradient of the discriminator. We demonstrate the benefits of significant improved generation on both synthetic data and several real-world image generation benchmarks.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2004.01704},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/song et al_2020_discriminator contrastive divergence.pdf},
  journal = {arXiv:2004.01704 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sorrenson20_DisentanglementNonlinearICA,
  title = {Disentanglement by {{Nonlinear ICA}} with {{General Incompressible}}-Flow {{Networks}} ({{GIN}})},
  author = {Sorrenson, Peter and Rother, Carsten and K{\"o}the, Ullrich},
  year = {2020},
  month = jan,
  abstract = {A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.},
  archivePrefix = {arXiv},
  eprint = {2001.04872},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sorrenson et al_2020_disentanglement by nonlinear ica with general incompressible-flow networks (gin).pdf},
  journal = {arXiv:2001.04872 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{sorrenson20_DisentanglementNonlinearICAa,
  title = {Disentanglement by {{Nonlinear ICA}} with {{General Incompressible}}-Flow {{Networks}} ({{GIN}})},
  author = {Sorrenson, Peter and Rother, Carsten and K{\"o}the, Ullrich},
  year = {2020},
  month = jan,
  abstract = {A central question of representation learning asks under which conditions it is possible to reconstruct the true latent variables of an arbitrarily complex generative process. Recent breakthrough work by Khemakhem et al. (2019) on nonlinear ICA has answered this question for a broad class of conditional generative processes. We extend this important result in a direction relevant for application to real-world data. First, we generalize the theory to the case of unknown intrinsic problem dimension and prove that in some special (but not very restrictive) cases, informative latent variables will be automatically separated from noise by an estimating model. Furthermore, the recovered informative latent variables will be in one-to-one correspondence with the true latent variables of the generating process, up to a trivial component-wise transformation. Second, we introduce a modification of the RealNVP invertible neural network architecture (Dinh et al. (2016)) which is particularly suitable for this type of problem: the General Incompressible-flow Network (GIN). Experiments on artificial data and EMNIST demonstrate that theoretical predictions are indeed verified in practice. In particular, we provide a detailed set of exactly 22 informative latent variables extracted from EMNIST.},
  archivePrefix = {arXiv},
  eprint = {2001.04872},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sorrenson et al_2020_disentanglement by nonlinear ica with general incompressible-flow networks (gin)2.pdf},
  journal = {arXiv:2001.04872 [cs, stat]},
  primaryClass = {cs, stat}
}

@techreport{speir20_UCSCCellBrowser,
  title = {{{UCSC Cell Browser}}: {{Visualize Your Single}}-{{Cell Data}}},
  shorttitle = {{{UCSC Cell Browser}}},
  author = {Speir, Matthew L and Bhaduri, Aparna and Markov, Nikolay S and Moreno, Pablo and Nowakowski, Tomasz J and Papatheodorou, Irene and Pollen, Alex A and Seninge, Lucas and Kent, W James and Haeussler, Maximilian},
  year = {2020},
  month = oct,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.10.30.361162},
  abstract = {Abstract                        Summary             As the use of single-cell technologies has grown, so has the need for tools to explore these large, complicated datasets. The UCSC Cell Browser is a tool that allows scientists to visualize gene expression and metadata annotation distribution throughout a single-cell dataset or multiple datasets.                                   Availability and implementation                            We provide the UCSC Cell Browser as a free website where users can explore a growing collection of single-cell datasets and a freely available python package for scientists to create stable, self-contained visualizations for their own single-cell datasets. Learn more at               https://cells.ucsc.edu               .                                                Contact                            cells@ucsc.edu},
  file = {/home/trung/Zotero/storage/YYGUYB9I/Speir et al. - 2020 - UCSC Cell Browser Visualize Your Single-Cell Data.pdf},
  language = {en},
  type = {Preprint}
}

@article{spellings19_AgglomerativeAttention,
  title = {Agglomerative {{Attention}}},
  author = {Spellings, Matthew},
  year = {2019},
  month = jul,
  abstract = {Neural networks using transformer-based architectures have recently demonstrated great power and flexibility in modeling sequences of many types. One of the core components of transformer networks is the attention layer, which allows contextual information to be exchanged among sequence elements. While many of the prevalent network structures thus far have utilized full attention -- which operates on all pairs of sequence elements -- the quadratic scaling of this attention mechanism significantly constrains the size of models that can be trained. In this work, we present an attention model that has only linear requirements in memory and computation time. We show that, despite the simpler attention model, networks using this attention mechanism can attain comparable performance to full attention networks on language modeling tasks.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1907.06607},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/spellings_2019_agglomerative attention.pdf;/home/trung/Zotero/storage/QKQRIKSP/1907.html},
  journal = {arXiv:1907.06607 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{sreekar20_MutualInformationBased,
  title = {Mutual {{Information Based Method}} for {{Unsupervised Disentanglement}} of {{Video Representation}}},
  author = {Sreekar, P. Aditya and Tiwari, Ujjwal and Namboodiri, Anoop},
  year = {2020},
  month = nov,
  abstract = {Video Prediction is an interesting and challenging task of predicting future frames from a given set context frames that belong to a video sequence. Video prediction models have found prospective applications in Maneuver Planning, Health care, Autonomous Navigation and Simulation. One of the major challenges in future frame generation is due to the high dimensional nature of visual data. In this work, we propose Mutual Information Predictive Auto-Encoder (MIPAE) framework, that reduces the task of predicting high dimensional video frames by factorising video representations into content and low dimensional pose latent variables that are easy to predict. A standard LSTM network is used to predict these low dimensional pose representations. Content and the predicted pose representations are decoded to generate future frames. Our approach leverages the temporal structure of the latent generative factors of a video and a novel mutual information loss to learn disentangled video representations. We also propose a metric based on mutual information gap (MIG) to quantitatively access the effectiveness of disentanglement on DSprites and MPI3D-real datasets. MIG scores corroborate with the visual superiority of frames predicted by MIPAE. We also compare our method quantitatively on evaluation metrics LPIPS, SSIM and PSNR.},
  archivePrefix = {arXiv},
  eprint = {2011.08614},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sreekar et al_2020_mutual information based method for unsupervised disentanglement of video representation.pdf},
  journal = {arXiv:2011.08614 [cs]},
  keywords = {_tablet,disentanglement,information},
  primaryClass = {cs}
}

@article{srivastava00_DropoutSimpleWay,
  title = {Dropout: {{A Simple Way}} to {{Prevent Neural Networks}} from {{Overfitting}}},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  pages = {30},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different ``thinned'' networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  annotation = {ZSCC: 0014989},
  file = {/home/trung/GoogleDrive/Zotero/srivastava et al_dropout.pdf;/home/trung/GoogleDrive/Zotero/srivastava et al_dropout2.pdf;/home/trung/GoogleDrive/Zotero/srivastava et al_dropout3.pdf},
  keywords = {dropout},
  language = {en}
}

@article{srivastava15_UnderstandingLocallyCompetitive,
  title = {Understanding {{Locally Competitive Networks}}},
  author = {Srivastava, Rupesh Kumar and Masci, Jonathan and Gomez, Faustino and Schmidhuber, J{\"u}rgen},
  year = {2015},
  month = apr,
  abstract = {Recently proposed neural network activation functions such as rectified linear, maxout, and local winner-take-all have allowed for faster and more effective training of deep neural architectures on large and complex datasets. The common trait among these functions is that they implement local competition between small groups of computational units within a layer, so that only part of the network is activated for any given input pattern. In this paper, we attempt to visualize and understand this self-modularization, and suggest a unified explanation for the beneficial properties of such networks. We also show how our insights can be directly useful for efficiently performing retrieval over large datasets using neural networks.},
  archivePrefix = {arXiv},
  eprint = {1410.1165},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/srivastava et al_2015_understanding locally competitive networks.pdf;/home/trung/Zotero/storage/WNEEDZMA/1410.html},
  journal = {arXiv:1410.1165 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,I.2.6},
  primaryClass = {cs}
}

@article{srivastava17_AutoencodingVariationalInference,
  title = {Autoencoding {{Variational Inference For Topic Models}}},
  author = {Srivastava, Akash and Sutton, Charles},
  year = {2017},
  month = mar,
  abstract = {Topic models are one of the most popular methods for learning representations of text, but a major challenge is that any change to the topic model requires mathematically deriving a new inference algorithm. A promising approach to address this problem is autoencoding variational Bayes (AEVB), but it has proven difficult to apply to topic models in practice. We present what is to our knowledge the first effective AEVB based inference method for latent Dirichlet allocation (LDA), which we call Autoencoded Variational Inference For Topic Model (AVITM). This model tackles the problems caused for AEVB by the Dirichlet prior and by component collapsing. We find that AVITM matches traditional methods in accuracy with much better inference time. Indeed, because of the inference network, we find that it is unnecessary to pay the computational cost of running variational optimization on test data. Because AVITM is black box, it is readily applied to new topic models. As a dramatic illustration of this, we present a new topic model called ProdLDA, that replaces the mixture model in LDA with a product of experts. By changing only one line of code from LDA, we find that ProdLDA yields much more interpretable topics, even if LDA is trained via collapsed Gibbs sampling.},
  archivePrefix = {arXiv},
  eprint = {1703.01488},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/srivastava et al_2017_autoencoding variational inference for topic models.pdf},
  journal = {arXiv:1703.01488 [stat]},
  keywords = {Statistics - Machine Learning},
  language = {en},
  primaryClass = {stat}
}

@article{srivastava17_VEEGANReducingMode,
  title = {{{VEEGAN}}: {{Reducing Mode Collapse}} in {{GANs}} Using {{Implicit Variational Learning}}},
  shorttitle = {{{VEEGAN}}},
  author = {Srivastava, Akash and Valkov, Lazar and Russell, Chris and Gutmann, Michael U. and Sutton, Charles},
  year = {2017},
  month = nov,
  abstract = {Deep generative models provide powerful tools for distributions over complicated manifolds, such as those of natural images. But many of these methods, including generative adversarial networks (GANs), can be difficult to train, in part because they are prone to mode collapse, which means that they characterize only a few modes of the true distribution. To address this, we introduce VEEGAN, which features a reconstructor network, reversing the action of the generator by mapping from data to noise. Our training objective retains the original asymptotic consistency guarantee of GANs, and can be interpreted as a novel autoencoder loss over the noise. In sharp contrast to a traditional autoencoder over data points, VEEGAN does not require specifying a loss function over the data, but rather only over the representations, which are standard normal by assumption. On an extensive set of synthetic and real world image datasets, VEEGAN indeed resists mode collapsing to a far greater extent than other recent GAN variants, and produces more realistic samples.},
  archivePrefix = {arXiv},
  eprint = {1705.07761},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/srivastava et al_2017_veegan.pdf},
  journal = {arXiv:1705.07761 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{srivastava19_TrainingAgentsusing,
  title = {Training {{Agents}} Using {{Upside}}-{{Down Reinforcement Learning}}},
  author = {Srivastava, Rupesh Kumar and Shyam, Pranav and Mutz, Filipe and Ja{\'s}kowski, Wojciech and Schmidhuber, J{\"u}rgen},
  year = {2019},
  month = dec,
  abstract = {Traditional Reinforcement Learning (RL) algorithms either predict rewards with value functions or maximize them using policy search. We study an alternative: Upside-Down Reinforcement Learning (Upside-Down RL or UDRL), that solves RL problems primarily using supervised learning techniques. Many of its main principles are outlined in a companion report [34]. Here we present the first concrete implementation of UDRL and demonstrate its feasibility on certain episodic learning problems. Experimental results show that its performance can be surprisingly competitive with, and even exceed that of traditional baseline algorithms developed over decades of research.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1912.02877},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/srivastava et al_2019_training agents using upside-down reinforcement learning.pdf;/home/trung/Zotero/storage/AFZQKMTT/1912.html},
  journal = {arXiv:1912.02877 [cs]},
  primaryClass = {cs}
}

@article{stanic19_RSQAIRRelationalSequential,
  title = {R-{{SQAIR}}: {{Relational Sequential Attend}}, {{Infer}}, {{Repeat}}},
  shorttitle = {R-{{SQAIR}}},
  author = {Stani{\'c}, Aleksandar and Schmidhuber, J{\"u}rgen},
  year = {2019},
  month = oct,
  abstract = {Traditional sequential multi-object attention models rely on a recurrent mechanism to infer object relations. We propose a relational extension (R-SQAIR) of one such attention model (SQAIR) by endowing it with a module with strong relational inductive bias that computes in parallel pairwise interactions between inferred objects. Two recently proposed relational modules are studied on tasks of unsupervised learning from videos. We demonstrate gains over sequential relational mechanisms, also in terms of combinatorial generalization.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.05231},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/stanić et al_2019_r-sqair.pdf;/home/trung/GoogleDrive/Zotero/stanić et al_2019_r-sqair2.pdf;/home/trung/Zotero/storage/6JYIJPGE/1910.html},
  journal = {arXiv:1910.05231 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,graph,I.2.6,relational,sequential,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{steck18_Calibratedrecommendations,
  title = {Calibrated Recommendations},
  booktitle = {Proceedings of the 12th {{ACM Conference}} on {{Recommender Systems}}},
  author = {Steck, Harald},
  year = {2018},
  month = sep,
  pages = {154--162},
  publisher = {{ACM}},
  address = {{Vancouver British Columbia Canada}},
  doi = {10.1145/3240323.3240372},
  abstract = {When a user has watched, say, 70 romance movies and 30 action movies, then it is reasonable to expect the personalized list of recommended movies to be comprised of about 70\% romance and 30\% action movies as well. This important property is known as calibration, and recently received renewed attention in the context of fairness in machine learning. In the recommended list of items, calibration ensures that the various (past) areas of interest of a user are reflected with their corresponding proportions. Calibration is especially important in light of the fact that recommender systems optimized toward accuracy (e.g., ranking metrics) in the usual offline-setting can easily lead to recommendations where the lesser interests of a user get crowded out by the user's main interests\textendash which we show empirically as well as in thought-experiments. This can be prevented by calibrated recommendations. To this end, we outline metrics for quantifying the degree of calibration, as well as a simple yet effective re-ranking algorithm for post-processing the output of recommender systems.},
  file = {/home/trung/GoogleDrive/Zotero/steck_2018_calibrated recommendations.pdf},
  isbn = {978-1-4503-5901-6},
  language = {en}
}

@article{steenbrugge18_ImprovingGeneralizationAbstract,
  title = {Improving {{Generalization}} for {{Abstract Reasoning Tasks Using Disentangled Feature Representations}}},
  author = {Steenbrugge, Xander and Leroux, Sam and Verbelen, Tim and Dhoedt, Bart},
  year = {2018},
  month = nov,
  abstract = {In this work we explore the generalization characteristics of unsupervised representation learning by leveraging disentangled VAE's to learn a useful latent space on a set of relational reasoning problems derived from Raven Progressive Matrices. We show that the latent representations, learned by unsupervised training using the right objective function, significantly outperform the same architectures trained with purely supervised learning, especially when it comes to generalization.},
  annotation = {ZSCC: 0000010},
  archivePrefix = {arXiv},
  eprint = {1811.04784},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/steenbrugge et al_2018_improving generalization for abstract reasoning tasks using disentangled feature representations.pdf},
  journal = {arXiv:1811.04784 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{steinke20_ReasoningGeneralizationConditional,
  title = {Reasoning {{About Generalization}} via {{Conditional Mutual Information}}},
  author = {Steinke, Thomas and Zakynthinou, Lydia},
  year = {2020},
  month = jan,
  abstract = {We provide an information-theoretic framework for studying the generalization properties of machine learning algorithms. Our framework ties together existing approaches, including uniform convergence bounds and recent methods for adaptive data analysis.},
  archivePrefix = {arXiv},
  eprint = {2001.09122},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/steinke et al_2020_reasoning about generalization via conditional mutual information.pdf},
  journal = {arXiv:2001.09122 [cs, math, stat]},
  keywords = {disentanglement,information},
  language = {en},
  primaryClass = {cs, math, stat}
}

@inproceedings{stevens19_MannaAcceleratorMemoryAugmented,
  title = {Manna: {{An Accelerator}} for {{Memory}}-{{Augmented Neural Networks}}},
  shorttitle = {Manna},
  booktitle = {Proceedings of the 52nd {{Annual IEEE}}/{{ACM International Symposium}} on {{Microarchitecture}}  - {{MICRO}} '52},
  author = {Stevens, Jacob R. and Ranjan, Ashish and Das, Dipankar and Kaul, Bharat and Raghunathan, Anand},
  year = {2019},
  pages = {794--806},
  publisher = {{ACM Press}},
  address = {{Columbus, OH, USA}},
  doi = {10.1145/3352460.3358304},
  abstract = {Memory-augmented neural networks (MANNs)\textendash{} which augment a traditional Deep Neural Network (DNN) with an external, differentiable memory\textendash{} are emerging as a promising direction in machine learning. MANNs have been shown to achieve one-shot learning and complex cognitive capabilities that are well beyond those of classical DNNs. We analyze the computational characteristics of MANNs and observe that they present a unique challenge due to soft reads and writes to the differentiable memory, each of which requires access to all the memory locations. This results in poor performance of MANNs on modern CPUs, GPUs, and other accelerators. To address this, we present Manna, a specialized hardware inference accelerator for MANNs. Manna is a memory-centric design that focuses on maximizing performance in an extremely low FLOPS/Byte context. The key architectural features from which Manna derives efficiency are: (i) investing most of the die area and power in highly banked on-chip memories that provide ample bandwidth rather than large matrix-multiply units that would be underutilized due to the low reuse (ii) a hardware-assisted transpose mechanism for accommodating the diverse memory access patterns observed in MANNs, (iii) a specialized processing tile that is equipped to handle the nearly-equal mix of MAC and non-MAC computations present in MANNs, and (iv) methods to map MANNs to Manna that minimize data movement while fully exploiting the little reuse present. We evaluate Manna by developing a detailed architectural simulator with timing and power models calibrated by synthesis to the 15 nm Nangate Open Cell library. Across a suite of 10 benchmarks, Manna demonstrates average speedups of 39x with average energy improvements of 122x over an NVIDIA 1080-Ti Pascal GPU and average speedups of 24x with average energy improvements of 86x over a state-of-the-art NVIDIA 2080-Ti Turing GPU.},
  file = {/home/trung/GoogleDrive/Zotero/stevens et al_2019_manna.pdf},
  isbn = {978-1-4503-6938-1},
  keywords = {attention,memory,recurrent,turing machine},
  language = {en}
}

@article{stier19_StructuralAnalysisSparse,
  title = {Structural {{Analysis}} of {{Sparse Neural Networks}}},
  author = {Stier, Julian and Granitzer, Michael},
  year = {2019},
  volume = {159},
  pages = {107--116},
  issn = {18770509},
  doi = {10.1016/j.procs.2019.09.165},
  abstract = {Sparse Neural Networks regained attention due to their potential for mathematical and computational advantages. We give motivation to study Artificial Neural Networks (ANNs) from a network science perspective, provide a technique to embed arbitrary Directed Acyclic Graphs into ANNs and report study results on predicting the performance of image classifiers based on the structural properties of the networks' underlying graph. Results could further progress neuroevolution and add explanations for the success of distinct architectures from a structural perspective.},
  archivePrefix = {arXiv},
  eprint = {1910.07225},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/stier et al_2019_structural analysis of sparse neural networks.pdf},
  journal = {Procedia Computer Science}
}

@article{stirn19_NewDistributionSimplex,
  title = {A {{New Distribution}} on the {{Simplex}} with {{Auto}}-{{Encoding Applications}}},
  author = {Stirn, Andrew and Jebara, Tony and Knowles, David A.},
  year = {2019},
  month = dec,
  abstract = {We construct a new distribution for the simplex using the Kumaraswamy distribution and an ordered stick-breaking process. We explore and develop the theoretical properties of this new distribution and prove that it exhibits symmetry (exchangeability) under the same conditions as the well-known Dirichlet. Like the Dirichlet, the new distribution is adept at capturing sparsity but, unlike the Dirichlet, has an exact and closed form reparameterization\textendash making it well suited for deep variational Bayesian modeling. We demonstrate the distribution's utility in a variety of semi-supervised auto-encoding tasks. In all cases, the resulting models achieve competitive performance commensurate with their simplicity, use of explicit probability models, and abstinence from adversarial training.},
  archivePrefix = {arXiv},
  eprint = {1905.12052},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/stirn et al_2019_a new distribution on the simplex with auto-encoding applications.pdf},
  journal = {arXiv:1905.12052 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{stoeckius17_Largescalesimultaneousmeasurement,
  title = {Large-Scale Simultaneous Measurement of Epitopes and Transcriptomes in Single Cells.},
  author = {Stoeckius, Marlon and Hafemeister, Christoph and Stephenson, William and {Houck-Loomis}, Brian and Chattopadhyay, Pratip K and Swerdlow, Harold and Satija, Rahul and Smibert, Peter},
  year = {2017},
  month = jun,
  doi = {10.1101/113068},
  abstract = {Recent high-throughput single-cell sequencing approaches have been transformative for understanding complex cell populations, but are unable to provide additional phenotypic information, such as protein levels of cell-surface markers. Using oligonucleotide-labeled antibodies, we integrate measurements of cellular proteins and transcriptomes into an efficient, sequencing-based readout of single cells. This method is compatible with existing single-cell sequencing approaches and will readily scale as the throughput of these methods increase.},
  file = {/home/trung/GoogleDrive/Zotero/stoeckius et al_2017_large-scale simultaneous measurement of epitopes and transcriptomes in single cells.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{stooke20_DecouplingRepresentationLearning,
  title = {Decoupling {{Representation Learning}} from {{Reinforcement Learning}}},
  author = {Stooke, Adam and Lee, Kimin and Abbeel, Pieter and Laskin, Michael},
  year = {2020},
  month = sep,
  abstract = {In an effort to overcome limitations of reward-driven feature learning in deep reinforcement learning (RL) from images, we propose decoupling representation learning from policy learning. To this end, we introduce a new unsupervised learning (UL) task, called Augmented Temporal Contrast (ATC), which trains a convolutional encoder to associate pairs of observations separated by a short time difference, under image augmentations and using a contrastive loss. In online RL experiments, we show that training the encoder exclusively using ATC matches or outperforms end-to-end RL in most environments. Additionally, we benchmark several leading UL algorithms by pre-training encoders on expert demonstrations and using them, with weights frozen, in RL agents; we find that agents using ATC-trained encoders outperform all others. We also train multi-task encoders on data from multiple environments and show generalization to different downstream RL tasks. Finally, we ablate components of ATC, and introduce a new data augmentation to enable replay of (compressed) latent images from pre-trained encoders when RL requires augmentation. Our experiments span visually diverse RL benchmarks in DeepMind Control, DeepMind Lab, and Atari, and our complete code is available at https://github.com/astooke/rlpyt/rlpyt/ul.},
  archivePrefix = {arXiv},
  eprint = {2009.08319},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/stooke et al_2020_decoupling representation learning from reinforcement learning.pdf},
  journal = {arXiv:2009.08319 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{stough19_CoQ10CognitionReview,
  title = {{{CoQ10}} and {{Cognition}} a {{Review}} and {{Study Protocol}} for a 90-{{Day Randomized Controlled Trial Investigating}} the {{Cognitive Effects}} of {{Ubiquinol}} in the {{Healthy Elderly}}},
  author = {Stough, Con and Nankivell, Madeleine and Camfield, David A. and Perry, Naomi L. and Pipingas, Andrew and Macpherson, Helen and Wesnes, Keith and Ou, Ruchong and Hare, David and {de Haan}, Judy and Head, Geoffrey and Lansjoen, Peter and Langsjoen, Alena and Tan, Brendan and Pase, Matthew P. and King, Rebecca and Rowsell, Renee and Zwalf, Oliver and Rathner, Yossi and Cooke, Matthew and Rosenfeldt, Franklin},
  year = {2019},
  month = may,
  volume = {11},
  pages = {103},
  issn = {1663-4365},
  doi = {10.3389/fnagi.2019.00103},
  file = {/home/trung/GoogleDrive/Zotero/stough et al_2019_coq10 and cognition a review and study protocol for a 90-day randomized controlled trial investigating the cognitive effects of ubiquinol in the healthy elderly.pdf},
  journal = {Frontiers in Aging Neuroscience},
  language = {en}
}

@article{stuhmer19_IndependentSubspaceAnalysis,
  title = {Independent {{Subspace Analysis}} for {{Unsupervised Learning}} of {{Disentangled Representations}}},
  author = {St{\"u}hmer, Jan and Turner, Richard E. and Nowozin, Sebastian},
  year = {2019},
  month = sep,
  abstract = {Recently there has been an increased interest in unsupervised learning of disentangled representations using the Variational Autoencoder (VAE) framework. Most of the existing work has focused largely on modifying the variational cost function to achieve this goal. We first show that these modifications, e.g. beta-VAE, simplify the tendency of variational inference to underfit causing pathological over-pruning and over-orthogonalization of learned components. Second we propose a complementary approach: to modify the probabilistic model with a structured latent prior. This prior allows to discover latent variable representations that are structured into a hierarchy of independent vector spaces. The proposed prior has three major advantages: First, in contrast to the standard VAE normal prior the proposed prior is not rotationally invariant. This resolves the problem of unidentifiability of the standard VAE normal prior. Second, we demonstrate that the proposed prior encourages a disentangled latent representation which facilitates learning of disentangled representations. Third, extensive quantitative experiments demonstrate that the prior significantly mitigates the trade-off between reconstruction loss and disentanglement over the state of the art.},
  archivePrefix = {arXiv},
  eprint = {1909.05063},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/stühmer et al_2019_independent subspace analysis for unsupervised learning of disentangled representations.pdf},
  journal = {arXiv:1909.05063 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{stuhr20_DonmissMismatch,
  title = {Don't Miss the {{Mismatch}}: {{Investigating}} the {{Objective Function Mismatch}} for {{Unsupervised Representation Learning}}},
  shorttitle = {Don't Miss the {{Mismatch}}},
  author = {Stuhr, Bonifaz and Brauer, J{\"u}rgen},
  year = {2020},
  month = sep,
  abstract = {Finding general evaluation metrics for unsupervised representation learning techniques is a challenging open research question, which recently has become more and more necessary due to the increasing interest in unsupervised methods. Even though these methods promise beneficial representation characteristics, most approaches currently suffer from the objective function mismatch. This mismatch states that the performance on a desired target task can decrease when the unsupervised pretext task is learned too long - especially when both tasks are ill-posed. In this work, we build upon the widely used linear evaluation protocol and define new general evaluation metrics to quantitatively capture the objective function mismatch and the more generic metrics mismatch. We discuss the usability and stability of our protocols on a variety of pretext and target tasks and study mismatches in a wide range of experiments. Thereby we disclose dependencies of the objective function mismatch across several pretext and target tasks with respect to the pretext model's representation size, target model complexity, pretext and target augmentations as well as pretext and target task types.},
  archivePrefix = {arXiv},
  eprint = {2009.02383},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/stuhr et al_2020_don't miss the mismatch.pdf},
  journal = {arXiv:2009.02383 [cs]},
  primaryClass = {cs}
}

@article{subakan20_AttentionAllYou,
  title = {Attention Is {{All You Need}} in {{Speech Separation}}},
  author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Bronzi, Mirko and Zhong, Jianyuan},
  year = {2020},
  month = oct,
  abstract = {Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism. In this paper, we propose the `SepFormer', a novel RNN-free Transformer-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model matches or overtakes the state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It indeed achieves an SI-SNRi of 20.2 dB on WSJ0-2mix matching the SOTA, and an SI-SNRi of 17.6 dB on WSJ0-3mix, a SOTA result. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest RNN-based systems.},
  archivePrefix = {arXiv},
  eprint = {2010.13154},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/subakan et al_2020_attention is all you need in speech separation.pdf},
  journal = {arXiv:2010.13154 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{subakan20_AttentionAllYoua,
  title = {Attention Is {{All You Need}} in {{Speech Separation}}},
  author = {Subakan, Cem and Ravanelli, Mirco and Cornell, Samuele and Bronzi, Mirko and Zhong, Jianyuan},
  year = {2020},
  month = oct,
  abstract = {Recurrent Neural Networks (RNNs) have long been the dominant architecture in sequence-to-sequence learning. RNNs, however, are inherently sequential models that do not allow parallelization of their computations. Transformers are emerging as a natural alternative to standard RNNs, replacing recurrent computations with a multi-head attention mechanism. In this paper, we propose the `SepFormer', a novel RNN-free Transformer-based neural network for speech separation. The SepFormer learns short and long-term dependencies with a multi-scale approach that employs transformers. The proposed model matches or overtakes the state-of-the-art (SOTA) performance on the standard WSJ0-2/3mix datasets. It indeed achieves an SI-SNRi of 20.2 dB on WSJ0-2mix matching the SOTA, and an SI-SNRi of 17.6 dB on WSJ0-3mix, a SOTA result. The SepFormer inherits the parallelization advantages of Transformers and achieves a competitive performance even when downsampling the encoded representation by a factor of 8. It is thus significantly faster and it is less memory-demanding than the latest RNN-based systems.},
  archivePrefix = {arXiv},
  eprint = {2010.13154},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/subakan et al_2020_attention is all you need in speech separation2.pdf},
  journal = {arXiv:2010.13154 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{such19_GenerativeTeachingNetworks,
  title = {Generative {{Teaching Networks}}: {{Accelerating Neural Architecture Search}} by {{Learning}} to {{Generate Synthetic Training Data}}},
  shorttitle = {Generative {{Teaching Networks}}},
  author = {Such, Felipe Petroski and Rawal, Aditya and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff},
  year = {2019},
  month = dec,
  abstract = {This paper investigates the intriguing question of whether we can create learning algorithms that automatically generate training data, learning environments, and curricula in order to help AI agents rapidly learn. We show that such algorithms are possible via Generative Teaching Networks (GTNs), a general approach that is, in theory, applicable to supervised, unsupervised, and reinforcement learning, although our experiments only focus on the supervised case. GTNs are deep neural networks that generate data and/or training environments that a learner (e.g. a freshly initialized neural network) trains on for a few SGD steps before being tested on a target task. We then differentiate through the entire learning process via meta-gradients to update the GTN parameters to improve performance on the target task. GTNs have the beneficial property that they can theoretically generate any type of data or training environment, making their potential impact large. This paper introduces GTNs, discusses their potential, and showcases that they can substantially accelerate learning. We also demonstrate a practical and exciting application of GTNs: accelerating the evaluation of candidate architectures for neural architecture search (NAS), which is rate-limited by such evaluations, enabling massive speed-ups in NAS. GTN-NAS improves the NAS state of the art, finding higher performing architectures when controlling for the search proposal mechanism. GTN-NAS also is competitive with the overall state of the art approaches, which achieve top performance while using orders of magnitude less computation than typical NAS methods. Speculating forward, GTNs may represent a first step toward the ambitious goal of algorithms that generate their own training data and, in doing so, open a variety of interesting new research questions and directions.},
  archivePrefix = {arXiv},
  eprint = {1912.07768},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/F58UI7QC/Such et al. - 2019 - Generative Teaching Networks Accelerating Neural .pdf},
  journal = {arXiv:1912.07768 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sugiyama12_DensityratiomatchingBregman,
  title = {Density-Ratio Matching under the {{Bregman}} Divergence: A Unified Framework of Density-Ratio Estimation},
  shorttitle = {Density-Ratio Matching under the {{Bregman}} Divergence},
  author = {Sugiyama, Masashi and Suzuki, Taiji and Kanamori, Takafumi},
  year = {2012},
  month = oct,
  volume = {64},
  pages = {1009--1044},
  issn = {0020-3157, 1572-9052},
  doi = {10.1007/s10463-011-0343-8},
  abstract = {Estimation of the ratio of probability densities has attracted a great deal of attention since it can be used for addressing various statistical paradigms. A naive approach to density-ratio approximation is to first estimate numerator and denominator densities separately and then take their ratio. However, this two-step approach does not perform well in practice, and methods for directly estimating density ratios without density estimation have been explored. In this paper, we first give a comprehensive review of existing density-ratio estimation methods and discuss their pros and cons. Then we propose a new framework of density-ratio estimation in which a density-ratio model is fitted to the true density-ratio under the Bregman divergence. Our new framework includes existing approaches as special cases, and is substantially more general. Finally, we develop a robust density-ratio estimation method under the power divergence, which is a novel instance in our framework.},
  annotation = {ZSCC: 0000093},
  file = {/home/trung/Zotero/storage/5667LBIH/Sugiyama et al. - 2012 - Density-ratio matching under the Bregman divergenc.pdf},
  journal = {Annals of the Institute of Statistical Mathematics},
  language = {en},
  number = {5}
}

@article{sukhbaatar19_AugmentingSelfattentionPersistent,
  title = {Augmenting {{Self}}-Attention with {{Persistent Memory}}},
  author = {Sukhbaatar, Sainbayar and Grave, Edouard and Lample, Guillaume and Jegou, Herve and Joulin, Armand},
  year = {2019},
  month = jul,
  abstract = {Transformer networks have lead to important progress in language modeling and machine translation. These models include two consecutive modules, a feed-forward layer and a self-attention layer. The latter allows the network to capture long term dependencies and are often regarded as the key ingredient in the success of Transformers. Building upon this intuition, we propose a new model that solely consists of attention layers. More precisely, we augment the self-attention layers with persistent memory vectors that play a similar role as the feed-forward layer. Thanks to these vectors, we can remove the feed-forward layer without degrading the performance of a transformer. Our evaluation shows the benefits brought by our model on standard character and word level language modeling benchmarks.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1907.01470},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sukhbaatar et al_2019_augmenting self-attention with persistent memory.pdf;/home/trung/GoogleDrive/Zotero/sukhbaatar et al_2019_augmenting self-attention with persistent memory2.pdf;/home/trung/Zotero/storage/MLC9IGLD/1907.html;/home/trung/Zotero/storage/N8UMT5UP/1907.html},
  journal = {arXiv:1907.01470 [cs, stat]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,memory,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{sun17_LearningStructuredWeight,
  title = {Learning {{Structured Weight Uncertainty}} in {{Bayesian Neural Networks}}},
  booktitle = {Artificial {{Intelligence}} and {{Statistics}}},
  author = {Sun, Shengyang and Chen, Changyou and Carin, Lawrence},
  year = {2017},
  month = apr,
  pages = {1283--1292},
  abstract = {Deep neural networks (DNNs) are increasingly popular in modern machine learning. Bayesian learning affords the opportunity to quantify posterior uncertainty on DNN model parameters. Most existing w...},
  file = {/home/trung/GoogleDrive/Zotero/sun et al_2017_learning structured weight uncertainty in bayesian neural networks.pdf;/home/trung/Zotero/storage/X84J2DD2/sun17b.html},
  keywords = {variational},
  language = {en}
}

@article{sun19_Accuracyrobustnessscalability,
  title = {Accuracy, Robustness and Scalability of Dimensionality Reduction Methods for Single-Cell {{RNA}}-Seq Analysis},
  author = {Sun, Shiquan and Zhu, Jiaqiang and Ma, Ying and Zhou, Xiang},
  year = {2019},
  month = dec,
  volume = {20},
  pages = {269},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1898-6},
  abstract = {Background: Dimensionality reduction is an indispensable analytic component for many areas of single-cell RNA sequencing (scRNA-seq) data analysis. Proper dimensionality reduction can allow for effective noise removal and facilitate many downstream analyses that include cell clustering and lineage reconstruction. Unfortunately, despite the critical importance of dimensionality reduction in scRNA-seq analysis and the vast number of dimensionality reduction methods developed for scRNA-seq studies, few comprehensive comparison studies have been performed to evaluate the effectiveness of different dimensionality reduction methods in scRNA-seq. Results: We aim to fill this critical knowledge gap by providing a comparative evaluation of a variety of commonly used dimensionality reduction methods for scRNA-seq studies. Specifically, we compare 18 different dimensionality reduction methods on 30 publicly available scRNA-seq datasets that cover a range of sequencing techniques and sample sizes. We evaluate the performance of different dimensionality reduction methods for neighborhood preserving in terms of their ability to recover features of the original expression matrix, and for cell clustering and lineage reconstruction in terms of their accuracy and robustness. We also evaluate the computational scalability of different dimensionality reduction methods by recording their computational cost. Conclusions: Based on the comprehensive evaluation results, we provide important guidelines for choosing dimensionality reduction methods for scRNA-seq data analysis. We also provide all analysis scripts used in the present study at www.xzlab.org/reproduce.html.},
  annotation = {ZSCC: 0000006},
  file = {/home/trung/Zotero/storage/CFQLYJGG/Sun et al. - 2019 - Accuracy, robustness and scalability of dimensiona.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{sun19_Bayesianmixturemodel,
  title = {A {{Bayesian}} Mixture Model for Clustering Droplet-Based Single-Cell Transcriptomic Data from Population Studies},
  author = {Sun, Zhe and Chen, Li and Xin, Hongyi and Jiang, Yale and Huang, Qianhui and Cillo, Anthony R. and Tabib, Tracy and Kolls, Jay K. and Bruno, Tullia C. and Lafyatis, Robert and Vignali, Dario A. A. and Chen, Kong and Ding, Ying and Hu, Ming and Chen, Wei},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {1649},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-09639-3},
  file = {/home/trung/GoogleDrive/Zotero/sun et al_2019_a bayesian mixture model for clustering droplet-based single-cell transcriptomic data from population studies.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{sun19_FunctionalVariationalBayesian,
  title = {Functional {{Variational Bayesian Neural Networks}}},
  author = {Sun, Shengyang and Zhang, Guodong and Shi, Jiaxin and Grosse, Roger},
  year = {2019},
  month = mar,
  abstract = {Variational Bayesian neural networks (BNNs) perform variational inference over weights, but it is difficult to specify meaningful priors and approximate posteriors in a high-dimensional weight space. We introduce functional variational Bayesian neural networks (fBNNs), which maximize an Evidence Lower BOund (ELBO) defined directly on stochastic processes, i.e. distributions over functions. We prove that the KL divergence between stochastic processes equals the supremum of marginal KL divergences over all finite sets of inputs. Based on this, we introduce a practical training objective which approximates the functional ELBO using finite measurement sets and the spectral Stein gradient estimator. With fBNNs, we can specify priors entailing rich structures, including Gaussian processes and implicit stochastic processes. Empirically, we find fBNNs extrapolate well using various structured priors, provide reliable uncertainty estimates, and scale to large datasets.},
  archivePrefix = {arXiv},
  eprint = {1903.05779},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sun et al_2019_functional variational bayesian neural networks.pdf;/home/trung/Zotero/storage/BCYNM2VH/1903.html},
  journal = {arXiv:1903.05779 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@misc{sun19_TutorialSurveyProbabilistic,
  title = {Tutorial and {{Survey}} on {{Probabilistic Graphical Model}} and {{Variational Inference}} in {{Deep Reinforcement Learning}}},
  author = {Sun, Xudong and Bischl, Bernd},
  year = {2019},
  file = {/home/trung/GoogleDrive/Zotero/sun et al_2019_tutorial and survey on probabilistic graphical model and variational inference in deep reinforcement learning.pdf}
}

@article{sun20_Poisonedclassifiersare,
  title = {Poisoned Classifiers Are Not Only Backdoored, They Are Fundamentally Broken},
  author = {Sun, Mingjie and Agarwal, Siddhant and Kolter, J. Zico},
  year = {2020},
  month = oct,
  abstract = {Under a commonly-studied "backdoor" poisoning attack against classification models, an attacker adds a small "trigger" to a subset of the training data, such that the presence of this trigger at test time causes the classifier to always predict some target class. It is often implicitly assumed that the poisoned classifier is vulnerable exclusively to the adversary who possesses the trigger. In this paper, we show empirically that this view of backdoored classifiers is fundamentally incorrect. We demonstrate that anyone with access to the classifier, even without access to any original training data or trigger, can construct several alternative triggers that are as effective or more so at eliciting the target class at test time. We construct these alternative triggers by first generating adversarial examples for a smoothed version of the classifier, created with a recent process called Denoised Smoothing, and then extracting colors or cropped portions of adversarial images. We demonstrate the effectiveness of our attack through extensive experiments on ImageNet and TrojAI datasets, including a user study which demonstrates that our method allows users to easily determine the existence of such backdoors in existing poisoned classifiers. Furthermore, we demonstrate that our alternative triggers can in fact look entirely different from the original trigger, highlighting that the backdoor actually learned by the classifier differs substantially from the trigger image itself. Thus, we argue that there is no such thing as a "secret" backdoor in poisoned classifiers: poisoning a classifier invites attacks not just by the party that possesses the trigger, but from anyone with access to the classifier. Code is available at https://github.com/locuslab/breaking-poisoned-classifier.},
  archivePrefix = {arXiv},
  eprint = {2010.09080},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sun et al_2020_poisoned classifiers are not only backdoored, they are fundamentally broken.pdf},
  journal = {arXiv:2010.09080 [cs]},
  primaryClass = {cs}
}

@misc{sungman.cho20_SungmanChoAwesomeSelfSupervisedPapers,
  title = {Sungman-{{Cho}}/{{Awesome}}-{{Self}}-{{Supervised}}-{{Papers}}},
  author = {Sungman.Cho},
  year = {2020},
  month = apr,
  abstract = {Paper bank for Self-Supervised Learning. Contribute to Sungman-Cho/Awesome-Self-Supervised-Papers development by creating an account on GitHub.},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{suresh20_FrameworkUnderstandingUnintended,
  title = {A {{Framework}} for {{Understanding Unintended Consequences}} of {{Machine Learning}}},
  author = {Suresh, Harini and Guttag, John V.},
  year = {2020},
  month = feb,
  abstract = {As machine learning increasingly affects people and society, it is important that we strive for a comprehensive and unified understanding of potential sources of unwanted consequences. For instance, downstream harms to particular groups are often blamed on "biased data," but this concept encompass too many issues to be useful in developing solutions. In this paper, we provide a framework that partitions sources of downstream harm in machine learning into six distinct categories spanning the data generation and machine learning pipeline. We describe how these issues arise, how they are relevant to particular applications, and how they motivate different solutions. In doing so, we aim to facilitate the development of solutions that stem from an understanding of application-specific populations and data generation processes, rather than relying on general statements about what may or may not be "fair."},
  archivePrefix = {arXiv},
  eprint = {1901.10002},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/suresh et al_2020_a framework for understanding unintended consequences of machine learning.pdf},
  journal = {arXiv:1901.10002 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{suter19_RobustlyDisentangledCausal,
  title = {Robustly {{Disentangled Causal Mechanisms}}: {{Validating Deep Representations}} for {{Interventional Robustness}}},
  shorttitle = {Robustly {{Disentangled Causal Mechanisms}}},
  author = {Suter, Raphael and Miladinovi{\'c}, {\DJ}or{\dj}e and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  year = {2019},
  month = may,
  abstract = {The ability to learn disentangled representations that split underlying sources of variation in high dimensional, unstructured data is important for data efficient and robust use of neural networks. While various approaches aiming towards this goal have been proposed in recent times, a commonly accepted definition and validation procedure is missing. We provide a causal perspective on representation learning which covers disentanglement and domain shift robustness as special cases. Our causal framework allows us to introduce a new metric for the quantitative evaluation of deep latent variable models. We show how this metric can be estimated from labeled observational data and further provide an efficient estimation algorithm that scales linearly in the dataset size.},
  archivePrefix = {arXiv},
  eprint = {1811.00007},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/suter et al_2019_robustly disentangled causal mechanisms.pdf},
  journal = {arXiv:1811.00007 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{sutskever00_importanceinitializationmomentum,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  author = {Sutskever, Ilya and Martens, James and Dahl, George},
  pages = {9},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.},
  annotation = {ZSCC: 0002175},
  file = {/home/trung/GoogleDrive/Zotero/sutskever et al_on the importance of initialization and momentum in deep learning.pdf},
  keywords = {gradient,initialization,momentum,recurrent},
  language = {en}
}

@article{sutter20_MultimodalGenerativeLearning,
  title = {Multimodal {{Generative Learning Utilizing Jensen}}-{{Shannon}}-{{Divergence}}},
  author = {Sutter, Thomas M. and Daunhawer, Imant and Vogt, Julia E.},
  year = {2020},
  month = oct,
  abstract = {Learning from different data types is a long-standing goal in machine learning research, as multiple information sources co-occur when describing natural phenomena. However, existing generative models that approximate a multimodal ELBO rely on difficult or inefficient training schemes to learn a joint distribution and the dependencies between modalities. In this work, we propose a novel, efficient objective function that utilizes the Jensen-Shannon divergence for multiple distributions. It simultaneously approximates the unimodal and joint multimodal posteriors directly via a dynamic prior. In addition, we theoretically prove that the new multimodal JS-divergence (mmJSD) objective optimizes an ELBO. In extensive experiments, we demonstrate the advantage of the proposed mmJSD model compared to previous work in unsupervised, generative learning tasks.},
  archivePrefix = {arXiv},
  eprint = {2006.08242},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sutter et al_2020_multimodal generative learning utilizing jensen-shannon-divergence.pdf},
  journal = {arXiv:2006.08242 [cs, stat]},
  keywords = {_tablet,information},
  primaryClass = {cs, stat}
}

@book{sutton18_Reinforcementlearningintroduction,
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  annotation = {ZSCC: 0034258},
  file = {/home/trung/GoogleDrive/Zotero/sutton et al_2018_reinforcement learning.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@misc{sutton19_BitterLesson,
  title = {The {{Bitter Lesson}}},
  author = {Sutton, Richard S.},
  year = {2019},
  file = {/home/trung/Zotero/storage/MHKURH42/BitterLesson.html},
  howpublished = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
  keywords = {favorite}
}

@misc{sutton84_TemporalCreditsAssignment,
  title = {Temporal {{Credits Assignment}} in {{Reinforcement Learning}}},
  author = {Sutton, Richard S.},
  year = {1984},
  file = {/home/trung/GoogleDrive/Zotero/sutton_1984_temporal credits assignment in reinforcement learning.pdf},
  howpublished = {http://incompleteideas.net/papers/Sutton-PhD-thesis.pdf}
}

@article{suzuki16_EstimatorMutualInformation,
  title = {An {{Estimator}} of {{Mutual Information}} and Its {{Application}} to {{Independence Testing}}},
  author = {Suzuki, Joe},
  year = {2016},
  volume = {18},
  issn = {1099-4300},
  doi = {10.3390/e18040109},
  abstract = {This paper proposes a novel estimator of mutual information for discrete and continuous variables. The main feature of this estimator is that it is zero for a large sample size n if and only if the two variables are independent. The estimator can be used to construct several histograms, compute estimations of mutual information, and choose the maximum value. We prove that the number of histograms constructed has an upper bound of O(log n) and apply this fact to the search. We compare the performance of the proposed estimator with an estimator of the Hilbert-Schmidt independence criterion (HSIC), though the proposed method is based on the minimum description length (MDL) principle and the HSIC provides a statistical test. The proposed method completes the estimation in O(n log n) time, whereas the HSIC kernel computation requires O(n3) time. We also present examples in which the HSIC fails to detect independence but the proposed method successfully detects it.},
  file = {/home/trung/GoogleDrive/Zotero/suzuki_2016_an estimator of mutual information and its application to independence testing.pdf},
  journal = {Entropy},
  keywords = {Hilbert-Schmidt independence criterion (HSIC),histogram,independence testing,information,kernel,minimum description length (MDL) principle,mutual information},
  number = {4}
}

@techreport{svensson19_Interpretablefactormodelsa,
  title = {Interpretable Factor Models of Single-Cell {{RNA}}-Seq via Variational Autoencoders},
  author = {Svensson, Valentine and Pachter, Lior},
  year = {2019},
  month = aug,
  institution = {{Bioinformatics}},
  doi = {10.1101/737601},
  abstract = {Single cell RNA-seq makes possible the investigation of variability in gene expression among cells, and dependence of variation on cell type. Statistical inference methods for such analyses must be scalable, and ideally interpretable. We present an approach based on a modification of a recently published highly scalable variational autoencoder framework that provides interpretability without sacrificing much accuracy. We demonstrate that our approach enables identification of gene programs in massive datasets. Our strategy, namely the learning of factor models with the auto-encoding variational Bayes framework, is not domain specific and may be of interest for other applications.},
  file = {/home/trung/GoogleDrive/Zotero/svensson et al_2019_interpretable factor models of single-cell rna-seq via variational autoencoders.pdf},
  language = {en},
  type = {Preprint}
}

@article{sweigart00_AutomateBoringStuff,
  title = {Automate the {{Boring Stuff}} with {{Python}}: {{Practical Programming}} for {{Total Beginners}}},
  author = {Sweigart, Albert},
  pages = {603},
  file = {/home/trung/GoogleDrive/Zotero/sweigart_automate the boring stuff with python.pdf},
  language = {en}
}

@article{swiatkowski20_ktiedNormalDistribution,
  title = {The K-Tied {{Normal Distribution}}: {{A Compact Parameterization}} of {{Gaussian Mean Field Posteriors}} in {{Bayesian Neural Networks}}},
  shorttitle = {The K-Tied {{Normal Distribution}}},
  author = {Swiatkowski, Jakub and Roth, Kevin and Veeling, Bastiaan S. and Tran, Linh and Dillon, Joshua V. and Snoek, Jasper and Mandt, Stephan and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  year = {2020},
  month = jul,
  abstract = {Variational Bayesian Inference is a popular methodology for approximating posterior distributions over Bayesian neural network weights. Recent work developing this class of methods has explored ever richer parameterizations of the approximate posterior in the hope of improving performance. In contrast, here we share a curious experimental finding that suggests instead restricting the variational distribution to a more compact parameterization. For a variety of deep Bayesian neural networks trained using Gaussian mean-field variational inference, we find that the posterior standard deviations consistently exhibit strong low-rank structure after convergence. This means that by decomposing these variational parameters into a low-rank factorization, we can make our variational approximation more compact without decreasing the models' performance. Furthermore, we find that such factorized parameterizations improve the signal-to-noise ratio of stochastic gradient estimates of the variational lower bound, resulting in faster convergence.},
  archivePrefix = {arXiv},
  eprint = {2002.02655},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/swiatkowski et al_2020_the k-tied normal distribution.pdf},
  journal = {arXiv:2002.02655 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{sylvain20_CrossModalInformationMaximization,
  title = {Cross-{{Modal Information Maximization}} for {{Medical Imaging}}: {{CMIM}}},
  shorttitle = {Cross-{{Modal Information Maximization}} for {{Medical Imaging}}},
  author = {Sylvain, Tristan and Dutil, Francis and Berthier, Tess and Di Jorio, Lisa and Luck, Margaux and Hjelm, Devon and Bengio, Yoshua},
  year = {2020},
  month = oct,
  abstract = {In hospitals, data are siloed to specific information systems that make the same information available under different modalities such as the different medical imaging exams the patient undergoes (CT scans, MRI, PET, Ultrasound, etc.) and their associated radiology reports. This offers unique opportunities to obtain and use at train-time those multiple views of the same information that might not always be available at test-time. In this paper, we propose an innovative framework that makes the most of available data by learning good representations of a multi-modal input that are resilient to modality dropping at test-time, using recent advances in mutual information maximization. By maximizing cross-modal information at train time, we are able to outperform several state-of-the-art baselines in two different settings, medical image classification, and segmentation. In particular, our method is shown to have a strong impact on the inference-time performance of weaker modalities.},
  archivePrefix = {arXiv},
  eprint = {2010.10593},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/sylvain et al_2020_cross-modal information maximization for medical imaging.pdf},
  journal = {arXiv:2010.10593 [cs]},
  keywords = {information,semi-supervised},
  primaryClass = {cs}
}

@misc{synced18_SynchronizedSGDOutdated,
  title = {Synchronized {{SGD Outdated}}? {{Yoshua Bengio}} on {{Hardware}}-{{Friendly DL}}},
  shorttitle = {Synchronized {{SGD Outdated}}?},
  author = {Synced},
  year = {2018},
  month = jul,
  abstract = {DeepMind's 2018 AlphaGo Zero requires 300,000 times more computing power than AlexNet did in 2013. With larger-than-ever datasets and\ldots},
  howpublished = {https://medium.com/syncedreview/synchronized-sgd-outdated-yoshua-bengio-on-hardware-friendly-dl-9fba2ec06977},
  journal = {Medium},
  language = {en}
}

@article{synoradzki19_CiticolineSuperiorForm,
  title = {Citicoline: {{A Superior Form}} of {{Choline}}?},
  shorttitle = {Citicoline},
  author = {Synoradzki, Kamil and Grieb, Pawe{\l}},
  year = {2019},
  month = jul,
  volume = {11},
  issn = {2072-6643},
  doi = {10.3390/nu11071569},
  abstract = {Medicines containing citicoline (cytidine-diphosphocholine) as an active principle have been marketed since the 1970s as nootropic and psychostimulant drugs available on prescription. Recently, the inner salt variant of this substance was pronounced a food ingredient in the major world markets. However, in the EU no nutrition or health claim has been authorized for use in commercial communications concerning its properties. Citicoline is considered a dietetic source of choline and cytidine. Cytidine does not have any health claim authorized either, but there are claims authorized for choline, concerning its contribution to normal lipid metabolism, maintenance of normal liver function, and normal homocysteine metabolism. The applicability of these claims to citicoline is discussed, leading to the conclusion that the issue is not a trivial one. Intriguing data, showing that on a molar mass basis citicoline is significantly less toxic than choline, are also analyzed. It is hypothesized that, compared to choline moiety in other dietary sources such as phosphatidylcholine, choline in citicoline is less prone to conversion to trimethylamine (TMA) and its putative atherogenic N-oxide (TMAO). Epidemiological studies have suggested that choline supplementation may improve cognitive performance, and for this application citicoline may be safer and more efficacious.},
  file = {/home/trung/GoogleDrive/Zotero/synoradzki et al_2019_citicoline.pdf},
  journal = {Nutrients},
  number = {7},
  pmcid = {PMC6683073},
  pmid = {31336819}
}

@article{szabo17_ChallengesDisentanglingIndependent,
  title = {Challenges in {{Disentangling Independent Factors}} of {{Variation}}},
  author = {Szab{\'o}, Attila and Hu, Qiyang and Portenier, Tiziano and Zwicker, Matthias and Favaro, Paolo},
  year = {2017},
  month = nov,
  abstract = {We study the problem of building models that disentangle independent factors of variation. Such models could be used to encode features that can efficiently be used for classification and to transfer attributes between different images in image synthesis. As data we use a weakly labeled training set. Our weak labels indicate what single factor has changed between two data samples, although the relative value of the change is unknown. This labeling is of particular interest as it may be readily available without annotation costs. To make use of weak labels we introduce an autoencoder model and train it through constraints on image pairs and triplets. We formally prove that without additional knowledge there is no guarantee that two images with the same factor of variation will be mapped to the same feature. We call this issue the reference ambiguity. Moreover, we show the role of the feature dimensionality and adversarial training. We demonstrate experimentally that the proposed model can successfully transfer attributes on several datasets, but show also cases when the reference ambiguity occurs.},
  archivePrefix = {arXiv},
  eprint = {1711.02245},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/szabó et al_2017_challenges in disentangling independent factors of variation.pdf},
  journal = {arXiv:1711.02245 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{szegedy14_GoingDeeperConvolutions,
  title = {Going {{Deeper}} with {{Convolutions}}},
  author = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2014},
  month = sep,
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception, which was responsible for setting the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. This was achieved by a carefully crafted design that allows for increasing the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  annotation = {ZSCC: 0000039},
  archivePrefix = {arXiv},
  eprint = {1409.4842},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/ERMILHZV/Szegedy et al. - 2014 - Going Deeper with Convolutions.pdf},
  journal = {arXiv:1409.4842 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{taghanaki19_InfoMaskMaskedVariational,
  title = {{{InfoMask}}: {{Masked Variational Latent Representation}} to {{Localize Chest Disease}}},
  shorttitle = {{{InfoMask}}},
  author = {Taghanaki, Saeid Asgari and Havaei, Mohammad and Berthier, Tess and Dutil, Francis and Di Jorio, Lisa and Hamarneh, Ghassan and Bengio, Yoshua},
  year = {2019},
  month = jun,
  abstract = {The scarcity of richly annotated medical images is limiting supervised deep learning based solutions to medical image analysis tasks, such as localizing discriminatory radiomic disease signatures. Therefore, it is desirable to leverage unsupervised and weakly supervised models. Most recent weakly supervised localization methods apply attention maps or region proposals in a multiple instance learning formulation. While attention maps can be noisy, leading to erroneously highlighted regions, it is not simple to decide on an optimal window/bag size for multiple instance learning approaches. In this paper, we propose a learned spatial masking mechanism to filter out irrelevant background signals from attention maps. The proposed method minimizes mutual information between a masked variational representation and the input while maximizing the information between the masked representation and class labels. This results in more accurate localization of discriminatory regions. We tested the proposed model on the ChestX-ray8 dataset to localize pneumonia from chest X-ray images without using any pixel-level or bounding-box annotations.},
  archivePrefix = {arXiv},
  eprint = {1903.11741},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/taghanaki et al_2019_infomask.pdf},
  journal = {arXiv:1903.11741 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{taghanaki20_JigsawVAEBalancingFeatures,
  title = {Jigsaw-{{VAE}}: {{Towards Balancing Features}} in {{Variational Autoencoders}}},
  shorttitle = {Jigsaw-{{VAE}}},
  author = {Taghanaki, Saeid Asgari and Havaei, Mohammad and Lamb, Alex and Sanghi, Aditya and Danielyan, Ara and Custis, Tonya},
  year = {2020},
  month = may,
  abstract = {The latent variables learned by VAEs have seen considerable interest as an unsupervised way of extracting features, which can then be used for downstream tasks. There is a growing interest in the question of whether features learned on one environment will generalize across different environments. We demonstrate here that VAE latent variables often focus on some factors of variation at the expense of others - in this case we refer to the features as ``imbalanced''. Feature imbalance leads to poor generalization when the latent variables are used in an environment where the presence of features changes. Similarly, latent variables trained with imbalanced features induce the VAE to generate less diverse (i.e. biased towards dominant features) samples. To address this, we propose a regularization scheme for VAEs, which we show substantially addresses the feature imbalance problem. We also introduce a simple metric to measure the balance of features in generated images.},
  archivePrefix = {arXiv},
  eprint = {2005.05496},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/taghanaki et al_2020_jigsaw-vae.pdf},
  journal = {arXiv:2005.05496 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{takahashi20_PartiallySharedVariationalAutoencoders,
  title = {Partially-{{Shared Variational Auto}}-Encoders for {{Unsupervised Domain Adaptation}} with {{Target Shift}}},
  author = {Takahashi, Ryuhei and Hashimoto, Atsushi and Sonogashira, Motoharu and Iiyama, Masaaki},
  year = {2020},
  month = jan,
  abstract = {This paper proposes a novel approach for unsupervised domain adaptation (UDA) with target shift. Target shift is a problem of mismatch in label distribution between source and target domains. Typically it appears as class-imbalance in target domain. In practice, this is an important problem in UDA; as we do not know labels in target domain datasets, we do not know whether or not its distribution is identical to that in the source domain dataset. Many traditional approaches achieve UDA with distribution matching by minimizing mean maximum discrepancy or adversarial training; however these approaches implicitly assume a coincidence in the distributions and do not work under situations with target shift. Some recent UDA approaches focus on class boundary and some of them are robust to target shift, but they are only applicable to classification and not to regression. To overcome the target shift problem in UDA, the proposed method, partially shared variational autoencoders (PS-VAEs), uses pair-wise feature alignment instead of feature distribution matching. PS-VAEs inter-convert domain of each sample by a CycleGAN-based architecture while preserving its label-related content. To evaluate the performance of PS-VAEs, we carried out two experiments: UDA with class-unbalanced digits datasets (classification), and UDA from synthesized data to real observation in human-pose-estimation (regression). The proposed method presented its robustness against the class-imbalance in the classification task, and outperformed the other methods in the regression task with a large margin.},
  archivePrefix = {arXiv},
  eprint = {2001.07895},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/takahashi et al_2020_partially-shared variational auto-encoders for unsupervised domain adaptation with target shift.pdf},
  journal = {arXiv:2001.07895 [cs]},
  primaryClass = {cs}
}

@article{talmor19_oLMpicswhatLanguage,
  title = {{{oLMpics}} -- {{On}} What {{Language Model Pre}}-Training {{Captures}}},
  author = {Talmor, Alon and Elazar, Yanai and Goldberg, Yoav and Berant, Jonathan},
  year = {2019},
  month = dec,
  abstract = {Recent success of pre-trained language models (LMs) has spurred widespread interest in the language capabilities that they possess. However, efforts to understand whether LM representations are useful for symbolic reasoning tasks have been limited and scattered. In this work, we propose eight reasoning tasks, which conceptually require operations such as comparison, conjunction, and composition. A fundamental challenge is to understand whether the performance of a LM on a task should be attributed to the pre-trained representations or to the process of fine-tuning on the task data. To address this, we propose an evaluation protocol that includes both zero-shot evaluation (no fine-tuning), as well as comparing the learning curve of a fine-tuned LM to the learning curve of multiple controls, which paints a rich picture of the LM capabilities. Our main findings are that: (a) different LMs exhibit qualitatively different reasoning abilities, e.g., RoBERTa succeeds in reasoning tasks where BERT fails completely; (b) LMs do not reason in an abstract manner and are context-dependent, e.g., while RoBERTa can compare ages, it can do so only when the ages are in the typical range of human ages; (c) On half of our reasoning tasks all models fail completely. Our findings and infrastructure can help future work on designing new datasets, models and objective functions for pre-training.},
  archivePrefix = {arXiv},
  eprint = {1912.13283},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/talmor et al_2019_olmpics -- on what language model pre-training captures.pdf;/home/trung/Zotero/storage/8EIKLY3U/1912.html},
  journal = {arXiv:1912.13283 [cs]},
  primaryClass = {cs}
}

@article{tan19_EfficientNetRethinkingModel,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
  archivePrefix = {arXiv},
  eprint = {1905.11946},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/I99NSEYB/Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf},
  journal = {arXiv:1905.11946 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{tan19_MixConvMixedDepthwise,
  title = {{{MixConv}}: {{Mixed Depthwise Convolutional Kernels}}},
  shorttitle = {{{MixConv}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2019},
  month = jul,
  abstract = {Depthwise convolution is becoming increasingly popular in modern efficient ConvNets, but its kernel size is often overlooked. In this paper, we systematically study the impact of different kernel sizes, and observe that combining the benefits of multiple kernel sizes can lead to better accuracy and efficiency. Based on this observation, we propose a new mixed depthwise convolution (MixConv), which naturally mixes up multiple kernel sizes in a single convolution. As a simple drop-in replacement of vanilla depthwise convolution, our MixConv improves the accuracy and efficiency for existing MobileNets on both ImageNet classification and COCO object detection. To demonstrate the effectiveness of MixConv, we integrate it into AutoML search space and develop a new family of models, named as MixNets, which outperform previous mobile models including MobileNetV2 [20] (ImageNet top-1 accuracy +4.2\%), ShuffleNetV2 [16] (+3.5\%), MnasNet [26] (+1.3\%), ProxylessNAS [2] (+2.2\%), and FBNet [27] (+2.0\%). In particular, our MixNet-L achieves a new state-of-the-art 78.9\% ImageNet top-1 accuracy under typical mobile settings ({$<$}600M FLOPS). Code is at https://github.com/ tensorflow/tpu/tree/master/models/official/mnasnet/mixnet},
  archivePrefix = {arXiv},
  eprint = {1907.09595},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tan et al_2019_mixconv.pdf;/home/trung/Zotero/storage/TPUCAGPT/1907.html},
  journal = {arXiv:1907.09595 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{tan20_MusicFaderNetsControllable,
  title = {Music {{FaderNets}}: {{Controllable Music Generation Based On High}}-{{Level Features}} via {{Low}}-{{Level Feature Modelling}}},
  shorttitle = {Music {{FaderNets}}},
  author = {Tan, Hao Hao and Herremans, Dorien},
  year = {2020},
  month = jul,
  abstract = {High-level musical qualities (such as emotion) are often abstract, subjective, and hard to quantify. Given these difficulties, it is not easy to learn good feature representations with supervised learning techniques, either because of the insufficiency of labels, or the subjectiveness (and hence large variance) in human-annotated labels. In this paper, we present a framework that can learn high-level feature representations with a limited amount of data, by first modelling their corresponding quantifiable low-level attributes. We refer to our proposed framework as Music FaderNets, which is inspired by the fact that low-level attributes can be continuously manipulated by separate "sliding faders" through feature disentanglement and latent regularization techniques. High-level features are then inferred from the low-level representations through semi-supervised clustering using Gaussian Mixture Variational Autoencoders (GM-VAEs). Using arousal as an example of a high-level feature, we show that the "faders" of our model are disentangled and change linearly w.r.t. the modelled low-level attributes of the generated output music. Furthermore, we demonstrate that the model successfully learns the intrinsic relationship between arousal and its corresponding low-level attributes (rhythm and note density), with only 1\% of the training set being labelled. Finally, using the learnt high-level feature representations, we explore the application of our framework in style transfer tasks across different arousal states. The effectiveness of this approach is verified through a subjective listening test.},
  archivePrefix = {arXiv},
  eprint = {2007.15474},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tan et al_2020_music fadernets.pdf},
  journal = {arXiv:2007.15474 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{tan20_RecursiveTopDownProduction,
  title = {Recursive {{Top}}-{{Down Production}} for {{Sentence Generation}} with {{Latent Trees}}},
  author = {Tan, Shawn and Shen, Yikang and O'Donnell, Timothy J. and Sordoni, Alessandro and Courville, Aaron},
  year = {2020},
  month = oct,
  abstract = {We model the recursive production property of context-free grammars for natural and synthetic languages. To this end, we present a dynamic programming algorithm that marginalises over latent binary tree structures with \$N\$ leaves, allowing us to compute the likelihood of a sequence of \$N\$ tokens under a latent tree model, which we maximise to train a recursive neural function. We demonstrate performance on two synthetic tasks: SCAN (Lake and Baroni, 2017), where it outperforms previous models on the LENGTH split, and English question formation (McCoy et al., 2020), where it performs comparably to decoders with the ground-truth tree structure. We also present experimental results on German-English translation on the Multi30k dataset (Elliott et al., 2016), and qualitatively analyse the induced tree structures our model learns for the SCAN tasks and the German-English translation task.},
  archivePrefix = {arXiv},
  eprint = {2010.04704},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tan et al_2020_recursive top-down production for sentence generation with latent trees.pdf},
  journal = {arXiv:2010.04704 [cs]},
  primaryClass = {cs}
}

@article{tang19_LearningCorrelatedLatent,
  title = {Learning {{Correlated Latent Representations}} with {{Adaptive Priors}}},
  author = {Tang, Da and Liang, Dawen and Ruozzi, Nicholas and Jebara, Tony},
  year = {2019},
  month = dec,
  abstract = {Variational Auto-Encoders (VAEs) have been widely applied for learning compact, low-dimensional latent representations of high-dimensional data. When the correlation structure among data points is available, previous work proposed Correlated Variational Auto-Encoders (CVAEs), which employ a structured mixture model as prior and a structured variational posterior for each mixture component to enforce that the learned latent representations follow the same correlation structure. However, as we demonstrate in this work, such a choice cannot guarantee that CVAEs capture all the correlations. Furthermore, it prevents us from obtaining a tractable joint and marginal variational distribution. To address these issues, we propose Adaptive Correlated Variational Auto-Encoders (ACVAEs), which apply an adaptive prior distribution that can be adjusted during training and can learn a tractable joint variational distribution. Its tractable form also enables further refinement with belief propagation. Experimental results on link prediction and hierarchical clustering show that ACVAEs significantly outperform CVAEs among other benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1906.06419},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tang et al_2019_learning correlated latent representations with adaptive priors.pdf},
  journal = {arXiv:1906.06419 [cs, stat]},
  keywords = {_tablet},
  primaryClass = {cs, stat}
}

@article{tang19_MultipleFuturesPrediction,
  title = {Multiple {{Futures Prediction}}},
  author = {Tang, Yichuan Charlie and Salakhutdinov, Ruslan},
  year = {2019},
  month = nov,
  abstract = {Temporal prediction is critical for making intelligent and robust decisions in complex dynamic environments. Motion prediction needs to model the inherently uncertain future which often contains multiple potential outcomes, due to multi-agent interactions and the latent goals of others. Towards these goals, we introduce a probabilistic framework that efficiently learns latent variables to jointly model the multi-step future motions of agents in a scene. Our framework is data-driven and learns semantically meaningful latent variables to represent the multimodal future, without requiring explicit labels. Using a dynamic attention-based state encoder, we learn to encode the past as well as the future interactions among agents, efficiently scaling to any number of agents. Finally, our model can be used for planning via computing a conditional probability density over the trajectories of other agents given a hypothetical rollout of the 'self' agent. We demonstrate our algorithms by predicting vehicle trajectories of both simulated and real data, demonstrating the state-of-the-art results on several vehicle trajectory datasets.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1911.00997},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tang et al_2019_multiple futures prediction.pdf;/home/trung/Zotero/storage/CNS5YVXQ/1911.html},
  journal = {arXiv:1911.00997 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics,semi-supervised,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{tang20_crazyhottommyscATACseqanalysisnotes,
  title = {Crazyhottommy/{{scATACseq}}-Analysis-Notes},
  author = {Tang, Ming},
  year = {2020},
  month = nov,
  abstract = {my notes for scATACseq analysis. Contribute to crazyhottommy/scATACseq-analysis-notes development by creating an account on GitHub.}
}

@article{tangnguyen19_MarkovInformationBottleneck,
  title = {Markov {{Information Bottleneck}} to {{Improve Information Flow}} in {{Stochastic Neural Networks}}},
  author = {Tang Nguyen, Thanh and Choi, Jaesik},
  year = {2019},
  volume = {21},
  issn = {1099-4300},
  doi = {10.3390/e21100976},
  abstract = {While rate distortion theory compresses data under a distortion constraint, information bottleneck (IB) generalizes rate distortion theory to learning problems by replacing a distortion constraint with a constraint of relevant information. In this work, we further extend IB to multiple Markov bottlenecks (i.e., latent variables that form a Markov chain), namely Markov information bottleneck (MIB), which particularly fits better in the context of stochastic neural networks (SNNs) than the original IB. We show that Markov bottlenecks cannot simultaneously achieve their information optimality in a non-collapse MIB, and thus devise an optimality compromise. With MIB, we take the novel perspective that each layer of an SNN is a bottleneck whose learning goal is to encode relevant information in a compressed form from the data. The inference from a hidden layer to the output layer is then interpreted as a variational approximation to the layer\&rsquo;s decoding of relevant information in the MIB. As a consequence of this perspective, the maximum likelihood estimate (MLE) principle in the context of SNNs becomes a special case of the variational MIB. We show that, compared to MLE, the variational MIB can encourage better information flow in SNNs in both principle and practice, and empirically improve performance in classification, adversarial robustness, and multi-modal learning in MNIST.},
  file = {/home/trung/GoogleDrive/Zotero/tang nguyen et al_2019_markov information bottleneck to improve information flow in stochastic neural networks.pdf},
  journal = {Entropy},
  keywords = {information,information bottleneck,machine learning,stochastic neural networks,variational inference},
  number = {10}
}

@misc{target_blank[link]19_InfoVAEBalancingLearning,
  title = {{{InfoVAE}}: {{Balancing Learning}} and {{Inference}} in {{Variational Autoencoders}}},
  shorttitle = {{{InfoVAE}}},
  author = {et al {target='\_blank'{$>$}[link]{$<$}/a{$>$}}, 2017, , Zhao},
  year = {2019},
  month = may,
  abstract = {Two known shortcomings of VAEs are that (i) The variational bound (ELBO) can lead to poor approximation of the true likelihood and inacurrate models and (ii) the model can ignore the learned latent representation when the decoder is too powerful. In this work, the author propose to tackle these problems by adding an explicit mutual information term to the standard VAE objective. Pros (+): Well justified, fast implementation trick, semi-supervised setting. Cons (-): requires explicit knowledge of the sensitive attribute.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/Z9BVKEYU/infovae.html},
  howpublished = {https://ameroyer.github.io/reading-notes/representation\%20learning/2019/05/02/infovae.html},
  journal = {Reading Notes},
  language = {en-US}
}

@article{tavakoli20_ContinuousRepresentationMolecules,
  title = {Continuous {{Representation}} of {{Molecules Using Graph Variational Autoencoder}}},
  author = {Tavakoli, Mohammadamin and Baldi, Pierre},
  year = {2020},
  month = apr,
  abstract = {In order to continuously represent molecules, we propose a generative model in the form of a VAE which is operating on the 2D-graph structure of molecules. A side predictor is employed to prune the latent space and help the decoder in generating meaningful adjacency tensor of molecules. Other than the potential applicability in drug design and property prediction, we show the superior performance of this technique in comparison to other similar methods based on the SMILES representation of the molecules with RNN based encoder and decoder.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {2004.08152},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/JKQRR6HI/Tavakoli and Baldi - 2020 - Continuous Representation of Molecules Using Graph.pdf},
  journal = {arXiv:2004.08152 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{tay18_DenselyConnectedAttention,
  title = {Densely {{Connected Attention Propagation}} for {{Reading Comprehension}}},
  author = {Tay, Yi and Tuan, Luu Anh and Hui, Siu Cheung and Su, Jian},
  year = {2018},
  month = nov,
  abstract = {We propose DecaProp (Densely Connected Attention Propagation), a new densely connected neural architecture for reading comprehension (RC). There are two distinct characteristics of our model. Firstly, our model densely connects all pairwise layers of the network, modeling relationships between passage and query across all hierarchical levels. Secondly, the dense connectors in our network are learned via attention instead of standard residual skip-connectors. To this end, we propose novel Bidirectional Attention Connectors (BAC) for efficiently forging connections throughout the network. We conduct extensive experiments on four challenging RC benchmarks. Our proposed approach achieves state-of-the-art results on all four, outperforming existing baselines by up to \$2.6\textbackslash\%-14.2\textbackslash\%\$ in absolute F1 score.},
  annotation = {ZSCC: 0000006},
  archivePrefix = {arXiv},
  eprint = {1811.04210},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tay et al_2018_densely connected attention propagation for reading comprehension.pdf;/home/trung/Zotero/storage/CJTDZNEY/1811.html},
  journal = {arXiv:1811.04210 [cs]},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing,information},
  primaryClass = {cs}
}

@article{tay20_EfficientTransformersSurvey,
  title = {Efficient {{Transformers}}: {{A Survey}}},
  shorttitle = {Efficient {{Transformers}}},
  author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
  year = {2020},
  month = sep,
  abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
  archivePrefix = {arXiv},
  eprint = {2009.06732},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tay et al_2020_efficient transformers.pdf},
  journal = {arXiv:2009.06732 [cs]},
  primaryClass = {cs}
}

@article{teh06_HierarchicalDirichletProcesses,
  title = {Hierarchical {{Dirichlet Processes}}},
  author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
  year = {2006},
  month = dec,
  volume = {101},
  pages = {1566--1581},
  issn = {0162-1459, 1537-274X},
  doi = {10.1198/016214506000000302},
  abstract = {We propose the hierarchical Dirichlet process (HDP), a nonparametric Bayesian model for clustering problems involving multiple groups of data. Each group of data is modeled with a mixture, with the number of components being open-ended and inferred automatically by the model. Further, components can be shared across groups, allowing dependencies across groups to be modeled effectively as well as conferring generalization to new groups. Such grouped clustering problems occur often in practice, e.g. in the problem of topic discovery in document corpora. We report experimental results on three text corpora showing the effective and superior performance of the HDP over previous models.},
  file = {/home/trung/GoogleDrive/Zotero/teh et al_2006_hierarchical dirichlet processes.pdf},
  journal = {Journal of the American Statistical Association},
  language = {en},
  number = {476}
}

@incollection{teh07_collapsedvariationalbayesian,
  title = {A Collapsed Variational Bayesian Inference Algorithm for Latent Dirichlet Allocation},
  booktitle = {Advances in Neural Information Processing Systems 19},
  author = {Teh, Yee W. and Newman, David and Welling, Max},
  editor = {Sch{\"o}lkopf, B. and Platt, J. C. and Hoffman, T.},
  year = {2007},
  pages = {1353--1360},
  publisher = {{MIT Press}},
  file = {/home/trung/GoogleDrive/Zotero/teh et al_2007_a collapsed variational bayesian inference algorithm for latent dirichlet allocation.pdf}
}

@article{tenenbaum00_SeparatingStyleContent,
  title = {Separating {{Style}} and {{Content}} with {{Bilinear Models}}},
  author = {Tenenbaum, Joshua B. and Freeman, William T.},
  year = {2000},
  month = jun,
  volume = {12},
  pages = {1247--1283},
  issn = {0899-7667, 1530-888X},
  doi = {10.1162/089976600300015349},
  abstract = {Perceptual systems routinely separate ``content'' from ``style,'' classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, \& Bibby, 1979; Hinton \& Zemel, 1994; Ghahramani, 1995; Bell \& Sejnowski, 1995; Hinton, Dayan, Frey, \& Neal, 1995; Dayan, Hinton, Neal, \& Zemel, 1995; Hinton \& Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants.},
  file = {/home/trung/GoogleDrive/Zotero/tenenbaum et al_2000_separating style and content with bilinear models.pdf},
  journal = {Neural Computation},
  keywords = {disentanglement},
  language = {en},
  number = {6}
}

@article{teye18_BayesianUncertaintyEstimation,
  title = {Bayesian {{Uncertainty Estimation}} for {{Batch Normalized Deep Networks}}},
  author = {Teye, Mattias and Azizpour, Hossein and Smith, Kevin},
  year = {2018},
  month = feb,
  abstract = {We show that training a deep network using batch normalization is equivalent to approximate inference in Bayesian models. We further demonstrate that this finding allows us to make meaningful estimates of the model uncertainty using conventional architectures, without modifications to the network or the training procedure. Our approach is thoroughly validated by measuring the quality of uncertainty in a series of empirical experiments on different tasks. It outperforms baselines with strong statistical significance, and displays competitive performance with recent Bayesian approaches.},
  archivePrefix = {arXiv},
  eprint = {1802.06455},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/teye et al_2018_bayesian uncertainty estimation for batch normalized deep networks.pdf},
  journal = {arXiv:1802.06455 [stat]},
  keywords = {Statistics - Machine Learning,variational},
  primaryClass = {stat}
}

@article{thanh-tung00_CatastrophicForgettingMode,
  title = {On {{Catastrophic Forgetting}} and {{Mode Collapse}} in {{GANs}}},
  author = {{Thanh-Tung}, Hoang and Tran, Truyen},
  pages = {28},
  abstract = {In this paper, we show that Generative Adversarial Networks (GANs) suffer from catastrophic forgetting even when they are trained to approximate a single target distribution. We show that GAN training is a continual learning problem in which the sequence of changing model distributions is the sequence of tasks to the discriminator. The level of mismatch between tasks in the sequence determines the level of forgetting. Catastrophic forgetting is interrelated to mode collapse and can make the training of GANs nonconvergent. We investigate the landscape of the discriminator's output in different variants of GANs and find that when a GAN converges to a good equilibrium, real training datapoints are wide local maxima of the discriminator. Catastrophic forgetting prevents the discriminator from making real datapoints its local maxima. Finally, we study methods for preventing catastrophic forgetting in GANs.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/WS8I53XB/Thanh-Tung and Tran - On Catastrophic Forgetting and Mode Collapse in GA.pdf},
  language = {en}
}

@article{thanh-tung00_ImprovingGeneralizationStability,
  title = {Improving {{Generalization}} and {{Stability}} of {{GANs}}},
  author = {{Thanh-Tung}, Hoang and Tran, Truyen and Venkatesh, Svetha},
  pages = {1},
  file = {/home/trung/GoogleDrive/Zotero/thanh-tung et al_improving generalization and stability of gans.pdf},
  language = {en}
}

@article{theis16_noteevaluationgenerative,
  title = {A Note on the Evaluation of Generative Models},
  author = {Theis, Lucas and van den Oord, A{\"a}ron and Bethge, Matthias},
  year = {2016},
  month = apr,
  abstract = {Probabilistic generative models can be used for compression, denoising, inpainting, texture synthesis, semi-supervised learning, unsupervised feature learning, and other tasks. Given this wide range of applications, it is not surprising that a lot of heterogeneity exists in the way these models are formulated, trained, and evaluated. As a consequence, direct comparison between models is often difficult. This article reviews mostly known but often underappreciated properties relating to the evaluation and interpretation of generative models with a focus on image models. In particular, we show that three of the currently most commonly used criteria\textemdash average log-likelihood, Parzen window estimates, and visual fidelity of samples\textemdash are largely independent of each other when the data is high-dimensional. Good performance with respect to one criterion therefore need not imply good performance with respect to the other criteria. Our results show that extrapolation from one criterion to another is not warranted and generative models need to be evaluated directly with respect to the application(s) they were intended for. In addition, we provide examples demonstrating that Parzen window estimates should generally be avoided.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1511.01844},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/theis et al_2016_a note on the evaluation of generative models.pdf},
  journal = {arXiv:1511.01844 [cs, stat]},
  keywords = {disentanglement,favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{thio19_vibertthioawesomemachinelearningart,
  title = {Vibertthio/Awesome-Machine-Learning-Art},
  author = {Thio, Vibert},
  year = {2019},
  month = dec,
  abstract = {:robot::art::guitar:A curated list of awesome projects, works, people, articles, and resource for creating art (including music) with machine learning. It\&\#39;s machine learning art.}
}

@article{thomas13_AscorbicAcidSupplements,
  title = {Ascorbic {{Acid Supplements}} and {{Kidney Stone Incidence Among Men}}: {{A Prospective Study}}},
  shorttitle = {Ascorbic {{Acid Supplements}} and {{Kidney Stone Incidence Among Men}}},
  author = {Thomas, Laura D. K. and Elinder, Carl-Gustaf and Tiselius, Hans-G{\"o}ran and Wolk, Alicja and {\AA}kesson, Agneta},
  year = {2013},
  month = mar,
  volume = {173},
  pages = {386--388},
  issn = {2168-6106},
  doi = {10.1001/jamainternmed.2013.2296},
  abstract = {Urinary oxalate is an important determinant of calcium oxalate kidney stone formation.1 Vitamin C is excreted in urine both in its unmetabolized form and as oxalate; however, there remains considerable uncertainty over the kidney stone risk that may be associated with ascorbic acid supplement use.2},
  annotation = {ZSCC: 0000089},
  file = {/home/trung/GoogleDrive/Zotero/thomas et al_2013_ascorbic acid supplements and kidney stone incidence among men.pdf;/home/trung/Zotero/storage/EHXS7FBF/1568519.html},
  journal = {JAMA Internal Medicine},
  language = {en},
  number = {5}
}

@article{thomas19_interplaynoisecurvature,
  title = {On the Interplay between Noise and Curvature and Its Effect on Optimization and Generalization},
  author = {Thomas, Valentin and Pedregosa, Fabian and {van Merri{\"e}nboer}, Bart and Mangazol, Pierre-Antoine and Bengio, Yoshua and Roux, Nicolas Le},
  year = {2019},
  month = jun,
  abstract = {The speed at which one can minimize an expected loss using stochastic methods depends on two properties: the curvature of the loss and the variance of the gradients. While most previous works focus on one or the other of these properties, we explore how their interaction affects optimization speed. Further, as the ultimate goal is good generalization performance, we clarify how both curvature and noise are relevant to properly estimate the generalization gap. Realizing that the limitations of some existing works stems from a confusion between these matrices, we also clarify the distinction between the Fisher matrix, the Hessian, and the covariance matrix of the gradients.},
  archivePrefix = {arXiv},
  eprint = {1906.07774},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/thomas et al_2019_on the interplay between noise and curvature and its effect on optimization and generalization.pdf},
  journal = {arXiv:1906.07774 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{thomas19_UsingArtificialIntelligence,
  title = {Using {{Artificial Intelligence}} to {{Augment Science Prioritization}} for {{Astro2020}}},
  author = {Thomas, Brian and Thronson, Harley and Adrian, Andrew and Lowndes, Alison and Mason, James and Memarsadeghi, Nargess and Samadi, Shahin and Varsi, Giulio},
  year = {2019},
  month = aug,
  abstract = {Science funding agencies (NASA, DOE, and NSF), the science community, and the US taxpayer have all benefited enormously from the several-decade series of National Academies Decadal Surveys. These Surveys are one of the primary means whereby these agencies may align multi-year strategic priorities and funding to guide the scientific community. They comprise highly regarded subject matter experts whose goal is to develop a set of science and program priorities that are recommended for major investments in the subsequent 10+ years. They do this using both their own professional knowledge and by synthesizing details from many thousands of existing and solicited documents. Congress, the relevant funding agencies, and the scientific community have placed great respect and value on these recommendations. Consequently, any significant changes to the process of determining these recommendations should be scrutinized carefully. That said, we believe that there is currently sufficient justification for the National Academies to consider some changes. We advocate that they supplement the established survey process with predictions of promising science priorities identified by application of current Artificial Intelligence (AI) techniques These techniques are being applied elsewhere in long-range planning and prioritization. We present a proposal to apply AI to aid the Decadal Survey panel in prioritizing science objectives. We emphasize that while AI can assist a mass review of papers, the decision-making remains with humans. In our paper below we summarize the case for using AI in this manner and suggest small inexpensive demonstration trials, including an AI/ML assessment of the white papers submitted to Astro2020 and backcasting to evaluate AI in making predictions for the 2010 Decadal Survey.},
  archivePrefix = {arXiv},
  eprint = {1908.00369},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/thomas et al_2019_using artificial intelligence to augment science prioritization for astro2020.pdf;/home/trung/Zotero/storage/C72EAD5V/1908.html},
  journal = {arXiv:1908.00369 [astro-ph]},
  primaryClass = {astro-ph}
}

@article{tian19_LatentTranslationCrossing,
  title = {Latent {{Translation}}: {{Crossing Modalities}} by {{Bridging Generative Models}}},
  shorttitle = {Latent {{Translation}}},
  author = {Tian, Yingtao and Engel, Jesse},
  year = {2019},
  month = feb,
  abstract = {End-to-end optimization has achieved state-of-the-art performance on many specific problems, but there is no straight-forward way to combine pretrained models for new problems. Here, we explore improving modularity by learning a post-hoc interface between two existing models to solve a new task. Specifically, we take inspiration from neural machine translation, and cast the challenging problem of cross-modal domain transfer as unsupervised translation between the latent spaces of pretrained deep generative models. By abstracting away the data representation, we demonstrate that it is possible to transfer across different modalities (e.g., image-to-audio) and even different types of generative models (e.g., VAE-to-GAN). We compare to state-of-the-art techniques and find that a straight-forward variational autoencoder is able to best bridge the two generative models through learning a shared latent space. We can further impose supervised alignment of attributes in both domains with a classifier in the shared latent space. Through qualitative and quantitative evaluations, we demonstrate that locality and semantic alignment are preserved through the transfer process, as indicated by high transfer accuracies and smooth interpolations within a class. Finally, we show this modular structure speeds up training of new interface models by several orders of magnitude by decoupling it from expensive retraining of base generative models.},
  archivePrefix = {arXiv},
  eprint = {1902.08261},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tian et al_2019_latent translation.pdf},
  journal = {arXiv:1902.08261 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{tian20_UnderstandingSelfsupervisedLearning,
  title = {Understanding {{Self}}-Supervised {{Learning}} with {{Dual Deep Networks}}},
  author = {Tian, Yuandong and Yu, Lantao and Chen, Xinlei and Ganguli, Surya},
  year = {2020},
  month = oct,
  abstract = {We propose a novel theoretical framework to understand self-supervised learning methods that employ dual pairs of deep ReLU networks (e.g., SimCLR, BYOL). First, we prove that in each SGD update of SimCLR, the weights at each layer are updated by a \textbackslash emph\{covariance operator\} that specifically amplifies initial random selectivities that vary across data samples but survive averages over data augmentations, which we show leads to the emergence of hierarchical features, if the input data are generated from a hierarchical latent tree model. With the same framework, we also show analytically that BYOL works due to an implicit contrastive term, acting as an approximate covariance operator. The term is formed by the inter-play between the zero-mean operation of BatchNorm and the extra predictor in the online network. Extensive ablation studies justify our theoretical findings.},
  archivePrefix = {arXiv},
  eprint = {2010.00578},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tian et al_2020_understanding self-supervised learning with dual deep networks.pdf},
  journal = {arXiv:2010.00578 [cs, stat]},
  keywords = {self-supervised},
  primaryClass = {cs, stat}
}

@article{tildesley03_SalvialavandulaefoliaSpanish,
  title = {Salvia Lavandulaefolia ({{Spanish Sage}}) Enhances Memory in Healthy Young Volunteers},
  author = {Tildesley, N. T. J and Kennedy, D. O and Perry, E. K and Ballard, C. G and Savelev, S and Wesnes, K. A and Scholey, A. B},
  year = {2003},
  month = jun,
  volume = {75},
  pages = {669--674},
  issn = {0091-3057},
  doi = {10.1016/S0091-3057(03)00122-9},
  abstract = {Sage (Salvia) has a longstanding reputation in British herbal encyclopaedias as an agent that enhances memory, although there is little evidence regarding the efficacy of sage from systematized trials. Based on known pharmacokinetic and binding properties, it was hypothesised that acute administration of sage would enhance memory in young adult volunteers. Two experiments utilised a placebo-controlled, double-blind, balanced, crossover methodology. In Trial 1, 20 participants received 50, 100 and 150 {$\mu$}l of a standardised essential oil extract of Salvia lavandulaefolia and placebo. In Trial 2, 24 participants received 25 and 50 {$\mu$}l of a standardised essential oil extract of S. lavandulaefolia and placebo. Doses were separated by a 7-day washout period with treatment order determined by Latin squares. Assessment was undertaken using the Cognitive Drug Research computerised test battery prior to treatment and 1, 2.5, 4 and 6 h thereafter. The primary outcome measures were immediate and delayed word recall. The 50 {$\mu$}l dose of Salvia essential oil significantly improved immediate word recall in both studies. These results represent the first systematic evidence that Salvia is capable of acute modulation of cognition in healthy young adults.},
  file = {/home/trung/GoogleDrive/Zotero/tildesley et al_2003_salvia lavandulaefolia (spanish sage) enhances memory in healthy young volunteers.pdf},
  journal = {Pharmacology Biochemistry and Behavior},
  language = {en},
  number = {3},
  series = {Plants and the {{Central Nervous System}}}
}

@article{timmers20_Multivariategenomicscan,
  title = {Multivariate Genomic Scan Implicates Novel Loci and Haem Metabolism in Human Ageing},
  author = {Timmers, Paul R. H. J. and Wilson, James F. and Joshi, Peter K. and Deelen, Joris},
  year = {2020},
  month = jul,
  volume = {11},
  pages = {3570},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-17312-3},
  abstract = {Ageing phenotypes, such as years lived in good health (healthspan), total years lived (lifespan), and survival until an exceptional old age (longevity), are of interest to us all but require exceptionally large sample sizes to study genetically. Here we combine existing genome-wide association summary statistics for healthspan, parental lifespan, and longevity in a multivariate framework, increasing statistical power, and identify 10 genomic loci which influence all three phenotypes, of which five (near FOXO3, SLC4A7, LINC02513, ZW10, and FGD6) have not been reported previously at genome-wide significance. The majority of these 10 loci are associated with cardiovascular disease and some affect the expression of genes known to change their activity with age. In total, we implicate 78 genes, and find these to be enriched for ageing pathways previously highlighted in model organisms, such as the response to DNA damage, apoptosis, and homeostasis. Finally, we identify a pathway worthy of further study: haem metabolism.},
  file = {/home/trung/GoogleDrive/Zotero/timmers et al_2020_multivariate genomic scan implicates novel loci and haem metabolism in human ageing.pdf},
  journal = {Nature Communications},
  number = {1}
}

@article{tishby15_DeepLearningInformation,
  title = {Deep {{Learning}} and the {{Information Bottleneck Principle}}},
  author = {Tishby, Naftali and Zaslavsky, Noga},
  year = {2015},
  month = mar,
  abstract = {Deep Neural Networks (DNNs) are analyzed via the theoretical framework of the information bottleneck (IB) principle. We first show that any DNN can be quantified by the mutual information between the layers and the input and output variables. Using this representation we can calculate the optimal information theoretic limits of the DNN and obtain finite sample generalization bounds. The advantage of getting closer to the theoretical limit is quantifiable both by the generalization bound and by the network's simplicity. We argue that both the optimal architecture, number of layers and features/connections at each layer, are related to the bifurcation points of the information bottleneck tradeoff, namely, relevant compression of the input layer with respect to the output layer. The hierarchical representations at the layered network naturally correspond to the structural phase transitions along the information curve. We believe that this new insight can lead to new optimality bounds and deep learning algorithms.},
  annotation = {ZSCC: 0000350},
  archivePrefix = {arXiv},
  eprint = {1503.02406},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tishby et al_2015_deep learning and the information bottleneck principle.pdf;/home/trung/Zotero/storage/QXZKTLY8/1503.html},
  journal = {arXiv:1503.02406 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@inproceedings{tishby99_informationbottleneckmethod,
  title = {The Information Bottleneck Method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = {1999},
  pages = {368--377},
  file = {/home/trung/GoogleDrive/Zotero/tishby et al_1999_the information bottleneck method.pdf},
  keywords = {information}
}

@inproceedings{tishby99_informationbottleneckmethoda,
  title = {The Information Bottleneck Method},
  booktitle = {Proc. of the 37-Th Annual Allerton Conference on Communication, Control and Computing},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = {1999},
  pages = {368--377},
  added-at = {2017-09-27T13:23:06.000+0200},
  biburl = {https://www.bibsonomy.org/bibtex/2c61af806ab3a8fe92154753e84736818/mo\textsubscript{x}ime},
  file = {/home/trung/GoogleDrive/Zotero/tishby et al_1999_the information bottleneck method2.pdf},
  interhash = {15bd5efbf394791da00b09839b9a5757},
  intrahash = {c61af806ab3a8fe92154753e84736818},
  keywords = {information},
  timestamp = {2017-09-27T15:48:05.000+0200}
}

@article{tixier18_NotesDeepLearning,
  title = {Notes on {{Deep Learning}} for {{NLP}}},
  author = {Tixier, Antoine J.-P.},
  year = {2018},
  month = aug,
  abstract = {My notes on Deep Learning for NLP.},
  archivePrefix = {arXiv},
  eprint = {1808.09772},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tixier_2018_notes on deep learning for nlp.pdf},
  journal = {arXiv:1808.09772 [cs]},
  primaryClass = {cs}
}

@article{tjandra19_DejavuDoubleFeature,
  title = {Deja-vu: {{Double Feature Presentation}} in {{Deep Transformer Networks}}},
  shorttitle = {Deja-Vu},
  author = {Tjandra, Andros and Liu, Chunxi and Zhang, Frank and Zhang, Xiaohui and Wang, Yongqiang and Synnaeve, Gabriel and Nakamura, Satoshi and Zweig, Geoffrey},
  year = {2019},
  month = oct,
  abstract = {Deep acoustic models typically receive features in the first layer of the network, and process increasingly abstract representations in the subsequent layers. Here, we propose to feed the input features at multiple depths in the acoustic model. As our motivation is to allow acoustic models to re-examine their input features in light of partial hypotheses we introduce intermediate model heads and loss function. We study this architecture in the context of deep Transformer networks, and we use an attention mechanism over both the previous layer activations and the input features. To train this model's intermediate output hypothesis, we apply the objective function at each layer right before feature re-use. We find that the use of such intermediate losses significantly improves performance by itself, as well as enabling input feature re-use. We present results on both Librispeech, and a large scale video dataset, with relative improvements of 10 - 20\% for Librispeech and 3.2 - 13\% for videos.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1910.10324},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tjandra et al_2019_deja-vu.pdf;/home/trung/Zotero/storage/3KR5KVNR/1910.html},
  journal = {arXiv:1910.10324 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,transformer},
  primaryClass = {cs, eess}
}

@article{tjandra20_TransformerVQVAEUnsupervised,
  title = {Transformer {{VQ}}-{{VAE}} for {{Unsupervised Unit Discovery}} and {{Speech Synthesis}}: {{ZeroSpeech}} 2020 {{Challenge}}},
  shorttitle = {Transformer {{VQ}}-{{VAE}} for {{Unsupervised Unit Discovery}} and {{Speech Synthesis}}},
  author = {Tjandra, Andros and Sakti, Sakriani and Nakamura, Satoshi},
  year = {2020},
  month = may,
  abstract = {In this paper, we report our submitted system for the ZeroSpeech 2020 challenge on Track 2019. The main theme in this challenge is to build a speech synthesizer without any textual information or phonetic labels. In order to tackle those challenges, we build a system that must address two major components such as 1) given speech audio, extract subword units in an unsupervised way and 2) resynthesize the audio from novel speakers. The system also needs to balance the codebook performance between the ABX error rate and the bitrate compression rate. Our main contribution here is we proposed Transformer-based VQ-VAE for unsupervised unit discovery and Transformerbased inverter for the speech synthesis given the extracted codebook. Additionally, we also explored several regularization methods to improve performance even further.},
  archivePrefix = {arXiv},
  eprint = {2005.11676},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tjandra et al_2020_transformer vq-vae for unsupervised unit discovery and speech synthesis.pdf},
  journal = {arXiv:2005.11676 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{tjandra20_UnsupervisedLearningDisentangled,
  title = {Unsupervised {{Learning}} of {{Disentangled Speech Content}} and {{Style Representation}}},
  author = {Tjandra, Andros and Pang, Ruoming and Zhang, Yu and Karita, Shigeki},
  year = {2020},
  month = oct,
  abstract = {We present an approach for unsupervised learning of speech representation disentangling contents and styles. Our model consists of: (1) a local encoder that captures per-frame information; (2) a global encoder that captures per-utterance information; and (3) a conditional decoder that reconstructs speech given local and global latent variables. Our experiments show that (1) the local latent variables encode speech contents, as reconstructed speech can be recognized by ASR with low word error rates (WER), even with a different global encoding; (2) the global latent variables encode speaker style, as reconstructed speech shares speaker identity with the source utterance of the global encoding. Additionally, we demonstrate an useful application from our pre-trained model, where we can train a speaker recognition model from the global latent variables and achieve high accuracy by fine-tuning with as few data as one label per speaker.},
  archivePrefix = {arXiv},
  eprint = {2010.12973},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tjandra et al_2020_unsupervised learning of disentangled speech content and style representation.pdf},
  journal = {arXiv:2010.12973 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{tobin00_TroubleshootingDeepNeural,
  title = {Troubleshooting {{Deep Neural Networks}}},
  author = {Tobin, Josh},
  pages = {141},
  file = {/home/trung/GoogleDrive/Zotero/tobin_troubleshooting deep neural networks.pdf},
  language = {en}
}

@article{todorovic20_Amplificationfreesinglecellwholegenome,
  title = {Amplification-Free Single-Cell Whole-Genome Sequencing Gets a Makeover},
  author = {Todorovic, Vesna},
  year = {2020},
  month = jan,
  volume = {17},
  pages = {27--27},
  issn = {1548-7091, 1548-7105},
  doi = {10.1038/s41592-019-0722-2},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/M5IGF945/Todorovic - 2020 - Amplification-free single-cell whole-genome sequen.pdf},
  journal = {Nature Methods},
  language = {en},
  number = {1}
}

@article{tolias18_AmortizedContextVector,
  title = {Amortized {{Context Vector Inference}} for {{Sequence}}-to-{{Sequence Networks}}},
  author = {Tolias, Kyriacos and Kourouklides, Ioannis and Chatzis, Sotirios},
  year = {2018},
  month = may,
  abstract = {Neural attention (NA) has become a key component of sequence-to-sequence models that yield state-of-the-art performance in as hard tasks as abstractive document summarization (ADS) and video captioning (VC). NA mechanisms perform inference of context vectors; these constitute weighted sums of deterministic input sequence encodings, adaptively sourced over long temporal horizons. Inspired from recent work in the field of amortized variational inference (AVI), in this work we consider treating the context vectors generated by soft-attention (SA) models as latent variables, with approximate finite mixture model posteriors inferred via AVI. We posit that this formulation may yield stronger generalization capacity, in line with the outcomes of existing applications of AVI to deep networks. To illustrate our method, we implement it and experimentally evaluate it considering challenging ADS, VC, and MT benchmarks. This way, we exhibit its improved effectiveness over state-of-the-art alternatives.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1805.09039},
  eprinttype = {arxiv},
  journal = {arXiv:1805.09039 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{tolosana20_DeepFakesSurveyFace,
  title = {{{DeepFakes}} and {{Beyond}}: {{A Survey}} of {{Face Manipulation}} and {{Fake Detection}}},
  shorttitle = {{{DeepFakes}} and {{Beyond}}},
  author = {Tolosana, Ruben and {Vera-Rodriguez}, Ruben and Fierrez, Julian and Morales, Aythami and {Ortega-Garcia}, Javier},
  year = {2020},
  month = jan,
  abstract = {The free access to large-scale public databases, together with the fast progress of deep learning techniques, in particular Generative Adversarial Networks, have led to the generation of very realistic fake contents with its corresponding implications towards society in this era of fake news. This survey provides a thorough review of techniques for manipulating face images including DeepFake methods, and methods to detect such manipulations. In particular, four types of facial manipulation are reviewed: i) entire face synthesis, ii) face identity swap (DeepFakes), iii) facial attributes manipulation, and iv) facial expression manipulation. For each manipulation type, we provide details regarding manipulation techniques, existing public databases, and key benchmarks for technology evaluation of fake detection methods, including a summary of results from those evaluations. Among the different databases available and discussed in the survey, FaceForensics++ is for example one of the most widely used for detecting both face identity swap and facial expression manipulations, with results in the literature in the range of 90-100\% of manipulation detection accuracy. In addition to the survey information, we also discuss trends and provide an outlook of the ongoing work in this field, e.g., the recently announced DeepFake Detection Challenge (DFDC).},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2001.00179},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tolosana et al_2020_deepfakes and beyond.pdf;/home/trung/Zotero/storage/UKXKHZYK/2001.html},
  journal = {arXiv:2001.00179 [cs]},
  primaryClass = {cs}
}

@article{tomczak18_VAEVampPrior,
  title = {{{VAE}} with a {{VampPrior}}},
  author = {Tomczak, Jakub M. and Welling, Max},
  year = {2018},
  month = feb,
  abstract = {Many different methods to train deep generative models have been introduced in the past. In this paper, we propose to extend the variational auto-encoder (VAE) framework with a new type of prior which we call "Variational Mixture of Posteriors" prior, or VampPrior for short. The VampPrior consists of a mixture distribution (e.g., a mixture of Gaussians) with components given by variational posteriors conditioned on learnable pseudo-inputs. We further extend this prior to a two layer hierarchical model and show that this architecture with a coupled prior and posterior, learns significantly better models. The model also avoids the usual local optima issues related to useless latent dimensions that plague VAEs. We provide empirical studies on six datasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes, Frey Faces and Histopathology patches, and show that applying the hierarchical VampPrior delivers state-of-the-art results on all datasets in the unsupervised permutation invariant setting and the best results or comparable to SOTA methods for the approach with convolutional networks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1705.07120},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tomczak et al_2018_vae with a vampprior.pdf;/home/trung/Zotero/storage/QQLQ4TUB/1705.html},
  journal = {arXiv:1705.07120 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{tong20_TrajectoryNetDynamicOptimal,
  title = {{{TrajectoryNet}}: {{A Dynamic Optimal Transport Network}} for {{Modeling Cellular Dynamics}}},
  shorttitle = {{{TrajectoryNet}}},
  author = {Tong, Alexander and Huang, Jessie and Wolf, Guy and {van Dijk}, David and Krishnaswamy, Smita},
  year = {2020},
  month = jul,
  abstract = {It is increasingly common to encounter data from dynamic processes captured by static cross-sectional measurements over time, particularly in biomedical settings. Recent attempts to model individual trajectories from this data use optimal transport to create pairwise matchings between time points. However, these methods cannot model continuous dynamics and non-linear paths that entities can take in these systems. To address this issue, we establish a link between continuous normalizing flows and dynamic optimal transport, that allows us to model the expected paths of points over time. Continuous normalizing flows are generally under constrained, as they are allowed to take an arbitrary path from the source to the target distribution. We present TrajectoryNet, which controls the continuous paths taken between distributions to produce dynamic optimal transport. We show how this is particularly applicable for studying cellular dynamics in data from single-cell RNA sequencing (scRNA-seq) technologies, and that TrajectoryNet improves upon recently proposed static optimal transport-based models that can be used for interpolating cellular distributions.},
  archivePrefix = {arXiv},
  eprint = {2002.04461},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tong et al_2020_trajectorynet.pdf},
  journal = {arXiv:2002.04461 [cs, q-bio, stat]},
  primaryClass = {cs, q-bio, stat}
}

@article{torkzadehmahani20_DPCGANDifferentiallyPrivate,
  title = {{{DP}}-{{CGAN}}: {{Differentially Private Synthetic Data}} and {{Label Generation}}},
  shorttitle = {{{DP}}-{{CGAN}}},
  author = {Torkzadehmahani, Reihaneh and Kairouz, Peter and Paten, Benedict},
  year = {2020},
  month = jan,
  abstract = {Generative Adversarial Networks (GANs) are one of the well-known models to generate synthetic data including images, especially for research communities that cannot use original sensitive datasets because they are not publicly accessible. One of the main challenges in this area is to preserve the privacy of individuals who participate in the training of the GAN models. To address this challenge, we introduce a Differentially Private Conditional GAN (DP-CGAN) training framework based on a new clipping and perturbation strategy, which improves the performance of the model while preserving privacy of the training dataset. DP-CGAN generates both synthetic data and corresponding labels and leverages the recently introduced Renyi differential privacy accountant to track the spent privacy budget. The experimental results show that DP-CGAN can generate visually and empirically promising results on the MNIST dataset with a single-digit epsilon parameter in differential privacy.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {2001.09700},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/torkzadehmahani et al_2020_dp-cgan.pdf;/home/trung/Zotero/storage/6CUUUP6M/2001.html},
  journal = {arXiv:2001.09700 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{townes20_ReviewProbabilityDistributions,
  title = {Review of {{Probability Distributions}} for {{Modeling Count Data}}},
  author = {Townes, F. William},
  year = {2020},
  month = jan,
  abstract = {Count data take on non-negative integer values and are challenging to properly analyze using standard linear-Gaussian methods such as linear regression and principal components analysis. Generalized linear models enable direct modeling of counts in a regression context using distributions such as the Poisson and negative binomial. When counts contain only relative information, multinomial or Dirichlet-multinomial models can be more appropriate. We review some of the fundamental connections between multinomial and count models from probability theory, providing detailed proofs. These relationships are useful for methods development in applications such as topic modeling of text data and genomics.},
  archivePrefix = {arXiv},
  eprint = {2001.04343},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/townes_2020_review of probability distributions for modeling count data.pdf},
  journal = {arXiv:2001.04343 [stat]},
  keywords = {_tablet},
  language = {en},
  primaryClass = {stat}
}

@article{tran00_BayesianLayersModule,
  title = {Bayesian {{Layers}}: {{A Module}} for {{Neural Network Uncertainty}}},
  author = {Tran, Dustin and Dusenberry, Mike},
  pages = {13},
  annotation = {ZSCC: 0000006},
  file = {/home/trung/GoogleDrive/Zotero/tran et al_bayesian layers.pdf;/home/trung/GoogleDrive/Zotero/tran et al_bayesian layers2.pdf;/home/trung/Zotero/storage/JJ5EAB89/1812.html},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,favorite,Statistics - Machine Learning,uncertainty,variational},
  language = {en}
}

@misc{tran00_DeepLearning,
  title = {Deep {{Learning}} 1.0 and {{Beyond}}},
  author = {Tran, Truyen},
  howpublished = {https://truyentran.github.io/ssci2020-tute.html?fbclid=IwAR3BJd\_Ja\_I890srN7rS2zOoJl9dexQiuZePSbp0-4S8rcJIMIXJu3gZUdQ}
}

@article{tran00_HierarchicalImplicitModels,
  title = {Hierarchical {{Implicit Models}} and {{Likelihood}}-{{Free Variational Inference}}},
  author = {Tran, Dustin and Ranganath, Rajesh and Blei, David},
  pages = {11},
  abstract = {Implicit probabilistic models are a flexible class of models defined by a simulation process for data. They form the basis for theories which encompass our understanding of the physical world. Despite this fundamental nature, the use of implicit models remains limited due to challenges in specifying complex latent structure in them, and in performing inferences in such models with large data sets. In this paper, we first introduce hierarchical implicit models (HIMs). HIMs combine the idea of implicit densities with hierarchical Bayesian modeling, thereby defining models via simulators of data with rich hidden structure. Next, we develop likelihood-free variational inference (LFVI), a scalable variational inference algorithm for HIMs. Key to LFVI is specifying a variational family that is also implicit. This matches the model's flexibility and allows for accurate approximation of the posterior. We demonstrate diverse applications: a large-scale physical simulator for predator-prey populations in ecology; a Bayesian generative adversarial network for discrete data; and a deep implicit model for text generation.},
  file = {/home/trung/GoogleDrive/Zotero/tran et al_hierarchical implicit models and likelihood-free variational inference.pdf},
  language = {en}
}

@article{tran17_DeepProbabilisticProgramming,
  title = {Deep {{Probabilistic Programming}}},
  author = {Tran, Dustin and Hoffman, Matthew D. and Saurous, Rif A. and Brevdo, Eugene and Murphy, Kevin and Blei, David M.},
  year = {2017},
  month = mar,
  abstract = {We propose Edward, a Turing-complete probabilistic programming language. Edward defines two compositional representations---random variables and inference. By treating inference as a first class citizen, on a par with modeling, we show that probabilistic programming can be as flexible and computationally efficient as traditional deep learning. For flexibility, Edward makes it easy to fit the same model using a variety of composable inference methods, ranging from point estimation to variational inference to MCMC. In addition, Edward can reuse the modeling representation as part of inference, facilitating the design of rich variational models and generative adversarial networks. For efficiency, Edward is integrated into TensorFlow, providing significant speedups over existing probabilistic systems. For example, we show on a benchmark logistic regression task that Edward is at least 35x faster than Stan and 6x faster than PyMC3. Further, Edward incurs no runtime overhead: it is as fast as handwritten TensorFlow.},
  annotation = {ZSCC: 0000123},
  archivePrefix = {arXiv},
  eprint = {1701.03757},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tran et al_2017_deep probabilistic programming.pdf;/home/trung/Zotero/storage/TWX9N8IX/1701.html},
  journal = {arXiv:1701.03757 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{tran17_ImplicitCausalModels,
  title = {Implicit {{Causal Models}} for {{Genome}}-Wide {{Association Studies}}},
  author = {Tran, Dustin and Blei, David M.},
  year = {2017},
  month = oct,
  abstract = {Progress in probabilistic generative models has accelerated, developing richer models with neural architectures, implicit densities, and with scalable algorithms for their Bayesian inference. However, there has been limited progress in models that capture causal relationships, for example, how individual genetic factors cause major human diseases. In this work, we focus on two challenges in particular: How do we build richer causal models, which can capture highly nonlinear relationships and interactions between multiple causes? How do we adjust for latent confounders, which are variables influencing both cause and effect and which prevent learning of causal relationships? To address these challenges, we synthesize ideas from causality and modern probabilistic modeling. For the first, we describe implicit causal models, a class of causal models that leverages neural architectures with an implicit density. For the second, we describe an implicit causal model that adjusts for confounders by sharing strength across examples. In experiments, we scale Bayesian inference on up to a billion genetic measurements. We achieve state of the art accuracy for identifying causal factors: we significantly outperform existing genetics methods by an absolute difference of 15-45.3\%.},
  archivePrefix = {arXiv},
  eprint = {1710.10742},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tran et al_2017_implicit causal models for genome-wide association studies.pdf},
  journal = {arXiv:1710.10742 [cs, q-bio, stat]},
  keywords = {causal,Computer Science - Machine Learning,Quantitative Biology - Genomics,Statistics - Applications,Statistics - Machine Learning,Statistics - Methodology},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{tran18_SimpleDistributedAccelerated,
  title = {Simple, {{Distributed}}, and {{Accelerated Probabilistic Programming}}},
  author = {Tran, Dustin and Hoffman, Matthew and Moore, Dave and Suter, Christopher and Vasudevan, Srinivas and Radul, Alexey and Johnson, Matthew and Saurous, Rif A.},
  year = {2018},
  month = nov,
  abstract = {We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction\textemdash the random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (VAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multiGPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.},
  archivePrefix = {arXiv},
  eprint = {1811.02091},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tran et al_2018_simple, distributed, and accelerated probabilistic programming.pdf;/home/trung/GoogleDrive/Zotero/tran et al_2018_simple, distributed, and accelerated probabilistic programming2.pdf},
  journal = {arXiv:1811.02091 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Programming Languages,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{tran20_benchmarkbatcheffectcorrection,
  title = {A Benchmark of Batch-Effect Correction Methods for Single-Cell {{RNA}} Sequencing Data},
  author = {Tran, Hoa Thi Nhu and Ang, Kok Siong and Chevrier, Marion and Zhang, Xiaomeng and Lee, Nicole Yee Shin and Goh, Michelle and Chen, Jinmiao},
  year = {2020},
  month = dec,
  volume = {21},
  pages = {12},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1850-9},
  abstract = {Background: Large-scale single-cell transcriptomic datasets generated using different technologies contain batchspecific systematic variations that present a challenge to batch-effect removal and data integration. With continued growth expected in scRNA-seq data, achieving effective batch integration with available computational resources is crucial. Here, we perform an in-depth benchmark study on available batch correction methods to determine the most suitable method for batch-effect removal. Results: We compare 14 methods in terms of computational runtime, the ability to handle large datasets, and batch-effect correction efficacy while preserving cell type purity. Five scenarios are designed for the study: identical cell types with different technologies, non-identical cell types, multiple batches, big data, and simulated data. Performance is evaluated using four benchmarking metrics including kBET, LISI, ASW, and ARI. We also investigate the use of batch-corrected data to study differential gene expression. Conclusion: Based on our results, Harmony, LIGER, and Seurat 3 are the recommended methods for batch integration. Due to its significantly shorter runtime, Harmony is recommended as the first method to try, with the other methods as viable alternatives.},
  annotation = {ZSCC: 0000009},
  file = {/home/trung/Zotero/storage/342ADBJQ/Tran et al. - 2020 - A benchmark of batch-effect correction methods for.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@misc{tran20_dustinvtranapproximateinference,
  title = {Dustinvtran/Approximateinference},
  author = {Tran, Dustin},
  year = {2020},
  month = feb,
  abstract = {NeurIPS workshop on Advances in Approximate Bayesian Inference},
  annotation = {ZSCC: NoCitationData[s0]}
}

@misc{tran20_dustinvtranmlvideos,
  title = {Dustinvtran/Ml-Videos},
  author = {Tran, Dustin},
  year = {2020},
  month = oct,
  abstract = {A collection of video resources for machine learning}
}

@phdthesis{tran20_ProbabilisticProgrammingDeep,
  title = {Probabilistic {{Programming}} for {{Deep Learning}}},
  author = {Tran, Dustin},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/tran_2020_probabilistic programming for deep learning.pdf},
  keywords = {_tablet,favorite},
  language = {en}
}

@article{trauble20_Independenceallyou,
  title = {Is {{Independence}} All You Need? {{On}} the {{Generalization}} of {{Representations Learned}} from {{Correlated Data}}},
  shorttitle = {Is {{Independence}} All You Need?},
  author = {Tr{\"a}uble, Frederik and Creager, Elliot and Kilbertus, Niki and Goyal, Anirudh and Locatello, Francesco and Sch{\"o}lkopf, Bernhard and Bauer, Stefan},
  year = {2020},
  month = jun,
  abstract = {Despite impressive progress in the last decade, it still remains an open challenge to build models that generalize well across multiple tasks and datasets. One path to achieve this is to learn meaningful and compact representations, in which different semantic aspects of data are structurally disentangled. The focus of disentanglement approaches has been on separating independent factors of variation despite the fact that real-world observations are often not structured into meaningful independent causal variables to begin with. In this work we bridge the gap to real-world scenarios by analyzing the behavior of most prominent methods and disentanglement scores on correlated data in a large scale empirical study (including 3900 models). We show that systematically induced correlations in the dataset are being learned and reflected in the latent representations, while widely used disentanglement scores fall short of capturing these latent correlations. Finally, we demonstrate how to disentangle these latent correlations using weak supervision, even if we constrain this supervision to be causally plausible. Our results thus support the argument to learn independent mechanisms rather than independent factors of variations.},
  archivePrefix = {arXiv},
  eprint = {2006.07886},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/träuble et al_2020_is independence all you need.pdf},
  journal = {arXiv:2006.07886 [cs, stat]},
  keywords = {_tablet,disentanglement,favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{trinh19_NestedVariationalAutoencoder,
  title = {Nested {{Variational Autoencoder}} for {{Topic Modeling}} on {{Microtexts}} with {{Word Vectors}}},
  author = {Trinh, Trung and Quan, Tho and Mai, Trung},
  year = {2019},
  month = sep,
  abstract = {Most of the information on the Internet is represented in the form of microtexts, which are short text snippets such as news headlines or tweets. These sources of information are abundant, and mining these data could uncover meaningful insights. Topic modeling is one of the popular methods to extract knowledge from a collection of documents; however, conventional topic models such as latent Dirichlet allocation (LDA) are unable to perform well on short documents, mostly due to the scarcity of word co-occurrence statistics embedded in the data. The objective of our research is to create a topic model that can achieve great performances on microtexts while requiring a small runtime for scalability to large datasets. To solve the lack of information of microtexts, we allow our method to take advantage of word embeddings for additional knowledge of relationships between words. For speed and scalability, we apply autoencoding variational Bayes, an algorithm that can perform efficient black-box inference in probabilistic models. The result of our work is a novel topic model called the nested variational autoencoder, which is a distribution that takes into account word vectors and is parameterized by a neural network architecture. For optimization, the model is trained to approximate the posterior distribution of the original LDA model. Experiments show the improvements of our model on microtexts as well as its runtime advantage.},
  archivePrefix = {arXiv},
  eprint = {1905.00195},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/trinh et al_2019_nested variational autoencoder for topic modeling on microtexts with word vectors.pdf},
  journal = {arXiv:1905.00195 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{trinh19_SelfieSelfsupervisedPretraining,
  title = {Selfie: {{Self}}-Supervised {{Pretraining}} for {{Image Embedding}}},
  shorttitle = {Selfie},
  author = {Trinh, Trieu H. and Luong, Minh-Thang and Le, Quoc V.},
  year = {2019},
  month = jun,
  abstract = {We introduce a pretraining technique called Selfie, which stands for SELFie supervised Image Embedding. Selfie generalizes the concept of masked language modeling of BERT (Devlin et al., 2019) to continuous data, such as images, by making use of the Contrastive Predictive Coding loss (Oord et al., 2018). Given masked-out patches in an input image, our method learns to select the correct patch, among other "distractor" patches sampled from the same image, to fill in the masked location. This classification objective sidesteps the need for predicting exact pixel values of the target patches. The pretraining architecture of Selfie includes a network of convolutional blocks to process patches followed by an attention pooling network to summarize the content of unmasked patches before predicting masked ones. During finetuning, we reuse the convolutional weights found by pretraining. We evaluate Selfie on three benchmarks (CIFAR-10, ImageNet 32 x 32, and ImageNet 224 x 224) with varying amounts of labeled data, from 5\% to 100\% of the training sets. Our pretraining method provides consistent improvements to ResNet-50 across all settings compared to the standard supervised training of the same network. Notably, on ImageNet 224 x 224 with 60 examples per class (5\%), our method improves the mean accuracy of ResNet-50 from 35.6\% to 46.7\%, an improvement of 11.1 points in absolute accuracy. Our pretraining method also improves ResNet-50 training stability, especially on low data regime, by significantly lowering the standard deviation of test accuracies across different runs.},
  archivePrefix = {arXiv},
  eprint = {1906.02940},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/trinh et al_2019_selfie.pdf;/home/trung/Zotero/storage/ARBMUAAT/1906.html},
  journal = {arXiv:1906.02940 [cs, eess, stat]},
  keywords = {attention,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{trippe18_ConditionalDensityEstimation,
  title = {Conditional {{Density Estimation}} with {{Bayesian Normalising Flows}}},
  author = {Trippe, Brian L. and Turner, Richard E.},
  year = {2018},
  month = feb,
  abstract = {Modeling complex conditional distributions is critical in a variety of settings. Despite a long tradition of research into conditional density estimation, current methods employ either simple parametric forms or are difficult to learn in practice. This paper employs normalising flows as a flexible likelihood model and presents an efficient method for fitting them to complex densities. These estimators must trade-off between modeling distributional complexity, functional complexity and heteroscedasticity without overfitting. We recognize these trade-offs as modeling decisions and develop a Bayesian framework for placing priors over these conditional density estimators using variational Bayesian neural networks. We evaluate this method on several small benchmark regression datasets, on some of which it obtains state of the art performance. Finally, we apply the method to two spatial density modeling tasks with over 1 million datapoints using the New York City yellow taxi dataset and the Chicago crime dataset.},
  annotation = {ZSCC: 0000011},
  archivePrefix = {arXiv},
  eprint = {1802.04908},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/trippe et al_2018_conditional density estimation with bayesian normalising flows.pdf},
  journal = {arXiv:1802.04908 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{tripuraneni20_TheoryTransferLearning,
  title = {On the {{Theory}} of {{Transfer Learning}}: {{The Importance}} of {{Task Diversity}}},
  shorttitle = {On the {{Theory}} of {{Transfer Learning}}},
  author = {Tripuraneni, Nilesh and Jordan, Michael I. and Jin, Chi},
  year = {2020},
  month = jun,
  abstract = {We provide new statistical guarantees for transfer learning via representation learning\textendash when transfer is achieved by learning a feature representation shared across different tasks. This enables learning on new tasks using far less data than is required to learn them in isolation. Formally, we consider t + 1 tasks parameterized by functions of the form fj {$\smwhtcircle$} h in a general function class F {$\smwhtcircle$} H, where each fj is a task-specific function in F and h is the shared representation in H. Letting C({$\cdot$}) denote the complexity measure of the function class, we show that for diverse training tasks (1) the sample complexity needed to learn the shared representation across the first t training tasks scales as C(H) + tC(F), despite no explicit access to a signal from the feature representation and (2) with an accurate estimate of the representation, the sample complexity needed to learn a new task scales only with C(F). Our results depend upon a new general notion of task diversity\textendash applicable to models with general tasks, features, and losses\textendash as well as a novel chain rule for Gaussian complexities. Finally, we exhibit the utility of our general framework in several models of importance in the literature.},
  archivePrefix = {arXiv},
  eprint = {2006.11650},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tripuraneni et al_2020_on the theory of transfer learning.pdf},
  journal = {arXiv:2006.11650 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@techreport{trivedi20_Crispr2vecMachineLearning,
  title = {Crispr2vec: {{Machine Learning Model Predicts Off}}-{{Target Cuts}} of {{CRISPR}} Systems},
  shorttitle = {Crispr2vec},
  author = {Trivedi, Tara Basu and Boger, Ron and Kamath, Govinda M. and Evangelopoulos, Georgios and Cate, Jamie and Doudna, Jennifer and Hidary, Jack},
  year = {2020},
  month = oct,
  institution = {{Synthetic Biology}},
  doi = {10.1101/2020.10.28.359885},
  abstract = {1           Abstract           Clustered Regularly Interspaced Short Palindromic Repeats (CRISPR)/CRISPR-Cas systems have revolutionized gene editing, with applications in therapeutics, diagnostics, agriculture, and developing disease models. However, CRISPR-Cas suffers from off-target effects \textemdash{} unintended genetic modifications in the genome that arise from its use. In this work, we present crispr2vec: a deep metric learning approach for embedding CRISPR single guide RNA (sgRNA) sequences and predicting off-target cuts. Given a fixed target sequence, we show that our learned embedding yields a faithful representation of potential off-targets. We present a new triplet sampling strategy specifically for CRISPR sequences that improves the quality of our embedding. We show the resulting embedding generalizes across different off-target cut detection assays. Finally, we demonstrate the superiority of our deep metric learning method in its ability to predict off-target cuts compared to previous literature in cross fold validation across different datasets for both seen and unseen sgRNAs.},
  file = {/home/trung/Zotero/storage/EAQARJ8H/Trivedi et al. - 2020 - Crispr2vec Machine Learning Model Predicts Off-Ta.pdf},
  language = {en},
  type = {Preprint}
}

@article{trofimov18_LatentTranscriptome,
  title = {Towards the {{Latent Transcriptome}}},
  author = {Trofimov, Assya and Dutil, Francis and Perreault, Claude and Lemieux, Sebastien and Bengio, Yoshua and Cohen, Joseph Paul},
  year = {2018},
  month = oct,
  abstract = {In this work we propose a method to compute continuous embeddings for kmers from raw RNA-seq data, in a reference-free fashion. We report that our model captures information of both DNA sequence similarity as well as DNA sequence abundance in the embedding latent space. We confirm the quality of these vectors by comparing them to known gene sub-structures and report that the latent space recovers exon information from raw RNA-Seq data from acute myeloid leukemia patients. Furthermore we show that this latent space allows the detection of genomic abnormalities such as translocations as well as patient-specific mutations, making this representation space both useful for visualization as well as analysis.},
  archivePrefix = {arXiv},
  eprint = {1810.03442},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/trofimov et al_2018_towards the latent transcriptome.pdf},
  journal = {arXiv:1810.03442 [cs, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Quantitative Biology - Genomics,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, q-bio, stat}
}

@article{trofimov18_LatentTranscriptomea,
  title = {Towards the {{Latent Transcriptome}}},
  author = {Trofimov, Assya and Dutil, Francis and Perreault, Claude and Lemieux, Sebastien and Bengio, Yoshua and Cohen, Joseph Paul},
  year = {2018},
  month = dec,
  abstract = {In this work we propose a method to compute continuous embeddings for kmers from raw RNA-seq data, without the need for alignment to a reference genome. The approach uses an RNN to transform kmers of the RNA-seq reads into a 2 dimensional representation that is used to predict abundance of each kmer. We report that our model captures information of both DNA sequence similarity as well as DNA sequence abundance in the embedding latent space, that we call the Latent Transcriptome. We confirm the quality of these vectors by comparing them to known gene sub-structures and report that the latent space recovers exon information from raw RNA-Seq data from acute myeloid leukemia patients. Furthermore we show that this latent space allows the detection of genomic abnormalities such as translocations as well as patient-specific mutations, making this representation space both useful for visualization as well as analysis.},
  archivePrefix = {arXiv},
  eprint = {1810.03442},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/trofimov et al_2018_towards the latent transcriptome2.pdf},
  journal = {arXiv:1810.03442 [cs, q-bio, stat]},
  primaryClass = {cs, q-bio, stat}
}

@article{trofimov20_Factorizedembeddingslearns,
  title = {Factorized Embeddings Learns Rich and Biologically Meaningful Embedding Spaces Using Factorized Tensor Decomposition},
  author = {Trofimov, Assya and Cohen, Joseph Paul and Bengio, Yoshua and Perreault, Claude and Lemieux, S{\'e}bastien},
  year = {2020},
  month = jul,
  volume = {36},
  pages = {i417-i426},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/btaa488},
  abstract = {Motivation: The recent development of sequencing technologies revolutionized our understanding of the inner workings of the cell as well as the way disease is treated. A single RNA sequencing (RNA-Seq) experiment, however, measures tens of thousands of parameters simultaneously. While the results are information rich, data analysis provides a challenge. Dimensionality reduction methods help with this task by extracting patterns from the data by compressing it into compact vector representations.},
  file = {/home/trung/GoogleDrive/Zotero/trofimov et al_2020_factorized embeddings learns rich and biologically meaningful embedding spaces using factorized tensor decomposition.pdf},
  journal = {Bioinformatics},
  language = {en},
  number = {Supplement\_1}
}

@article{trong00_burdenbackpain,
  title = {The Burden of Back Pain {{Back}} Pain Is a Massive Problem Which Is Badly Treated},
  author = {Trong, Trung Ngo},
  pages = {13},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/DZP5VRAQ/Trong - The burden of back pain Back pain is a massive pro.pdf},
  language = {en}
}

@article{trong00_comprehensivedeeplearning,
  title = {A Comprehensive Deep Learning Approach to End-to-End Language Identification},
  author = {Trong, Trung Ngo},
  pages = {94},
  abstract = {A new machine learning paradigm, called deep learning, has accelerated the development of state-of-the-art systems in various research domains. Deep learning leverages a sophisticated network of non-linear units and their connections to learn multiple levels of abstraction from the data. Each of these units is inspired by a model of organic brain cell which is known as a neuron. A deep neural network (DNN), which contains millions of neurons with their own parameters, allows the model freely optimizes its feature representations for particular tasks. This capability has been proven to be an universal function approximators which properly benefits the processing of complex signals, for instance, the voice signal. These architectures and algorithms have been the core of the recent ground breaking approaches to automatic speech processing which includes automatic speech recognition (ASR), speaker recognition (SR) and language identification (LID) tasks. In particular, a combination of both deep learning and acoustic modeling has brought about breakthroughs in ASR, especially, an end-to-end SR system can interpret speech directly from its most primitive spectral form. The end-to-end design enhances completeness and is more flexible adapting to wide range of voice signals without the requirement of excessive hand-engineering features, moreover, it also increase the reliability by reducing the stacked deficiency of multiple components. However, similar exploration in LID is quite limited by the attention of research community, regardless the connection to speech technology of all three research fields.},
  file = {/home/trung/GoogleDrive/Zotero/trong_a comprehensive deep learning approach to end-to-end language identiﬁcation.pdf},
  language = {en}
}

@article{trong00_HowDoesCoronavirus,
  title = {How {{Does}} the {{Coronavirus Behave Inside}} a {{Patient}}? | {{The New Yorker}}},
  author = {Trong, Trung Ngo},
  pages = {2},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/trong_how does the coronavirus behave inside a patient.pdf},
  language = {en}
}

@article{trong00_ProbabilisticSemisupervisedLearning,
  title = {Probabilistic {{Semi}}-Supervised {{Learning}} for {{Signal Processing}}},
  author = {Trong, Trung Ngo},
  pages = {2},
  file = {/home/trung/GoogleDrive/Zotero/trong_probabilistic semi-supervised learning for signal processing.pdf},
  language = {en}
}

@article{trong17_comprehensivedeeplearning,
  title = {A Comprehensive Deep Learning Approach to End-to-End Language Identification},
  author = {Trong, Trung Ngo},
  year = {2017},
  pages = {94},
  abstract = {A new machine learning paradigm, called deep learning, has accelerated the development of state-of-the-art systems in various research domains. Deep learning leverages a sophisticated network of non-linear units and their connections to learn multiple levels of abstraction from the data. Each of these units is inspired by a model of organic brain cell which is known as a neuron. A deep neural network (DNN), which contains millions of neurons with their own parameters, allows the model freely optimizes its feature representations for particular tasks. This capability has been proven to be an universal function approximators which properly benefits the processing of complex signals, for instance, the voice signal. These architectures and algorithms have been the core of the recent ground breaking approaches to automatic speech processing which includes automatic speech recognition (ASR), speaker recognition (SR) and language identification (LID) tasks. In particular, a combination of both deep learning and acoustic modeling has brought about breakthroughs in ASR, especially, an end-to-end SR system can interpret speech directly from its most primitive spectral form. The end-to-end design enhances completeness and is more flexible adapting to wide range of voice signals without the requirement of excessive hand-engineering features, moreover, it also increase the reliability by reducing the stacked deficiency of multiple components. However, similar exploration in LID is quite limited by the attention of research community, regardless the connection to speech technology of all three research fields.},
  file = {/home/trung/GoogleDrive/Zotero/trong_2017_a comprehensive deep learning approach to end-to-end language identiﬁcation.pdf},
  language = {en}
}

@article{trong17_TacklingDatasetImbalance,
  title = {Tackling {{Dataset Imbalance}} for {{End}}-to-{{End Deep Learning Approach}} to {{Language Identification}}},
  author = {Trong, Trung Ngo and Hautamaki, Ville and Kukanov, Ivan and Lee, Kong Aik and Jokinen, Kristiina},
  year = {2017},
  pages = {6},
  abstract = {Deep learning is a universal approximator. It leverages multiple non-linear layers to learn distributed representation of data with high level of abstraction. The approach is the key to achieved the state-of-the-art in many domains including speech processing. However, its strong nonlinearity is also emphasized as serious drawback on handling imbalanced dataset. In this work, we tackle non-uniform distribution of classes for an end-to-end language identification task. Our approach optimizes convolutional architecture by better understanding of the convolution behaviour on signal in a reconstruction network. As the convolutional layers captured essence of both development and evaluation set, we train a discriminative network on top it to recognize languages. Key to our approach is the calibrated objective using the prior distribution of training data for regularization. Our system obtains 10\% of relative improvement on NIST 2015 Language Recognition Evaluation.},
  file = {/home/trung/GoogleDrive/Zotero/trong et al_2017_tackling dataset imbalance for end-to-end deep learning approach to language identiﬁcation.pdf},
  language = {en}
}

@article{trong18_EnablingSpokenDialogue,
  title = {Enabling {{Spoken Dialogue Systems}} for {{Low}}- {{Resourced Languages}} \textendash{} {{End}}-to-{{End Dialect Recognition}} for {{North Sami}}},
  author = {Trong, Trung Ngo and Jokinen, Kristiina and Hautam{\"a}ki, Ville},
  year = {2018},
  pages = {14},
  abstract = {In this paper, we tackle the challenge of identifying dialects using deep learning for under-resourced languages. Recent advances in spoken dialogue technology have been strongly influenced by the availability of big corpora, while our goal is to work on the spoken interactive application for the North Sami language, which is classified as one of the less-resourced languages spoken in Northern Europe. North Sami has various variations and dialects which are influenced by the majority languages of the areas in which it is spoken: Finnish and Norwegian. To provide reliable and accurate speech components for an interactive system, it is important to recognize the speakers with their Finnish or Norwegian accent. Conventional approaches compute universal statistical models which require a large amount of data to form reliable statistics, and thus they are vulnerable to small data where there is only a limited number of utterances and speakers available. In this paper we will discuss dialect and accent recognition in under-resourced context, and focus on training an attentive network for leveraging unlabeled data in a semi-supervised scenario for robust feature learning. Validation of our approach is done via two DigiSami datasets: conversational and read corpus.},
  file = {/home/trung/GoogleDrive/Zotero/trong et al_2018_enabling spoken dialogue systems for low- resourced languages – end-to-end dialect recognition for north sami.pdf},
  language = {en}
}

@inproceedings{trong18_StaircaseNetworkstructural,
  title = {Staircase {{Network}}: Structural Language Identification via Hierarchical Attentive Units},
  shorttitle = {Staircase {{Network}}},
  booktitle = {Odyssey 2018 {{The Speaker}} and {{Language Recognition Workshop}}},
  author = {Trong, Trung Ngo and Hautamaki, Ville and Jokinen, Kristiina},
  year = {2018},
  month = jun,
  pages = {60--67},
  publisher = {{ISCA}},
  doi = {10.21437/Odyssey.2018-9},
  abstract = {Language recognition system is typically trained directly to optimize classification error on the target language labels, without using the external, or meta-information in the estimation of the model parameters. However labels are not independent of each other, there is a dependency enforced by, for example, the language family, which affects negatively on classification. The other external information sources (e.g. audio encoding, telephony or video speech) can also decrease classification accuracy. In this paper, we attempt to solve these issues by constructing a deep hierarchical neural network, where different levels of meta-information are encapsulated by attentive prediction units and also embedded into the training progress. The proposed method learns auxiliary tasks to obtain robust internal representation and to construct a variant of attentive units within the hierarchical model. The final result is the structural prediction of the target language and a closely related language family. The algorithm reflects a ``staircase'' way of learning in both its architecture and training, advancing from the fundamental audio encoding to the language family level and finally to the target language level. This process not only improves generalization but also tackles the issues of imbalanced class priors and channel variability in the deep neural network model. Our experimental findings show that the proposed architecture outperforms the state-of-the-art i-vector approaches on both small and big language corpora by a significant margin.},
  file = {/home/trung/GoogleDrive/Zotero/trong et al_2018_staircase network.pdf},
  language = {en}
}

@article{trong19_SemisupervisedGenerativeAutoencoder,
  title = {Semisupervised {{Generative Autoencoder}} for {{Single}}-{{Cell Data}}},
  author = {Trong, Trung Ngo and Mehtonen, Juha and Gonz{\'a}lez, Gerardo and Kramer, Roger and Hautam{\"a}ki, Ville and Hein{\"a}niemi, Merja},
  year = {2019},
  month = dec,
  volume = {27},
  pages = {1190--1203},
  publisher = {{Mary Ann Liebert, Inc., publishers}},
  doi = {10.1089/cmb.2019.0337},
  abstract = {Single-cell transcriptomics offers a tool to study the diversity of cell phenotypes through snapshots of the abundance of mRNA in individual cells. Often there is additional information available besides the single-cell gene expression counts, such as bulk transcriptome data from the same tissue, or quantification of surface protein levels from the same cells. In this study, we propose models based on the Bayesian deep learning approach, where protein quantification, available as CITE-seq counts, from the same cells is used to constrain the learning process, thus forming a SemI-SUpervised generative Autoencoder (SISUA) model. The generative model is based on the deep variational autoencoder (VAE) neural network architecture.},
  journal = {Journal of Computational Biology},
  number = {8}
}

@article{trong20_OrganizedDigitalIntelligent,
  title = {O.{{D}}.{{I}}.{{N}} - {{Organized Digital Intelligent Networks}}},
  author = {Trong, Trung Ngo},
  year = {2020},
  howpublished = {Github}
}

@article{tsai00_LearningMultimodalRepresentations,
  title = {Learning {{Multimodal Representations}} with {{Factorized Deep Generative Models}}},
  author = {Tsai, Yao-Hung Hubert and Liang, Paul Pu and Zadeh, Amir and Morency, Louis-Philippe and Salakhutdinov, Ruslan},
  pages = {19},
  abstract = {Learning multimodal representations is a fundamentally complex research problem due to the presence of multiple heterogeneous sources of information. Although the presence of multiple modalities provides additional valuable information, there are two key challenges to address when learning from multimodal data: 1) models must learn the complex intra-modal and cross-modal interactions for prediction and 2) models must be robust to unexpected missing or noisy modalities during testing. In this paper, we propose to optimize for a joint generative-discriminative objective across multimodal data and labels. We introduce a model that factorizes representations into two sets of independent factors: multimodal discriminative and modality-specific generative factors. Multimodal discriminative factors are shared across all modalities and contain joint multimodal features required for discriminative tasks such as sentiment prediction. Modality-specific generative factors are unique for each modality and contain the information required for generating data. Experimental results show that our model is able to learn meaningful multimodal representations that achieve state-of-the-art or competitive performance on six multimodal datasets. Our model demonstrates flexible generative capabilities by conditioning on independent factors and can reconstruct missing modalities without significantly impacting performance. Lastly, we interpret our factorized representations to understand the interactions that influence multimodal learning.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/tsai et al_learning multimodal representations with factorized deep generative models.pdf},
  keywords = {semi-supervised,variational,wasserstein distance},
  language = {en}
}

@article{tsai20_DemystifyingSelfSupervisedLearning,
  title = {Demystifying {{Self}}-{{Supervised Learning}}: {{An Information}}-{{Theoretical Framework}}},
  shorttitle = {Demystifying {{Self}}-{{Supervised Learning}}},
  author = {Tsai, Yao-Hung Hubert and Wu, Yue and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2020},
  month = jun,
  abstract = {Self-supervised representation learning adopts self-defined signals as supervision and uses the learned representation for downstream tasks, such as masked language modeling (e.g., BERT) for natural language processing and contrastive visual representation learning (e.g., SimCLR) for computer vision applications. In this paper, we present a theoretical framework explaining that self-supervised learning is likely to work under the assumption that only the shared information (e.g., contextual information or content) between the input (e.g., non-masked words or original images) and self-supervised signals (e.g., masked-words or augmented images) contributes to downstream tasks. Under this assumption, we demonstrate that self-supervisedly learned representation can extract task-relevant and discard task-irrelevant information. We further connect our theoretical analysis to popular contrastive and predictive (self-supervised) learning objectives. In the experimental section, we provide controlled experiments on two popular tasks: 1) visual representation learning with various self-supervised learning objectives to empirically support our analysis; and 2) visual-textual representation learning to challenge that input and self-supervised signal lie in different modalities.},
  archivePrefix = {arXiv},
  eprint = {2006.05576},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tsai et al_2020_demystifying self-supervised learning.pdf},
  journal = {arXiv:2006.05576 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{tsai20_MultimodalRoutingImproving,
  title = {Multimodal {{Routing}}: {{Improving Local}} and {{Global Interpretability}} of {{Multimodal Language Analysis}}},
  shorttitle = {Multimodal {{Routing}}},
  author = {Tsai, Yao-Hung Hubert and Ma, Martin Q. and Yang, Muqiao and Salakhutdinov, Ruslan and Morency, Louis-Philippe},
  year = {2020},
  month = oct,
  abstract = {The human language can be expressed through multiple sources of information known as modalities, including tones of voice, facial gestures, and spoken language. Recent multimodal learning with strong performances on human-centric tasks such as sentiment analysis and emotion recognition are often blackbox, with very limited interpretability. In this paper we propose Multimodal Routing, which dynamically adjusts weights between input modalities and output representations differently for each input sample. Multimodal routing can identify relative importance of both individual modalities and cross-modality features. Moreover, the weight assignment by routing allows us to interpret modalityprediction relationships not only globally (i.e. general trends over the whole dataset), but also locally for each single input sample, meanwhile keeping competitive performance compared to state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {2004.14198},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tsai et al_2020_multimodal routing.pdf},
  journal = {arXiv:2004.14198 [cs]},
  keywords = {semi-supervised},
  language = {en},
  primaryClass = {cs}
}

@article{tschannen19_MutualInformationMaximization,
  title = {On {{Mutual Information Maximization}} for {{Representation Learning}}},
  author = {Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K. and Gelly, Sylvain and Lucic, Mario},
  year = {2019},
  month = jul,
  abstract = {Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods might be only loosely attributed to the properties of MI, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1907.13625},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tschannen et al_2019_on mutual information maximization for representation learning.pdf;/home/trung/Zotero/storage/GJIJPQP9/1907.html},
  journal = {arXiv:1907.13625 [cs, stat]},
  keywords = {Computer Science - Machine Learning,favorite,information,mutual information,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{tschannen20_MutualInformationMaximization,
  title = {On {{Mutual Information Maximization}} for {{Representation Learning}}},
  author = {Tschannen, Michael and Djolonga, Josip and Rubenstein, Paul K. and Gelly, Sylvain and Lucic, Mario},
  year = {2020},
  month = jan,
  abstract = {Many recent methods for unsupervised or self-supervised representation learning train feature extractors by maximizing an estimate of the mutual information (MI) between different views of the data. This comes with several immediate problems: For example, MI is notoriously hard to estimate, and using it as an objective for representation learning may lead to highly entangled representations due to its invariance under arbitrary invertible transformations. Nevertheless, these methods have been repeatedly shown to excel in practice. In this paper we argue, and provide empirical evidence, that the success of these methods cannot be attributed to the properties of MI alone, and that they strongly depend on the inductive bias in both the choice of feature extractor architectures and the parametrization of the employed MI estimators. Finally, we establish a connection to deep metric learning and argue that this interpretation may be a plausible explanation for the success of the recently introduced methods.},
  archivePrefix = {arXiv},
  eprint = {1907.13625},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/tschannen et al_2020_on mutual information maximization for representation learning.pdf},
  journal = {arXiv:1907.13625 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{tshitoyan19_Unsupervisedwordembeddings,
  title = {Unsupervised Word Embeddings Capture Latent Knowledge from Materials Science Literature},
  author = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A. and Ceder, Gerbrand and Jain, Anubhav},
  year = {2019},
  month = jul,
  volume = {571},
  pages = {95--98},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1335-8},
  file = {/home/trung/GoogleDrive/Zotero/tshitoyan et al_2019_unsupervised word embeddings capture latent knowledge from materials science literature.pdf},
  journal = {Nature},
  language = {en},
  number = {7763}
}

@article{tsipras19_RobustnessMayBe,
  title = {Robustness {{May Be}} at {{Odds}} with {{Accuracy}}},
  author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  year = {2019},
  month = sep,
  abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Specifically, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These findings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classifiers learning fundamentally different feature representations than standard classifiers. These differences, in particular, seem to result in unexpected benefits: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
  archivePrefix = {arXiv},
  eprint = {1805.12152},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/LVDTJ8J3/Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf},
  journal = {arXiv:1805.12152 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{tsuyuzaki20_Benchmarkingprincipalcomponent,
  title = {Benchmarking Principal Component Analysis for Large-Scale Single-Cell {{RNA}}-Sequencing},
  author = {Tsuyuzaki, Koki and Sato, Hiroyuki and Sato, Kenta and Nikaido, Itoshi},
  year = {2020},
  month = dec,
  volume = {21},
  pages = {9},
  issn = {1474-760X},
  doi = {10.1186/s13059-019-1900-3},
  abstract = {Background: Principal component analysis (PCA) is an essential method for analyzing single-cell RNA-seq (scRNA-seq) datasets, but for large-scale scRNA-seq datasets, computation time is long and consumes large amounts of memory. Results: In this work, we review the existing fast and memory-efficient PCA algorithms and implementations and evaluate their practical application to large-scale scRNA-seq datasets. Our benchmark shows that some PCA algorithms based on Krylov subspace and randomized singular value decomposition are fast, memory-efficient, and more accurate than the other algorithms. Conclusion: We develop a guideline to select an appropriate PCA implementation based on the differences in the computational environment of users and developers.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/Zotero/storage/LN53KHTT/Tsuyuzaki et al. - 2020 - Benchmarking principal component analysis for larg.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{tucha04_Effectsnicotinechewing,
  title = {Effects of Nicotine Chewing Gum on a Real-Life Motor Task: A Kinematic Analysis of Handwriting Movements in Smokers and Non-Smokers},
  shorttitle = {Effects of Nicotine Chewing Gum on a Real-Life Motor Task},
  author = {Tucha, Oliver and Lange, Klaus W.},
  year = {2004},
  month = apr,
  volume = {173},
  pages = {49--56},
  issn = {0033-3158},
  doi = {10.1007/s00213-003-1690-9},
  abstract = {RATIONALE: In laboratory tasks nicotine has consistently been shown to improve psychomotor performance. OBJECTIVES: The aim of the present experiment was to assess the effects of nicotine on a skilled task of everyday life in smoking and non-smoking healthy adults. METHODS: Assessment of handwriting movements of 38 non-deprived smokers and 38 non-smokers was performed following the chewing of gum containing 0 mg, 2 mg or 4 mg of nicotine. A digitising tablet was used for the assessment of fine motor movements. Subjects were asked to perform a simple writing task. Movement time, velocity and acceleration of the handwriting movements were measured. Furthermore, every writing specimen was independently rated by two examiners regarding the quality of handwriting. RESULTS: Kinematic analysis of writing movements revealed that nicotine could produce absolute improvements in handwriting. Following nicotine administration, reduced movement times, increased velocities and more fluent handwriting movements were observed. These improvements were more striking in smokers than in non-smokers. No effects of nicotine were found with regard to the quality of handwriting. CONCLUSION: The results suggest that nicotine can enhance psychomotor performance to a significant degree in a real-life motor task.},
  journal = {Psychopharmacology},
  language = {eng},
  number = {1-2},
  pmid = {14668975}
}

@article{turchini13_FishOilsMisconceptions,
  title = {Fish {{Oils}}, {{Misconceptions}} and the {{Environment}}},
  author = {Turchini, Giovanni M.},
  year = {2013},
  month = nov,
  volume = {103},
  pages = {e4},
  issn = {0090-0036},
  doi = {10.2105/AJPH.2013.301510},
  file = {/home/trung/GoogleDrive/Zotero/turchini_2013_fish oils, misconceptions and the environment.pdf},
  journal = {American Journal of Public Health},
  number = {11},
  pmcid = {PMC3828714},
  pmid = {24028240}
}

@article{turing50_ComputingMachineryIntelligence,
  title = {Computing {{Machinery}} and {{Intelligence}}},
  author = {Turing, A. M.},
  year = {1950},
  volume = {59},
  pages = {433--460},
  file = {/home/trung/GoogleDrive/Zotero/turing_1950_computing machinery and intelligence.pdf},
  journal = {Mind, New Series},
  number = {236}
}

@article{turing50_ComputingMachineryIntelligencea,
  title = {Computing {{Machinery}} and {{Intelligence}} - {{New}}},
  author = {TURING, A. M.},
  year = {1950},
  month = oct,
  volume = {LIX},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  file = {/home/trung/GoogleDrive/Zotero/turing_1950_computing machinery and intelligence - new.pdf},
  journal = {Mind},
  number = {236}
}

@article{tuszynska14_Computationalmodelingprotein,
  title = {Computational Modeling of Protein\textendash{{RNA}} Complex Structures},
  author = {Tuszynska, Irina and Matelska, Dorota and Magnus, Marcin and Chojnowski, Grzegorz and Kasprzak, Joanna M. and Kozlowski, Lukasz P. and {Dunin-Horkawicz}, Stanislaw and Bujnicki, Janusz M.},
  year = {2014},
  month = feb,
  volume = {65},
  pages = {310--319},
  issn = {10462023},
  doi = {10.1016/j.ymeth.2013.09.014},
  abstract = {Protein\textendash RNA interactions play fundamental roles in many biological processes, such as regulation of gene expression, RNA splicing, and protein synthesis. The understanding of these processes improves as new structures of protein\textendash RNA complexes are solved and the molecular details of interactions analyzed. However, experimental determination of protein\textendash RNA complex structures by high-resolution methods is tedious and difficult. Therefore, studies on protein\textendash RNA recognition and complex formation present major technical challenges for macromolecular structural biology. Alternatively, protein\textendash RNA interactions can be predicted by computational methods. Although less accurate than experimental measurements, theoretical models of macromolecular structures can be sufficiently accurate to prompt functional hypotheses and guide e.g. identification of important amino acid or nucleotide residues. In this article we present an overview of strategies and methods for computational modeling of protein\textendash RNA complexes, including software developed in our laboratory, and illustrate it with practical examples of structural predictions.},
  file = {/home/trung/GoogleDrive/Zotero/tuszynska et al_2014_computational modeling of protein–rna complex structures.pdf},
  journal = {Methods},
  language = {en},
  number = {3}
}

@article{ucar19_BridgingELBOMMD,
  title = {Bridging the {{ELBO}} and {{MMD}}},
  author = {Ucar, Talip},
  year = {2019},
  month = oct,
  abstract = {One of the challenges in training generative models such as the variational auto encoder (VAE) is avoiding posterior collapse. When the generator has too much capacity, it is prone to ignoring latent code. This problem is exacerbated when the dataset is small, and the latent dimension is high. The root of the problem is the ELBO objective, specifically the Kullback-Leibler (KL) divergence term in objective function \textbackslash citep\{zhao2019infovae\}. This paper proposes a new objective function to replace the KL term with one that emulates the maximum mean discrepancy (MMD) objective. It also introduces a new technique, named latent clipping, that is used to control distance between samples in latent space. A probabilistic autoencoder model, named \$\textbackslash mu\$-VAE, is designed and trained on MNIST and MNIST Fashion datasets, using the new objective function and is shown to outperform models trained with ELBO and \$\textbackslash beta\$-VAE objective. The \$\textbackslash mu\$-VAE is less prone to posterior collapse, and can generate reconstructions and new samples in good quality. Latent representations learned by \$\textbackslash mu\$-VAE are shown to be good and can be used for downstream tasks such as classification.},
  archivePrefix = {arXiv},
  eprint = {1910.13181},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ucar_2019_bridging the elbo and mmd.pdf},
  journal = {arXiv:1910.13181 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@article{ucar19_InverseGraphicsUnsupervised,
  title = {Inverse {{Graphics}}: {{Unsupervised Learning}} of {{3D Shapes}} from {{Single Images}}},
  shorttitle = {Inverse {{Graphics}}},
  author = {Ucar, Talip},
  year = {2019},
  month = dec,
  abstract = {Using generative models for Inverse Graphics is an active area of research. However, most works focus on developing models for supervised and semi-supervised methods. In this paper, we study the problem of unsupervised learning of 3D geometry from single images. Our approach is to use a generative model that produces 2-D images as projections of a latent 3D voxel grid, which we train either as a variational auto-encoder or using adversarial methods. Our contributions are as follows: First, we show how to recover 3D shape and pose from general datasets such as MNIST, and MNIST Fashion in good quality. Second, we compare the shapes learned using adversarial and variational methods. Adversarial approach gives denser 3D shapes. Third, we explore the idea of modelling the pose of an object as uniform distribution to recover 3D shape from a single image. Our experiment with the CelebA dataset \textbackslash cite\{liu2015faceattributes\} proves that we can recover complete 3D shape from a single image when the object is symmetric along one, or more axis whilst results obtained using ModelNet40 \textbackslash cite\{wu20153d\} show the potential side-effects, in which the model learns 3D shapes such that it can render the same image from any viewpoint. Forth, we present a general end-to-end approach to learning 3D shapes from single images in a completely unsupervised fashion by modelling the factors of variation such as azimuth as independent latent variables. Our method makes no assumptions about the dataset, and can work with synthetic as well as real images (i.e. unsupervised in true sense). We present our results, by training the model using the \$\textbackslash mu\$-VAE objective \textbackslash cite\{ucar2019bridging\} and a dataset combining all images from MNIST, MNIST Fashion, CelebA and six categories of ModelNet40. The model is able to learn 3D shapes and the pose in qood quality and leverages information learned across all datasets.},
  archivePrefix = {arXiv},
  eprint = {1911.07937},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ucar_2019_inverse graphics.pdf},
  journal = {arXiv:1911.07937 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{ugras13_GlucosamineSulfateFractureHealing,
  title = {Glucosamine-{{Sulfate On Fracture Healing}}},
  author = {Ugras, Akin and Guzel, Elif and Korkusuz, Petek and Kaya, Ibrahim and Dikici, Fatih and Demirbas, Emrah and Cetinus, Ercan},
  year = {2013},
  volume = {19},
  pages = {8--12},
  issn = {1306-696X},
  doi = {10.5505/tjtes.2013.03256},
  abstract = {BACKGROUND The aim of this study is to determine whether glucosaminesulfate has any effects on bone-healing. METHODS A unilateral fracture was created in the tibia of sixty-one female rats. Rats were given no drug or 230 mg/kg glucosamine-sulfate daily. Fractures were analyzed during the first, second and fourth weeks after creation of fracture. Quantitative measurement for new bone formation and osteoblast lining were determined histologically. Semiquantitative score for fracture healing was used for histomorphometric analyses. Bridging bone formation was assessed radiographically. RESULTS New bone formation and osteoblast lining were significantly higher in glucosamine-treated group at week 1. Surrounding connective tissue was more cellular and vascular, and the newly formed bone trabecules were present in greater amounts in glucosamine-treated group, compared to control group at week 1 and 4. But radiologically, the control group had better scores than that of the glucosamine-treated group at week 4. CONCLUSION These data demonstrate that daily glucosamine-sulfate administration accelerates early phase of fracture repair in the rat tibia, with increased new bone formation and osteoblast lining histologically, but radiologic bone union is not favored on radiographs.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/Zotero/storage/6WN4MFEU/Ugras et al. - 2013 - Glucosamine-Sulfate On Fracture Healing.pdf},
  journal = {Turkish Journal of Trauma and Emergency Surgery},
  language = {en},
  number = {1}
}

@article{ullah19_GraphConvolutionalNetworks,
  title = {Graph {{Convolutional Networks}}: Analysis, Improvements and Results},
  shorttitle = {Graph {{Convolutional Networks}}},
  author = {Ullah, Ihsan and Manzo, Mario and Shah, Mitul and Madden, Michael},
  year = {2019},
  month = dec,
  abstract = {In the current era of neural networks and big data, higher dimensional data is processed for automation of different application areas. Graphs represent a complex data organization in which dependencies between more than one object or activity occur. Due to the high dimensionality, this data creates challenges for machine learning algorithms. Graph convolutional networks were introduced to utilize the convolutional models concepts that shows good results. In this context, we enhanced two of the existing Graph convolutional network models by proposing four enhancements. These changes includes: hyper parameters optimization, convex combination of activation functions, topological information enrichment through clustering coefficients measure, and structural redesign of the network through addition of dense layers. We present extensive results on four state-of-art benchmark datasets. The performance is notable not only in terms of lesser computational cost compared to competitors, but also achieved competitive results for three of the datasets and state-of-the-art for the fourth dataset.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1912.09592},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/27THYAZ6/Ullah et al. - 2019 - Graph Convolutional Networks analysis, improvemen.pdf},
  journal = {arXiv:1912.09592 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ulyanov16_TextureNetworksFeedforward,
  title = {Texture {{Networks}}: {{Feed}}-Forward {{Synthesis}} of {{Textures}} and {{Stylized Images}}},
  shorttitle = {Texture {{Networks}}},
  author = {Ulyanov, Dmitry and Lebedev, Vadim and Vedaldi, Andrea and Lempitsky, Victor},
  year = {2016},
  month = mar,
  abstract = {Gatys et al. recently demonstrated that deep networks can generate beautiful textures and stylized images from a single texture example. However, their methods requires a slow and memory-consuming optimization process. We propose here an alternative approach that moves the computational burden to a learning stage. Given a single example of a texture, our approach trains compact feed-forward convolutional networks to generate multiple samples of the same texture of arbitrary size and to transfer artistic style from a given image to any other image. The resulting networks are remarkably light-weight and can generate textures of quality comparable to Gatys\textasciitilde et\textasciitilde al., but hundreds of times faster. More generally, our approach highlights the power and flexibility of generative feed-forward models trained with complex and expressive loss functions.},
  archivePrefix = {arXiv},
  eprint = {1603.03417},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ulyanov et al_2016_texture networks.pdf},
  journal = {arXiv:1603.03417 [cs]},
  primaryClass = {cs}
}

@misc{um19_TopCitedDeep,
  title = {Top {{Cited Deep Learning Papers}}},
  author = {Um, Terry Taewoong},
  year = {2019},
  month = dec,
  abstract = {The most cited deep learning papers. Contribute to terryum/awesome-deep-learning-papers development by creating an account on GitHub.},
  annotation = {ZSCC: NoCitationData[s0]}
}

@book{unpingco16_PythonProbabilityStatistics,
  title = {Python for {{Probability}}, {{Statistics}}, and {{Machine Learning}}},
  author = {Unpingco, Jos{\'e}},
  year = {2016},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-30717-6},
  file = {/home/trung/GoogleDrive/Zotero/unpingco_2016_python for probability, statistics, and machine learning.pdf},
  isbn = {978-3-319-30715-2 978-3-319-30717-6},
  language = {en}
}

@article{urasaki20_Akt3RegulatesTissueSpecific,
  title = {Akt3 {{Regulates}} the {{Tissue}}-{{Specific Response}} to {{Copaiba Essential Oil}}},
  author = {Urasaki, Yasuyo and Beaumont, Cody and Talbot, Jeffery N. and Hill, David K. and Le, Thuc T.},
  year = {2020},
  month = apr,
  volume = {21},
  issn = {1422-0067},
  doi = {10.3390/ijms21082851},
  abstract = {This study reports a relationship between Akt3 expression and tissue-specific regulation of the pI3K/Akt/mTOR signaling pathway by copaiba essential oil. Akt3, a protein kinase B isoform important for the regulation of neuronal development, exhibited differential expression levels in cells of various origins. In neuronal and microglial cells, where Akt3 is present, copaiba essential oil positively regulated the pI3K/Akt/mTOR signaling pathway. In contrast, in liver cells and T lymphocytes, where Akt3 is absent, copaiba essential oil negatively regulated the pI3K/Akt/mTOR signaling pathway. The expression of Akt3 via plasmid DNA in liver cells led to positive regulatory effects by copaiba essential oil on the pI3K/Akt/mTOR signaling pathway. In contrast, inhibition of Akt3 expression in neuronal cells via small interfering RNA molecules targeting Akt3 transcripts abrogated the regulatory effects of copaiba essential oil on the pI3K/Akt/mTOR signaling pathway. Interestingly, Akt3 expression did not impact the regulatory effects of copaiba essential oil on other signaling pathways. For example, copaiba essential oil consistently upregulated the MAPK and JAK/STAT signaling pathways in all evaluated cell types, independent of the Akt3 expression level. Collectively, the data indicated that Akt3 expression was required for the positive regulatory effects of copaiba essential oil, specifically on the pI3K/Akt/mTOR signaling pathway.},
  file = {/home/trung/GoogleDrive/Zotero/urasaki et al_2020_akt3 regulates the tissue-specific response to copaiba essential oil.pdf},
  journal = {International Journal of Molecular Sciences},
  number = {8},
  pmcid = {PMC7216139},
  pmid = {32325885}
}

@article{urasaki20_FastActingReceptorMediatedRegulation,
  title = {Fast-{{Acting}} and {{Receptor}}-{{Mediated Regulation}} of {{Neuronal Signaling Pathways}} by {{Copaiba Essential Oil}}},
  author = {Urasaki, Yasuyo and Beaumont, Cody and Workman, Michelle and Talbot, Jeffery N. and Hill, David K. and Le, Thuc T.},
  year = {2020},
  month = mar,
  volume = {21},
  issn = {1422-0067},
  doi = {10.3390/ijms21072259},
  abstract = {This study examined the biological activities of copaiba essential oil via measurement of its effects on signaling pathways in the SH-SY5Y neuronal cell line. Nanofluidic proteomic technologies were deployed to measure the phosphorylation of biomarker proteins within the signaling cascades. Interestingly, copaiba essential oil upregulated the pI3K/Akt/mTOR, MAPK, and JAK/STAT signaling pathways in neuronal cells. The effects of copaiba essential oil peaked at 30 min post-treatment, with a half-maximal effective concentration (EC50) of approximately 80 ng/mL. Treatment with cannabinoid receptor 2 (CB2) agonist AM1241 or the inverse agonist BML190 abrogated the regulatory effects of copaiba essential oil on the pI3K/Akt/mTOR signaling pathway. Surprisingly, copaiba essential oil also activated the apoptosis signaling pathway and reduced the viability of SH-SY5Y cells with an EC50 of approximately 400 ng/mL. Furthermore, {$\beta$}-caryophyllene, a principal constituent of copaiba essential oil, downregulated the pI3K/Akt/mTOR signaling pathway. Taken together, the findings indicated that copaiba essential oil upregulated signaling pathways associated with cell metabolism, growth, immunity, and apoptosis. The biological activities of copaiba essential oil were determined to be fast acting, CB2 mediated, and dependent on multiple chemical constituents of the oil. Nanofluidic proteomics provided a powerful means to assess the biological activities of copaiba essential oil.},
  file = {/home/trung/GoogleDrive/Zotero/urasaki et al_2020_fast-acting and receptor-mediated regulation of neuronal signaling pathways by copaiba essential oil.pdf},
  journal = {International Journal of Molecular Sciences},
  number = {7},
  pmcid = {PMC7177672},
  pmid = {32218156}
}

@article{uria16_NeuralAutoregressiveDistribution,
  title = {Neural {{Autoregressive Distribution Estimation}}},
  author = {Uria, Benigno and C{\^o}t{\'e}, Marc-Alexandre and Gregor, Karol and Murray, Iain and Larochelle, Hugo},
  year = {2016},
  month = may,
  abstract = {We present Neural Autoregressive Distribution Estimation (NADE) models, which are neural network architectures applied to the problem of unsupervised distribution and density estimation. They leverage the probability product rule and a weight sharing scheme inspired from restricted Boltzmann machines, to yield an estimator that is both tractable and has good generalization performance. We discuss how they achieve competitive performance in modeling both binary and real-valued observations. We also present how deep NADE models can be trained to be agnostic to the ordering of input dimensions used by the autoregressive product rule decomposition. Finally, we also show how to exploit the topological structure of pixels in images using a deep convolutional architecture for NADE.},
  annotation = {ZSCC: 0000085},
  archivePrefix = {arXiv},
  eprint = {1605.02226},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/uria et al_2016_neural autoregressive distribution estimation.pdf;/home/trung/Zotero/storage/IDR67YZV/1605.html},
  journal = {arXiv:1605.02226 [cs]},
  keywords = {autogressive,Computer Science - Machine Learning,nade},
  primaryClass = {cs}
}

@article{vahdat00_DVAEDiscreteVariational,
  title = {{{DVAE}}++: {{Discrete Variational Autoencoders}} with {{Overlapping Transformations}}},
  author = {Vahdat, Arash and Macready, William G and Bian, Zhengbing and Khoshaman, Amir and Andriyash, Evgeny},
  pages = {10},
  abstract = {Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult. We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors. We derive a new variational bound to efficiently train with Boltzmann machine priors. Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables. Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and discrete variational autoencoders (Rolfe, 2016).},
  file = {/home/trung/GoogleDrive/Zotero/vahdat et al_dvae++.pdf},
  language = {en}
}

@article{vahdat18_DVAEDiscreteVariational,
  title = {{{DVAE}}\#: {{Discrete Variational Autoencoders}} with {{Relaxed Boltzmann Priors}}},
  shorttitle = {{{DVAE}}\#},
  author = {Vahdat, Arash and Andriyash, Evgeny and Macready, William G.},
  year = {2018},
  month = may,
  abstract = {Boltzmann machines are powerful distributions that have been shown to be an effective prior over binary latent variables in variational autoencoders (VAEs). However, previous methods for training discrete VAEs have used the evidence lower bound and not the tighter importance-weighted bound. We propose two approaches for relaxing Boltzmann machines to continuous distributions that permit training with importance-weighted bounds. These relaxations are based on generalized overlapping transformations and the Gaussian integral trick. Experiments on the MNIST and OMNIGLOT datasets show that these relaxations outperform previous discrete VAEs with Boltzmann priors. An implementation which reproduces these results is available at https://github.com/QuadrantAI/dvae .},
  archivePrefix = {arXiv},
  eprint = {1805.07445},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vahdat et al_2018_dvae#.pdf;/home/trung/Zotero/storage/LVRC7XTU/1805.html},
  journal = {arXiv:1805.07445 [cs, stat]},
  keywords = {Computer Science - Machine Learning,discrete,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{vahdat20_NVAEDeepHierarchical,
  title = {{{NVAE}}: {{A Deep Hierarchical Variational Autoencoder}}},
  shorttitle = {{{NVAE}}},
  author = {Vahdat, Arash and Kautz, Jan},
  year = {2020},
  month = jul,
  abstract = {Normalizing flows, autoregressive models, variational autoencoders (VAEs), and deep energy-based models are among competing likelihood-based frameworks for deep generative learning. Among them, VAEs have the advantage of fast and tractable sampling and easy-to-access encoding networks. However, they are currently outperformed by other models such as normalizing flows and autoregressive models. While the majority of the research in VAEs is focused on the statistical challenges, we explore the orthogonal direction of carefully designing neural architectures for hierarchical VAEs. We propose Nouveau VAE (NVAE), a deep hierarchical VAE built for image generation using depth-wise separable convolutions and batch normalization. NVAE is equipped with a residual parameterization of Normal distributions and its training is stabilized by spectral regularization. We show that NVAE achieves state-of-the-art results among non-autoregressive likelihood-based models on the MNIST, CIFAR-10, and CelebA HQ datasets and it provides a strong baseline on FFHQ. For example, on CIFAR-10, NVAE pushes the state-of-the-art from 2.98 to 2.91 bits per dimension, and it produces high-quality images on CelebA HQ as shown in Fig. 1. To the best of our knowledge, NVAE is the first successful VAE applied to natural images as large as 256\$\textbackslash times\$256 pixels.},
  archivePrefix = {arXiv},
  eprint = {2007.03898},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vahdat et al_2020_nvae.pdf},
  journal = {arXiv:2007.03898 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{vainer20_SpeedySpeechEfficientNeural,
  title = {{{SpeedySpeech}}: {{Efficient Neural Speech Synthesis}}},
  shorttitle = {{{SpeedySpeech}}},
  author = {Vainer, Jan and Du{\v s}ek, Ond{\v r}ej},
  year = {2020},
  month = aug,
  abstract = {While recent neural sequence-to-sequence models have greatly improved the quality of speech synthesis, there has not been a system capable of fast training, fast inference and high-quality audio synthesis at the same time. We propose a student-teacher network capable of high-quality faster-than-real-time spectrogram synthesis, with low requirements on computational resources and fast training time. We show that self-attention layers are not necessary for generation of high quality audio. We utilize simple convolutional blocks with residual connections in both student and teacher networks and use only a single attention layer in the teacher model. Coupled with a MelGAN vocoder, our model's voice quality was rated significantly higher than Tacotron 2. Our model can be efficiently trained on a single GPU and can run in real time even on a CPU. We provide both our source code and audio samples in our GitHub repository.},
  archivePrefix = {arXiv},
  eprint = {2008.03802},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vainer et al_2020_speedyspeech.pdf},
  journal = {arXiv:2008.03802 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{valle-perez19_Deeplearninggeneralizes,
  title = {Deep Learning Generalizes Because the Parameter-Function Map Is Biased towards Simple Functions},
  author = {{Valle-P{\'e}rez}, Guillermo and Camargo, Chico Q. and Louis, Ard A.},
  year = {2019},
  month = apr,
  abstract = {Deep neural networks (DNNs) generalize remarkably well without explicit regularization even in the strongly over-parametrized regime where classical learning theory would instead predict that they would severely overfit. While many proposals for some kind of implicit regularization have been made to rationalise this success, there is no consensus for the fundamental reason why DNNs do not strongly overfit. In this paper, we provide a new explanation. By applying a very general probability-complexity bound recently derived from algorithmic information theory (AIT), we argue that the parameter-function map of many DNNs should be exponentially biased towards simple functions. We then provide clear evidence for this strong simplicity bias in a model DNN for Boolean functions, as well as in much larger fully connected and convolutional networks applied to CIFAR10 and MNIST. As the target functions in many real problems are expected to be highly structured, this intrinsic simplicity bias helps explain why deep networks generalize well on real world problems. This picture also facilitates a novel PAC-Bayes approach where the prior is taken over the DNN input-output function space, rather than the more conventional prior over parameter space. If we assume that the training algorithm samples parameters close to uniformly within the zero-error region then the PAC-Bayes theorem can be used to guarantee good expected generalization for target functions producing high-likelihood training sets. By exploiting recently discovered connections between DNNs and Gaussian processes to estimate the marginal likelihood, we produce relatively tight generalization PAC-Bayes error bounds which correlate well with the true error on realistic datasets such as MNIST and CIFAR10 and for architectures including convolutional and fully connected networks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1805.08522},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/valle-pérez et al_2019_deep learning generalizes because the parameter-function map is biased towards simple functions.pdf;/home/trung/Zotero/storage/KYM9YYBN/1805.html},
  journal = {arXiv:1805.08522 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{valle19_MellotronMultispeakerexpressive,
  title = {Mellotron: {{Multispeaker}} Expressive Voice Synthesis by Conditioning on Rhythm, Pitch and Global Style Tokens},
  shorttitle = {Mellotron},
  author = {Valle, Rafael and Li, Jason and Prenger, Ryan and Catanzaro, Bryan},
  year = {2019},
  month = oct,
  abstract = {Mellotron is a multispeaker voice synthesis model based on Tacotron 2 GST that can make a voice emote and sing without emotive or singing training data. By explicitly conditioning on rhythm and continuous pitch contours from an audio signal or music score, Mellotron is able to generate speech in a variety of styles ranging from read speech to expressive speech, from slow drawls to rap and from monotonous voice to singing voice. Unlike other methods, we train Mellotron using only read speech data without alignments between text and audio. We evaluate our models using the LJSpeech and LibriTTS datasets. We provide F0 Frame Errors and synthesized samples that include style transfer from other speakers, singers and styles not seen during training, procedural manipulation of rhythm and pitch and choir synthesis.},
  archivePrefix = {arXiv},
  eprint = {1910.11997},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/valle et al_2019_mellotron.pdf},
  journal = {arXiv:1910.11997 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{valle20_FlowtronAutoregressiveFlowbased,
  title = {Flowtron: An {{Autoregressive Flow}}-Based {{Generative Network}} for {{Text}}-to-{{Speech Synthesis}}},
  shorttitle = {Flowtron},
  author = {Valle, Rafael and Shih, Kevin and Prenger, Ryan and Catanzaro, Bryan},
  year = {2020},
  month = jul,
  abstract = {In this paper we propose Flowtron: an autoregressive flow-based generative network for text-to-speech synthesis with control over speech variation and style transfer. Flowtron borrows insights from IAF and revamps Tacotron in order to provide high-quality and expressive mel-spectrogram synthesis. Flowtron is optimized by maximizing the likelihood of the training data, which makes training simple and stable. Flowtron learns an invertible mapping of data to a latent space that can be manipulated to control many aspects of speech synthesis (pitch, tone, speech rate, cadence, accent). Our mean opinion scores (MOS) show that Flowtron matches state-of-the-art TTS models in terms of speech quality. In addition, we provide results on control of speech variation, interpolation between samples and style transfer between speakers seen and unseen during training. Code and pre-trained models will be made publicly available at https://github.com/NVIDIA/flowtron},
  archivePrefix = {arXiv},
  eprint = {2005.05957},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/valle et al_2020_flowtron.pdf},
  journal = {arXiv:2005.05957 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{vallejos15_BASiCSBayesianAnalysis,
  title = {{{BASiCS}}: {{Bayesian Analysis}} of {{Single}}-{{Cell Sequencing Data}}},
  shorttitle = {{{BASiCS}}},
  author = {Vallejos, Catalina A. and Marioni, John C. and Richardson, Sylvia},
  editor = {Morris, Quaid},
  year = {2015},
  month = jun,
  volume = {11},
  pages = {e1004333},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1004333},
  file = {/home/trung/GoogleDrive/Zotero/vallejos et al_2015_basics.pdf},
  journal = {PLOS Computational Biology},
  language = {en},
  number = {6}
}

@article{vallejos17_NormalizingsinglecellRNA,
  title = {Normalizing Single-Cell {{RNA}} Sequencing Data: Challenges and Opportunities},
  author = {Vallejos, Catalina A and Risso, Davide and Scialdone, Antonio and Dudoit, Sandrine and Marioni, John C},
  year = {2017},
  month = jun,
  volume = {14},
  pages = {565--571},
  issn = {1548-7105},
  doi = {10.1038/nmeth.4292},
  abstract = {This Perspective examines single-cell RNA-seq data challenges and the need for normalization methods designed specifically for single-cell data in order to remove technical biases.},
  file = {/home/trung/GoogleDrive/Zotero/vallejos et al_2017_normalizing single-cell rna sequencing data.pdf},
  journal = {Nature Methods},
  keywords = {_tablet},
  number = {6}
}

@article{vanamersfoort20_UncertaintyEstimationUsing,
  title = {Uncertainty {{Estimation Using}} a {{Single Deep Deterministic Neural Network}}},
  author = {{van Amersfoort}, Joost and Smith, Lewis and Teh, Yee Whye and Gal, Yarin},
  year = {2020},
  month = jun,
  abstract = {We propose a method for training a deterministic deep model that can find and reject out of distribution data points at test time with a single forward pass. Our approach, deterministic uncertainty quantification (DUQ), builds upon ideas of RBF networks. We scale training in these with a novel loss function and centroid updating scheme and match the accuracy of softmax models. By enforcing detectability of changes in the input using a gradient penalty, we are able to reliably detect out of distribution data. Our uncertainty quantification scales well to large datasets, and using a single model, we improve upon or match Deep Ensembles in out of distribution detection on notable difficult dataset pairs such as FashionMNIST vs. MNIST, and CIFAR-10 vs. SVHN.},
  archivePrefix = {arXiv},
  eprint = {2003.02037},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/van amersfoort et al_2020_uncertainty estimation using a single deep deterministic neural network.pdf},
  journal = {arXiv:2003.02037 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{vandegar20_MaximeVandegarNormalizingFlows,
  title = {{{MaximeVandegar}}/{{Normalizing}}-{{Flows}}},
  author = {Vandegar, Maxime},
  year = {2020},
  month = may,
  abstract = {A repository to learn about Flows, mostly from papers},
  annotation = {ZSCC: NoCitationData[s0]}
}

@article{vandemeent18_IntroductionProbabilisticProgramming,
  title = {An {{Introduction}} to {{Probabilistic Programming}}},
  author = {{van de Meent}, Jan-Willem and Paige, Brooks and Yang, Hongseok and Wood, Frank},
  year = {2018},
  month = sep,
  abstract = {This document is designed to be a first-year graduate-level introduction to probabilistic programming. It not only provides a thorough background for anyone wishing to use a probabilistic programming system, but also introduces the techniques needed to design and build these systems. It is aimed at people who have an undergraduate-level understanding of either or, ideally, both probabilistic machine learning and programming languages. We start with a discussion of model-based reasoning and explain why conditioning as a foundational computation is central to the fields of probabilistic machine learning and artificial intelligence. We then introduce a simple first-order probabilistic programming language (PPL) whose programs define static-computation-graph, finite-variable-cardinality models. In the context of this restricted PPL we introduce fundamental inference algorithms and describe how they can be implemented in the context of models denoted by probabilistic programs. In the second part of this document, we introduce a higher-order probabilistic programming language, with a functionality analogous to that of established programming languages. This affords the opportunity to define models with dynamic computation graphs, at the cost of requiring inference methods that generate samples by repeatedly executing the program. Foundational inference algorithms for this kind of probabilistic programming language are explained in the context of an interface between program executions and an inference controller. This document closes with a chapter on advanced topics which we believe to be, at the time of writing, interesting directions for probabilistic programming research; directions that point towards a tight integration with deep neural network research and the development of systems for next-generation artificial intelligence applications.},
  archivePrefix = {arXiv},
  eprint = {1809.10756},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/van de meent et al_2018_an introduction to probabilistic programming.pdf},
  journal = {arXiv:1809.10756 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{vandenberge20_Trajectorybaseddifferentialexpression,
  title = {Trajectory-Based Differential Expression Analysis for Single-Cell Sequencing Data},
  author = {{Van den Berge}, Koen and {Roux de B{\'e}zieux}, Hector and Street, Kelly and Saelens, Wouter and Cannoodt, Robrecht and Saeys, Yvan and Dudoit, Sandrine and Clement, Lieven},
  year = {2020},
  month = mar,
  volume = {11},
  pages = {1201},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14766-3},
  abstract = {Trajectory inference has radically enhanced single-cell RNA-seq research by enabling the study of dynamic changes in gene expression. Downstream of trajectory inference, it is vital to discover genes that are (i) associated with the lineages in the trajectory, or (ii) differentially expressed between lineages, to illuminate the underlying biological processes. Current data analysis procedures, however, either fail to exploit the continuous resolution provided by trajectory inference, or fail to pinpoint the exact types of differential expression. We introduce tradeSeq, a powerful generalized additive model framework based on the negative binomial distribution that allows flexible inference of both within-lineage and between-lineage differential expression. By incorporating observation-level weights, the model additionally allows to account for zero inflation. We evaluate the method on simulated datasets and on real datasets from droplet-based and full-length protocols, and show that it yields biological insights through a clear interpretation of the data.},
  file = {/home/trung/GoogleDrive/Zotero/van den berge et al_2020_trajectory-based differential expression analysis for single-cell sequencing data.pdf},
  journal = {Nature Communications},
  number = {1}
}

@article{vandepoll06_AdequateRangeSulfurContaining,
  title = {Adequate {{Range}} for {{Sulfur}}-{{Containing Amino Acids}} and {{Biomarkers}} for {{Their Excess}}: {{Lessons}} from {{Enteral}} and {{Parenteral Nutrition}}},
  shorttitle = {Adequate {{Range}} for {{Sulfur}}-{{Containing Amino Acids}} and {{Biomarkers}} for {{Their Excess}}},
  author = {{van de Poll}, Marcel C. G. and Dejong, Cornelis H. C. and Soeters, Peter B.},
  year = {2006},
  month = jun,
  volume = {136},
  pages = {1694S-1700S},
  issn = {0022-3166, 1541-6100},
  doi = {10.1093/jn/136.6.1694S},
  abstract = {The adequacy range of dietary requirements of specific amino acids in disease states is difficult to determine. In health, several techniques are available allowing rather precise quantification of requirements based on growth of the organism, rises in plasma concentration, or increases in the oxidation of marker amino acids during incremental administration of the amino acid under study. Requirements may not be similar in disease with regard to protein synthesis or with regard to specific functions such as scavenging of reactive oxygen species by compounds including glutathione. Requirements for this purpose can be assessed only when such a function can be measured and related to clinical outcome. There is apparent consensus concerning normal sulfur amino acid (SAA) requirements. WHO recommendations amount to 13 mg/kg per 24 h in healthy adults. This amount is roughly doubled in artificial nutrition regimens. In disease or after trauma, requirements may be altered for methionine, cysteine, and taurine. Although in specific cases of congenital enzyme deficiency, prematurity, or diminished liver function, hypermethionemia or hyperhomocysteinemia may occur, SAA supplementation can be considered safe in amounts exceeding 2\textendash 3 times the minimal recommended daily intake. Apart from some very specific indications (e.g., acetaminophen poisoning), the usefulness of SAA supplementation is not yet established. There is a growing body of data pointing out the potential importance of oxidative stress and resulting changes in redox state in numerous diseases including sepsis, chronic inflammation, cancer, AIDS/HIV, and aging. These observations warrant continued attention for the potential role of SAA supplementation. In particular, N-acetylcysteine remains promising for these conditions. J. Nutr. 136: 1694S\textendash 1700S, 2006.},
  annotation = {ZSCC: 0000054},
  file = {/home/trung/Zotero/storage/QQQGS54U/van de Poll et al. - 2006 - Adequate Range for Sulfur-Containing Amino Acids a.pdf},
  journal = {The Journal of Nutrition},
  language = {en},
  number = {6}
}

@article{vandermerwe16_InfluenceMethylsulfonylmethaneInflammationAssociated,
  title = {The {{Influence}} of {{Methylsulfonylmethane}} on {{Inflammation}}-{{Associated Cytokine Release}} before and Following {{Strenuous Exercise}}},
  author = {{van der Merwe}, Mari{\`e} and Bloomer, Richard J.},
  year = {2016},
  volume = {2016},
  issn = {2356-7651},
  doi = {10.1155/2016/7498359},
  abstract = {Background. Inflammation is associated with strenuous exercise and methylsulfonylmethane (MSM) has been shown to have anti-inflammatory properties. Methods. Physically active men were supplemented with either placebo or MSM (3 grams per day) for 28 days before performing 100 repetitions of eccentric knee extension exercise. Ex vivo and in vitro testing consisted of evaluating cytokine production in blood (whole blood and isolated peripheral blood mononuclear cells (PBMCs)) exposed to lipopolysaccharide (LPS), before and through 72 hours after exercise, while in vivo testing included the evaluation of cytokines before and through 72 hours after exercise. Results. LPS stimulation of whole blood after MSM supplementation resulted in decreased induction of IL-1{$\beta$}, with no effect on IL-6, TNF-{$\alpha$}, or IL-8. After exercise, there was a reduced response to LPS in the placebo, but MSM resulted in robust release of IL-6 and TNF-{$\alpha$}. A small decrease in resting levels of proinflammatory cytokines was noted with MSM, while an acute postexercise increase in IL-10 was observed with MSM. Conclusion. Strenuous exercise causes a robust inflammatory reaction that precludes the cells from efficiently responding to additional stimuli. MSM appears to dampen the release of inflammatory molecules in response to exercise, resulting in a less incendiary environment, allowing cells to still have the capacity to mount an appropriate response to an additional stimulus after exercise.},
  annotation = {ZSCC: 0000022},
  file = {/home/trung/GoogleDrive/Zotero/van der merwe et al_2016_the influence of methylsulfonylmethane on inflammation-associated cytokine release before and following strenuous exercise.pdf},
  journal = {Journal of Sports Medicine},
  pmcid = {PMC5097813},
  pmid = {27844051}
}

@inproceedings{vanderschaar20_AutoMLinterpretabilityPowering,
  title = {{{AutoML}} and Interpretability: {{Powering}} the Machine Learning Revolution in Healthcare},
  booktitle = {Proceedings of the 2020 {{ACM}}-{{IMS}} on Foundations of Data Science Conference},
  author = {{van der Schaar}, Mihaela},
  year = {2020},
  pages = {1},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3412815.3416879},
  abstract = {An AutoML and interpretability are both fundamental to the successful uptake of machine learning by non-expert end users. The former will lower barriers to entry and unlock potent new capabilities that are out of reach when working with ad-hoc models, while the latter will ensure that outputs are transparent, trustworthy, and meaningful. In healthcare, AutoML and interpretability are already beginning to empower the clinical community by enabling the crafting of actionable analytics that can inform and improve decision-making by clinicians, administrators, researchers, policymakers, and beyond.This keynote presents state-of-the-art AutoML and interpretability methods for healthcare developed in our lab and how they have been applied in various clinical settings (including cancer, cardiovascular disease, cystic fibrosis, and recently Covid-19), and then explains how these approaches form part of a broader vision for the future of machine learning in healthcare.},
  file = {/home/trung/GoogleDrive/Zotero/van der schaar_2020_automl and interpretability.pdf},
  isbn = {978-1-4503-8103-1},
  keywords = {automated machine learning,interpretability,machine learning,machine learning for healthcare},
  series = {{{FODS}} '20}
}

@article{vandijk18_MAGICRecoveringGene,
  title = {{{MAGIC}}: {{Recovering Gene Interactions}} from {{Single}}-{{Cell Data Using Data Diffusion}}},
  author = {{van Dijk}, David and Sharma, Roshan and Nainys, Juozas and Yim, Kristina and Kathail, Pooja and Carr, Ambrose J. and Burdziak, Cassandra and Moon, Kevin R. and Chaffer, Christine L. and Pattabiraman, Diwakar and Bierie, Brian and Mazutis, Linas and Wolf, Guy and Krishnaswamy, Smita and Pe'er, Dana},
  year = {2018},
  month = jul,
  volume = {174},
  pages = {716-729.e27},
  issn = {00928674},
  doi = {10.1016/j.cell.2018.05.061},
  abstract = {Single-cell RNA sequencing technologies suffer from many sources of technical noise, including under-sampling of mRNA molecules, often termed ``dropout,'' which can severely obscure important gene-gene relationships. To address this, we developed MAGIC (Markov affinity-based graph imputation of cells), a method that shares information across similar cells, via data diffusion, to denoise the cell count matrix and fill in missing transcripts. We validate MAGIC on several biological systems and find it effective at recovering gene-gene relationships and additional structures. Applied to the epithilial to mesenchymal transition, MAGIC reveals a phenotypic continuum, with the majority of cells residing in intermediate states that display stemlike signatures, and infers known and previously uncharacterized regulatory interactions, demonstrating that our approach can successfully uncover regulatory relations without perturbations.},
  file = {/home/trung/GoogleDrive/Zotero/van dijk et al_2018_magic.pdf},
  journal = {Cell},
  language = {en},
  number = {3}
}

@article{vandoremalen20_AerosolSurfaceStability,
  title = {Aerosol and {{Surface Stability}} of {{SARS}}-{{CoV}}-2 as {{Compared}} with {{SARS}}-{{CoV}}-1},
  author = {{van Doremalen}, Neeltje and Bushmaker, Trenton and Morris, Dylan H. and Holbrook, Myndi G. and Gamble, Amandine and Williamson, Brandi N. and Tamin, Azaibi and Harcourt, Jennifer L. and Thornburg, Natalie J. and Gerber, Susan I. and {Lloyd-Smith}, James O. and {de Wit}, Emmie and Munster, Vincent J.},
  year = {2020},
  month = apr,
  volume = {382},
  pages = {1564--1567},
  issn = {0028-4793, 1533-4406},
  doi = {10.1056/NEJMc2004973},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/U5PHJJNL/van Doremalen et al. - 2020 - Aerosol and Surface Stability of SARS-CoV-2 as Com.pdf},
  journal = {New England Journal of Medicine},
  language = {en},
  number = {16}
}

@article{vandusseldorp18_EffectBranchedChainAmino,
  title = {Effect of {{Branched}}-{{Chain Amino Acid Supplementation}} on {{Recovery Following Acute Eccentric Exercise}}},
  author = {VanDusseldorp, Trisha A. and Escobar, Kurt A. and Johnson, Kelly E. and Stratton, Matthew T. and Moriarty, Terence and Cole, Nathan and McCormick, James J. and Kerksick, Chad M. and Vaughan, Roger A. and Dokladny, Karol and Kravitz, Len and Mermier, Christine M.},
  year = {2018},
  month = oct,
  volume = {10},
  issn = {2072-6643},
  doi = {10.3390/nu10101389},
  abstract = {This study investigated the effect of branched-chain amino acid (BCAA) supplementation on recovery from eccentric exercise. Twenty males ingested either a BCAA supplement or placebo (PLCB) prior to and following eccentric exercise. Creatine kinase (CK), vertical jump (VJ), maximal voluntary isometric contraction (MVIC), jump squat (JS) and perceived soreness were assessed. No significant (p {$>$} 0.05) group by time interaction effects were observed for CK, soreness, MVIC, VJ, or JS. CK concentrations were elevated above baseline (p {$<$} 0.001) in both groups at 4, 24, 48 and 72 hr, while CK was lower (p = 0.02) in the BCAA group at 48 hr compared to PLCB. Soreness increased significantly from baseline (p {$<$} 0.01) in both groups at all time-points; however, BCAA supplemented individuals reported less soreness (p {$<$} 0.01) at the 48 and 72 hr time-points. MVIC force output returned to baseline levels (p {$>$} 0.05) at 24, 48 and 72 hr for BCAA individuals. No significant difference between groups (p {$>$} 0.05) was detected for VJ or JS. BCAA supplementation may mitigate muscle soreness following muscle-damaging exercise. However, when consumed with a diet consisting of \textasciitilde 1.2 g/kg/day protein, the attenuation of muscular performance decrements or corresponding plasma CK levels are likely negligible.},
  file = {/home/trung/GoogleDrive/Zotero/vandusseldorp et al_2018_effect of branched-chain amino acid supplementation on recovery following acute eccentric exercise.pdf},
  journal = {Nutrients},
  number = {10},
  pmcid = {PMC6212987},
  pmid = {30275356}
}

@article{vanengelen20_surveysemisupervisedlearning,
  title = {A Survey on Semi-Supervised Learning},
  author = {{van Engelen}, Jesper E. and Hoos, Holger H.},
  year = {2020},
  month = feb,
  volume = {109},
  pages = {373--440},
  issn = {0885-6125, 1573-0565},
  doi = {10.1007/s10994-019-05855-6},
  abstract = {Semi-supervised learning is the branch of machine learning concerned with using labelled as well as unlabelled data to perform certain learning tasks. Conceptually situated between supervised and unsupervised learning, it permits harnessing the large amounts of unlabelled data available in many use cases in combination with typically smaller sets of labelled data. In recent years, research in this area has followed the general trends observed in machine learning, with much attention directed at neural network-based models and generative learning. The literature on the topic has also expanded in volume and scope, now encompassing a broad spectrum of theory, algorithms and applications. However, no recent surveys exist to collect and organize this knowledge, impeding the ability of researchers and engineers alike to utilize it. Filling this void, we present an up-to-date overview of semi-supervised learning methods, covering earlier work as well as more recent advances. We focus primarily on semi-supervised classification, where the large majority of semi-supervised learning research takes place. Our survey aims to provide researchers and practitioners new to the field as well as more advanced readers with a solid understanding of the main approaches and algorithms developed over the past two decades, with an emphasis on the most prominent and currently relevant work. Furthermore, we propose a new taxonomy of semi-supervised classification algorithms, which sheds light on the different conceptual and methodological approaches for incorporating unlabelled data into the training process. Lastly, we show how the fundamental assumptions underlying most semi-supervised learning algorithms are closely connected to each other, and how they relate to the well-known semi-supervised clustering assumption.},
  file = {/home/trung/GoogleDrive/Zotero/van engelen et al_2020_a survey on semi-supervised learning.pdf},
  journal = {Machine Learning},
  language = {en},
  number = {2}
}

@misc{vanhoucke18_YouWantBe,
  title = {So {{You Want}} to {{Be}} a {{Research Scientist}}},
  author = {Vanhoucke, Vincent},
  year = {2018},
  month = dec,
  abstract = {Here's what they don't teach you in graduate school},
  file = {/home/trung/Zotero/storage/TFPY72MK/so-you-want-to-be-a-research-scientist-363c075d3d4c.html},
  howpublished = {https://medium.com/s/story/so-you-want-to-be-a-research-scientist-363c075d3d4c},
  journal = {Medium},
  language = {en}
}

@article{vania19_systematiccomparisonmethods,
  title = {A Systematic Comparison of Methods for Low-Resource Dependency Parsing on Genuinely Low-Resource Languages},
  author = {Vania, Clara and Kementchedjhieva, Yova and S{\o}gaard, Anders and Lopez, Adam},
  year = {2019},
  month = sep,
  abstract = {Parsers are available for only a handful of the world's languages, since they require lots of training data. How far can we get with just a small amount of training data? We systematically compare a set of simple strategies for improving low-resource parsers: data augmentation, which has not been tested before; cross-lingual training; and transliteration. Experimenting on three typologically diverse low-resource languages---North S\textbackslash 'ami, Galician, and Kazah---We find that (1) when only the low-resource treebank is available, data augmentation is very helpful; (2) when a related high-resource treebank is available, cross-lingual training is helpful and complements data augmentation; and (3) when the high-resource treebank uses a different writing system, transliteration into a shared orthographic spaces is also very helpful.},
  archivePrefix = {arXiv},
  eprint = {1909.02857},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vania et al_2019_a systematic comparison of methods for low-resource dependency parsing on genuinely low-resource languages.pdf;/home/trung/Zotero/storage/H5KZEED7/1909.html},
  journal = {arXiv:1909.02857 [cs]},
  keywords = {Computer Science - Computation and Language,small data},
  primaryClass = {cs}
}

@article{vansteenkiste19_AreDisentangledRepresentations,
  title = {Are {{Disentangled Representations Helpful}} for {{Abstract Visual Reasoning}}?},
  author = {{van Steenkiste}, Sjoerd and Locatello, Francesco and Schmidhuber, J{\"u}rgen and Bachem, Olivier},
  year = {2019},
  month = may,
  abstract = {A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world up-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better up-stream performance. In particular, they appear to enable quicker learning using fewer samples.},
  archivePrefix = {arXiv},
  eprint = {1905.12506},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2019/false},
  journal = {arXiv:1905.12506 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,disentanglement,I.2.6,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{varady20_NaturalWakeSleepAlgorithm,
  title = {Natural {{Wake}}-{{Sleep Algorithm}}},
  author = {V{\'a}rady, Csongor and Volpi, Riccardo and Malag{\`o}, Luigi and Ay, Nihat},
  year = {2020},
  month = aug,
  abstract = {The benefits of using the natural gradient are well known in a wide range of optimization problems. However, for the training of common neural networks the resulting increase in computational complexity sets a limitation to its practical application. Helmholtz Machines are a particular type of generative model composed of two Sigmoid Belief Networks (SBNs), acting as an encoder and a decoder, commonly trained using the Wake-Sleep (WS) algorithm and its reweighted version RWS. For SBNs, it has been shown how the locality of the connections in the graphical structure induces sparsity in the Fisher information matrix. The resulting block diagonal structure can be efficiently exploited to reduce the computational complexity of the Fisher matrix inversion and thus compute the natural gradient exactly, without the need of approximations. We present a geometric adaptation of well-known methods from the literature, introducing the Natural Wake-Sleep (NWS) and the Natural Reweighted Wake-Sleep (NRWS) algorithms. We present an experimental analysis of the novel geometrical algorithms based on the convergence speed and the value of the log-likelihood, both with respect to the number of iterations and the time complexity and demonstrating improvements on these aspects over their respective non-geometric baselines.},
  archivePrefix = {arXiv},
  eprint = {2008.06687},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/várady et al_2020_natural wake-sleep algorithm.pdf},
  journal = {arXiv:2008.06687 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{varano00_DisentanglingVariationalAutoencoders,
  title = {Disentangling {{Variational Autoencoders}} for {{Image Classification}}},
  author = {Varano, Chris},
  pages = {7},
  abstract = {In this paper, I investigate the use of a disentangled VAE for downstream image classification tasks. I train a disentangled VAE in an unsupervised manner, and use the learned encoder as a feature extractor on top of which a linear classifier is learned. The models are trained and evaluated on the MNIST handwritten digits dataset. Experiments compared the disentangled VAE with both a standard (entangled) VAE and a vanilla supervised model. Results show that the disentangled VAE significantly outperforms the other two models when the proportion of labelled data is artificially reduced, while it loses this advantage when the amount of labelled data increases, and instead matches the performance of the other models. These results suggest that the disentangled VAE may be useful in situations where labelled data is scarce but unlabelled data is abundant.},
  file = {/home/trung/GoogleDrive/Zotero/varano_disentangling variational autoencoders for image classiﬁcation.pdf},
  keywords = {disentanglement,variational},
  language = {en}
}

@article{vasilache18_TensorComprehensionsFrameworkAgnostic,
  title = {Tensor {{Comprehensions}}: {{Framework}}-{{Agnostic High}}-{{Performance Machine Learning Abstractions}}},
  shorttitle = {Tensor {{Comprehensions}}},
  author = {Vasilache, Nicolas and Zinenko, Oleksandr and Theodoridis, Theodoros and Goyal, Priya and DeVito, Zachary and Moses, William S. and Verdoolaege, Sven and Adams, Andrew and Cohen, Albert},
  year = {2018},
  month = jun,
  abstract = {Deep learning models with convolutional and recurrent networks are now ubiquitous and analyze massive amounts of audio, image, video, text and graph data, with applications in automatic translation, speech-to-text, scene understanding, ranking user preferences, ad placement, etc. Competing frameworks for building these networks such as TensorFlow, Chainer, CNTK, Torch/PyTorch, Caffe1/2, MXNet and Theano, explore different tradeoffs between usability and expressiveness, research or production orientation and supported hardware. They operate on a DAG of computational operators, wrapping high-performance libraries such as CUDNN (for NVIDIA GPUs) or NNPACK (for various CPUs), and automate memory allocation, synchronization, distribution. Custom operators are needed where the computation does not fit existing high-performance library calls, usually at a high engineering cost. This is frequently required when new operators are invented by researchers: such operators suffer a severe performance penalty, which limits the pace of innovation. Furthermore, even if there is an existing runtime call these frameworks can use, it often doesn't offer optimal performance for a user's particular network architecture and dataset, missing optimizations between operators as well as optimizations that can be done knowing the size and shape of data. Our contributions include (1) a language close to the mathematics of deep learning called Tensor Comprehensions, (2) a polyhedral Just-In-Time compiler to convert a mathematical description of a deep learning DAG into a CUDA kernel with delegated memory management and synchronization, also providing optimizations such as operator fusion and specialization for specific sizes, (3) a compilation cache populated by an autotuner. [Abstract cutoff]},
  archivePrefix = {arXiv},
  eprint = {1802.04730},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vasilache et al_2018_tensor comprehensions.pdf},
  journal = {arXiv:1802.04730 [cs]},
  primaryClass = {cs}
}

@article{vasquez19_MelNetGenerativeModel,
  title = {{{MelNet}}: {{A Generative Model}} for {{Audio}} in the {{Frequency Domain}}},
  shorttitle = {{{MelNet}}},
  author = {Vasquez, Sean and Lewis, Mike},
  year = {2019},
  month = jun,
  abstract = {Capturing high-level structure in audio waveforms is challenging because a single second of audio spans tens of thousands of timesteps. While long-range dependencies are difficult to model directly in the time domain, we show that they can be more tractably modelled in two-dimensional time-frequency representations such as spectrograms. By leveraging this representational advantage, in conjunction with a highly expressive probabilistic model and a multiscale generation procedure, we design a model capable of generating high-fidelity audio samples which capture structure at timescales that time-domain models have yet to achieve. We apply our model to a variety of audio generation tasks, including unconditional speech generation, music generation, and text-to-speech synthesis---showing improvements over previous approaches in both density estimates and human judgments.},
  archivePrefix = {arXiv},
  eprint = {1906.01083},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vasquez et al_2019_melnet.pdf},
  journal = {arXiv:1906.01083 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{vaswani17_AttentionAllYou,
  title = {Attention {{Is All You Need}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  year = {2017},
  month = jun,
  abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.},
  annotation = {ZSCC: 0003784},
  archivePrefix = {arXiv},
  eprint = {1706.03762},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vaswani et al_2017_attention is all you need.pdf;/home/trung/Zotero/storage/PN2XH3H5/1706.html},
  journal = {arXiv:1706.03762 [cs]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,favorite},
  primaryClass = {cs}
}

@article{vehtari17_PracticalBayesianmodel,
  title = {Practical {{Bayesian}} Model Evaluation Using Leave-One-out Cross-Validation and {{WAIC}}},
  author = {Vehtari, Aki and Gelman, Andrew and Gabry, Jonah},
  year = {2017},
  month = sep,
  volume = {27},
  pages = {1413--1432},
  issn = {0960-3174, 1573-1375},
  doi = {10.1007/s11222-016-9696-4},
  file = {/home/trung/GoogleDrive/Zotero/vehtari et al_2017_practical bayesian model evaluation using leave-one-out cross-validation and waic.pdf},
  journal = {Statistics and Computing},
  language = {en},
  number = {5}
}

@article{veitch19_UsingTextEmbeddings,
  title = {Using {{Text Embeddings}} for {{Causal Inference}}},
  author = {Veitch, Victor and Sridhar, Dhanya and Blei, David M.},
  year = {2019},
  month = may,
  abstract = {We address causal inference with text documents. For example, does adding a theorem to a paper affect its chance of acceptance? Does reporting the gender of a forum post author affect the popularity of the post? We estimate these effects from observational data, where they may be confounded by features of the text such as the subject or writing quality. Although the text suffices for causal adjustment, it is prohibitively high-dimensional. The challenge is to find a low-dimensional text representation that can be used in causal inference. A key insight is that causal adjustment requires only the aspects of text that are predictive of both the treatment and outcome. Our proposed method adapts deep language models to learn low-dimensional embeddings from text that predict these values well; these embeddings suffice for causal adjustment. We establish theoretical properties of this method. We study it empirically on semi-simulated and real data on paper acceptance and forum post popularity. Code is available at https://github.com/blei-lab/causal-text-embeddings.},
  archivePrefix = {arXiv},
  eprint = {1905.12741},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/veitch et al_2019_using text embeddings for causal inference.pdf;/home/trung/Zotero/storage/A5N27IWL/1905.html},
  journal = {arXiv:1905.12741 [cs, stat]},
  keywords = {causal,Computer Science - Computation and Language,Computer Science - Machine Learning,embedding,lda,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{velasquez16_TrimethylamineNOxideGood,
  title = {Trimethylamine {{N}}-{{Oxide}}: {{The Good}}, the {{Bad}} and the {{Unknown}}},
  shorttitle = {Trimethylamine {{N}}-{{Oxide}}},
  author = {Velasquez, Manuel T. and Ramezani, Ali and Manal, Alotaibi and Raj, Dominic S.},
  year = {2016},
  month = nov,
  volume = {8},
  issn = {2072-6651},
  doi = {10.3390/toxins8110326},
  abstract = {Trimethylamine N-oxide (TMAO) is a small colorless amine oxide generated from choline, betaine, and carnitine by gut microbial metabolism. It accumulates in the tissue of marine animals in high concentrations and protects against the protein-destabilizing effects of urea. Plasma level of TMAO is determined by a number of factors including diet, gut microbial flora and liver flavin monooxygenase activity. In humans, a positive correlation between elevated plasma levels of TMAO and an increased risk for major adverse cardiovascular events and death is reported. The atherogenic effect of TMAO is attributed to alterations in cholesterol and bile acid metabolism, activation of inflammatory pathways and promotion foam cell formation. TMAO levels increase with decreasing levels of kidney function and is associated with mortality in patients with chronic kidney disease. A number of therapeutic strategies are being explored to reduce TMAO levels, including use of oral broad spectrum antibiotics, promoting the growth of bacteria that utilize TMAO as substrate and the development of target-specific molecules with varying level of success. Despite the accumulating evidence, it is questioned whether TMAO is the mediator of a bystander in the disease process. Thus, it is important to undertake studies examining the cellular signaling in physiology and pathological states in order to establish the role of TMAO in health and disease in humans.},
  file = {/home/trung/GoogleDrive/Zotero/velasquez et al_2016_trimethylamine n-oxide.pdf},
  journal = {Toxins},
  number = {11},
  pmcid = {PMC5127123},
  pmid = {27834801}
}

@article{velickovic18_DeepGraphInfomax,
  title = {Deep {{Graph Infomax}}},
  author = {Veli{\v c}kovi{\'c}, Petar and Fedus, William and Hamilton, William L. and Li{\`o}, Pietro and Bengio, Yoshua and Hjelm, R. Devon},
  year = {2018},
  month = dec,
  abstract = {We present Deep Graph Infomax (DGI), a general approach for learning node representations within graph-structured data in an unsupervised manner. DGI relies on maximizing mutual information between patch representations and corresponding high-level summaries of graphs---both derived using established graph convolutional network architectures. The learnt patch representations summarize subgraphs centered around nodes of interest, and can thus be reused for downstream node-wise learning tasks. In contrast to most prior approaches to unsupervised learning with GCNs, DGI does not rely on random walk objectives, and is readily applicable to both transductive and inductive learning setups. We demonstrate competitive performance on a variety of node classification benchmarks, which at times even exceeds the performance of supervised learning.},
  archivePrefix = {arXiv},
  eprint = {1809.10341},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/veličković et al_2018_deep graph infomax.pdf},
  journal = {arXiv:1809.10341 [cs, math, stat]},
  keywords = {information},
  primaryClass = {cs, math, stat}
}

@article{vera18_RoleInformationComplexity,
  title = {The {{Role}} of {{Information Complexity}} and {{Randomization}} in {{Representation Learning}}},
  author = {Vera, Mat{\'i}as and Piantanida, Pablo and Vega, Leonardo Rey},
  year = {2018},
  month = feb,
  abstract = {A grand challenge in representation learning is to learn the different explanatory factors of variation behind the high dimen- sional data. Encoder models are often determined to optimize performance on training data when the real objective is to generalize well to unseen data. Although there is enough numerical evidence suggesting that noise injection (during training) at the representation level might improve the generalization ability of encoders, an information-theoretic understanding of this principle remains elusive. This paper presents a sample-dependent bound on the generalization gap of the cross-entropy loss that scales with the information complexity (IC) of the representations, meaning the mutual information between inputs and their representations. The IC is empirically investigated for standard multi-layer neural networks with SGD on MNIST and CIFAR-10 datasets; the behaviour of the gap and the IC appear to be in direct correlation, suggesting that SGD selects encoders to implicitly minimize the IC. We specialize the IC to study the role of Dropout on the generalization capacity of deep encoders which is shown to be directly related to the encoder capacity, being a measure of the distinguishability among samples from their representations. Our results support some recent regularization methods.},
  archivePrefix = {arXiv},
  eprint = {1802.05355},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vera et al_2018_the role of information complexity and randomization in representation learning.pdf},
  journal = {arXiv:1802.05355 [cs, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, stat}
}

@article{verdu19_EmpiricalEstimationInformation,
  title = {Empirical {{Estimation}} of {{Information Measures}}: {{A Literature Guide}}},
  author = {Verd{\'u}, Sergio},
  year = {2019},
  volume = {21},
  issn = {1099-4300},
  doi = {10.3390/e21080720},
  abstract = {We give a brief survey of the literature on the empirical estimation of entropy, differential entropy, relative entropy, mutual information and related information measures. While those quantities are of central importance in information theory, universal algorithms for their estimation are increasingly important in data science, machine learning, biology, neuroscience, economics, language, and other experimental sciences.},
  file = {/home/trung/GoogleDrive/Zotero/verdú_2019_empirical estimation of information measures.pdf},
  journal = {Entropy},
  keywords = {empirical estimators,entropy,information,information measures,mutual information,relative entropy,universal estimation},
  number = {8}
}

@article{verfaillie15_Decodingregulatorylandscape,
  title = {Decoding the Regulatory Landscape of Melanoma Reveals {{TEADS}} as Regulators of the Invasive Cell State},
  author = {Verfaillie, Annelien and Imrichova, Hana and Atak, Zeynep Kalender and Dewaele, Michael and Rambow, Florian and Hulselmans, Gert and Christiaens, Valerie and Svetlichnyy, Dmitry and Luciani, Flavie and {Van den Mooter}, Laura and Claerhout, Sofie and Fiers, Mark and Journe, Fabrice and Ghanem, Ghanem-Elias and Herrmann, Carl and Halder, Georg and Marine, Jean-Christophe and Aerts, Stein},
  year = {2015},
  month = apr,
  volume = {6},
  issn = {2041-1723},
  doi = {10.1038/ncomms7683},
  abstract = {Transcriptional reprogramming of proliferative melanoma cells into a phenotypically distinct invasive cell subpopulation is a critical event at the origin of metastatic spreading. Here we generate transcriptome, open chromatin and histone modification maps of melanoma cultures; and integrate this data with existing transcriptome and DNA methylation profiles from tumour biopsies to gain insight into the mechanisms underlying this key reprogramming event. This shows thousands of genomic regulatory regions underlying the proliferative and invasive states, identifying SOX10/MITF and AP-1/TEAD as regulators, respectively. Knockdown of TEADs shows a previously unrecognized role in the invasive gene network and establishes a causative link between these transcription factors, cell invasion and sensitivity to MAPK inhibitors. Using regulatory landscapes and in silico analysis, we show that transcriptional reprogramming underlies the distinct cellular states present in melanoma. Furthermore, it reveals an essential role for the TEADs, linking it to clinically relevant mechanisms such as invasion and resistance.,  The key regulators that allow transition from proliferative to invasive phenotype in melanoma cells have not been identified yet. The authors perform chromatin and transcriptome profiling followed by comprehensive bioinformatics analysis identifying new candidate regulators for two distinct cell states of melanoma.},
  file = {/home/trung/GoogleDrive/Zotero/verfaillie et al_2015_decoding the regulatory landscape of melanoma reveals teads as regulators of the invasive cell state.pdf},
  journal = {Nature Communications},
  pmcid = {PMC4403341},
  pmid = {25865119}
}

@article{verma18_GeneralizedZeroShotLearning,
  title = {Generalized {{Zero}}-{{Shot Learning}} via {{Synthesized Examples}}},
  author = {Verma, Vinay Kumar and Arora, Gundeep and Mishra, Ashish and Rai, Piyush},
  year = {2018},
  month = jun,
  abstract = {We present a generative framework for generalized zeroshot learning where the training and test classes are not necessarily disjoint. Built upon a variational autoencoder based architecture, consisting of a probabilistic encoder and a probabilistic conditional decoder, our model can generate novel exemplars from seen/unseen classes, given their respective class attributes. These exemplars can subsequently be used to train any off-the-shelf classification model. One of the key aspects of our encoder-decoder architecture is a feedback-driven mechanism in which a discriminator (a multivariate regressor) learns to map the generated exemplars to the corresponding class attribute vectors, leading to an improved generator. Our model's ability to generate and leverage examples from unseen classes to train the classification model naturally helps to mitigate the bias towards predicting seen classes in generalized zeroshot learning settings. Through a comprehensive set of experiments, we show that our model outperforms several state-of-the-art methods, on several benchmark datasets, for both standard as well as generalized zero-shot learning.},
  annotation = {ZSCC: 0000116},
  archivePrefix = {arXiv},
  eprint = {1712.03878},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/VCBQVT6K/Verma et al. - 2018 - Generalized Zero-Shot Learning via Synthesized Exa.pdf},
  journal = {arXiv:1712.03878 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{verma19_InterpolationConsistencyTraining,
  title = {Interpolation {{Consistency Training}} for {{Semi}}-{{Supervised Learning}}},
  author = {Verma, Vikas and Lamb, Alex and Kannala, Juho and Bengio, Yoshua and {Lopez-Paz}, David},
  year = {2019},
  month = may,
  abstract = {We introduce Interpolation Consistency Training (ICT), a simple and computation efficient algorithm for training Deep Neural Networks in the semi-supervised learning paradigm. ICT encourages the prediction at an interpolation of unlabeled points to be consistent with the interpolation of the predictions at those points. In classification problems, ICT moves the decision boundary to low-density regions of the data distribution. Our experiments show that ICT achieves state-of-the-art performance when applied to standard neural network architectures on the CIFAR-10 and SVHN benchmark datasets.},
  annotation = {ZSCC: 0000037},
  archivePrefix = {arXiv},
  eprint = {1903.03825},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/verma et al_2019_interpolation consistency training for semi-supervised learning.pdf;/home/trung/Zotero/storage/3B59MGX8/1903.html},
  journal = {arXiv:1903.03825 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{verma19_ManifoldMixupBetter,
  title = {Manifold {{Mixup}}: {{Better Representations}} by {{Interpolating Hidden States}}},
  shorttitle = {Manifold {{Mixup}}},
  author = {Verma, Vikas and Lamb, Alex and Beckham, Christopher and Najafi, Amir and Mitliagkas, Ioannis and Courville, Aaron and {Lopez-Paz}, David and Bengio, Yoshua},
  year = {2019},
  month = may,
  abstract = {Deep neural networks excel at learning the training data, but often provide incorrect and confident predictions when evaluated on slightly different test examples. This includes distribution shifts, outliers, and adversarial examples. To address these issues, we propose Manifold Mixup, a simple regularizer that encourages neural networks to predict less confidently on interpolations of hidden representations. Manifold Mixup leverages semantic interpolations as additional training signal, obtaining neural networks with smoother decision boundaries at multiple levels of representation. As a result, neural networks trained with Manifold Mixup learn class-representations with fewer directions of variance. We prove theory on why this flattening happens under ideal conditions, validate it on practical situations, and connect it to previous works on information theory and generalization. In spite of incurring no significant computation and being implemented in a few lines of code, Manifold Mixup improves strong baselines in supervised learning, robustness to single-step adversarial attacks, and test log-likelihood.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1806.05236},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/verma et al_2019_manifold mixup.pdf;/home/trung/Zotero/storage/GUHMKYBS/1806.html},
  journal = {arXiv:1806.05236 [cs, stat]},
  primaryClass = {cs, stat}
}

@techreport{verma20_Bayesiannonparametricsemisupervised,
  title = {A {{Bayesian}} Nonparametric Semi-Supervised Model for Integration of Multiple Single-Cell Experiments},
  author = {Verma, Archit and Engelhardt, Barbara},
  year = {2020},
  month = jan,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.01.14.906313},
  abstract = {Joint analysis of multiple single cell RNA-sequencing (scRNA-seq) data is confounded by technical batch effects across experiments, biological or environmental variability across cells, and different capture processes across sequencing platforms. Manifold alignment is a principled, effective tool for integrating multiple data sets and controlling for confounding factors. We demonstrate that the semi-supervised t-distributed Gaussian process latent variable model (sstGPLVM), which projects the data onto a mixture of fixed and latent dimensions, can learn a unified low-dimensional embedding for multiple single cell experiments with minimal assumptions. We show the efficacy of the model as compared with state-of-the-art methods for single cell data integration on simulated data, pancreas cells from four sequencing technologies, induced pluripotent stem cells from male and female donors, and mouse brain cells from both spatial seqFISH             +             and traditional scRNA-seq.                                   Code and data is available at             https://github.com/architverma1/sc-manifold-alignment},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/92DI3CKJ/Verma and Engelhardt - 2020 - A Bayesian nonparametric semi-supervised model for.pdf},
  language = {en},
  type = {Preprint}
}

@article{verma20_DomainAgnosticContrastiveLearning,
  title = {Towards {{Domain}}-{{Agnostic Contrastive Learning}}},
  author = {Verma, Vikas and Luong, Minh-Thang and Kawaguchi, Kenji and Pham, Hieu and Le, Quoc V.},
  year = {2020},
  month = nov,
  abstract = {Despite recent success, most contrastive self-supervised learning methods are domain-specific, relying heavily on data augmentation techniques that require knowledge about a particular domain, such as image cropping and rotation. To overcome such limitation, we propose a novel domain-agnostic approach to contrastive learning, named DACL, that is applicable to domains where invariances, and thus, data augmentation techniques, are not readily available. Key to our approach is the use of Mixup noise to create similar and dissimilar examples by mixing data samples differently either at the input or hidden-state levels. To demonstrate the effectiveness of DACL, we conduct experiments across various domains such as tabular data, images, and graphs. Our results show that DACL not only outperforms other domain-agnostic noising methods, such as Gaussian-noise, but also combines well with domain-specific methods, such as SimCLR, to improve self-supervised visual representation learning. Finally, we theoretically analyze our method and show advantages over the Gaussian-noise based contrastive learning approach.},
  archivePrefix = {arXiv},
  eprint = {2011.04419},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/verma et al_2020_towards domain-agnostic contrastive learning.pdf},
  journal = {arXiv:2011.04419 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{verma20_GraphMixRegularizedTraining,
  title = {{{GraphMix}}: {{Regularized Training}} of {{Graph Neural Networks}} for {{Semi}}-{{Supervised Learning}}},
  shorttitle = {{{GraphMix}}},
  author = {Verma, Vikas and Qu, Meng and Lamb, Alex and Bengio, Yoshua and Kannala, Juho and Tang, Jian},
  year = {2020},
  month = apr,
  abstract = {We present GraphMix, a regularization technique for Graph Neural Network based semi-supervised object classification, leveraging the recent advances in the regularization of classical deep neural networks. Specifically, we propose a unified approach in which we train a fully-connected network jointly with the graph neural network via parameter sharing, interpolation-based regularization, and self-predicted-targets. Our proposed method is architecture agnostic in the sense that it can be applied to any variant of graph neural networks which applies a parametric transformation to the features of the graph nodes. Despite its simplicity, with GraphMix we can consistently improve results and achieve or closely match state-of-the-art performance using even simpler architectures such as Graph Convolutional Networks, across three established graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as three newly proposed datasets : Cora-Full, Co-author-CS and Co-author-Physics.},
  archivePrefix = {arXiv},
  eprint = {1909.11715},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/verma et al_2020_graphmix.pdf},
  journal = {arXiv:1909.11715 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{vertes00_Flexibleaccurateinference,
  title = {Flexible and Accurate Inference and Learning for Deep Generative Models},
  author = {V{\'e}rtes, Eszter and Sahani, Maneesh},
  pages = {10},
  abstract = {We introduce a new approach to learning in hierarchical latent-variable generative models called the ``distributed distributional code Helmholtz machine'', which emphasises flexibility and accuracy in the inferential process. Like the original Helmholtz machine and later variational autoencoder algorithms (but unlike adversarial methods) our approach learns an explicit inference or ``recognition'' model to approximate the posterior distribution over the latent variables. Unlike these earlier methods, it employs a posterior representation that is not limited to a narrow tractable parametrised form (nor is it represented by samples). To train the generative and recognition models we develop an extended wake-sleep algorithm inspired by the original Helmholtz machine. This makes it possible to learn hierarchical latent models with both discrete and continuous variables, where an accurate posterior representation is essential. We demonstrate that the new algorithm outperforms current state-of-the-art methods on synthetic, natural image patch and the MNIST data sets.},
  file = {/home/trung/GoogleDrive/Zotero/vértes et al_flexible and accurate inference and learning for deep generative models.pdf},
  language = {en}
}

@article{vidal17_MathematicsDeepLearning,
  title = {Mathematics of {{Deep Learning}}},
  author = {Vidal, Rene and Bruna, Joan and Giryes, Raja and Soatto, Stefano},
  year = {2017},
  month = dec,
  abstract = {Recently there has been a dramatic increase in the performance of recognition systems due to the introduction of deep architectures for representation learning and classification. However, the mathematical reasons for this success remain elusive. This tutorial will review recent work that aims to provide a mathematical justification for several properties of deep networks, such as global optimality, geometric stability, and invariance of the learned representations.},
  archivePrefix = {arXiv},
  eprint = {1712.04741},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/DQ6KITF4/Vidal et al. - 2017 - Mathematics of Deep Learning.pdf},
  journal = {arXiv:1712.04741 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{vig19_VisualizingAttentionTransformerBased,
  title = {Visualizing {{Attention}} in {{Transformer}}-{{Based Language Representation Models}}},
  author = {Vig, Jesse},
  year = {2019},
  month = apr,
  abstract = {We present an open-source tool for visualizing multi-head self-attention in Transformer-based language representation models. The tool extends earlier work by visualizing attention at three levels of granularity: the attention-head level, the model level, and the neuron level. We describe how each of these views can help to interpret the model, and we demonstrate the tool on the BERT model and the OpenAI GPT-2 model. We also present three use cases for analyzing GPT-2: detecting model bias, identifying recurring patterns, and linking neurons to model behavior.},
  annotation = {ZSCC: 0000005},
  archivePrefix = {arXiv},
  eprint = {1904.02679},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vig_2019_visualizing attention in transformer-based language representation models.pdf;/home/trung/Zotero/storage/R9QGFD2V/1904.html},
  journal = {arXiv:1904.02679 [cs, stat]},
  keywords = {attention,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,explain,Statistics - Machine Learning,transformer,visualization},
  primaryClass = {cs, stat}
}

@inproceedings{villalba19_StateoftheArtSpeakerRecognition,
  title = {State-of-the-{{Art Speaker Recognition}} for {{Telephone}} and {{Video Speech}}: {{The JHU}}-{{MIT Submission}} for {{NIST SRE18}}},
  shorttitle = {State-of-the-{{Art Speaker Recognition}} for {{Telephone}} and {{Video Speech}}},
  booktitle = {Interspeech 2019},
  author = {Villalba, Jes{\'u}s and Chen, Nanxin and Snyder, David and {Garcia-Romero}, Daniel and McCree, Alan and Sell, Gregory and Borgstrom, Jonas and Richardson, Fred and Shon, Suwon and Grondin, Fran{\c c}ois and Dehak, R{\'e}da and {Garc{\'i}a-Perera}, Leibny Paola and Povey, Daniel and {Torres-Carrasquillo}, Pedro A. and Khudanpur, Sanjeev and Dehak, Najim},
  year = {2019},
  month = sep,
  pages = {1488--1492},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-2713},
  abstract = {We present a condensed description of the joint effort of JHUCLSP, JHU-HLTCOE, MIT-LL., MIT CSAIL and LSE-EPITA for NIST SRE18. All the developed systems consisted of xvector/i-vector embeddings with some flavor of PLDA backend. Very deep x-vector architectures\textendash Extended and Factorized TDNN, and ResNets\textendash{} clearly outperformed shallower xvectors and i-vectors. The systems were tailored to the video (VAST) or to the telephone (CMN2) condition. The VAST data was challenging, yielding 4 times worse performance than other video based datasets like Speakers in the Wild. We were able to calibrate the VAST data with very few development trials by using careful adaptation and score normalization methods. The VAST primary fusion yielded EER=10.18\% and Cprimary=0.431. By improving calibration in post-eval, we reached Cprimary=0.369. In CMN2, we used unsupervised SPLDA adaptation based on agglomerative clustering and score normalization to correct the domain shift between English and Tunisian Arabic models. The CMN2 primary fusion yielded EER=4.5\% and Cprimary=0.313. Extended TDNN x-vector was the best single system obtaining EER=11.1\% and Cprimary=0.452 in VAST; and 4.95\% and 0.354 in CMN2.},
  file = {/home/trung/GoogleDrive/Zotero/villalba et al_2019_state-of-the-art speaker recognition for telephone and video speech.pdf;/home/trung/GoogleDrive/Zotero/villalba et al_2019_state-of-the-art speaker recognition for telephone and video speech2.pdf},
  language = {en}
}

@article{villalba20_Stateoftheartspeakerrecognition,
  title = {State-of-the-Art Speaker Recognition with Neural Network Embeddings in {{NIST SRE18}} and {{Speakers}} in the {{Wild}} Evaluations},
  author = {Villalba, Jes{\'u}s and Chen, Nanxin and Snyder, David and {Garcia-Romero}, Daniel and McCree, Alan and Sell, Gregory and Borgstrom, Jonas and {Garc{\'i}a-Perera}, Leibny Paola and Richardson, Fred and Dehak, R{\'e}da and {Torres-Carrasquillo}, Pedro A. and Dehak, Najim},
  year = {2020},
  month = mar,
  volume = {60},
  pages = {101026},
  issn = {08852308},
  doi = {10.1016/j.csl.2019.101026},
  abstract = {We present a thorough analysis of the systems developed by the JHU-MIT consortium in the context of NIST speaker recognition evaluation 2018. In the previous NIST evaluation, in 2016, i-vectors were the speaker recognition state-of-the-art. However now, neural network embeddings (a.k.a. x-vectors) rise as the best performing approach. We show that in some conditions, x-vectors' detection error reduces by 2 w.r.t. i-vectors. In this work, we experimented on the Speakers In The Wild evaluation (SITW), NIST SRE18 VAST (Video Annotation for Speech Technology), and SRE18 CMN2 (Call My Net 2, telephone Tunisian Arabic) to compare network architectures, pooling layers, training objectives, back-end adaptation methods, and calibration techniques. x-Vectors based on factorized and extended TDNN networks achieved performance without parallel on SITW and CMN2 data. However for VAST, performance was significantly worse than for SITW. We noted that the VAST audio quality was severely degraded compared to the SITW, even though they both consist of Internet videos. This degradation caused strong domain mismatch between training and VAST data. Due to this mismatch, large networks performed just slightly better than smaller networks. This also complicated VAST calibration. However, we managed to calibrate VAST by adapting SITW scores distribution to VAST, using a small amount of in-domain development data.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/villalba et al_2020_state-of-the-art speaker recognition with neural network embeddings in nist sre18 and speakers in the wild evaluations.pdf},
  journal = {Computer Speech \& Language},
  language = {en}
}

@article{vinyals19_GrandmasterlevelStarCraft,
  title = {Grandmaster Level in {{StarCraft II}} Using Multi-Agent Reinforcement Learning},
  author = {Vinyals, Oriol and Babuschkin, Igor and Czarnecki, Wojciech M. and Mathieu, Micha{\"e}l and Dudzik, Andrew and Chung, Junyoung and Choi, David H. and Powell, Richard and Ewalds, Timo and Georgiev, Petko and Oh, Junhyuk and Horgan, Dan and Kroiss, Manuel and Danihelka, Ivo and Huang, Aja and Sifre, Laurent and Cai, Trevor and Agapiou, John P. and Jaderberg, Max and Vezhnevets, Alexander S. and Leblond, R{\'e}mi and Pohlen, Tobias and Dalibard, Valentin and Budden, David and Sulsky, Yury and Molloy, James and Paine, Tom L. and Gulcehre, Caglar and Wang, Ziyu and Pfaff, Tobias and Wu, Yuhuai and Ring, Roman and Yogatama, Dani and W{\"u}nsch, Dario and McKinney, Katrina and Smith, Oliver and Schaul, Tom and Lillicrap, Timothy and Kavukcuoglu, Koray and Hassabis, Demis and Apps, Chris and Silver, David},
  year = {2019},
  month = oct,
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-019-1724-z},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/vinyals et al_2019_grandmaster level in starcraft ii using multi-agent reinforcement learning.pdf;/home/trung/GoogleDrive/Zotero/vinyals et al_2019_grandmaster level in starcraft ii using multi-agent reinforcement learning2.pdf},
  journal = {Nature},
  language = {en}
}

@article{voita19_AnalyzingMultiHeadSelfAttention,
  title = {Analyzing {{Multi}}-{{Head Self}}-{{Attention}}: {{Specialized Heads Do}} the {{Heavy Lifting}}, the {{Rest Can Be Pruned}}},
  shorttitle = {Analyzing {{Multi}}-{{Head Self}}-{{Attention}}},
  author = {Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
  year = {2019},
  month = may,
  abstract = {Multi-head self-attention is a key component of the Transformer, a state-of-the-art architecture for neural machine translation. In this work we evaluate the contribution made by individual attention heads in the encoder to the overall performance of the model and analyze the roles played by them. We find that the most important and confident heads play consistent and often linguistically-interpretable roles. When pruning heads using a method based on stochastic gates and a differentiable relaxation of the L0 penalty, we observe that specialized heads are last to be pruned. Our novel pruning method removes the vast majority of heads without seriously affecting performance. For example, on the English-Russian WMT dataset, pruning 38 out of 48 encoder heads results in a drop of only 0.15 BLEU.},
  annotation = {ZSCC: 0000021},
  archivePrefix = {arXiv},
  eprint = {1905.09418},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/voita et al_2019_analyzing multi-head self-attention.pdf;/home/trung/Zotero/storage/SUEF8E78/1905.html},
  journal = {arXiv:1905.09418 [cs]},
  keywords = {attention,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{voita19_BottomupEvolutionRepresentations,
  title = {The {{Bottom}}-up {{Evolution}} of {{Representations}} in the {{Transformer}}: {{A Study}} with {{Machine Translation}} and {{Language Modeling Objectives}}},
  shorttitle = {The {{Bottom}}-up {{Evolution}} of {{Representations}} in the {{Transformer}}},
  author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
  year = {2019},
  month = sep,
  abstract = {We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We focus on the Transformers for our analysis as they have been shown effective on various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and how this process depends on the choice of learning objective. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.},
  archivePrefix = {arXiv},
  eprint = {1909.01380},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/4S2CS84C/Voita et al. - 2019 - The Bottom-up Evolution of Representations in the .pdf},
  journal = {arXiv:1909.01380 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{voita19_BottomupEvolutionRepresentationsa,
  title = {The {{Bottom}}-up {{Evolution}} of {{Representations}} in the {{Transformer}}: {{A Study}} with {{Machine Translation}} and {{Language Modeling Objectives}}},
  shorttitle = {The {{Bottom}}-up {{Evolution}} of {{Representations}} in the {{Transformer}}},
  author = {Voita, Elena and Sennrich, Rico and Titov, Ivan},
  year = {2019},
  month = sep,
  abstract = {We seek to understand how the representations of individual tokens and the structure of the learned feature space evolve between layers in deep neural networks under different learning objectives. We focus on the Transformers for our analysis as they have been shown effective on various tasks, including machine translation (MT), standard left-to-right language models (LM) and masked language modeling (MLM). Previous work used black-box probing tasks to show that the representations learned by the Transformer differ significantly depending on the objective. In this work, we use canonical correlation analysis and mutual information estimators to study how information flows across Transformer layers and how this process depends on the choice of learning objective. For example, as you go from bottom to top layers, information about the past in left-to-right language models gets vanished and predictions about the future get formed. In contrast, for MLM, representations initially acquire information about the context around the token, partially forgetting the token identity and producing a more generalized token representation. The token identity then gets recreated at the top MLM layers.},
  archivePrefix = {arXiv},
  eprint = {1909.01380},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/voita et al_2019_the bottom-up evolution of representations in the transformer.pdf},
  journal = {arXiv:1909.01380 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{voita20_InformationTheoreticProbingMinimum,
  title = {Information-{{Theoretic Probing}} with {{Minimum Description Length}}},
  author = {Voita, Elena and Titov, Ivan},
  year = {2020},
  month = mar,
  abstract = {To measure how well pretrained representations encode some linguistic property, it is common to use accuracy of a probe, i.e. a classifier trained to predict the property from the representations. Despite widespread adoption of probes, differences in their accuracy fail to adequately reflect differences in representations. For example, they do not substantially favour pretrained representations over randomly initialized ones. Analogously, their accuracy can be similar when probing for genuine linguistic labels and probing for random synthetic tasks. To see reasonable differences in accuracy with respect to these random baselines, previous work had to constrain either the amount of probe training data or its model size. Instead, we propose an alternative to the standard probes, information-theoretic probing with minimum description length (MDL). With MDL probing, training a probe to predict labels is recast as teaching it to effectively transmit the data. Therefore, the measure of interest changes from probe accuracy to the description length of labels given representations. In addition to probe quality, the description length evaluates "the amount of effort" needed to achieve the quality. This amount of effort characterizes either (i) size of a probing model, or (ii) the amount of data needed to achieve the high quality. We consider two methods for estimating MDL which can be easily implemented on top of the standard probing pipelines: variational coding and online coding. We show that these methods agree in results and are more informative and stable than the standard probes.},
  archivePrefix = {arXiv},
  eprint = {2003.12298},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2020/false},
  journal = {arXiv:2003.12298 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{voloshynovskiy19_Informationbottleneckvariational,
  title = {Information Bottleneck through Variational Glasses},
  author = {Voloshynovskiy, Slava and Kondah, Mouad and Rezaeifar, Shideh and Taran, Olga and Holotyak, Taras and Rezende, Danilo Jimenez},
  year = {2019},
  month = dec,
  abstract = {Information bottleneck (IB) principle [1] has become an important element in information-theoretic analysis of deep models. Many state-of-the-art generative models of both Variational Autoencoder (VAE) [2; 3] and Generative Adversarial Networks (GAN) [4] families use various bounds on mutual information terms to introduce certain regularization constraints [5; 6; 7; 8; 9; 10]. Accordingly, the main difference between these models consists in add regularization constraints and targeted objectives. In this work, we will consider the IB framework for three classes of models that include supervised, unsupervised and adversarial generative models. We will apply a variational decomposition leading a common structure and allowing easily establish connections between these models and analyze underlying assumptions. Based on these results, we focus our analysis on unsupervised setup and reconsider the VAE family. In particular, we present a new interpretation of VAE family based on the IB framework using a direct decomposition of mutual information terms and show some interesting connections to existing methods such as VAE [2; 3], beta-VAE [11], AAE [12], InfoVAE [5] and VAE/GAN [13]. Instead of adding regularization constraints to an evidence lower bound (ELBO) [2; 3], which itself is a lower bound, we show that many known methods can be considered as a product of variational decomposition of mutual information terms in the IB framework. The proposed decomposition might also contribute to the interpretability of generative models of both VAE and GAN families and create a new insights to a generative compression [14; 15; 16; 17]. It can also be of interest for the analysis of novelty detection based on one-class classifiers [18] with the IB based discriminators.},
  archivePrefix = {arXiv},
  eprint = {1912.00830},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/voloshynovskiy et al_2019_information bottleneck through variational glasses.pdf},
  journal = {arXiv:1912.00830 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{voloshynovskiy20_VariationalInformationBottleneck,
  title = {Variational {{Information Bottleneck}} for {{Semi}}-{{Supervised Classification}}},
  author = {Voloshynovskiy, Slava and Taran, Olga and Kondah, Mouad and Holotyak, Taras and Rezende, Danilo},
  year = {2020},
  month = aug,
  volume = {22},
  pages = {943},
  issn = {1099-4300},
  doi = {10.3390/e22090943},
  abstract = {In this paper, we consider an information bottleneck (IB) framework for semi-supervised classification with several families of priors on latent space representation. We apply a variational decomposition of mutual information terms of IB. Using this decomposition we perform an analysis of several regularizers and practically demonstrate an impact of different components of variational model on the classification accuracy. We propose a new formulation of semi-supervised IB with hand crafted and learnable priors and link it to the previous methods such as semi-supervised versions of VAE (M1 + M2), AAE, CatGAN, etc. We show that the resulting model allows better understand the role of various previously proposed regularizers in semi-supervised classification task in the light of IB framework. The proposed IB semi-supervised model with hand-crafted and learnable priors is experimentally validated on MNIST under different amount of labeled data.},
  file = {/home/trung/GoogleDrive/Zotero/voloshynovskiy et al_2020_variational information bottleneck for semi-supervised classification.pdf},
  journal = {Entropy},
  keywords = {_tablet,information},
  language = {en},
  number = {9}
}

@article{vonkugelgen20_SimpsonparadoxCovid19,
  title = {Simpson's Paradox in {{Covid}}-19 Case Fatality Rates: A Mediation Analysis of Age-Related Causal Effects},
  shorttitle = {Simpson's Paradox in {{Covid}}-19 Case Fatality Rates},
  author = {{von K{\"u}gelgen}, Julius and Gresele, Luigi and Sch{\"o}lkopf, Bernhard},
  year = {2020},
  month = jun,
  abstract = {We point out an example of Simpson's paradox in COVID-19 case fatality rates (CFRs): comparing data from {$>$} 72, 000 cases from China with data from Italy reported on March 9th, we find that CFRs are lower in Italy for each age group, but higher overall. This phenomenon can be explained by a stark difference in case demographic between the two countries. Using this as a motivating example, we review basic concepts from mediation analysis and show how these can be used to quantify different direct and indirect effects when assuming a coarse-grained causal graph involving country, age, and mortality. As a case study, we then investigate how total, direct, and indirect (age-mediated) causal effects between China and Italy evolve over two months until May 7th 2020.},
  archivePrefix = {arXiv},
  eprint = {2005.07180},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/von kügelgen et al_2020_simpson's paradox in covid-19 case fatality rates.pdf},
  journal = {arXiv:2005.07180 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{vonrueden20_InformedMachineLearning,
  title = {Informed {{Machine Learning}} -- {{A Taxonomy}} and {{Survey}} of {{Integrating Knowledge}} into {{Learning Systems}}},
  author = {{von Rueden}, Laura and Mayer, Sebastian and Beckh, Katharina and Georgiev, Bogdan and Giesselbach, Sven and Heese, Raoul and Kirsch, Birgit and Pfrommer, Julius and Pick, Annika and Ramamurthy, Rajkumar and Walczak, Michal and Garcke, Jochen and Bauckhage, Christian and Schuecker, Jannis},
  year = {2020},
  month = feb,
  abstract = {Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process, which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. First, we provide a definition and propose a concept for informed machine learning, which illustrates its building blocks and distinguishes it from conventional machine learning. Second, we introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Third, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning.},
  archivePrefix = {arXiv},
  eprint = {1903.12394},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/von rueden et al_2020_informed machine learning -- a taxonomy and survey of integrating knowledge into learning systems.pdf},
  journal = {arXiv:1903.12394 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{vowels19_GatedVariationalAutoEncoders,
  title = {Gated {{Variational AutoEncoders}}: {{Incorporating Weak Supervision}} to {{Encourage Disentanglement}}},
  shorttitle = {Gated {{Variational AutoEncoders}}},
  author = {Vowels, Matthew J. and Camgoz, Necati Cihan and Bowden, Richard},
  year = {2019},
  month = nov,
  abstract = {Variational AutoEncoders (VAEs) provide a means to generate representational latent embeddings. Previous research has highlighted the benefits of achieving representations that are disentangled, particularly for downstream tasks. However, there is some debate about how to encourage disentanglement with VAEs and evidence indicates that existing implementations of VAEs do not achieve disentanglement consistently. The evaluation of how well a VAE's latent space has been disentangled is often evaluated against our subjective expectations of which attributes should be disentangled for a given problem. Therefore, by definition, we already have domain knowledge of what should be achieved and yet we use unsupervised approaches to achieve it. We propose a weakly-supervised approach that incorporates any available domain knowledge into the training process to form a Gated-VAE. The process involves partitioning the representational embedding and gating backpropagation. All partitions are utilised on the forward pass but gradients are backpropagated through different partitions according to selected image/target pairings. The approach can be used to modify existing VAE models such as beta-VAE, InfoVAE and DIP-VAE-II. Experiments demonstrate that using gated backpropagation, latent factors are represented in their intended partition. The approach is applied to images of faces for the purpose of disentangling head-pose from facial expression. Quantitative metrics show that using Gated-VAE improves average disentanglement, completeness and informativeness, as compared with un-gated implementations. Qualitative assessment of latent traversals demonstrate its disentanglement of head-pose from expression, even when only weak/noisy supervision is available.},
  archivePrefix = {arXiv},
  eprint = {1911.06443},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/vowels et al_2019_gated variational autoencoders.pdf},
  journal = {arXiv:1911.06443 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@misc{vowels20_matthewvowels1AwesomeVAEs,
  title = {Matthewvowels1/{{Awesome}}-{{VAEs}}},
  author = {Vowels, Matthew},
  year = {2020},
  month = feb,
  abstract = {A curated list of awesome work on VAEs, disentanglement, representation learning, and generative models.},
  annotation = {ZSCC: NoCitationData[s0]}
}

@inproceedings{vowels20_NestedVAEIsolatingCommon,
  title = {{{NestedVAE}}: {{Isolating Common Factors}} via {{Weak Supervision}}},
  shorttitle = {{{NestedVAE}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Vowels, Matthew J. and Cihan Camgoz, Necati and Bowden, Richard},
  year = {2020},
  month = jun,
  pages = {9199--9209},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00922},
  abstract = {Fair and unbiased machine learning is an important and active field of research, as decision processes are increasingly driven by models that learn from data. Unfortunately, any biases present in the data may be learned by the model, thereby inappropriately transferring that bias into the decision making process. We identify the connection between the task of bias reduction and that of isolating factors common between domains whilst encouraging domain specific invariance. To isolate the common factors we combine the theory of deep latent variable models with information bottleneck theory for scenarios whereby data may be naturally paired across domains and no additional supervision is required. The result is the Nested Variational AutoEncoder (NestedVAE). Two outer VAEs with shared weights attempt to reconstruct the input and infer a latent space, whilst a nested VAE attempts to reconstruct the latent representation of one image, from the latent representation of its paired image. In so doing, the nested VAE isolates the common latent factors/causes and becomes invariant to unwanted factors that are not shared between paired images. We also propose a new metric to provide a balanced method of evaluating consistency and classifier performance across domains which we refer to as the Adjusted Parity metric. An evaluation of NestedVAE on both domain and attribute invariance, change detection, and learning common factors for the prediction of biological sex demonstrates that NestedVAE significantly outperforms alternative methods.},
  file = {/home/trung/GoogleDrive/Zotero/vowels et al_2020_nestedvae.pdf},
  isbn = {978-1-72817-168-5},
  keywords = {disentanglement},
  language = {en}
}

@inproceedings{vuddagiri19_MultiHeadSelfAttentionNetworks,
  title = {Multi-{{Head Self}}-{{Attention Networks}} for {{Language Identification}}},
  booktitle = {2019 {{Twelfth International Conference}} on {{Contemporary Computing}} ({{IC3}})},
  author = {Vuddagiri, Ravi Kumar and Mandava, Tirusha and Vydana, Hari Krishna and Vuppala, Anil Kumar},
  year = {2019},
  month = aug,
  pages = {1--5},
  publisher = {{IEEE}},
  address = {{Noida, India}},
  doi = {10.1109/IC3.2019.8844925},
  abstract = {Self-attention networks are being popularly employed in sequence classification and sequence summarization tasks. State-of-the-art models use sequential models to capture the high-level information, but these models are sensitive to length of utterance and do not equally generalize over variable length utterances. This work explores to study the efficiency of recent advancements in self-attentive networks for improving the performance of the LID system. In self-attentive network, variable length input sequence is converted to fixed dimensional vector which represents the whole sequence. The weighted mean of input sequence is considered as utterance level representation. Along with the mean, a standard deviation is employed to represent the whole input sequence. Experiments are performed using AP17-OLR database. Use of mean with standard deviation has reduced the equal error rate (EER) with an 8\% relative improvement. A multi-head attention mechanism is introduced in self-attention networks with an assumption that each head captures the distinct information to discriminate languages. Use of multi-head self-attention has further reduced the EER with a 13\% relative improvement. Best performance is achieved with multi-head self-attention network with residual connections. Shifted delta cepstral features (SDC) and stacked SDC features are used for developing LID systems.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/vuddagiri et al_2019_multi-head self-attention networks for language identification.pdf},
  isbn = {978-1-72813-591-5},
  keywords = {attention},
  language = {en}
}

@article{w.li18_SupervisedTopicModeling,
  title = {Supervised {{Topic Modeling Using Hierarchical Dirichlet Process}}-{{Based Inverse Regression}}: {{Experiments}} on {{E}}-{{Commerce Applications}}},
  author = {{W. Li} and {J. Yin} and {H. Chen}},
  year = {2018},
  month = jun,
  volume = {30},
  pages = {1192--1205},
  issn = {1558-2191},
  file = {/home/trung/GoogleDrive/Zotero/w. li et al_2018_supervised topic modeling using hierarchical dirichlet process-based inverse regression.pdf},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  number = {6}
}

@article{wadia20_Whiteningsecondorder,
  title = {Whitening and Second Order Optimization Both Destroy Information about the Dataset, and Can Make Generalization Impossible},
  author = {Wadia, Neha S. and Duckworth, Daniel and Schoenholz, Samuel S. and Dyer, Ethan and {Sohl-Dickstein}, Jascha},
  year = {2020},
  month = aug,
  abstract = {Machine learning is predicated on the concept of generalization: a model achieving low error on a sufficiently large training set should also perform well on novel samples from the same distribution. We show that both data whitening and second order optimization can harm or entirely prevent generalization. In general, model training harnesses information contained in the sample-sample second moment matrix of a dataset. We prove that for models with a fully connected first layer, the information contained in this matrix is the only information which can be used to generalize. Models trained using whitened data, or with certain second order optimization schemes, have less access to this information; in the high dimensional regime they have no access at all, producing models that generalize poorly or not at all. We experimentally verify these predictions for several architectures, and further demonstrate that generalization continues to be harmed even when theoretical requirements are relaxed. However, we also show experimentally that regularized second order optimization can provide a practical tradeoff, where training is still accelerated but less information is lost, and generalization can in some circumstances even improve.},
  archivePrefix = {arXiv},
  eprint = {2008.07545},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wadia et al_2020_whitening and second order optimization both destroy information about the dataset, and can make generalization impossible.pdf},
  journal = {arXiv:2008.07545 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wainwright07_GraphicalModelsExponential,
  title = {Graphical {{Models}}, {{Exponential Families}}, and {{Variational Inference}}},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2007},
  volume = {1},
  pages = {1--305},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000001},
  file = {/home/trung/GoogleDrive/Zotero/wainwright et al_2007_graphical models, exponential families, and variational inference.pdf},
  journal = {Foundations and Trends\textregistered{} in Machine Learning},
  language = {en},
  number = {1\textendash 2}
}

@article{wan20_OldPhotoRestoration,
  title = {Old {{Photo Restoration}} via {{Deep Latent Space Translation}}},
  author = {Wan, Ziyu and Zhang, Bo and Chen, Dongdong and Zhang, Pan and Chen, Dong and Liao, Jing and Wen, Fang},
  year = {2020},
  month = sep,
  abstract = {We propose to restore old photos that suffer from severe degradation through a deep learning approach. Unlike conventional restoration tasks that can be solved through supervised learning, the degradation in real photos is complex and the domain gap between synthetic images and real old photos makes the network fail to generalize. Therefore, we propose a novel triplet domain translation network by leveraging real photos along with massive synthetic image pairs. Specifically, we train two variational autoencoders (VAEs) to respectively transform old photos and clean photos into two latent spaces. And the translation between these two latent spaces is learned with synthetic paired data. This translation generalizes well to real photos because the domain gap is closed in the compact latent space. Besides, to address multiple degradations mixed in one old photo, we design a global branch with apartial nonlocal block targeting to the structured defects, such as scratches and dust spots, and a local branch targeting to the unstructured defects, such as noises and blurriness. Two branches are fused in the latent space, leading to improved capability to restore old photos from multiple defects. Furthermore, we apply another face refinement network to recover fine details of faces in the old photos, thus ultimately generating photos with enhanced perceptual quality. With comprehensive experiments, the proposed pipeline demonstrates superior performance over state-of-the-art methods as well as existing commercial tools in terms of visual quality for old photos restoration.},
  archivePrefix = {arXiv},
  eprint = {2009.07047},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wan et al_2020_old photo restoration via deep latent space translation.pdf},
  journal = {arXiv:2009.07047 [cs]},
  primaryClass = {cs}
}

@article{wang00_ContinuousTimeDynamic,
  title = {Continuous {{Time Dynamic Topic Models}}},
  author = {Wang, Chong and Blei, David and Heckerman, David},
  pages = {9},
  abstract = {In this paper, we develop the continuous time dynamic topic model (cDTM). The cDTM is a dynamic topic model that uses Brownian motion to model the latent topics through a sequential collection of documents, where a ``topic'' is a pattern of word use that we expect to evolve over the course of the collection. We derive an efficient variational approximate inference algorithm that takes advantage of the sparsity of observations in text, a property that lets us easily handle many time points. In contrast to the cDTM, the original discrete-time dynamic topic model (dDTM) requires that time be discretized. Moreover, the complexity of variational inference for the dDTM grows quickly as time granularity increases, a drawback which limits fine-grained discretization. We demonstrate the cDTM on two news corpora, reporting both predictive perplexity and the novel task of time stamp prediction.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_continuous time dynamic topic models.pdf},
  language = {en}
}

@article{wang00_GreenGenerativeModeling,
  title = {Green {{Generative Modeling}}: {{Recycling Dirty Data}} Using {{Recurrent Variational Autoencoders}}},
  author = {Wang, Yu and Dai, Bin and Hua, Gang and Aston, John and Wipf, David},
  pages = {10},
  abstract = {This paper explores two useful modifications of the recent variational autoencoder (VAE), a popular deep generative modeling framework that dresses traditional autoencoders with probabilistic attire. The first involves a specially-tailored form of conditioning that allows us to simplify the VAE decoder structure while simultaneously introducing robustness to outliers. In a related vein, a second, complementary alteration is proposed to further build invariance to contaminated or dirty samples via a data augmentation process that amounts to recycling. In brief, to the extent that the VAE is legitimately a representative generative model, then each output from the decoder should closely resemble an authentic sample, which can then be resubmitted as a novel input ad infinitum. Moreover, this can be accomplished via special recurrent connections without the need for additional parameters to be trained. We evaluate these proposals on multiple practical outlier-removal and generative modeling tasks, demonstrating considerable improvements over existing algorithms.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_green generative modeling.pdf},
  language = {en}
}

@inproceedings{wang08_GenerativeProbabilisticModel,
  title = {A {{Generative Probabilistic Model}} for {{Multi}}-Label {{Classification}}},
  booktitle = {2008 {{Eighth IEEE International Conference}} on {{Data Mining}}},
  author = {Wang, Hongning and Huang, Minlie and Zhu, Xiaoyan},
  year = {2008},
  month = dec,
  pages = {628--637},
  publisher = {{IEEE}},
  address = {{Pisa, Italy}},
  doi = {10.1109/ICDM.2008.86},
  abstract = {Traditional discriminative classification method makes little attempt to reveal the probabilistic structure and the correlation within both input and output spaces. In the scenario of multi-label classification, most of the classifiers simply assume the predefined classes are independently distributed, which would definitely hinder the classification performance when there are intrinsic correlations between the classes. In this article, we propose a generative probabilistic model, the Correlated Labeling Model (CoL Model), to formulate the correlation between different classes. The CoL model is presented to capture the correlation between classes and the underlying structures via the latent random variables in a supervised manner. We develop a variational procedure to approximate the posterior distribution and employ the EM algorithm for the empirical Bayes parameter estimation. In our evaluations, the proposed model achieved promising results on various data sets.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2008_a generative probabilistic model for multi-label classification.pdf},
  isbn = {978-0-7695-3502-9},
  language = {en}
}

@inproceedings{wang17_FastAssociativeAttentive,
  title = {Fast {{Associative Attentive Memory Network}}},
  booktitle = {Proceedings of the 2017 2nd {{International Conference}} on {{Automatic Control}} and {{Information Engineering}} ({{ICACIE}} 2017)},
  author = {Wang, Xiaomin and Cheng, Samuel},
  year = {2017},
  publisher = {{Atlantis Press}},
  address = {{Hong Kong}},
  doi = {10.2991/icacie-17.2017.37},
  abstract = {To solve the Cloze-style reading comprehension task, a challenging task to test the understanding and reasoning abilities of model, we propose a general and novel model called Fast Associative with Attention Memory Network in this paper. Unlike regular language model, we use fast weights to store associative memory for the recent past instead of activity hidden units which pay attention to the recent past. Our preliminary experiments indicate that our model outperforms regular RNN and LSTM.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2017_fast associative attentive memory network.pdf},
  isbn = {978-94-6252-398-2},
  keywords = {attention,memory},
  language = {en}
}

@article{wang17_TacotronEndtoEndSpeech,
  title = {Tacotron: {{Towards End}}-to-{{End Speech Synthesis}}},
  shorttitle = {Tacotron},
  author = {Wang, Yuxuan and {Skerry-Ryan}, R. J. and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J. and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and Le, Quoc and Agiomyrgiannakis, Yannis and Clark, Rob and Saurous, Rif A.},
  year = {2017},
  month = apr,
  abstract = {A text-to-speech synthesis system typically consists of multiple stages, such as a text analysis frontend, an acoustic model and an audio synthesis module. Building these components often requires extensive domain expertise and may contain brittle design choices. In this paper, we present Tacotron, an end-to-end generative text-to-speech model that synthesizes speech directly from characters. Given {$<$}text, audio{$>$} pairs, the model can be trained completely from scratch with random initialization. We present several key techniques to make the sequence-to-sequence framework perform well for this challenging task. Tacotron achieves a 3.82 subjective 5-scale mean opinion score on US English, outperforming a production parametric system in terms of naturalness. In addition, since Tacotron generates speech at the frame level, it's substantially faster than sample-level autoregressive methods.},
  archivePrefix = {arXiv},
  eprint = {1703.10135},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2017_tacotron.pdf},
  journal = {arXiv:1703.10135 [cs]},
  primaryClass = {cs}
}

@article{wang17_Visualizationanalysissinglecell,
  title = {Visualization and Analysis of Single-Cell {{RNA}}-Seq Data by Kernel-Based Similarity Learning},
  author = {Wang, Bo and Zhu, Junjie and Pierson, Emma and Ramazzotti, Daniele and Batzoglou, Serafim},
  year = {2017},
  month = feb,
  doi = {10.1101/052225},
  abstract = {Single-cell RNA-seq technologies enable high throughput gene expression measurement of individual cells, and allow the discovery of heterogeneity within cell populations. Measurement of cell-to-cell gene expression similarity is critical to identification, visualization and analysis of cell populations. However, single-cell data introduce challenges to conventional measures of gene expression similarity because of the high level of noise, outliers and dropouts. Here, we propose a novel similarity-learning framework, SIMLR (single-cell interpretation via multikernel learning), which learns an appropriate distance metric from the data for dimension reduction, clustering and visualization applications. Benchmarking against state-of-the-art methods for these applications, we used SIMLR to re-analyse seven representative single-cell data sets, including high-throughput droplet-based data sets with tens of thousands of cells. We show that SIMLR greatly improves clustering sensitivity and accuracy, as well as the visualization and interpretability of the data.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2017_visualization and analysis of single-cell rna-seq data by kernel-based similarity learning.pdf},
  journal = {bioRxiv},
  language = {en}
}

@article{wang18_AttentionMechanismSpeaker,
  title = {Attention {{Mechanism}} in {{Speaker Recognition}}: {{What Does It Learn}} in {{Deep Speaker Embedding}}?},
  shorttitle = {Attention {{Mechanism}} in {{Speaker Recognition}}},
  author = {Wang, Qiongqiong and Okabe, Koji and Lee, Kong Aik and Yamamoto, Hitoshi and Koshinaka, Takafumi},
  year = {2018},
  month = sep,
  abstract = {This paper presents an experimental study on deep speaker embedding with an attention mechanism that has been found to be a powerful representation learning technique in speaker recognition. In this framework, an attention model works as a frame selector that computes an attention weight for each frame-level feature vector, in accord with which an utterancelevel representation is produced at the pooling layer in a speaker embedding network. In general, an attention model is trained together with the speaker embedding network on a single objective function, and thus those two components are tightly bound to one another. In this paper, we consider the possibility that the attention model might be decoupled from its parent network and assist other speaker embedding networks and even conventional i-vector extractors. This possibility is demonstrated through a series of experiments on a NIST Speaker Recognition Evaluation (SRE) task, with 9.0\% EER reduction and 3.8\% min\_Cprimary reduction when the attention weights are applied to i-vector extraction. Another experiment shows that DNN-based soft voice activity detection (VAD) can be effectively combined with the attention mechanism to yield further reduction of minCprimary by 6.6\% and 1.6\% in deep speaker embedding and i-vector systems, respectively.},
  archivePrefix = {arXiv},
  eprint = {1809.09311},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2018_attention mechanism in speaker recognition.pdf;/home/trung/Zotero/storage/R7RQ8M6H/1809.html},
  journal = {arXiv:1809.09311 [cs, eess]},
  keywords = {attention,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{wang18_Conditionalgenerativeadversarial,
  title = {Conditional Generative Adversarial Network for Gene Expression Inference},
  author = {Wang, Xiaoqian and Ghasedi Dizaji, Kamran and Huang, Heng},
  year = {2018},
  month = sep,
  volume = {34},
  pages = {i603-i611},
  issn = {1367-4803, 1460-2059},
  doi = {10.1093/bioinformatics/bty563},
  abstract = {Motivation: The rapid progress of gene expression profiling has facilitated the prosperity of recent biological studies in various fields, where gene expression data characterizes various cell conditions and regulatory mechanisms under different experimental circumstances. Despite the widespread application of gene expression profiling and advances in high-throughput technologies, profiling in genome-wide level is still expensive and difficult. Previous studies found that high correlation exists in the expression pattern of different genes, such that a small subset of genes can be informative to approximately describe the entire transcriptome. In the Library of Integrated Network-based Cell-Signature program, a set of \$1000 landmark genes have been identified that contain \$80\% information of the whole genome and can be used to predict the expression of remaining genes. For a cost-effective profiling strategy, traditional methods measure the profiles of landmark genes and then infer the expression of other target genes via linear models. However, linear models do not have the capacity to capture the non-linear associations in gene regulatory networks.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2018_conditional generative adversarial network for gene expression inference.pdf},
  journal = {Bioinformatics},
  language = {en},
  number = {17}
}

@article{wang18_GLUEMultiTaskBenchmark,
  title = {{{GLUE}}: {{A Multi}}-{{Task Benchmark}} and {{Analysis Platform}} for {{Natural Language Understanding}}},
  shorttitle = {{{GLUE}}},
  author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  year = {2018},
  month = apr,
  abstract = {For natural language understanding (NLU) technology to be maximally useful, both practically and as a scientific object of study, it must be general: it must be able to process language in a way that is not exclusively tailored to any one specific task or dataset. In pursuit of this objective, we introduce the General Language Understanding Evaluation benchmark (GLUE), a tool for evaluating and analyzing the performance of models across a diverse range of existing NLU tasks. GLUE is model-agnostic, but it incentivizes sharing knowledge across tasks because certain tasks have very limited training data. We further provide a hand-crafted diagnostic test suite that enables detailed linguistic analysis of NLU models. We evaluate baselines based on current methods for multi-task and transfer learning and find that they do not immediately give substantial improvements over the aggregate performance of training a separate model per task, indicating room for improvement in developing general and robust NLU systems.},
  archivePrefix = {arXiv},
  eprint = {1804.07461},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2018_glue.pdf;/home/trung/Zotero/storage/HGQ279R6/1804.html},
  journal = {arXiv:1804.07461 [cs]},
  keywords = {Computer Science - Computation and Language,language comprehension evaluation},
  primaryClass = {cs}
}

@article{wang18_IdentifyingGeneralizationProperties,
  title = {Identifying {{Generalization Properties}} in {{Neural Networks}}},
  author = {Wang, Huan and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  year = {2018},
  month = sep,
  abstract = {While it has not yet been proven, empirical evidence suggests that model generalization is related to local properties of the optima which can be described via the Hessian. We connect model generalization with the local property of a solution under the PAC-Bayes paradigm. In particular, we prove that model generalization ability is related to the Hessian, the higher-order "smoothness" terms characterized by the Lipschitz constant of the Hessian, and the scales of the parameters. Guided by the proof, we propose a metric to score the generalization capability of the model, as well as an algorithm that optimizes the perturbed model accordingly.},
  annotation = {ZSCC: 0000014},
  archivePrefix = {arXiv},
  eprint = {1809.07402},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2018_identifying generalization properties in neural networks.pdf;/home/trung/Zotero/storage/794YSLFC/1809.html},
  journal = {arXiv:1809.07402 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{wang18_SelfAttentiveHierarchicalModel,
  title = {A {{Self}}-{{Attentive Hierarchical Model}} for {{Jointly Improving Text Summarization}} and {{Sentiment Classification}}},
  booktitle = {{{ACML}}},
  author = {Wang, Hongli and Ren, Jiangtao},
  year = {2018},
  abstract = {Text summarization and sentiment classification, in NLP, are two main tasks implemented on text analysis, focusing on extracting the major idea of a text at different levels. Based on the characteristics of both, sentiment classification can be regarded as a more abstractive summarization task. According to the scheme, a Self-Attentive Hierarchical model for jointly improving text Summarization and Sentiment Classification (SAHSSC) is proposed in this paper. This model jointly performs abstractive text summarization and sentiment classification within a hierarchical end-to-end neural framework, in which the sentiment classification layer on top of the summarization layer predicts the sentiment label in the light of the text and the generated summary. Furthermore, a self-attention layer is also proposed in the hierarchical framework, which is the bridge that connects the summarization layer and the sentiment classification layer and aims at capturing emotional information at text-level as well as summary-level. The proposed model can generate a more relevant summary and lead to a more accurate summary-aware sentiment prediction. Experimental results evaluated on SNAP amazon online review datasets show that our model outperforms the state-of-the-art baselines on both abstractive text summarization and sentiment classification by a considerable margin.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2018_a self-attentive hierarchical model for jointly improving text summarization and sentiment classification.pdf},
  keywords = {Amazona,Artificial neural network,attention,Automatic summarization,End-to-end principle,Experiment,Hierarchical database model,Numerical weather prediction,Statistical classification}
}

@article{wang19_AdaptiveEnsembleClassifiers,
  title = {Adaptive {{Ensemble}} of {{Classifiers}} with {{Regularization}} for {{Imbalanced Data Classification}}},
  author = {Wang, Chen and Yu, Qin and Luo, Ruisen and Hui, Dafeng and Zhou, Kai and Yu, Yanmei and Sun, Chao and Gong, Xiaofeng},
  year = {2019},
  month = aug,
  abstract = {Dynamic ensembling of classifiers is an effective approach in processing label-imbalanced classifications. However, in dynamic ensemble methods, the combination of classifiers is usually determined by the local competence and conventional regularization methods are difficult to apply, leaving the technique prone to overfitting. In this paper, focusing on the binary label-imbalanced classification field, a novel method of Adaptive Ensemble of classifiers with Regularization (AER) has been proposed. The method deals with the overfitting problem from a perspective of implicit regularization. Specifically, it leverages the properties of Stochastic Gradient Descent (SGD) to obtain the solution with the minimum norm to achieve regularization, and interpolates ensemble weights via the global geometry of data to further prevent overfitting. The method enjoys a favorable time and memory complexity, and theoretical proofs show that algorithms implemented with AER paradigm have time and memory complexities upper-bounded by their original implementations. Furthermore, the proposed AER method is tested with a specific implementation based on Gradient Boosting Machine (XGBoost) on the three datasets: UCI Bioassay, KEEL Abalone19, and a set of GMM-sampled artificial dataset. Results show that the proposed AER algorithm can outperform the major existing algorithms based on multiple metrics, and Mcnemar's tests are applied to validate performance superiorities. To summarize, this work complements regularization for dynamic ensemble methods and develops an algorithm superior in grasping both the global and local geometry of data to alleviate overfitting in imbalanced data classification.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1908.03595},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_adaptive ensemble of classifiers with regularization for imbalanced data classification.pdf;/home/trung/Zotero/storage/C9AVN4QV/1908.html},
  journal = {arXiv:1908.03595 [cs, stat]},
  keywords = {adaptive,Computer Science - Machine Learning,ensemble,imbalance,regularization,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{wang19_BlessingsMultipleCauses,
  title = {The {{Blessings}} of {{Multiple Causes}}},
  author = {Wang, Yixin and Blei, David M.},
  year = {2019},
  month = apr,
  abstract = {Causal inference from observational data often assumes ``ignorability,'' that all confounders are observed. This assumption is standard yet untestable. However, many scientific studies involve multiple causes, different variables whose effects are simultaneously of interest. We propose the deconfounder, an algorithm that combines unsupervised machine learning and predictive model checking to perform causal inference in multiple-cause settings. The deconfounder infers a latent variable as a substitute for unobserved confounders and then uses that substitute to perform causal inference. We develop theory for the deconfounder, and show that it requires weaker assumptions than classical causal inference. We analyze its performance in three types of studies: semi-simulated data around smoking and lung cancer, semi-simulated data around genome-wide association studies, and a real dataset about actors and movie revenue. The deconfounder provides a checkable approach to estimating closer-to-truth causal effects.},
  archivePrefix = {arXiv},
  eprint = {1805.06826},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_the blessings of multiple causes.pdf},
  journal = {arXiv:1805.06826 [cs, stat]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wang19_Bulktissuecell,
  title = {Bulk Tissue Cell Type Deconvolution with Multi-Subject Single-Cell Expression Reference},
  author = {Wang, Xuran and Park, Jihwan and Susztak, Katalin and Zhang, Nancy R. and Li, Mingyao},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {380},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-08023-x},
  annotation = {ZSCC: 0000052},
  file = {/home/trung/Zotero/storage/G6WJ6Q5Q/Wang et al. - 2019 - Bulk tissue cell type deconvolution with multi-sub.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{wang19_CharacterizingDeepLearning,
  title = {Characterizing {{Deep Learning Training Workloads}} on {{Alibaba}}-{{PAI}}},
  author = {Wang, Mengdi and Meng, Chen and Long, Guoping and Wu, Chuan and Yang, Jun and Lin, Wei and Jia, Yangqing},
  year = {2019},
  month = oct,
  abstract = {Modern deep learning models have been exploited in various domains, including computer vision (CV), natural language processing (NLP), search and recommendation. In practical AI clusters, workloads training these models are run using software frameworks such as TensorFlow, Caffe, PyTorch and CNTK. One critical issue for efficiently operating practical AI clouds, is to characterize the computing and data transfer demands of these workloads, and more importantly, the training performance given the underlying software framework and hardware configurations. In this paper, we characterize deep learning training workloads from Platform of Artificial Intelligence (PAI) in Alibaba. We establish an analytical framework to investigate detailed execution time breakdown of various workloads using different training architectures, to identify performance bottleneck. Results show that weight/gradient communication during training takes almost 62\% of the total execution time among all our workloads on average. The computation part, involving both GPU computing and memory access, are not the biggest bottleneck based on collective behavior of the workloads. We further evaluate attainable performance of the workloads on various potential software/hardware mappings, and explore implications on software architecture selection and hardware configurations. We identify that 60\% of PS/Worker workloads can be potentially sped up when ported to the AllReduce architecture exploiting the high-speed NVLink for GPU interconnect, and on average 1.7X speedup can be achieved when Ethernet bandwidth is upgraded from 25 Gbps to 100 Gbps.},
  archivePrefix = {arXiv},
  eprint = {1910.05930},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_characterizing deep learning training workloads on alibaba-pai.pdf;/home/trung/Zotero/storage/FDEPYRKE/1910.html},
  journal = {arXiv:1910.05930 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Performance},
  primaryClass = {cs}
}

@inproceedings{wang19_CLVSAConvolutionalLSTM,
  title = {{{CLVSA}}: {{A Convolutional LSTM Based Variational Sequence}}-to-{{Sequence Model}} with {{Attention}} for {{Predicting Trends}} of {{Financial Markets}}},
  shorttitle = {{{CLVSA}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Wang, Jia and Sun, Tong and Liu, Benyuan and Cao, Yu and Zhu, Hongwei},
  year = {2019},
  month = aug,
  pages = {3705--3711},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/514},
  abstract = {Financial markets are a complex dynamical system. The complexity comes from the interaction between a market and its participants, in other words, the integrated outcome of activities of the entire participants determines the markets trend, while the markets trend affects activities of participants. These interwoven interactions make financial markets keep evolving. Inspired by stochastic recurrent models that successfully capture variability observed in natural sequential data such as speech and video, we propose CLVSA, a hybrid model that consists of stochastic recurrent networks, the sequence-to-sequence architecture, the self- and inter-attention mechanism, and convolutional LSTM units to capture variationally underlying features in raw financial trading data. Our model outperforms basic models, such as convolutional neural network, vanilla LSTM network, and sequence-to-sequence model with attention, based on backtesting results of six futures from January 2010 to December 2017. Our experimental results show that, by introducing an approximate posterior, CLVSA takes advantage of an extra regularizer based on the Kullback-Leibler divergence to prevent itself from overfitting traps.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_clvsa.pdf},
  isbn = {978-0-9992411-4-1},
  keywords = {attention,variational},
  language = {en}
}

@article{wang19_Comparativeanalysisdifferential,
  title = {Comparative Analysis of Differential Gene Expression Analysis Tools for Single-Cell {{RNA}} Sequencing Data},
  author = {Wang, Tianyu and Li, Boyang and Nelson, Craig E. and Nabavi, Sheida},
  year = {2019},
  month = dec,
  volume = {20},
  pages = {40},
  issn = {1471-2105},
  doi = {10.1186/s12859-019-2599-6},
  abstract = {Background: The analysis of single-cell RNA sequencing (scRNAseq) data plays an important role in understanding the intrinsic and extrinsic cellular processes in biological and biomedical research. One significant effort in this area is the detection of differentially expressed (DE) genes. scRNAseq data, however, are highly heterogeneous and have a large number of zero counts, which introduces challenges in detecting DE genes. Addressing these challenges requires employing new approaches beyond the conventional ones, which are based on a nonzero difference in average expression. Several methods have been developed for differential gene expression analysis of scRNAseq data. To provide guidance on choosing an appropriate tool or developing a new one, it is necessary to evaluate and compare the performance of differential gene expression analysis methods for scRNAseq data. Results: In this study, we conducted a comprehensive evaluation of the performance of eleven differential gene expression analysis software tools, which are designed for scRNAseq data or can be applied to them. We used simulated and real data to evaluate the accuracy and precision of detection. Using simulated data, we investigated the effect of sample size on the detection accuracy of the tools. Using real data, we examined the agreement among the tools in identifying DE genes, the run time of the tools, and the biological relevance of the detected DE genes. Conclusions: In general, agreement among the tools in calling DE genes is not high. There is a trade-off between true-positive rates and the precision of calling DE genes. Methods with higher true positive rates tend to show low precision due to their introducing false positives, whereas methods with high precision show low true positive rates due to identifying few DE genes. We observed that current methods designed for scRNAseq data do not tend to show better performance compared to methods designed for bulk RNAseq data. Data multimodality and abundance of zero read counts are the main characteristics of scRNAseq data, which play important roles in the performance of differential gene expression analysis methods and need to be considered in terms of the development of new methods.},
  annotation = {ZSCC: 0000023},
  file = {/home/trung/Zotero/storage/YPXV3TIU/Wang et al. - 2019 - Comparative analysis of differential gene expressi.pdf},
  journal = {BMC Bioinformatics},
  language = {en},
  number = {1}
}

@article{wang19_DeconfoundedRecommenderCausal,
  title = {The {{Deconfounded Recommender}}: {{A Causal Inference Approach}} to {{Recommendation}}},
  shorttitle = {The {{Deconfounded Recommender}}},
  author = {Wang, Yixin and Liang, Dawen and Charlin, Laurent and Blei, David M.},
  year = {2019},
  month = may,
  abstract = {The goal of recommendation is to show users items that they will like. Though usually framed as a prediction, the spirit of recommendation is to answer an interventional question---for each user and movie, what would the rating be if we "forced" the user to watch the movie? To this end, we develop a causal approach to recommendation, one where watching a movie is a "treatment" and a user's rating is an "outcome." The problem is there may be unobserved confounders, variables that affect both which movies the users watch and how they rate them; unobserved confounders impede causal predictions with observational data. To solve this problem, we develop the deconfounded recommender, a way to use classical recommendation models for causal recommendation. Following Wang \& Blei [23], the deconfounded recommender involves two probabilistic models. The first models which movies the users watch; it provides a substitute for the unobserved confounders. The second one models how each user rates each movie; it employs the substitute to help account for confounders. This two-stage approach removes bias due to confounding. It improves recommendation and enjoys stable performance against interventions on test sets.},
  archivePrefix = {arXiv},
  eprint = {1808.06581},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_the deconfounded recommender.pdf},
  journal = {arXiv:1808.06581 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{wang19_FUNCTIONSPACEPARTICLE,
  title = {{{FUNCTION SPACE PARTICLE OPTIMIZATION FOR BAYESIAN NEURAL NETWORKS}}},
  author = {Wang, Ziyu and Ren, Tongzheng and Zhu, Jun and Zhang, Bo},
  year = {2019},
  pages = {25},
  abstract = {While Bayesian neural networks (BNNs) have drawn increasing attention, their posterior inference remains challenging, due to the high-dimensional and overparameterized nature. Recently, several highly flexible and scalable variational inference procedures based on the idea of particle optimization have been proposed. These methods directly optimize a set of particles to approximate the target posterior. However, their application to BNNs often yields sub-optimal performance, as they have a particular failure mode on over-parameterized models. In this paper, we propose to solve this issue by performing particle optimization directly in the space of regression functions. We demonstrate through extensive experiments that our method successfully overcomes this issue, and outperforms strong baselines in a variety of tasks including prediction, defense against adversarial examples, and reinforcement learning.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_function space particle optimization for bayesian neural networks.pdf;/home/trung/GoogleDrive/Zotero/wang et al_2019_function space particle optimization for bayesian neural networks2.pdf;/home/trung/GoogleDrive/Zotero/wang et al_2019_function space particle optimization for bayesian neural networks3.pdf;/home/trung/Zotero/storage/U5H7IBWF/1902.html},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  language = {en}
}

@article{wang19_LearningPriorsAdversarial,
  title = {Learning {{Priors}} for {{Adversarial Autoencoders}}},
  author = {Wang, Hui-Po and Peng, Wen-Hsiao and Ko, Wei-Jan},
  year = {2019},
  month = sep,
  abstract = {Most deep latent factor models choose simple priors for simplicity, tractability or not knowing what prior to use. Recent studies show that the choice of the prior may have a profound effect on the expressiveness of the model,especially when its generative network has limited capacity. In this paper, we propose to learn a proper prior from data for adversarial autoencoders(AAEs). We introduce the notion of code generators to transform manually selected simple priors into ones that can better characterize the data distribution. Experimental results show that the proposed model can generate better image quality and learn better disentangled representations than AAEs in both supervised and unsupervised settings. Lastly, we present its ability to do cross-domain translation in a text-to-image synthesis task.},
  archivePrefix = {arXiv},
  eprint = {1909.04443},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_learning priors for adversarial autoencoders.pdf},
  journal = {arXiv:1909.04443 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wang19_MetaReasoningKnowledge,
  title = {Meta {{Reasoning}} over {{Knowledge Graphs}}},
  author = {Wang, Hong and Xiong, Wenhan and Yu, Mo and Guo, Xiaoxiao and Chang, Shiyu and Wang, William Yang},
  year = {2019},
  month = aug,
  abstract = {The ability to reason over learned knowledge is an innate ability for humans and humans can easily master new reasoning rules with only a few demonstrations. While most existing studies on knowledge graph (KG) reasoning assume enough training examples, we study the challenging and practical problem of few-shot knowledge graph reasoning under the paradigm of meta-learning. We propose a new meta learning framework that effectively utilizes the task-specific meta information such as local graph neighbors and reasoning paths in KGs. Specifically, we design a meta-encoder that encodes the meta information into task-specific initialization parameters for different tasks. This allows our reasoning module to have diverse starting points when learning to reason over different relations, which is expected to better fit the target task. On two few-shot knowledge base completion benchmarks, we show that the augmented task-specific meta-encoder yields much better initial point than MAML and outperforms several few-shot learning baselines.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1908.04877},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_meta reasoning over knowledge graphs.pdf},
  journal = {arXiv:1908.04877 [cs]},
  keywords = {attention,Computer Science - Computation and Language,graph},
  language = {en},
  primaryClass = {cs}
}

@article{wang19_MultipleCausesCausal,
  title = {Multiple {{Causes}}: {{A Causal Graphical View}}},
  shorttitle = {Multiple {{Causes}}},
  author = {Wang, Yixin and Blei, David M.},
  year = {2019},
  month = may,
  abstract = {Unobserved confounding is a major hurdle for causal inference from observational data. Confounders---the variables that affect both the causes and the outcome---induce spurious non-causal correlations between the two. Wang \& Blei (2018) lower this hurdle with "the blessings of multiple causes," where the correlation structure of multiple causes provides indirect evidence for unobserved confounding. They leverage these blessings with an algorithm, called the deconfounder, that uses probabilistic factor models to correct for the confounders. In this paper, we take a causal graphical view of the deconfounder. In a graph that encodes shared confounding, we show how the multiplicity of causes can help identify intervention distributions. We then justify the deconfounder, showing that it makes valid inferences of the intervention. Finally, we expand the class of graphs, and its theory, to those that include other confounders and selection variables. Our results expand the theory in Wang \& Blei (2018), justify the deconfounder for causal graphs, and extend the settings where it can be used.},
  archivePrefix = {arXiv},
  eprint = {1905.12793},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_multiple causes.pdf;/home/trung/Zotero/storage/WP52I495/1905.html},
  journal = {arXiv:1905.12793 [cs, stat]},
  keywords = {causal,Computer Science - Machine Learning,graph,Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {cs, stat}
}

@article{wang19_NeuralGaussianCopula,
  title = {Neural {{Gaussian Copula}} for {{Variational Autoencoder}}},
  author = {Wang, Prince Zizhuang and Wang, William Yang},
  year = {2019},
  month = sep,
  abstract = {Variational language models seek to estimate the posterior of latent variables with an approximated variational posterior. The model often assumes the variational posterior to be factorized even when the true posterior is not. The learned variational posterior under this assumption does not capture the dependency relationships over latent variables. We argue that this would cause a typical training problem called posterior collapse observed in all other variational language models. We propose Gaussian Copula Variational Autoencoder (VAE) to avert this problem. Copula is widely used to model correlation and dependencies of high-dimensional random variables, and therefore it is helpful to maintain the dependency relationships that are lost in VAE. The empirical results show that by modeling the correlation of latent variables explicitly using a neural parametric copula, we can avert this training difficulty while getting competitive results among all other VAE approaches.},
  archivePrefix = {arXiv},
  eprint = {1909.03569},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_neural gaussian copula for variational autoencoder.pdf},
  journal = {arXiv:1909.03569 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wang19_PaperRobotIncrementalDraft,
  title = {{{PaperRobot}}: {{Incremental Draft Generation}} of {{Scientific Ideas}}},
  shorttitle = {{{PaperRobot}}},
  author = {Wang, Qingyun and Huang, Lifu and Jiang, Zhiying and Knight, Kevin and Ji, Heng and Bansal, Mohit and Luan, Yi},
  year = {2019},
  month = may,
  abstract = {We present a PaperRobot who performs as an automatic research assistant by (1) conducting deep understanding of a large collection of human-written papers in a target domain and constructing comprehensive background knowledge graphs (KGs); (2) creating new ideas by predicting links from the background KGs, by combining graph attention and contextual text attention; (3) incrementally writing some key elements of a new paper based on memory-attention networks: from the input title along with predicted related entities to generate a paper abstract, from the abstract to generate conclusion and future work, and finally from future work to generate a title for a follow-on paper. Turing Tests, where a biomedical domain expert is asked to compare a system output and a human-authored string, show PaperRobot generated abstracts, conclusion and future work sections, and new titles are chosen over human-written ones up to 30\%, 24\% and 12\% of the time, respectively.},
  archivePrefix = {arXiv},
  eprint = {1905.07870},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_paperrobot.pdf;/home/trung/Zotero/storage/5BPL5N4M/1905.html},
  journal = {arXiv:1905.07870 [cs]},
  primaryClass = {cs}
}

@article{wang19_SATNetBridgingdeep,
  title = {{{SATNet}}: {{Bridging}} Deep Learning and Logical Reasoning Using a Differentiable Satisfiability Solver},
  shorttitle = {{{SATNet}}},
  author = {Wang, Po-Wei and Donti, Priya L. and Wilder, Bryan and Kolter, Zico},
  year = {2019},
  month = may,
  abstract = {Integrating logical reasoning within deep learning architectures has been a major goal of modern AI systems. In this paper, we propose a new direction toward this goal by introducing a differentiable (smoothed) maximum satisfiability (MAXSAT) solver that can be integrated into the loop of larger deep learning systems. Our (approximate) solver is based upon a fast coordinate descent approach to solving the semidefinite program (SDP) associated with the MAXSAT problem. We show how to analytically differentiate through the solution to this SDP and efficiently solve the associated backward pass. We demonstrate that by integrating this solver into end-to-end learning systems, we can learn the logical structure of challenging problems in a minimally supervised fashion. In particular, we show that we can learn the parity function using single-bit supervision (a traditionally hard task for deep networks) and learn how to play 9x9 Sudoku solely from examples. We also solve a "visual Sudok" problem that maps images of Sudoku puzzles to their associated logical solutions by combining our MAXSAT solver with a traditional convolutional architecture. Our approach thus shows promise in integrating logical structures within deep learning.},
  archivePrefix = {arXiv},
  eprint = {1905.12149},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_satnet.pdf;/home/trung/Zotero/storage/H27B7ZCE/1905.html},
  journal = {arXiv:1905.12149 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{wang19_SemisupervisedLearningContrastive,
  title = {Semi-Supervised {{Learning}} with {{Contrastive Predicative Coding}}},
  author = {Wang, Jiaxing and Zheng, Yin and Chen, Xiaoshuang and Huang, Junzhou and Cheng, Jian},
  year = {2019},
  month = may,
  abstract = {Semi-supervised learning (SSL) provides a powerful framework for leveraging unlabeled data when labels are limited or expensive to obtain. SSL algorithms based on deep neural networks have recently proven successful on standard benchmark tasks. However, many of them have thus far been either inflexible, inefficient or non-scalable. This paper explores recently developed contrastive predictive coding technique to improve discriminative power of deep learning models when large portion of labels are absent. Two models, cpc-SSL and a class conditional variant (ccpc-SSL) are presented. They effectively exploit the unlabeled data by extracting shared information between different parts of the (high-dimensional) data. The proposed approaches are inductive, and scale well to very large datasets like ImageNet, making them good candidates in real world large scale applications.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1905.10514},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/LGXELSW4/Wang et al. - 2019 - Semi-supervised Learning with Contrastive Predicat.pdf},
  journal = {arXiv:1905.10514 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wang19_TransformerbasedAcousticModeling,
  title = {Transformer-Based {{Acoustic Modeling}} for {{Hybrid Speech Recognition}}},
  author = {Wang, Yongqiang and Mohamed, Abdelrahman and Le, Duc and Liu, Chunxi and Xiao, Alex and Mahadeokar, Jay and Huang, Hongzhao and Tjandra, Andros and Zhang, Xiaohui and Zhang, Frank and Fuegen, Christian and Zweig, Geoffrey and Seltzer, Michael L.},
  year = {2019},
  month = oct,
  abstract = {We propose and evaluate transformer-based acoustic models (AMs) for hybrid speech recognition. Several modeling choices are discussed in this work, including various positional embedding methods and an iterated loss to enable training deep transformers. We also present a preliminary study of using limited right context in transformer models, which makes it possible for streaming applications. We demonstrate that on the widely used Librispeech benchmark, our transformer-based AM outperforms the best published hybrid result by 19\% to 26\% relative when the standard n-gram language model (LM) is used. Combined with neural network LM for rescoring, our proposed approach achieves state-of-the-art results on Librispeech. Our findings are also confirmed on a much larger internal dataset.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.09799},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2019_transformer-based acoustic modeling for hybrid speech recognition.pdf;/home/trung/Zotero/storage/SMZD6Q3F/1910.html},
  journal = {arXiv:1910.09799 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Electrical Engineering and Systems Science - Audio and Speech Processing,speech,transformer},
  primaryClass = {cs, eess}
}

@article{wang20_CausalInferenceRecommender,
  title = {Causal {{Inference}} for {{Recommender Systems}}},
  author = {Wang, Yixin and Liang, Dawen and Charlin, Laurent and Blei, David M},
  year = {2020},
  pages = {6},
  abstract = {The task of recommender systems is classically framed as a prediction of users' preferences and users' ratings. However, its spirit is to answer a counterfactual question: ``What would the rating be if we `forced' the user to watch the movie?'' This is a question about an intervention, that is a causal inference question. The key challenge of this causal inference is unobserved confounders, variables that affect both which items the users decide to interact with and how they rate them. To this end, we develop an algorithm that leverages classical recommendation models for causal recommendation. Across simulated and real datasets, we demonstrate that the proposed algorithm is more robust to unobserved confounders and improves recommendation.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_causal inference for recommender systems.pdf},
  keywords = {causal},
  language = {en}
}

@article{wang20_Doescomorbidityincrease,
  title = {Does Comorbidity Increase the Risk of Patients with {{COVID}}-19: Evidence from Meta-Analysis},
  shorttitle = {Does Comorbidity Increase the Risk of Patients with {{COVID}}-19},
  author = {Wang, Bolin and Li, Ruobao and Lu, Zhong and Huang, Yan},
  year = {2020},
  month = apr,
  issn = {1945-4589},
  doi = {10.18632/aging.103000},
  abstract = {Currently, the number of patients with coronavirus disease 2019 (COVID-19) has increased rapidly, but relationship between comorbidity and patients with COVID-19 still not clear. The aim was to explore whether the presence of common comorbidities increases COVID-19 patients' risk. A literature search was performed using the electronic platforms (PubMed, Cochrane Library, Embase, and other databases) to obtain relevant research studies published up to March 1, 2020. Relevant data of research endpoints in each study were extracted and merged. All data analysis was performed using Stata12.0 software. A total of 1558 patients with COVID-19 in 6 studies were enrolled in our meta-analysis eventually. Hypertension (OR: 2.29, P{$<$}0.001), diabetes (OR: 2.47, P{$<$}0.001), chronic obstructive pulmonary disease (COPD) (OR: 5.97, P{$<$}0.001), cardiovascular disease (OR: 2.93, P{$<$}0.001), and cerebrovascular disease (OR:3.89,P=0.002) were independent risk factors associated with COVID-19 patients. The meta-analysis revealed no correlation between increased risk of COVID-19 and liver disease, malignancy, or renal disease. Hypertension, diabetes, COPD, cardiovascular disease, and cerebrovascular disease are major risk factors for patients with COVID-19. Knowledge of these risk factors can be a resource for clinicians in the early appropriate medical management of patients with COVID-19.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_does comorbidity increase the risk of patients with covid-19.pdf},
  journal = {Aging},
  language = {en}
}

@inproceedings{wang20_G3ANDisentanglingAppearance,
  title = {{{G3AN}}: {{Disentangling Appearance}} and {{Motion}} for {{Video Generation}}},
  shorttitle = {{{G3AN}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wang, Yaohui and Bilinski, Piotr and Bremond, Francois and Dantcheva, Antitza},
  year = {2020},
  month = jun,
  pages = {5263--5272},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.00531},
  abstract = {Creating realistic human videos entails the challenge of being able to simultaneously generate both appearance, as well as motion. To tackle this challenge, we introduce G3AN, a novel spatio-temporal generative model, which seeks to capture the distribution of high dimensional video data and to model appearance and motion in disentangled manner. The latter is achieved by decomposing appearance and motion in a three-stream Generator, where the main stream aims to model spatio-temporal consistency, whereas the two auxiliary streams augment the main stream with multi-scale appearance and motion features, respectively. An extensive quantitative and qualitative analysis shows that our model systematically and significantly outperforms state-of-the-art methods on the facial expression datasets MUG and UvA-NEMO, as well as the Weizmann and UCF101 datasets on human action. Additional analysis on the learned latent representations confirms the successful decomposition of appearance and motion. Source code and pre-trained models are publicly available 1.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_g3an.pdf},
  isbn = {978-1-72817-168-5},
  language = {en}
}

@article{wang20_RegularizationMattersNonparametric,
  title = {Regularization {{Matters}}: {{A Nonparametric Perspective}} on {{Overparametrized Neural Network}}},
  shorttitle = {Regularization {{Matters}}},
  author = {Wang, Wenjia and Hu, Tianyang and Lin, Cong and Cheng, Guang},
  year = {2020},
  month = jul,
  abstract = {Overparametrized neural networks trained by gradient descent (GD) can provably overfit any training data. However, the generalization guarantee may not hold for noisy data. From a nonparametric perspective, this paper studies how well overparametrized neural networks can recover the true target function in the presence of random noises. We establish a lower bound on the L2 estimation error with respect to the GD iteration, which is away from zero without a delicate choice of early stopping. In turn, through a comprehensive analysis of 2-regularized GD trajectories, we prove that for overparametrized one-hidden-layer ReLU neural network with the 2 regularization: (1) the output is close to that of the kernel ridge regression with the corresponding neural tangent kernel; (2) minimax optimal rate of L2 estimation error is achieved. Numerical experiments confirm our theory and further demonstrate that the 2 regularization approach improves the training robustness and works for a wider range of neural networks.},
  archivePrefix = {arXiv},
  eprint = {2007.02486},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_regularization matters.pdf},
  journal = {arXiv:2007.02486 [cs, stat]},
  keywords = {early stopping},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{wang20_Relaxedmultivariatebernoulli,
  title = {Relaxed Multivariate Bernoulli Distribution and Its Applications to Deep Generative Models},
  author = {Wang, Xi and Yin, Junming},
  editor = {Peters, Jonas and Sontag, David},
  year = {2020},
  month = aug,
  volume = {124},
  pages = {500--509},
  publisher = {{PMLR}},
  address = {{Virtual}},
  abstract = {Recent advances in variational auto-encoder (VAE) have demonstrated the possibility of approximating the intractable posterior distribution with a variational distribution parameterized by a neural network. To optimize the variational objective of VAE, the reparameterization trick is commonly applied to obtain a low-variance estimator of the gradient. The main idea of the trick is to express the variational distribution as a differentiable function of parameters and a random variable with a fixed distribution. To extend the reparameterization trick to inference involving discrete latent variables, a common approach is to use a continuous relaxation of the categorical distribution as the approximate posterior. However, when applying continuous relaxation to the multivariate cases, multiple variables are typically assumed to be independent, making it suboptimal in applications where modeling dependency is crucial to the overall performance. In this work, we propose a multivariate generalization of the Relaxed Bernoulli distribution, which can be reparameterized and can capture the correlation between variables via a Gaussian copula. We demonstrate its effectiveness in two tasks: density estimation with Bernoulli VAE and semi-supervised multi-label classification.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_relaxed multivariate bernoulli distribution and its applications to deep generative models.pdf},
  pdf = {http://proceedings.mlr.press/v124/wang20b/wang20b.pdf},
  series = {Proceedings of Machine Learning Research}
}

@article{wang20_SmartManufacturingIntelligent,
  title = {Smart {{Manufacturing}} and {{Intelligent Manufacturing}}: {{A Comparative Review}}},
  author = {Wang, Baicun and Tao, Fei and Fang, Xudong and Liu, Chao and Liu, Yufei and Freiheit, Theodor},
  year = {2020},
  month = sep,
  issn = {2095-8099},
  doi = {10.1016/j.eng.2020.07.017},
  abstract = {The application of intelligence to manufacturing has emerged as a compelling topic for researchers and industries around the world. However, different terminologies, namely smart manufacturing (SM) and intelligent manufacturing (IM), have been applied to what may be broadly characterized as a similar paradigm by some researchers and practitioners. While SM and IM are similar, they are not identical. From an evolutionary perspective, there has been little consideration on whether the definition, thought, connotation, and technical development of the concepts of SM or IM are consistent in the literature. To address this gap, the work performs a qualitative and quantitative investigation of research literature to systematically compare inherent differences of SM and IM and clarify the relationship between SM and IM. A bibliometric analysis of publication sources, annual publication numbers, keywords frequency, and top regions of research and development establishes the scope and trends of the currently presented research. Critical topics discussed include origin, definitions, evolutionary path, and key technologies of SM and IM. The implementation architecture, standards, and national focus are also discussed. In this work, a basis to understand SM and IM is provided, which is increasingly important because the trend to merge both terminologies rises in Industry 4.0 as intelligence is being rapidly applied to modern manufacturing and human\textendash cyber\textendash physical systems.},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_smart manufacturing and intelligent manufacturing.pdf},
  journal = {Engineering}
}

@article{wang20_Toomanycooks,
  title = {Too Many Cooks: {{Bayesian}} Inference for Coordinating Multi-Agent Collaboration},
  shorttitle = {Too Many Cooks},
  author = {Wang, Rose E. and Wu, Sarah A. and Evans, James A. and Tenenbaum, Joshua B. and Parkes, David C. and {Kleiman-Weiner}, Max},
  year = {2020},
  month = jul,
  abstract = {Collaboration requires agents to coordinate their behavior on the fly, sometimes cooperating to solve a single task together and other times dividing it up into sub-tasks to work on in parallel. Underlying the human ability to collaborate is theory-of-mind, the ability to infer the hidden mental states that drive others to act. Here, we develop Bayesian Delegation, a decentralized multi-agent learning mechanism with these abilities. Bayesian Delegation enables agents to rapidly infer the hidden intentions of others by inverse planning. We test Bayesian Delegation in a suite of multi-agent Markov decision processes inspired by cooking problems. On these tasks, agents with Bayesian Delegation coordinate both their high-level plans (e.g. what sub-task they should work on) and their low-level actions (e.g. avoiding getting in each other's way). In a self-play evaluation, Bayesian Delegation outperforms alternative algorithms. Bayesian Delegation is also a capable ad-hoc collaborator and successfully coordinates with other agent types even in the absence of prior experience. Finally, in a behavioral experiment, we show that Bayesian Delegation makes inferences similar to human observers about the intent of others. Together, these results demonstrate the power of Bayesian Delegation for decentralized multi-agent collaboration.},
  archivePrefix = {arXiv},
  eprint = {2003.11778},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_too many cooks.pdf},
  journal = {arXiv:2003.11778 [cs]},
  primaryClass = {cs}
}

@article{wang20_UnifyingGraphConvolutional,
  title = {Unifying {{Graph Convolutional Neural Networks}} and {{Label Propagation}}},
  author = {Wang, Hongwei and Leskovec, Jure},
  year = {2020},
  month = feb,
  abstract = {Label Propagation (LPA) and Graph Convolutional Neural Networks (GCN) are both message passing algorithms on graphs. Both solve the task of node classification but LPA propagates node label information across the edges of the graph, while GCN propagates and transforms node feature information. However, while conceptually similar, theoretical relation between LPA and GCN has not yet been investigated. Here we study the relationship between LPA and GCN in terms of two aspects: (1) feature/label smoothing where we analyze how the feature/label of one node is spread over its neighbors; And, (2) feature/label influence of how much the initial feature/label of one node influences the final feature/label of another node. Based on our theoretical analysis, we propose an end-to-end model that unifies GCN and LPA for node classification. In our unified model, edge weights are learnable, and the LPA serves as regularization to assist the GCN in learning proper edge weights that lead to improved classification performance. Our model can also be seen as learning attention weights based on node labels, which is more task-oriented than existing feature-based attention models. In a number of experiments on real-world graphs, our model shows superiority over state-of-the-art GCN-based methods in terms of node classification accuracy.},
  annotation = {ZSCC: 0000003},
  archivePrefix = {arXiv},
  eprint = {2002.06755},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/5LD59CK6/Wang and Leskovec - 2020 - Unifying Graph Convolutional Neural Networks and L.pdf},
  journal = {arXiv:2002.06755 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wang20_unsuperviseddeeplearning,
  title = {An Unsupervised Deep Learning Framework via Integrated Optimization of Representation Learning and {{GMM}}-Based Modeling},
  author = {Wang, Jinghua and Jiang, Jianmin},
  year = {2020},
  month = sep,
  abstract = {While supervised deep learning has achieved great success in a range of applications, relatively little work has studied the discovery of knowledge from unlabeled data. In this paper, we propose an unsupervised deep learning framework to provide a potential solution for the problem that existing deep learning techniques require large labeled data sets for completing the training process. Our proposed introduces a new principle of joint learning on both deep representations and GMM (Gaussian Mixture Model)-based deep modeling, and thus an integrated objective function is proposed to facilitate the principle. In comparison with the existing work in similar areas, our objective function has two learning targets, which are created to be jointly optimized to achieve the best possible unsupervised learning and knowledge discovery from unlabeled data sets. While maximizing the first target enables the GMM to achieve the best possible modeling of the data representations and each Gaussian component corresponds to a compact cluster, maximizing the second term will enhance the separability of the Gaussian components and hence the inter-cluster distances. As a result, the compactness of clusters is significantly enhanced by reducing the intra-cluster distances, and the separability is improved by increasing the inter-cluster distances. Extensive experimental results show that the propose method can improve the clustering performance compared with benchmark methods.},
  archivePrefix = {arXiv},
  eprint = {2009.05234},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wang et al_2020_an unsupervised deep learning framework via integrated optimization of representation learning and gmm-based modeling.pdf},
  journal = {arXiv:2009.05234 [cs]},
  primaryClass = {cs}
}

@article{watson20_explanationgameformal,
  title = {The Explanation Game: A Formal Framework for Interpretable Machine Learning},
  shorttitle = {The Explanation Game},
  author = {Watson, David S. and Floridi, Luciano},
  year = {2020},
  month = apr,
  issn = {0039-7857, 1573-0964},
  doi = {10.1007/s11229-020-02629-9},
  abstract = {We propose a formal framework for interpretable machine learning. Combining elements from statistical learning, causal interventionism, and decision theory, we design an idealised explanation game in which players collaborate to find the best explanation(s) for a given algorithmic prediction. Through an iterative procedure of questions and answers, the players establish a three-dimensional Pareto frontier that describes the optimal trade-offs between explanatory accuracy, simplicity, and relevance. Multiple rounds are played at different levels of abstraction, allowing the players to explore overlapping causal patterns of variable granularity and scope. We characterise the conditions under which such a game is almost surely guaranteed to converge on a (conditionally) optimal explanation surface in polynomial time, and highlight obstacles that will tend to prevent the players from advancing beyond certain explanatory thresholds. The game serves a descriptive and a normative function, establishing a conceptual space in which to analyse and compare existing proposals, as well as design new and improved solutions.},
  file = {/home/trung/GoogleDrive/Zotero/watson et al_2020_the explanation game.pdf},
  journal = {Synthese},
  language = {en}
}

@article{welling00_westillneed,
  title = {Do We Still Need Models or Just More Data and Compute?},
  author = {Welling, Max},
  pages = {3},
  file = {/home/trung/GoogleDrive/Zotero/welling_do we still need models or just more data and compute.pdf},
  keywords = {favorite},
  language = {en}
}

@article{welling19_ArtificialIntelligenceIntelligence,
  title = {Artificial {{Intelligence}} versus {{Intelligence Engineering}}},
  author = {Welling, Max},
  year = {2019},
  month = jun,
  doi = {10.1162/99608f92.364ce476},
  file = {/home/trung/GoogleDrive/Zotero/welling_2019_artificial intelligence versus intelligence engineering.pdf},
  journal = {Harvard Data Science Review},
  language = {en}
}

@article{wen18_FlipoutEfficientPseudoIndependent,
  title = {Flipout: {{Efficient Pseudo}}-{{Independent Weight Perturbations}} on {{Mini}}-{{Batches}}},
  shorttitle = {Flipout},
  author = {Wen, Yeming and Vicol, Paul and Ba, Jimmy and Tran, Dustin and Grosse, Roger},
  year = {2018},
  month = mar,
  abstract = {Stochastic neural net weights are used in a variety of contexts, including regularization, Bayesian neural nets, exploration in reinforcement learning, and evolution strategies. Unfortunately, due to the large number of weights, all the examples in a mini-batch typically share the same weight perturbation, thereby limiting the variance reduction effect of large mini-batches. We introduce flipout, an efficient method for decorrelating the gradients within a mini-batch by implicitly sampling pseudo-independent weight perturbations for each example. Empirically, flipout achieves the ideal linear variance reduction for fully connected networks, convolutional networks, and RNNs. We find significant speedups in training neural networks with multiplicative Gaussian perturbations. We show that flipout is effective at regularizing LSTMs, and outperforms previous methods. Flipout also enables us to vectorize evolution strategies: in our experiments, a single GPU with flipout can handle the same throughput as at least 40 CPU cores using existing methods, equivalent to a factor-of-4 cost reduction on Amazon Web Services.},
  archivePrefix = {arXiv},
  eprint = {1803.04386},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wen et al_2018_flipout.pdf},
  journal = {arXiv:1803.04386 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wen18_LatentTopicConversational,
  title = {Latent {{Topic Conversational Models}}},
  author = {Wen, Tsung-Hsien and Luong, Minh-Thang},
  year = {2018},
  month = sep,
  abstract = {Latent variable models have been a preferred choice in conversational modeling compared to sequence-to-sequence (seq2seq) models which tend to generate generic and repetitive responses. Despite so, training latent variable models remains to be difficult. In this paper, we propose Latent Topic Conversational Model (LTCM) which augments seq2seq with a neural latent topic component to better guide response generation and make training easier. The neural topic component encodes information from the source sentence to build a global "topic" distribution over words, which is then consulted by the seq2seq model at each generation step. We study in details how the latent representation is learnt in both the vanilla model and LTCM. Our extensive experiments contribute to better understanding and training of conditional latent models for languages. Our results show that by sampling from the learnt latent representations, LTCM can generate diverse and interesting responses. In a subjective human evaluation, the judges also confirm that LTCM is the overall preferred option.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1809.07070},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wen et al_2018_latent topic conversational models.pdf;/home/trung/Zotero/storage/NZ8QN2D3/1809.html},
  journal = {arXiv:1809.07070 [cs]},
  keywords = {Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{wen20_CombiningEnsemblesData,
  title = {Combining {{Ensembles}} and {{Data Augmentation}} Can {{Harm}} Your {{Calibration}}},
  author = {Wen, Yeming and Jerfel, Ghassen and Muller, Rafael and Dusenberry, Michael W. and Snoek, Jasper and Lakshminarayanan, Balaji and Tran, Dustin},
  year = {2020},
  month = oct,
  abstract = {Ensemble methods which average over multiple neural network predictions are a simple approach to improve a model's calibration and robustness. Similarly, data augmentation techniques, which encode prior information in the form of invariant feature transformations, are effective for improving calibration and robustness. In this paper, we show a surprising pathology: combining ensembles and data augmentation can harm model calibration. This leads to a trade-off in practice, whereby improved accuracy by combining the two techniques comes at the expense of calibration. On the other hand, selecting only one of the techniques ensures good uncertainty estimates at the expense of accuracy. We investigate this pathology and identify a compounding under-confidence among methods which marginalize over sets of weights and data augmentation techniques which soften labels. Finally, we propose a simple correction, achieving the best of both worlds with significant accuracy and calibration gains over using only ensembles or data augmentation individually. Applying the correction produces new state-of-the art in uncertainty calibration across CIFAR-10, CIFAR-100, and ImageNet.},
  archivePrefix = {arXiv},
  eprint = {2010.09875},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wen et al_2020_combining ensembles and data augmentation can harm your calibration.pdf},
  journal = {arXiv:2010.09875 [cs, stat]},
  keywords = {favorite},
  primaryClass = {cs, stat}
}

@article{wen20_MutualInformationGradient,
  title = {Mutual {{Information Gradient Estimation}} for {{Representation Learning}}},
  author = {Wen, Liangjian and Zhou, Yiji and He, Lirong and Zhou, Mingyuan and Xu, Zenglin},
  year = {2020},
  month = may,
  abstract = {Mutual Information (MI) plays an important role in representation learning. However, MI is unfortunately intractable in continuous and high-dimensional settings. Recent advances establish tractable and scalable MI estimators to discover useful representation. However, most of the existing methods are not capable of providing an accurate estimation of MI with low-variance when the MI is large. We argue that directly estimating the gradients of MI is more appealing for representation learning than estimating MI in itself. To this end, we propose the Mutual Information Gradient Estimator (MIGE) for representation learning based on the score estimation of implicit distributions. MIGE exhibits a tight and smooth gradient estimation of MI in the high-dimensional and large-MI settings. We expand the applications of MIGE in both unsupervised learning of deep representations based on InfoMax and the Information Bottleneck method. Experimental results have indicated significant performance improvement in learning useful representation.},
  archivePrefix = {arXiv},
  eprint = {2005.01123},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wen et al_2020_mutual information gradient estimation for representation learning.pdf},
  journal = {arXiv:2005.01123 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@inproceedings{weng10_TwitterRankfindingtopicsensitive,
  title = {{{TwitterRank}}: Finding Topic-Sensitive Influential Twitterers},
  shorttitle = {{{TwitterRank}}},
  booktitle = {Proceedings of the Third {{ACM}} International Conference on {{Web}} Search and Data Mining - {{WSDM}} '10},
  author = {Weng, Jianshu and Lim, Ee-Peng and Jiang, Jing and He, Qi},
  year = {2010},
  pages = {261},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/1718487.1718520},
  abstract = {This paper focuses on the problem of identifying influential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called ``following'', in which each user can choose who she wants to ``follow'' to receive tweets from without requiring the latter to give permission first. In a dataset prepared for this study, it is observed that (1) 72.4\% of the users in Twitter follow more than 80\% of their followers, and (2) 80.5\% of the users have 80\% of users they are following follow them back. Our study reveals that the presence of ``reciprocity'' can be explained by phenomenon of homophily [14]. Based on this finding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the influence of users in Twitter. TwitterRank measures the influence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.},
  file = {/home/trung/GoogleDrive/Zotero/weng et al_2010_twitterrank.pdf},
  isbn = {978-1-60558-889-6},
  language = {en}
}

@article{weng20_KethseqtranscriptomewideRNA,
  title = {Keth-Seq for Transcriptome-Wide {{RNA}} Structure Mapping},
  author = {Weng, Xiaocheng and Gong, Jing and Chen, Yi and Wu, Tong and Wang, Fang and Yang, Shixi and Yuan, Yushu and Luo, Guanzheng and Chen, Kai and Hu, Lulu and Ma, Honghui and Wang, Pingluan and Zhang, Qiangfeng Cliff and Zhou, Xiang and He, Chuan},
  year = {2020},
  month = feb,
  issn = {1552-4450, 1552-4469},
  doi = {10.1038/s41589-019-0459-3},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/PNCRH6WJ/Weng et al. - 2020 - Keth-seq for transcriptome-wide RNA structure mapp.pdf},
  journal = {Nature Chemical Biology},
  language = {en}
}

@article{wenliang20_AmortisedLearningWakeSleep,
  title = {Amortised {{Learning}} by {{Wake}}-{{Sleep}}},
  author = {Wenliang, Li K. and Moskovitz, Theodore and Kanagawa, Heishiro and Sahani, Maneesh},
  year = {2020},
  month = aug,
  abstract = {Models that employ latent variables to capture structure in observed data lie at the heart of many current unsupervised learning algorithms, but exact maximum-likelihood learning for powerful and flexible latent-variable models is almost always intractable. Thus, state-of-the-art approaches either abandon the maximum-likelihood framework entirely, or else rely on a variety of variational approximations to the posterior distribution over the latents. Here, we propose an alternative approach that we call amortised learning. Rather than computing an approximation to the posterior over latents, we use a wake-sleep Monte-Carlo strategy to learn a function that directly estimates the maximum-likelihood parameter updates. Amortised learning is possible whenever samples of latents and observations can be simulated from the generative model, treating the model as a "black box". We demonstrate its effectiveness on a wide range of complex models, including those with latents that are discrete or supported on non-Euclidean spaces.},
  archivePrefix = {arXiv},
  eprint = {2002.09737},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wenliang et al_2020_amortised learning by wake-sleep.pdf},
  journal = {arXiv:2002.09737 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wenzel20_HowGoodBayes,
  title = {How {{Good}} Is the {{Bayes Posterior}} in {{Deep Neural Networks Really}}?},
  author = {Wenzel, Florian and Roth, Kevin and Veeling, Bastiaan S. and {\'S}wi{\k{a}}tkowski, Jakub and Tran, Linh and Mandt, Stephan and Snoek, Jasper and Salimans, Tim and Jenatton, Rodolphe and Nowozin, Sebastian},
  year = {2020},
  month = jul,
  abstract = {During the past five years the Bayesian deep learning community has developed increasingly accurate and efficient approximate inference procedures that allow for Bayesian inference in deep neural networks. However, despite this algorithmic progress and the promise of improved uncertainty quantification and sample efficiency there are---as of early 2020---no publicized deployments of Bayesian neural networks in industrial practice. In this work we cast doubt on the current understanding of Bayes posteriors in popular deep neural networks: we demonstrate through careful MCMC sampling that the posterior predictive induced by the Bayes posterior yields systematically worse predictions compared to simpler methods including point estimates obtained from SGD. Furthermore, we demonstrate that predictive performance is improved significantly through the use of a "cold posterior" that overcounts evidence. Such cold posteriors sharply deviate from the Bayesian paradigm but are commonly used as heuristic in Bayesian deep learning papers. We put forward several hypotheses that could explain cold posteriors and evaluate the hypotheses through experiments. Our work questions the goal of accurate posterior approximations in Bayesian deep learning: If the true Bayes posterior is poor, what is the use of more accurate approximations? Instead, we argue that it is timely to focus on understanding the origin of the improved performance of cold posteriors.},
  archivePrefix = {arXiv},
  eprint = {2002.02405},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wenzel et al_2020_how good is the bayes posterior in deep neural networks really.pdf},
  journal = {arXiv:2002.02405 [cs, stat]},
  keywords = {favorite},
  primaryClass = {cs, stat}
}

@article{west19_BottleSumUnsupervisedSelfsupervised,
  title = {{{BottleSum}}: {{Unsupervised}} and {{Self}}-Supervised {{Sentence Summarization}} Using the {{Information Bottleneck Principle}}},
  shorttitle = {{{BottleSum}}},
  author = {West, Peter and Holtzman, Ari and Buys, Jan and Choi, Yejin},
  year = {2019},
  month = sep,
  abstract = {The principle of the Information Bottleneck (Tishby et al., 1999) is to produce a summary of information X optimized to predict some other relevant information Y . In this paper, we propose a novel approach to unsupervised sentence summarization by mapping the Information Bottleneck principle to a conditional language modelling objective: given a sentence, our approach seeks a compressed sentence that can best predict the next sentence. Our iterative algorithm under the Information Bottleneck objective searches gradually shorter subsequences of the given sentence while maximizing the probability of the next sentence conditioned on the summary. Using only pretrained language models with no direct supervision, our approach can efficiently perform extractive sentence summarization over a large corpus.},
  archivePrefix = {arXiv},
  eprint = {1909.07405},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/west et al_2019_bottlesum.pdf},
  journal = {arXiv:1909.07405 [cs]},
  keywords = {information},
  language = {en},
  primaryClass = {cs}
}

@article{weston00_DeepLearningSemiSupervised,
  title = {Deep {{Learning}} via {{Semi}}-{{Supervised Embedding}}},
  author = {Weston, Jason and Ratle, Frederic and Collobert, Ronan},
  pages = {8},
  abstract = {We show how nonlinear embedding algorithms popular for use with shallow semisupervised learning techniques such as kernel methods can be applied to deep multilayer architectures, either as a regularizer at the output layer, or on each layer of the architecture. This provides a simple alternative to existing approaches to deep learning whilst yielding competitive error rates compared to those methods, and existing shallow semi-supervised techniques.},
  file = {/home/trung/GoogleDrive/Zotero/weston et al_deep learning via semi-supervised embedding.pdf},
  language = {en}
}

@article{whitney20_Evaluatingrepresentationscomplexity,
  title = {Evaluating Representations by the Complexity of Learning Low-Loss Predictors},
  author = {Whitney, William F. and Song, Min Jae and Brandfonbrener, David and Altosaar, Jaan and Cho, Kyunghyun},
  year = {2020},
  month = sep,
  abstract = {We consider the problem of evaluating representations of data for use in solving a downstream task. We propose to measure the quality of a representation by the complexity of learning a predictor on top of the representation that achieves low loss on a task of interest, and introduce two methods, surplus description length (SDL) and \$\textbackslash varepsilon\$ sample complexity (\$\textbackslash varepsilon\$SC). In contrast to prior methods, which measure the amount of information about the optimal predictor that is present in a specific amount of data, our methods measure the amount of information needed from the data to recover an approximation of the optimal predictor up to a specified tolerance. We present a framework to compare these methods based on plotting the validation loss versus training set size (the "loss-data" curve). Existing measures, such as mutual information and minimum description length probes, correspond to slices and integrals along the data-axis of the loss-data curve, while ours correspond to slices and integrals along the loss-axis. We provide experiments on real data to compare the behavior of each of these methods over datasets of varying size along with a high performance open source library for representation evaluation at https://github.com/willwhitney/reprieve.},
  archivePrefix = {arXiv},
  eprint = {2009.07368},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/whitney et al_2020_evaluating representations by the complexity of learning low-loss predictors.pdf},
  journal = {arXiv:2009.07368 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{whitney20_Evaluatingrepresentationscomplexitya,
  title = {Evaluating Representations by the Complexity of Learning Low-Loss Predictors},
  author = {Whitney, William F. and Song, Min Jae and Brandfonbrener, David and Altosaar, Jaan and Cho, Kyunghyun},
  year = {2020},
  month = sep,
  abstract = {We consider the problem of evaluating representations of data for use in solving a downstream task. We propose to measure the quality of a representation by the complexity of learning a predictor on top of the representation that achieves low loss on a task of interest, and introduce two methods, surplus description length (SDL) and \$\textbackslash varepsilon\$ sample complexity (\$\textbackslash varepsilon\$SC). In contrast to prior methods, which measure the amount of information about the optimal predictor that is present in a specific amount of data, our methods measure the amount of information needed from the data to recover an approximation of the optimal predictor up to a specified tolerance. We present a framework to compare these methods based on plotting the validation loss versus training set size (the "loss-data" curve). Existing measures, such as mutual information and minimum description length probes, correspond to slices and integrals along the data-axis of the loss-data curve, while ours correspond to slices and integrals along the loss-axis. We provide experiments on real data to compare the behavior of each of these methods over datasets of varying size along with a high performance open source library for representation evaluation at https://github.com/willwhitney/reprieve.},
  archivePrefix = {arXiv},
  eprint = {2009.07368},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2020/false},
  journal = {arXiv:2009.07368 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wicke20_ShowMustGo,
  title = {The {{Show Must Go On}}: {{On}} the {{Use}} of {{Embodiment}}, {{Space}} and {{Gesture}} in {{Computational Storytelling}}},
  shorttitle = {The {{Show Must Go On}}},
  author = {Wicke, Philipp and Veale, Tony},
  year = {2020},
  month = sep,
  issn = {0288-3635, 1882-7055},
  doi = {10.1007/s00354-020-00106-y},
  abstract = {Stories are made to be told, yet the computational generation of stories has principally focused on stories as textual artifacts, rather than on the telling, or indeed the performance, of stories to an audience. The performative aspect of stories, in which a teller brings a tale to life, requires more than the written word. We humans use our bodies to enact a story, through the apt use of motion, space, timing, orientation and gesture. This work explores the physical enactment of computer-generated stories using robots. These embodied agents narrate the tale, and take on the role of the characters within it. They use pantomime to enhance the drama of narrative events, and use naturalistic gestures for more subtle communicative effects. They also use space as a mirror for more abstract concerns such as affect and social relations. The paper outlines the Sc\textasciiacute ealability framework for turning story artifacts into performances, and presents empirical findings on the effectiveness of various embodied strategies. In particular, we show that audiences are sensitive to, and appreciate, the coherent use of spatial movement in embodied story-telling.},
  file = {/home/trung/GoogleDrive/Zotero/wicke et al_2020_the show must go on.pdf},
  journal = {New Generation Computing},
  language = {en}
}

@article{wikipedia20_Riemanniangeometry,
  title = {Riemannian Geometry},
  author = {{wikipedia}},
  year = {2020},
  month = may,
  abstract = {Riemannian geometry is the branch of differential geometry that studies Riemannian manifolds, smooth manifolds with a Riemannian metric, i.e. with an inner product on the tangent space at each point that varies smoothly from point to point. This gives, in particular, local notions of angle, length of curves, surface area and volume. From those, some other global quantities can be derived by integrating local contributions. Riemannian geometry originated with the vision of Bernhard Riemann expressed in his inaugural lecture "Ueber die Hypothesen, welche der Geometrie zu Grunde liegen" ("On the Hypotheses on which Geometry is Based"). It is a very broad and abstract generalization of the differential geometry of surfaces in R3. Development of Riemannian geometry resulted in synthesis of diverse results concerning the geometry of surfaces and the behavior of geodesics on them, with techniques that can be applied to the study of differentiable manifolds of higher dimensions. It enabled the formulation of Einstein's general theory of relativity, made profound impact on group theory and representation theory, as well as analysis, and spurred the development of algebraic and differential topology.},
  annotation = {Page Version ID: 954782703},
  copyright = {Creative Commons Attribution-ShareAlike License},
  journal = {Wikipedia},
  language = {en}
}

@article{willetts19_DisentanglingClusterGaussian,
  title = {Disentangling to {{Cluster}}: {{Gaussian Mixture Variational Ladder Autoencoders}}},
  shorttitle = {Disentangling to {{Cluster}}},
  author = {Willetts, Matthew and Roberts, Stephen and Holmes, Chris},
  year = {2019},
  month = sep,
  abstract = {In clustering we normally output one cluster variable for each datapoint. However it is not necessarily the case that there is only one way to partition a given dataset into cluster components. For example, one could cluster objects by their colour, or by their type. Different attributes form a hierarchy, and we could wish to cluster in any of them. By disentangling the learnt latent representations of some dataset into different layers for different attributes we can then cluster in those latent spaces. We call this "disentangled clustering". Extending Variational Ladder Autoencoders (Zhao et al., 2017), we propose a clustering algorithm, VLAC, that outperforms a Gaussian Mixture DGM in cluster accuracy over digit identity on the test set of SVHN. We also demonstrate learning clusters jointly over numerous layers of the hierarchy of latent variables for the data, and show component-wise generation from this hierarchical model.},
  archivePrefix = {arXiv},
  eprint = {1909.11501},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/willetts et al_2019_disentangling to cluster.pdf;/home/trung/Zotero/storage/ER9SJT8S/1909.html},
  journal = {arXiv:1909.11501 [cs, stat]},
  keywords = {Computer Science - Machine Learning,disentanglement,ladder,mixture,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{willetts19_DisentanglingClusterGaussiana,
  title = {Disentangling to {{Cluster}}: {{Gaussian Mixture Variational Ladder Autoencoders}}},
  shorttitle = {Disentangling to {{Cluster}}},
  author = {Willetts, Matthew and Roberts, Stephen and Holmes, Chris},
  year = {2019},
  month = dec,
  abstract = {In clustering we normally output one cluster variable for each datapoint. However it is not necessarily the case that there is only one way to partition a given dataset into cluster components. For example, one could cluster objects by their colour, or by their type. Different attributes form a hierarchy, and we could wish to cluster in any of them. By disentangling the learnt latent representations of some dataset into different layers for different attributes we can then cluster in those latent spaces. We call this "disentangled clustering". Extending Variational Ladder Autoencoders (Zhao et al., 2017), we propose a clustering algorithm, VLAC, that outperforms a Gaussian Mixture DGM in cluster accuracy over digit identity on the test set of SVHN. We also demonstrate learning clusters jointly over numerous layers of the hierarchy of latent variables for the data, and show component-wise generation from this hierarchical model.},
  archivePrefix = {arXiv},
  eprint = {1909.11501},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/willetts et al_2019_disentangling to cluster2.pdf},
  journal = {arXiv:1909.11501 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{willetts19_DisentanglingImprovesVAEs,
  title = {Disentangling {{Improves VAEs}}' {{Robustness}} to {{Adversarial Attacks}}},
  author = {Willetts, Matthew and Camuto, Alexander and Roberts, Stephen and Holmes, Chris},
  year = {2019},
  month = jun,
  abstract = {This paper is concerned with the robustness of VAEs to adversarial attacks. We highlight that conventional VAEs are brittle under attack but that methods recently introduced for disentanglement such as \$\textbackslash beta\$-TCVAE (Chen et al., 2018) improve robustness, as demonstrated through a variety of previously proposed adversarial attacks (Tabacof et al. (2016); Gondim-Ribeiro et al. (2018); Kos et al.(2018)). This motivated us to develop Seatbelt-VAE, a new hierarchical disentangled VAE that is designed to be significantly more robust to adversarial attacks than existing approaches, while retaining high quality reconstructions.},
  archivePrefix = {arXiv},
  eprint = {1906.00230},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/willetts et al_2019_disentangling improves vaes' robustness to adversarial attacks.pdf;/home/trung/Zotero/storage/U2YCM56H/1906.html},
  journal = {arXiv:1906.00230 [cs, stat]},
  keywords = {adversarial,Computer Science - Cryptography and Security,Computer Science - Machine Learning,disentanglement,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{willetts19_SemiUnsupervisedLearningDeep,
  title = {Semi-{{Unsupervised Learning}} with {{Deep Generative Models}}: {{Clustering}} and {{Classifying}} Using {{Ultra}}-{{Sparse Labels}}},
  shorttitle = {Semi-{{Unsupervised Learning}} with {{Deep Generative Models}}},
  author = {Willetts, Matthew and Roberts, Stephen J. and Holmes, Christopher C.},
  year = {2019},
  month = jan,
  abstract = {We introduce \$\textbackslash textit\{semi-unsupervised learning\}\$, an extreme case of semi-supervised learning with ultra-sparse categorisation where some classes have no labels in the training set. That is, in the training data some classes are sparsely labelled and other classes appear only as unlabelled data. Many real-world datasets are conceivably of this type. We demonstrate that effective learning in this regime is only possible when a model is capable of capturing both semi-supervised and unsupervised learning. We develop two deep generative models for classification in this regime that extend previous deep generative models designed for semi-supervised learning. By changing their probabilistic structure to contain a mixture of Gaussians in their continuous latent space, these new models can learn in both unsupervised and semi-unsupervised paradigms. We demonstrate their performance both for semi-unsupervised and unsupervised learning on various standard datasets. We show that our models can learn in an semi-unsupervised manner on Fashion-MNIST. Here we artificially mask out all labels for half of the classes of data and keep \$2\textbackslash\%\$ of labels for the remaining classes. Our model is able to learn effectively, obtaining a trained classifier with \$(77.2\textbackslash pm1.3)\textbackslash\%\$ test set accuracy. We also can train on Fashion-MNIST unsupervised, obtaining \$(75.2\textbackslash pm1.5)\textbackslash\%\$ test set accuracy. Additionally, doing the same for MNIST unsupervised we get \$(96.3\textbackslash pm0.9)\textbackslash\%\$ test set accuracy, which is state-of-the art for fully probabilistic deep generative models.},
  archivePrefix = {arXiv},
  eprint = {1901.08560},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/willetts et al_2019_semi-unsupervised learning with deep generative models.pdf},
  journal = {arXiv:1901.08560 [cs, stat]},
  keywords = {clustering,Computer Science - Machine Learning,semi-supervised,sparse,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{willetts20_RelaxedResponsibilityHierarchicalDiscrete,
  title = {Relaxed-{{Responsibility Hierarchical Discrete VAEs}}},
  author = {Willetts, Matthew and Miscouridou, Xenia and Roberts, Stephen and Holmes, Chris},
  year = {2020},
  month = jul,
  abstract = {Successfully training Variational Autoencoders (VAEs) with a hierarchy of discrete latent variables remains an area of active research. Leveraging insights from classical methods of inference we introduce Relaxed-Responsibility Vector-Quantisation, a novel way to parameterise discrete latent variables, a refinement of relaxed Vector-Quantisation. This enables a novel approach to hierarchical discrete variational autoencoder with numerous layers of latent variables that we train end-to-end. Unlike discrete VAEs with a single layer of latent variables, we can produce realistic-looking samples by ancestral sampling: it is not essential to train a second generative model over the learnt latent representations to then sample from and then decode. Further, we observe different layers of our model become associated with different aspects of the data.},
  archivePrefix = {arXiv},
  eprint = {2007.07307},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/willetts et al_2020_relaxed-responsibility hierarchical discrete vaes.pdf},
  journal = {arXiv:2007.07307 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{willi19_RecurrentNeuralProcesses,
  title = {Recurrent {{Neural Processes}}},
  author = {Willi, Timon and Masci, Jonathan and Schmidhuber, J{\"u}rgen and Osendorfer, Christian},
  year = {2019},
  month = nov,
  abstract = {We extend Neural Processes (NPs) to sequential data through Recurrent NPs or RNPs, a family of conditional state space models. RNPs model the state space with Neural Processes. Given time series observed on fast real-world time scales but containing slow long-term variabilities, RNPs may derive appropriate slow latent time scales. They do so in an efficient manner by establishing conditional independence among subsequences of the time series. Our theoretically grounded framework for stochastic processes expands the applicability of NPs while retaining their benefits of flexibility, uncertainty estimation, and favorable runtime with respect to Gaussian Processes (GPs). We demonstrate that state spaces learned by RNPs benefit predictive performance on real-world time-series data and nonlinear system identification, even in the case of limited data availability.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1906.05915},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/8YPEIDRL/Willi et al. - 2019 - Recurrent Neural Processes.pdf},
  journal = {arXiv:1906.05915 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{williams19_DisentanglingStyleFactors,
  title = {Disentangling {{Style Factors}} from {{Speaker Representations}}},
  booktitle = {Interspeech 2019},
  author = {Williams, Jennifer and King, Simon},
  year = {2019},
  month = sep,
  pages = {3945--3949},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2019-1769},
  abstract = {Our goal is to separate out speaking style from speaker identity in utterance-level representations of speech such as ivectors and x-vectors. We first show that both i-vectors and x-vectors contain information not only about speaker but also about speaking style (for one data set) or emotion (for another data set), even when projected into a low-dimensional space. To disentangle these factors, we use an autoencoder in which the latent space is split into two subspaces. The entangled information about speaker and style/emotion is pushed apart by the use of auxiliary classifiers that take one of the two latent subspaces as input and that are jointly learned with the autoencoder. We evaluate how well the latent subspaces separate the factors by using them as input to separate style/emotion classification tasks. In traditional speaker identification tasks, speakerinvariant characteristics are factorized from channel and then the channel information is ignored. Our results suggest that this so-called channel may contain exploitable information, which we refer to as style factors. Finally, we propose future work to use information theory to formalize style factors in the context of speaker identity.},
  file = {/home/trung/GoogleDrive/Zotero/williams et al_2019_disentangling style factors from speaker representations.pdf},
  language = {en}
}

@article{williams20_HierarchicalQuantizedAutoencoders,
  title = {Hierarchical {{Quantized Autoencoders}}},
  author = {Williams, Will and Ringer, Sam and Ash, Tom and Hughes, John and MacLeod, David and Dougherty, Jamie},
  year = {2020},
  month = jun,
  abstract = {Despite progress in training neural networks for lossy image compression, current approaches fail to maintain both perceptual quality and abstract features at very low bitrates. Encouraged by recent success in learning discrete representations with Vector Quantized Variational Autoencoders (VQ-VAEs), we motivate the use of a hierarchy of VQ-VAEs to attain high factors of compression. We show that the combination of stochastic quantization and hierarchical latent structure aids likelihood-based image compression. This leads us to introduce a novel objective for training hierarchical VQ-VAEs. Our resulting scheme produces a Markovian series of latent variables that reconstruct images of high-perceptual quality which retain semantically meaningful features. We provide qualitative and quantitative evaluations on the CelebA and MNIST datasets.},
  archivePrefix = {arXiv},
  eprint = {2002.08111},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/williams et al_2020_hierarchical quantized autoencoders.pdf},
  journal = {arXiv:2002.08111 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{williams20_LearningDisentangledPhone,
  title = {Learning {{Disentangled Phone}} and {{Speaker Representations}} in a {{Semi}}-{{Supervised VQ}}-{{VAE Paradigm}}},
  author = {Williams, Jennifer and Zhao, Yi and Cooper, Erica and Yamagishi, Junichi},
  year = {2020},
  month = oct,
  abstract = {We present a new approach to disentangle speaker voice and phone content by introducing new components to the VQ-VAE architecture for speech synthesis. The original VQ-VAE does not generalize well to unseen speakers or content. To alleviate this problem, we have incorporated a speaker encoder and speaker VQ codebook that learns global speaker characteristics entirely separate from the existing sub-phone codebooks. We also compare two training methods: self-supervised with global conditions and semi-supervised with speaker labels. Adding a speaker VQ component improves objective measures of speech synthesis quality (estimated MOS, speaker similarity, ASR-based intelligibility) and provides learned representations that are meaningful. Our speaker VQ codebook indices can be used in a simple speaker diarization task and perform slightly better than an x-vector baseline. Additionally, phones can be recognized from sub-phone VQ codebook indices in our semi-supervised VQ-VAE better than self-supervised with global conditions.},
  archivePrefix = {arXiv},
  eprint = {2010.10727},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/williams et al_2020_learning disentangled phone and speaker representations in a semi-supervised vq-vae paradigm.pdf},
  journal = {arXiv:2010.10727 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{wilson00_BayesianDeepLearning,
  title = {Bayesian {{Deep Learning}}},
  author = {Wilson, Andrew Gordon},
  pages = {66},
  file = {/home/trung/GoogleDrive/Zotero/wilson_bayesian deep learning.pdf},
  language = {en}
}

@article{wilson16_StochasticVariationalDeep,
  title = {Stochastic {{Variational Deep Kernel Learning}}},
  author = {Wilson, Andrew Gordon and Hu, Zhiting and Salakhutdinov, Ruslan and Xing, Eric P.},
  year = {2016},
  month = nov,
  abstract = {Deep kernel learning combines the non-parametric flexibility of kernel methods with the inductive biases of deep learning architectures. We propose a novel deep kernel learning model and stochastic variational inference procedure which generalizes deep kernel learning approaches to enable classification, multi-task learning, additive covariance structures, and stochastic gradient training. Specifically, we apply additive base kernels to subsets of output features from deep neural architectures, and jointly learn the parameters of the base kernels and deep network through a Gaussian process marginal likelihood objective. Within this framework, we derive an efficient form of stochastic variational inference which leverages local kernel interpolation, inducing points, and structure exploiting algebra. We show improved performance over stand alone deep networks, SVMs, and state of the art scalable Gaussian processes on several classification benchmarks, including an airline delay dataset containing 6 million training points, CIFAR, and ImageNet.},
  archivePrefix = {arXiv},
  eprint = {1611.00336},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wilson et al_2016_stochastic variational deep kernel learning.pdf;/home/trung/Zotero/storage/VR28RCXT/1611.html},
  journal = {arXiv:1611.00336 [cs, stat]},
  keywords = {Computer Science - Machine Learning,kernel,Statistics - Machine Learning,Statistics - Methodology,variational},
  primaryClass = {cs, stat}
}

@article{wilson19_EightyFivePercent,
  title = {The {{Eighty Five Percent Rule}} for Optimal Learning},
  author = {Wilson, Robert C. and Shenhav, Amitai and Straccia, Mark and Cohen, Jonathan D.},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {4646},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12552-4},
  annotation = {ZSCC: 0000002},
  file = {/home/trung/GoogleDrive/Zotero/wilson et al_2019_the eighty five percent rule for optimal learning.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{wilson20_BayesianDeepLearning,
  title = {Bayesian {{Deep Learning}} and a {{Probabilistic Perspective}} of {{Generalization}}},
  author = {Wilson, Andrew Gordon and Izmailov, Pavel},
  year = {2020},
  month = apr,
  abstract = {The key distinguishing property of a Bayesian approach is marginalization, rather than using a single setting of weights. Bayesian marginalization can particularly improve the accuracy and calibration of modern deep neural networks, which are typically underspecified by the data, and can represent many compelling but different solutions. We show that deep ensembles provide an effective mechanism for approximate Bayesian marginalization, and propose a related approach that further improves the predictive distribution by marginalizing within basins of attraction, without significant overhead. We also investigate the prior over functions implied by a vague distribution over neural network weights, explaining the generalization properties of such models from a probabilistic perspective. From this perspective, we explain results that have been presented as mysterious and distinct to neural network generalization, such as the ability to fit images with random labels, and show that these results can be reproduced with Gaussian processes. We also show that Bayesian model averaging alleviates double descent, resulting in monotonic performance improvements with increased flexibility. Finally, we provide a Bayesian perspective on tempering for calibrating predictive distributions.},
  archivePrefix = {arXiv},
  eprint = {2002.08791},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wilson et al_2020_bayesian deep learning and a probabilistic perspective of generalization.pdf},
  journal = {arXiv:2002.08791 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{wilson20_CaseBayesianDeep,
  title = {The {{Case}} for {{Bayesian Deep Learning}}},
  author = {Wilson, Andrew Gordon},
  year = {2020},
  month = jan,
  abstract = {The key distinguishing property of a Bayesian approach is marginalization instead of optimization, not the prior, or Bayes rule. Bayesian inference is especially compelling for deep neural networks. (1) Neural networks are typically underspecified by the data, and can represent many different but high performing models corresponding to different settings of parameters, which is exactly when marginalization will make the biggest difference for both calibration and accuracy. (2) Deep ensembles have been mistaken as competing approaches to Bayesian methods, but can be seen as approximate Bayesian marginalization. (3) The structure of neural networks gives rise to a structured prior in function space, which reflects the inductive biases of neural networks that help them generalize. (4) The observed correlation between parameters in flat regions of the loss and a diversity of solutions that provide good generalization is further conducive to Bayesian marginalization, as flat regions occupy a large volume in a high dimensional space, and each different solution will make a good contribution to a Bayesian model average. (5) Recent practical advances for Bayesian deep learning provide improvements in accuracy and calibration compared to standard training, while retaining scalability.},
  archivePrefix = {arXiv},
  eprint = {2001.10995},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/89JRCMRW/Wilson - 2020 - The Case for Bayesian Deep Learning.pdf},
  journal = {arXiv:2001.10995 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{winther07_Bayesianindependentcomponent,
  title = {Bayesian Independent Component Analysis: {{Variational}} Methods and Non-Negative Decompositions},
  author = {Winther, Ole and Petersen, Kaare Brandt},
  year = {2007},
  month = sep,
  volume = {17},
  pages = {858--872},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2007.01.003},
  abstract = {In this paper we present an empirical Bayesian framework for independent component analysis. The framework provides estimates of the sources, the mixing matrix and the noise parameters, and is flexible with respect to choice of source prior and the number of sources and sensors. Inside the engine of the method are two mean field techniques\textemdash the variational Bayes and the expectation consistent framework\textemdash and the cost function relating to these methods are optimized using the adaptive overrelaxed expectation maximization (EM) algorithm and the easy gradient recipe. The entire framework, implemented in a Matlab toolbox, is demonstrated for non-negative decompositions and compared with non-negative matrix factorization.},
  file = {/home/trung/GoogleDrive/Zotero/winther et al_2007_bayesian independent component analysis.pdf},
  journal = {Special Issue on Bayesian Source Separation},
  number = {5}
}

@article{winther07_Bayesianindependentcomponenta,
  title = {Bayesian Independent Component Analysis: {{Variational}} Methods and Non-Negative Decompositions},
  shorttitle = {Bayesian Independent Component Analysis},
  author = {Winther, Ole and Petersen, Kaare Brandt},
  year = {2007},
  month = sep,
  volume = {17},
  pages = {858--872},
  issn = {1051-2004},
  doi = {10.1016/j.dsp.2007.01.003},
  abstract = {In this paper we present an empirical Bayesian framework for independent component analysis. The framework provides estimates of the sources, the mixing matrix and the noise parameters, and is flexible with respect to choice of source prior and the number of sources and sensors. Inside the engine of the method are two mean field techniques\textemdash the variational Bayes and the expectation consistent framework\textemdash and the cost function relating to these methods are optimized using the adaptive overrelaxed expectation maximization (EM) algorithm and the easy gradient recipe. The entire framework, implemented in a Matlab toolbox, is demonstrated for non-negative decompositions and compared with non-negative matrix factorization.},
  annotation = {ZSCC: 0000017},
  file = {/home/trung/Zotero/storage/I3KJWD8Y/S1051200407000048.html},
  journal = {Digital Signal Processing},
  language = {en},
  number = {5},
  series = {Special {{Issue}} on {{Bayesian Source Separation}}}
}

@article{wipf00_DeepGenerativeModels,
  title = {Deep {{Generative Models}} for {{Signal Processing}} and {{Beyond}}},
  author = {Wipf, David},
  pages = {55},
  file = {/home/trung/GoogleDrive/Zotero/wipf_deep generative models for signal processing and beyond.pdf},
  language = {en}
}

@article{wiseman19_AmortizedBetheFree,
  title = {Amortized {{Bethe Free Energy Minimization}} for {{Learning MRFs}}},
  author = {Wiseman, Sam and Kim, Yoon},
  year = {2019},
  month = jun,
  abstract = {We propose to learn deep undirected graphical models (i.e., MRFs), with a non-ELBO objective for which we can calculate exact gradients. In particular, we optimize a saddle-point objective deriving from the Bethe free energy approximation to the partition function. Unlike much recent work in approximate inference, the derived objective requires no sampling, and can be efficiently computed even for very expressive MRFs. We furthermore amortize this optimization with trained inference networks. Experimentally, we find that the proposed approach compares favorably with loopy belief propagation, but is faster, and it allows for attaining better held out log likelihood than other recent approximate inference schemes.},
  archivePrefix = {arXiv},
  eprint = {1906.06399},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wiseman et al_2019_amortized bethe free energy minimization for learning mrfs.pdf;/home/trung/Zotero/storage/ANW9NLLE/1906.html},
  journal = {arXiv:1906.06399 [cs, stat]},
  keywords = {Computer Science - Machine Learning,graph,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{withee17_EffectsMethylsulfonylmethaneMSM,
  title = {Effects of {{Methylsulfonylmethane}} ({{MSM}}) on Exercise-Induced Oxidative Stress, Muscle Damage, and Pain Following a Half-Marathon: A Double-Blind, Randomized, Placebo-Controlled Trial},
  shorttitle = {Effects of {{Methylsulfonylmethane}} ({{MSM}}) on Exercise-Induced Oxidative Stress, Muscle Damage, and Pain Following a Half-Marathon},
  author = {Withee, Eric D. and Tippens, Kimberly M. and Dehen, Regina and Tibbitts, Deanne and Hanes, Douglas and Zwickey, Heather},
  year = {2017},
  volume = {14},
  pages = {24},
  issn = {1550-2783},
  doi = {10.1186/s12970-017-0181-z},
  abstract = {BACKGROUND: Oxidative stress and muscle damage occur during exhaustive bouts of exercise, and many runners report pain and soreness as major influences on changes or breaks in training regimens, creating a barrier to training persistence. Methylsulfonylmethane (MSM) is a sulfur-based nutritional supplement that is purported to have pain and inflammation-reducing effects. To investigate the effects of MSM in attenuating damage associated with physical exertion, this randomized, double-blind, placebo-controlled study evaluated the effects of MSM supplementation on exercise-induced pain, oxidative stress and muscle damage. METHODS: Twenty-two healthy females (n~=~17) and males (n~=~5) (age 33.7~{$\pm~$}6.9~yrs.) were recruited from the 2014 Portland Half-Marathon registrant pool. Participants were randomized to take either MSM (OptiMSM\textregistered ) (n~=~11), or a placebo (n~=~11) at 3~g/day for 21~days prior to the race and for two days after (23 total). Participants provided blood samples for measurement of markers of oxidative stress, and completed VAS surveys for pain approximately one month prior to the race (T0), and at 15~min (T1), 90~min (T2), 1~Day (T3), and 2~days (T4) after race finish. The primary outcome measure 8-hydroxy-2-deoxyguanine (8-OHdG) measured oxidative stress. Secondary outcomes included malondialdehyde (MDA) for oxidative stress, creatine kinase (CK) and lactate dehydrogenase (LDH) as measures of muscle damage, and muscle (MP) and joint pain (JP) recorded using a 100~mm Visual Analogue Scale (VAS). Data were analyzed using repeated and multivariate ANOVAs, and simple contrasts compared post-race time points to baseline, presented as mean (SD) or mean change (95\% CI) where appropriate. RESULTS: Running a half-marathon induced significant increases in all outcome measures (p~{$<~$}0.001). From baseline, 8-OHdG increased significantly at T1 by 1.53~ng/mL (0.86-2.20~ng/mL CI, p~{$<~$}0.001) and T2 by 1.19~ng/mL (0.37-2.01~ng/mL CI, p~{$<~$}0.01), and fell below baseline levels at T3 by -0.46~ng/mL (-1.18-0.26 CI, p~{$>~$}0.05) and T4 by -0.57~ng/mL (-1.27-0.13 CI, p~{$>~$}0.05). MDA increased significantly at T1 by 7.3~{$\mu$}M (3.9-10.7 CI, p~{$<~$}0.001). Muscle damage markers CK and LDH saw significant increases from baseline at all time-points (p~{$<~$}0.01). Muscle and joint pain increased significantly from baseline at T1, T2, and T3 (p~{$<~$}0.01) and returned to baseline levels at T4. Time-by-treatment results did not reach statistical significance for any outcome measure, however, the MSM group saw clinically significant ({$\Delta~>~$}10~mm) reductions in both muscle and joint pain. CONCLUSION: Participation in a half-marathon was associated with increased markers of oxidative stress, muscle damage, and pain. MSM supplementation was not associated with a decrease from pre-training levels of oxidative stress or muscle damage associated with an acute bout of exercise. MSM supplementation attenuated post-exercise muscle and joint pain at clinically, but not statistically significant levels.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/withee et al_2017_effects of methylsulfonylmethane (msm) on exercise-induced oxidative stress, muscle damage, and pain following a half-marathon.pdf},
  journal = {Journal of the International Society of Sports Nutrition},
  language = {eng},
  pmcid = {PMC5521097},
  pmid = {28736511}
}

@misc{wolchover00_NewTheoryCracks,
  title = {New {{Theory Cracks Open}} the {{Black Box}} of {{Deep Learning}}},
  author = {Wolchover, Natalie},
  abstract = {A new idea is helping to explain the puzzling success of today's artificial-intelligence algorithms \textemdash{} and might also explain how human brains learn.},
  annotation = {ZSCC: 0000018},
  file = {/home/trung/Zotero/storage/CING4UET/new-theory-cracks-open-the-black-box-of-deep-learning-20170921.html},
  howpublished = {https://www.quantamagazine.org/new-theory-cracks-open-the-black-box-of-deep-learning-20170921/},
  journal = {Quanta Magazine},
  language = {en}
}

@article{wolf18_SCANPYlargescalesinglecell,
  title = {{{SCANPY}}: Large-Scale Single-Cell Gene Expression Data Analysis},
  shorttitle = {{{SCANPY}}},
  author = {Wolf, F. Alexander and Angerer, Philipp and Theis, Fabian J.},
  year = {2018},
  month = dec,
  volume = {19},
  issn = {1474-760X},
  doi = {10.1186/s13059-017-1382-0},
  abstract = {SCANPY is a scalable toolkit for analyzing single-cell gene expression data. It includes methods for preprocessing, visualization, clustering, pseudotime and trajectory inference, differential expression testing, and simulation of gene regulatory networks. Its Python-based implementation efficiently deals with data sets of more than one million cells (https://github.com/theislab/Scanpy). Along with SCANPY, we present ANNDATA, a generic class for handling annotated data matrices (https://github.com/theislab/anndata).},
  file = {/home/trung/GoogleDrive/Zotero/wolf et al_2018_scanpy.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{wong20_Fastbetterfree,
  title = {Fast Is Better than Free: {{Revisiting}} Adversarial Training},
  shorttitle = {Fast Is Better than Free},
  author = {Wong, Eric and Rice, Leslie and Kolter, J. Zico},
  year = {2020},
  month = jan,
  abstract = {Adversarial training, a method for learning robust deep networks, is typically assumed to be more expensive than traditional training due to the necessity of constructing adversarial examples via a first-order method like projected gradient decent (PGD). In this paper, we make the surprising discovery that it is possible to train empirically robust models using a much weaker and cheaper adversary, an approach that was previously believed to be ineffective, rendering the method no more costly than standard training in practice. Specifically, we show that adversarial training with the fast gradient sign method (FGSM), when combined with random initialization, is as effective as PGD-based training but has significantly lower cost. Furthermore we show that FGSM adversarial training can be further accelerated by using standard techniques for efficient training of deep networks, allowing us to learn a robust CIFAR10 classifier with 45\% robust accuracy to PGD attacks with = 8/255 in 6 minutes, and a robust ImageNet classifier with 43\% robust accuracy at = 2/255 in 12 hours, in comparison to past work based on ``free'' adversarial training which took 10 and 50 hours to reach the same respective thresholds. Finally, we identify a failure mode referred to as ``catastrophic overfitting'' which may have caused previous attempts to use FGSM adversarial training to fail. All code for reproducing the experiments in this paper as well as pretrained model weights are at https://github.com/locuslab/fast\_adversarial.},
  archivePrefix = {arXiv},
  eprint = {2001.03994},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/MGKT6D3S/Wong et al. - 2020 - Fast is better than free Revisiting adversarial t.pdf},
  journal = {arXiv:2001.03994 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wongpanich20_TrainingEfficientNetsSupercomputer,
  title = {Training {{EfficientNets}} at {{Supercomputer Scale}}: 83\% {{ImageNet Top}}-1 {{Accuracy}} in {{One Hour}}},
  shorttitle = {Training {{EfficientNets}} at {{Supercomputer Scale}}},
  author = {Wongpanich, Arissa and Pham, Hieu and Demmel, James and Tan, Mingxing and Le, Quoc and You, Yang and Kumar, Sameer},
  year = {2020},
  month = nov,
  abstract = {EfficientNets are a family of state-of-the-art image classification models based on efficiently scaled convolutional neural networks. Currently, EfficientNets can take on the order of days to train; for example, training an EfficientNet-B0 model takes 23 hours on a Cloud TPU v2-8 node. In this paper, we explore techniques to scale up the training of EfficientNets on TPU-v3 Pods with 2048 cores, motivated by speedups that can be achieved when training at such scales. We discuss optimizations required to scale training to a batch size of 65536 on 1024 TPU-v3 cores, such as selecting large batch optimizers and learning rate schedules as well as utilizing distributed evaluation and batch normalization techniques. Additionally, we present timing and performance benchmarks for EfficientNet models trained on the ImageNet dataset in order to analyze the behavior of EfficientNets at scale. With our optimizations, we are able to train EfficientNet on ImageNet to an accuracy of 83\% in 1 hour and 4 minutes.},
  archivePrefix = {arXiv},
  eprint = {2011.00071},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wongpanich et al_2020_training efficientnets at supercomputer scale.pdf},
  journal = {arXiv:2011.00071 [cs]},
  keywords = {favorite},
  primaryClass = {cs}
}

@article{wu00_Estimatingclinicalseverity,
  title = {Estimating Clinical Severity of {{COVID}}-19 from the Transmission Dynamics in {{Wuhan}}, {{China}}},
  author = {Wu, Joseph T},
  pages = {13},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/wu_estimating clinical severity of covid-19 from the transmission dynamics in wuhan, china.pdf},
  journal = {Nature Medicine},
  language = {en}
}

@inproceedings{wu16_QuantizedConvolutionalNeural,
  title = {Quantized {{Convolutional Neural Networks}} for {{Mobile Devices}}},
  booktitle = {2016 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Wu, Jiaxiang and Leng, Cong and Wang, Yuhang and Hu, Qinghao and Cheng, Jian},
  year = {2016},
  month = jun,
  pages = {4820--4828},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.521},
  abstract = {Recently, convolutional neural networks (CNN) have demonstrated impressive performance in various computer vision tasks. However, high performance hardware is typically indispensable for the application of CNN models due to the high computation complexity, which prohibits their further extensions. In this paper, we propose an efficient framework, namely Quantized CNN, to simultaneously speed-up the computation and reduce the storage and memory overhead of CNN models. Both filter kernels in convolutional layers and weighting matrices in fully-connected layers are quantized, aiming at minimizing the estimation error of each layer's response. Extensive experiments on the ILSVRC-12 benchmark demonstrate 4 {$\sim$} 6\texttimes{} speed-up and 15 {$\sim$} 20\texttimes{} compression with merely one percentage loss of classification accuracy. With our quantized CNN model, even mobile devices can accurately classify images within one second.},
  annotation = {ZSCC: 0000384},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2016_quantized convolutional neural networks for mobile devices.pdf},
  isbn = {978-1-4673-8851-1},
  language = {en}
}

@article{wu17_QuantitativeAnalysisDecoderBased,
  title = {On the {{Quantitative Analysis}} of {{Decoder}}-{{Based Generative Models}}},
  author = {Wu, Yuhuai and Burda, Yuri and Salakhutdinov, Ruslan and Grosse, Roger},
  year = {2017},
  pages = {17},
  abstract = {The past several years have seen remarkable progress in generative models which produce convincing samples of images and other modalities. A shared component of many powerful generative models is a decoder network, a parametric deep neural net that defines a generative distribution. Examples include variational autoencoders, generative adversarial networks, and generative moment matching networks. Unfortunately, it can be difficult to quantify the performance of these models because of the intractability of log-likelihood estimation, and inspecting samples can be misleading. We propose to use Annealed Importance Sampling for evaluating log-likelihoods for decoder-based models and validate its accuracy using bidirectional Monte Carlo. The evaluation code is provided at https:// github.com/tonywu95/eval\_gen. Using this technique, we analyze the performance of decoder-based models, the effectiveness of existing log-likelihood estimators, the degree of overfitting, and the degree to which these models miss important modes of the data distribution.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/L7TUT7CD/Wu et al. - 2017 - ON THE QUANTITATIVE ANALYSIS OF DECODER- BASED GEN.pdf},
  keywords = {disentanglement},
  language = {en}
}

@article{wu18_MoleculeNetBenchmarkMolecular,
  title = {{{MoleculeNet}}: {{A Benchmark}} for {{Molecular Machine Learning}}},
  shorttitle = {{{MoleculeNet}}},
  author = {Wu, Zhenqin and Ramsundar, Bharath and Feinberg, Evan N. and Gomes, Joseph and Geniesse, Caleb and Pappu, Aneesh S. and Leswing, Karl and Pande, Vijay},
  year = {2018},
  month = oct,
  abstract = {Molecular machine learning has been maturing rapidly over the last few years. Improved methods and the presence of larger datasets have enabled machine learning algorithms to make increasingly accurate predictions about molecular properties. However, algorithmic progress has been limited due to the lack of a standard benchmark to compare the efficacy of proposed methods; most new algorithms are benchmarked on different datasets making it challenging to gauge the quality of proposed methods. This work introduces MoleculeNet, a large scale benchmark for molecular machine learning. MoleculeNet curates multiple public datasets, establishes metrics for evaluation, and offers high quality open-source implementations of multiple previously proposed molecular featurization and learning algorithms (released as part of the DeepChem open source library). MoleculeNet benchmarks demonstrate that learnable representations are powerful tools for molecular machine learning and broadly offer the best performance. However, this result comes with caveats. Learnable representations still struggle to deal with complex tasks under data scarcity and highly imbalanced classification. For quantum mechanical and biophysical datasets, the use of physics-aware featurizations can be more important than choice of particular learning algorithm.},
  archivePrefix = {arXiv},
  eprint = {1703.00564},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2018_moleculenet.pdf},
  journal = {arXiv:1703.00564 [physics, stat]},
  language = {en},
  primaryClass = {physics, stat}
}

@article{wu19_DeterministicVariationalInference,
  title = {Deterministic {{Variational Inference}} for {{Robust Bayesian Neural Networks}}},
  author = {Wu, Anqi and Nowozin, Sebastian and Meeds, Edward and Turner, Richard E and {Hernandez-Lobato}, Jose Miguel and Gaunt, Alexander L},
  year = {2019},
  pages = {24},
  abstract = {Bayesian neural networks (BNNs) hold great promise as a flexible and principled solution to deal with uncertainty when learning from finite data. Among approaches to realize probabilistic inference in deep neural networks, variational Bayes (VB) is theoretically grounded, generally applicable, and computationally efficient. With wide recognition of potential advantages, why is it that variational Bayes has seen very limited practical use for BNNs in real applications? We argue that variational inference in neural networks is fragile: successful implementations require careful initialization and tuning of prior variances, as well as controlling the variance of Monte Carlo gradient estimates. We provide two innovations that aim to turn VB into a robust inference tool for Bayesian neural networks: first, we introduce a novel deterministic method to approximate moments in neural networks, eliminating gradient variance; second, we introduce a hierarchical prior for parameters and a novel Empirical Bayes procedure for automatically selecting prior variances. Combining these two innovations, the resulting method is highly efficient and robust. On the application of heteroscedastic regression we demonstrate good predictive performance over alternative approaches.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2019_deterministic variational inference for robust bayesian neural networks.pdf},
  language = {en}
}

@article{wu19_LearnabilityInformationBottleneck,
  title = {Learnability for the {{Information Bottleneck}}},
  author = {Wu, Tailin and Fischer, Ian and Chuang, Isaac L. and Tegmark, Max},
  year = {2019},
  volume = {21},
  issn = {1099-4300},
  doi = {10.3390/e21100924},
  abstract = {The Information Bottleneck (IB) method provides an insightful and principled approach for balancing compression and prediction for representation learning. The IB objective     I ( X ; Z ) \&minus; \&beta; I ( Y ; Z )     employs a Lagrange multiplier    \&beta;    to tune this trade-off. However, in practice, not only is    \&beta;    chosen empirically without theoretical guidance, there is also a lack of theoretical understanding between    \&beta;   , learnability, the intrinsic nature of the dataset and model capacity. In this paper, we show that if    \&beta;    is improperly chosen, learning cannot happen\&mdash;the trivial representation     P ( Z | X ) = P ( Z )     becomes the global minimum of the IB objective. We show how this can be avoided, by identifying a sharp phase transition between the unlearnable and the learnable which arises as    \&beta;    is varied. This phase transition defines the concept of IB-Learnability. We prove several sufficient conditions for IB-Learnability, which provides theoretical guidance for choosing a good    \&beta;   . We further show that IB-learnability is determined by the largest confident, typical and imbalanced subset of the examples (the conspicuous subset), and discuss its relation with model capacity. We give practical algorithms to estimate the minimum    \&beta;    for a given dataset. We also empirically demonstrate our theoretical conditions with analyses of synthetic datasets, MNIST and CIFAR10.},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2019_learnability for the information bottleneck.pdf},
  journal = {Entropy},
  keywords = {conspicuous subset,information,information bottleneck,learnability,representation learning},
  number = {10}
}

@article{wu19_MetaAmortizedVariationalInference,
  title = {Meta-{{Amortized Variational Inference}} and {{Learning}}},
  author = {Wu, Mike and Choi, Kristy and Goodman, Noah and Ermon, Stefano},
  year = {2019},
  month = sep,
  abstract = {Despite the recent success in probabilistic modeling and their applications, generative models trained using traditional inference techniques struggle to adapt to new distributions, even when the target distribution may be closely related to the ones seen during training. In this work, we present a doublyamortized variational inference procedure as a way to address this challenge. By sharing computation across not only a set of query inputs, but also a set of different, related probabilistic models, we learn transferable latent representations that generalize across several related distributions. In particular, given a set of distributions over images, we find the learned representations to transfer to different data transformations. We empirically demonstrate the effectiveness of our method by introducing the MetaVAE, and show that it significantly outperforms baselines on downstream image classification tasks on MNIST (10-50\%) and NORB (10-35\%).},
  archivePrefix = {arXiv},
  eprint = {1902.01950},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2019_meta-amortized variational inference and learning.pdf},
  journal = {arXiv:1902.01950 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wu19_PayLessAttention,
  title = {Pay {{Less Attention}} with {{Lightweight}} and {{Dynamic Convolutions}}},
  author = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann N. and Auli, Michael},
  year = {2019},
  month = jan,
  abstract = {Self-attention is a useful mechanism to build generative models for language and images. It determines the importance of context elements by comparing each element to the current time step. In this paper, we show that a very lightweight convolution can perform competitively to the best reported self-attention results. Next, we introduce dynamic convolutions which are simpler and more efficient than self-attention. We predict separate convolution kernels based solely on the current time-step in order to determine the importance of context elements. The number of operations required by this approach scales linearly in the input length, whereas self-attention is quadratic. Experiments on large-scale machine translation, language modeling and abstractive summarization show that dynamic convolutions improve over strong self-attention models. On the WMT'14 English-German test set dynamic convolutions achieve a new state of the art of 29.7 BLEU.},
  annotation = {ZSCC: 0000033},
  archivePrefix = {arXiv},
  eprint = {1901.10430},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2019_pay less attention with lightweight and dynamic convolutions.pdf;/home/trung/Zotero/storage/AKRJM752/1901.html},
  journal = {arXiv:1901.10430 [cs]},
  keywords = {attention,Computer Science - Computation and Language},
  primaryClass = {cs}
}

@article{wu20_ComprehensiveSurveyGraph,
  title = {A {{Comprehensive Survey}} on {{Graph Neural Networks}}},
  author = {Wu, Zonghan and Pan, Shirui and Chen, Fengwen and Long, Guodong and Zhang, Chengqi and Yu, Philip S.},
  year = {2020},
  pages = {1--21},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2020.2978386},
  abstract = {Deep learning has revolutionized many machine learning tasks in recent years, ranging from image classification and video processing to speech recognition and natural language understanding. The data in these tasks are typically represented in the Euclidean space. However, there is an increasing number of applications where data are generated from non-Euclidean domains and are represented as graphs with complex relationships and interdependency between objects. The complexity of graph data has imposed significant challenges on existing machine learning algorithms. Recently, many studies on extending deep learning approaches for graph data have emerged. In this survey, we provide a comprehensive overview of graph neural networks (GNNs) in data mining and machine learning fields. We propose a new taxonomy to divide the state-of-the-art graph neural networks into four categories, namely recurrent graph neural networks, convolutional graph neural networks, graph autoencoders, and spatial-temporal graph neural networks. We further discuss the applications of graph neural networks across various domains and summarize the open source codes, benchmark data sets, and model evaluation of graph neural networks. Finally, we propose potential research directions in this rapidly growing field.},
  annotation = {ZSCC: 0000238},
  archivePrefix = {arXiv},
  eprint = {1901.00596},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/TAXUBCNM/Wu et al. - 2020 - A Comprehensive Survey on Graph Neural Networks.pdf},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  language = {en}
}

@article{wu20_EncoderDecoderIncompatibilityVariational,
  title = {On the {{Encoder}}-{{Decoder Incompatibility}} in {{Variational Text Modeling}} and {{Beyond}}},
  author = {Wu, Chen and Wang, Prince Zizhuang and Wang, William Yang},
  year = {2020},
  month = apr,
  abstract = {Variational autoencoders (VAEs) combine latent variables with amortized variational inference, whose optimization usually converges into a trivial local optimum termed posterior collapse, especially in text modeling. By tracking the optimization dynamics, we observe the encoder-decoder incompatibility that leads to poor parameterizations of the data manifold. We argue that the trivial local optimum may be avoided by improving the encoder and decoder parameterizations since the posterior network is part of a transition map between them. To this end, we propose Coupled-VAE, which couples a VAE model with a deterministic autoencoder with the same structure and improves the encoder and decoder parameterizations via encoder weight sharing and decoder signal matching. We apply the proposed Coupled-VAE approach to various VAE models with different regularization, posterior family, decoder structure, and optimization strategy. Experiments on benchmark datasets (i.e., PTB, Yelp, and Yahoo) show consistently improved results in terms of probability estimation and richness of the latent space. We also generalize our method to conditional language modeling and propose Coupled-CVAE, which largely improves the diversity of dialogue generation on the Switchboard dataset.},
  archivePrefix = {arXiv},
  eprint = {2004.09189},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2020_on the encoder-decoder incompatibility in variational text modeling and beyond.pdf},
  journal = {arXiv:2004.09189 [cs, stat]},
  keywords = {vae_issues},
  primaryClass = {cs, stat}
}

@techreport{wu20_G2S3genegraphbased,
  title = {{{G2S3}}: A Gene Graph-Based Imputation Method for Single-Cell {{RNA}} Sequencing Data},
  shorttitle = {{{G2S3}}},
  author = {Wu, Weimiao and Dai, Qile and Liu, Yunqing and Yan, Xiting and Wang, Zuoheng},
  year = {2020},
  month = apr,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.04.01.020586},
  abstract = {Single-cell RNA sequencing provides an opportunity to study gene expression at single-cell resolution. However, prevalent dropout events result in high data sparsity and noise that may obscure downstream analyses. We propose a novel method, G2S3, that imputes dropouts by borrowing information from adjacent genes in a sparse gene graph learned from gene expression profiles across cells. We applied G2S3 and other existing methods to seven single-cell datasets to compare their performance. Our results demonstrated that G2S3 is superior in recovering true expression levels, identifying cell subtypes, improving differential expression analyses, and recovering gene regulatory relationships, especially for mildly expressed genes.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/X2RLURAH/Wu et al. - 2020 - G2S3 a gene graph-based imputation method for sin.pdf},
  language = {en},
  type = {Preprint}
}

@article{wu20_ImprovingRobustnessGenerality,
  title = {Improving {{Robustness}} and {{Generality}} of {{NLP Models Using Disentangled Representations}}},
  author = {Wu, Jiawei and Li, Xiaoya and Ao, Xiang and Meng, Yuxian and Wu, Fei and Li, Jiwei},
  year = {2020},
  month = sep,
  abstract = {Supervised neural networks, which first map an input \$x\$ to a single representation \$z\$, and then map \$z\$ to the output label \$y\$, have achieved remarkable success in a wide range of natural language processing (NLP) tasks. Despite their success, neural models lack for both robustness and generality: small perturbations to inputs can result in absolutely different outputs; the performance of a model trained on one domain drops drastically when tested on another domain. In this paper, we present methods to improve robustness and generality of NLP models from the standpoint of disentangled representation learning. Instead of mapping \$x\$ to a single representation \$z\$, the proposed strategy maps \$x\$ to a set of representations \$\textbackslash\{z\_1,z\_2,...,z\_K\textbackslash\}\$ while forcing them to be disentangled. These representations are then mapped to different logits \$l\$s, the ensemble of which is used to make the final prediction \$y\$. We propose different methods to incorporate this idea into currently widely-used models, including adding an \$L\$2 regularizer on \$z\$s or adding Total Correlation (TC) under the framework of variational information bottleneck (VIB). We show that models trained with the proposed criteria provide better robustness and domain adaptation ability in a wide range of supervised learning tasks.},
  archivePrefix = {arXiv},
  eprint = {2009.09587},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2020_improving robustness and generality of nlp models using disentangled representations.pdf},
  journal = {arXiv:2009.09587 [cs]},
  keywords = {information},
  primaryClass = {cs}
}

@article{wu20_MemformerMemoryAugmentedTransformer,
  title = {Memformer: {{The Memory}}-{{Augmented Transformer}}},
  shorttitle = {Memformer},
  author = {Wu, Qingyang and Lan, Zhenzhong and Gu, Jing and Yu, Zhou},
  year = {2020},
  month = oct,
  abstract = {Transformer models have obtained remarkable accomplishments in various NLP tasks. However, these models have efficiency issues on long sequences, as the complexity of their self-attention module scales quadratically with the sequence length. To remedy the limitation, we present Memformer, a novel language model that utilizes a single unified memory to encode and retrieve past information. It includes a new optimization scheme, Memory Replay Back-Propagation, which promotes long-range back-propagation through time with a significantly reduced memory requirement. Memformer achieves \$\textbackslash mathcal\{O\}(n)\$ time complexity and \$\textbackslash mathcal\{O\}(1)\$ space complexity in processing long sequences, meaning that the model can handle an infinite length sequence during inference. Our model is also compatible with other self-supervised tasks to further improve the performance on language modeling. Experimental results show that Memformer outperforms the previous long-range sequence models on WikiText-103, including Transformer-XL and compressive Transformer.},
  archivePrefix = {arXiv},
  eprint = {2010.06891},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2020_memformer.pdf},
  journal = {arXiv:2010.06891 [cs]},
  primaryClass = {cs}
}

@inproceedings{wu20_NeuralMixedCounting,
  title = {Neural {{Mixed Counting Models}} for {{Dispersed Topic Discovery}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Jiemin and Rao, Yanghui and Zhang, Zusheng and Xie, Haoran and Li, Qing and Wang, Fu Lee and Chen, Ziye},
  year = {2020},
  pages = {6159--6169},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.548},
  abstract = {Mixed counting models that use the negative binomial distribution as the prior can well model over-dispersed and hierarchically dependent random variables; thus they have attracted much attention in mining dispersed document topics. However, the existing parameter inference method like Monte Carlo sampling is quite time-consuming. In this paper, we propose two efficient neural mixed counting models, i.e., the Negative BinomialNeural Topic Model (NB-NTM) and the Gamma Negative Binomial-Neural Topic Model (GNB-NTM) for dispersed topic discovery. Neural variational inference algorithms are developed to infer model parameters by using the reparameterization of Gamma distribution and the Gaussian approximation of Poisson distribution. Experiments on real-world datasets indicate that our models outperform state-of-theart baseline models in terms of perplexity and topic coherence. The results also validate that both NB-NTM and GNB-NTM can produce explainable intermediate variables by generating dispersed proportions of document topics.},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2020_neural mixed counting models for dispersed topic discovery.pdf},
  language = {en}
}

@article{wu20_QuantizationBasedRegularizationAutoencoders,
  title = {Quantization-{{Based Regularization}} for {{Autoencoders}}},
  author = {Wu, Hanwei and Flierl, Markus},
  year = {2020},
  month = jan,
  abstract = {Autoencoders and their variations provide unsupervised models for learning low-dimensional representations for downstream tasks. Without proper regularization, autoencoder models are susceptible to the overfitting problem and the so-called posterior collapse phenomenon. In this paper, we introduce a quantization-based regularizer in the bottleneck stage of autoencoder models to learn meaningful latent representations. We combine both perspectives of Vector Quantized-Variational AutoEncoders (VQ-VAE) and classical denoising regularization methods of neural networks. We interpret quantizers as regularizers that constrain latent representations while fostering a similarity-preserving mapping at the encoder. Before quantization, we impose noise on the latent codes and use a Bayesian estimator to optimize the quantizer-based representation. The introduced bottleneck Bayesian estimator outputs the posterior mean of the centroids to the decoder, and thus, is performing soft quantization of the noisy latent codes. We show that our proposed regularization method results in improved latent representations for both supervised learning and clustering downstream tasks when compared to autoencoders using other bottleneck structures.},
  archivePrefix = {arXiv},
  eprint = {1905.11062},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wu et al_2020_quantization-based regularization for autoencoders.pdf},
  journal = {arXiv:1905.11062 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{wulfmeier20_RepresentationMattersImproving,
  title = {Representation {{Matters}}: {{Improving Perception}} and {{Exploration}} for {{Robotics}}},
  shorttitle = {Representation {{Matters}}},
  author = {Wulfmeier, Markus and Byravan, Arunkumar and Hertweck, Tim and Higgins, Irina and Gupta, Ankush and Kulkarni, Tejas and Reynolds, Malcolm and Teplyashin, Denis and Hafner, Roland and Lampe, Thomas and Riedmiller, Martin},
  year = {2020},
  month = nov,
  abstract = {Projecting high-dimensional environment observations into lower-dimensional structured representations can considerably improve data-efficiency for reinforcement learning in domains with limited data such as robotics. Can a single generally useful representation be found? In order to answer this question, it is important to understand how the representation will be used by the agent and what properties such a 'good' representation should have. In this paper we systematically evaluate a number of common learnt and hand-engineered representations in the context of three robotics tasks: lifting, stacking and pushing of 3D blocks. The representations are evaluated in two use-cases: as input to the agent, or as a source of auxiliary tasks. Furthermore, the value of each representation is evaluated in terms of three properties: dimensionality, observability and disentanglement. We can significantly improve performance in both use-cases and demonstrate that some representations can perform commensurate to simulator states as agent inputs. Finally, our results challenge common intuitions by demonstrating that: 1) dimensionality strongly matters for task generation, but is negligible for inputs, 2) observability of task-relevant aspects mostly affects the input representation use-case, and 3) disentanglement leads to better auxiliary tasks, but has only limited benefits for input representations. This work serves as a step towards a more systematic understanding of what makes a 'good' representation for control in robotics, enabling practitioners to make more informed choices for developing new learned or hand-engineered representations.},
  archivePrefix = {arXiv},
  eprint = {2011.01758},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/wulfmeier et al_2020_representation matters.pdf},
  journal = {arXiv:2011.01758 [cs, stat]},
  keywords = {_tablet,favorite,representation},
  primaryClass = {cs, stat}
}

@article{xiao18_DirichletVariationalAutoencoder,
  title = {Dirichlet {{Variational Autoencoder}} for {{Text Modeling}}},
  author = {Xiao, Yijun and Zhao, Tiancheng and Wang, William Yang},
  year = {2018},
  month = oct,
  abstract = {We introduce an improved variational autoencoder (VAE) for text modeling with topic information explicitly modeled as a Dirichlet latent variable. By providing the proposed model topic awareness, it is more superior at reconstructing input texts. Furthermore, due to the inherent interactions between the newly introduced Dirichlet variable and the conventional multivariate Gaussian variable, the model is less prone to KL divergence vanishing. We derive the variational lower bound for the new model and conduct experiments on four different data sets. The results show that the proposed model is superior at text reconstruction across the latent space and classifications on learned representations have higher test accuracies.},
  archivePrefix = {arXiv},
  eprint = {1811.00135},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/2018/false;/home/trung/Zotero/storage/9ZG7L98J/1811.html},
  journal = {arXiv:1811.00135 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,dirichlet,discrete,variational},
  primaryClass = {cs}
}

@article{xiao19_DisentangledRepresentationLearning,
  title = {Disentangled {{Representation Learning}} with {{Wasserstein Total Correlation}}},
  author = {Xiao, Yijun and Wang, William Yang},
  year = {2019},
  month = dec,
  abstract = {Unsupervised learning of disentangled representations involves uncovering of different factors of variations that contribute to the data generation process. Total correlation penalization has been a key component in recent methods towards disentanglement. However, KullbackLeibler (KL) divergence-based total correlation is metric-agnostic and sensitive to data samples. In this paper, we introduce Wasserstein total correlation in both variational autoencoder and Wasserstein autoencoder settings to learn disentangled latent representations. A critic is adversarially trained along with the main objective to estimate the Wasserstein total correlation term. We discuss the benefits of using Wasserstein distance over KL divergence to measure independence and conduct quantitative and qualitative experiments on several data sets. Moreover, we introduce a new metric to measure disentanglement. We show that the proposed approach has comparable performances on disentanglement with smaller sacrifices in reconstruction abilities.},
  archivePrefix = {arXiv},
  eprint = {1912.12818},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2019_disentangled representation learning with wasserstein total correlation.pdf},
  journal = {arXiv:1912.12818 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{xiao19_DisentangledRepresentationLearninga,
  title = {Disentangled {{Representation Learning}} with {{Wasserstein Total Correlation}}},
  author = {Xiao, Yijun and Wang, William Yang},
  year = {2019},
  month = dec,
  abstract = {Unsupervised learning of disentangled representations involves uncovering of different factors of variations that contribute to the data generation process. Total correlation penalization has been a key component in recent methods towards disentanglement. However, Kullback-Leibler (KL) divergence-based total correlation is metric-agnostic and sensitive to data samples. In this paper, we introduce Wasserstein total correlation in both variational autoencoder and Wasserstein autoencoder settings to learn disentangled latent representations. A critic is adversarially trained along with the main objective to estimate the Wasserstein total correlation term. We discuss the benefits of using Wasserstein distance over KL divergence to measure independence and conduct quantitative and qualitative experiments on several data sets. Moreover, we introduce a new metric to measure disentanglement. We show that the proposed approach has comparable performances on disentanglement with smaller sacrifices in reconstruction abilities.},
  archivePrefix = {arXiv},
  eprint = {1912.12818},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2019_disentangled representation learning with wasserstein total correlation2.pdf},
  journal = {arXiv:1912.12818 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{xiao19_GenerativeLatentFlow,
  title = {Generative {{Latent Flow}}},
  author = {Xiao, Zhisheng and Yan, Qing and Amit, Yali},
  year = {2019},
  month = sep,
  abstract = {In this work, we propose the Generative Latent Flow (GLF), an algorithm for generative modeling of the data distribution. GLF uses an Auto-encoder (AE) to learn latent representations of the data, and a normalizing flow to map the distribution of the latent variables to that of simple i.i.d noise. In contrast to some other Auto-encoder based generative models, which use various regularizers that encourage the encoded latent distribution to match the prior distribution, our model explicitly constructs a mapping between these two distributions, leading to better density matching while avoiding over regularizing the latent variables. We compare our model with several related techniques, and show that it has many relative advantages including fast convergence, single stage training and minimal reconstruction trade-off. We also study the relationship between our model and its stochastic counterpart, and show that our model can be viewed as a vanishing noise limit of VAEs with flow prior. Quantitatively, under standardized evaluations, our method achieves state-of-the-art sample quality among AE based models on commonly used datasets, and is competitive with GANs' benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1905.10485},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2019_generative latent flow.pdf},
  journal = {arXiv:1905.10485 [cs]},
  primaryClass = {cs}
}

@article{xiao19_MethodModelConditional,
  title = {A {{Method}} to {{Model Conditional Distributions}} with {{Normalizing Flows}}},
  author = {Xiao, Zhisheng and Yan, Qing and Amit, Yali},
  year = {2019},
  month = nov,
  abstract = {In this work, we investigate the use of normalizing flows to model conditional distributions. In particular, we use our proposed method to analyze inverse problems with invertible neural networks by maximizing the posterior likelihood. Our method uses only a single loss and is easy to train. This is an improvement on the previous method that solves similar inverse problems with invertible neural networks but which involves a combination of several loss terms with ad-hoc weighting. In addition, our method provides a natural framework to incorporate conditioning in normalizing flows, and therefore, we can train an invertible network to perform conditional generation. We analyze our method and perform a careful comparison with previous approaches. Simple experiments show the effectiveness of our method, and more comprehensive experimental evaluations are undergoing.},
  archivePrefix = {arXiv},
  eprint = {1911.02052},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2019_a method to model conditional distributions with normalizing flows.pdf},
  journal = {arXiv:1911.02052 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xiao20_DisentanglingTrainabilityGeneralization,
  title = {Disentangling {{Trainability}} and {{Generalization}} in {{Deep Neural Networks}}},
  author = {Xiao, Lechao and Pennington, Jeffrey and Schoenholz, Samuel S.},
  year = {2020},
  month = jul,
  abstract = {A longstanding goal in the theory of deep learning is to characterize the conditions under which a given neural network architecture will be trainable, and if so, how well it might generalize to unseen data. In this work, we provide such a characterization in the limit of very wide and very deep networks, for which the analysis simplifies considerably. For wide networks, the trajectory under gradient descent is governed by the Neural Tangent Kernel (NTK), and for deep networks the NTK itself maintains only weak data dependence. By analyzing the spectrum of the NTK, we formulate necessary conditions for trainability and generalization across a range of architectures, including Fully Connected Networks (FCNs) and Convolutional Neural Networks (CNNs). We identify large regions of hyperparameter space for which networks can memorize the training set but completely fail to generalize. We find that CNNs without global average pooling behave almost identically to FCNs, but that CNNs with pooling have markedly different and often better generalization performance. These theoretical results are corroborated experimentally on CIFAR10 for a variety of network architectures and we include a colab1 notebook that reproduces the essential results of the paper.},
  archivePrefix = {arXiv},
  eprint = {1912.13053},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2020_disentangling trainability and generalization in deep neural networks.pdf},
  journal = {arXiv:1912.13053 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{xiao20_LikelihoodRegretOutofDistribution,
  title = {Likelihood {{Regret}}: {{An Out}}-of-{{Distribution Detection Score For Variational Auto}}-Encoder},
  shorttitle = {Likelihood {{Regret}}},
  author = {Xiao, Zhisheng and Yan, Qing and Amit, Yali},
  year = {2020},
  month = apr,
  abstract = {Deep probabilistic generative models enable modeling the likelihoods of very high dimensional data. An important application of generative modeling should be the ability to detect out-of-distribution (OOD) samples by setting a threshold on the likelihood. However, a recent study shows that probabilistic generative models can, in some cases, assign higher likelihoods on certain types of OOD samples, making the OOD detection rules based on likelihood threshold problematic. To address this issue, several OOD detection methods have been proposed for deep generative models. In this paper, we make the observation that some of these methods fail when applied to generative models based on Variational Auto-encoders (VAE). As an alternative, we propose Likelihood Regret, an efficient OOD score for VAEs. We benchmark our proposed method over existing approaches, and empirical results suggest that our method obtains the best overall OOD detection performances compared with other OOD method applied on VAE.},
  archivePrefix = {arXiv},
  eprint = {2003.02977},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2020_likelihood regret.pdf},
  journal = {arXiv:2003.02977 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xiao20_VAEBMSymbiosisVariational,
  title = {{{VAEBM}}: {{A Symbiosis}} between {{Variational Autoencoders}} and {{Energy}}-Based {{Models}}},
  shorttitle = {{{VAEBM}}},
  author = {Xiao, Zhisheng and Kreis, Karsten and Kautz, Jan and Vahdat, Arash},
  year = {2020},
  month = oct,
  abstract = {Energy-based models (EBMs) have recently been successful in representing complex distributions of small images. However, sampling from them requires expensive Markov chain Monte Carlo (MCMC) iterations that mix slowly in high dimensional pixel space. Unlike EBMs, variational autoencoders (VAEs) generate samples quickly and are equipped with a latent space that enables fast traversal of the data manifold. However, VAEs tend to assign high probability density to regions in data space outside the actual data distribution and often fail at generating sharp images. In this paper, we propose VAEBM, a symbiotic composition of a VAE and an EBM that offers the best of both worlds. VAEBM captures the overall mode structure of the data distribution using a state-of-the-art VAE and it relies on its EBM component to explicitly exclude non-data-like regions from the model and refine the image samples. Moreover, the VAE component in VAEBM allows us to speed up MCMC updates by reparameterizing them in the VAE's latent space. Our experimental results show that VAEBM outperforms state-of-the-art VAEs and EBMs in generative quality on several benchmark image datasets by a large margin. It can generate high-quality images as large as 256\$\textbackslash times\$256 pixels with short MCMC chains. We also demonstrate that VAEBM provides complete mode coverage and performs well in out-of-distribution detection.},
  archivePrefix = {arXiv},
  eprint = {2010.00654},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2020_vaebm.pdf},
  journal = {arXiv:2010.00654 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xiao20_WhatShouldNot,
  title = {What {{Should Not Be Contrastive}} in {{Contrastive Learning}}},
  author = {Xiao, Tete and Wang, Xiaolong and Efros, Alexei A. and Darrell, Trevor},
  year = {2020},
  month = aug,
  abstract = {Recent self-supervised contrastive methods have been able to produce impressive transferable visual representations by learning to be invariant to different data augmentations. However, these methods implicitly assume a particular set of representational invariances (e.g., invariance to color), and can perform poorly when a downstream task violates this assumption (e.g., distinguishing red vs. yellow cars). We introduce a contrastive learning framework which does not require prior knowledge of specific, task-dependent invariances. Our model learns to capture varying and invariant factors for visual representations by constructing separate embedding spaces, each of which is invariant to all but one augmentation. We use a multi-head network with a shared backbone which captures information across each augmentation and alone outperforms all baselines on downstream tasks. We further find that the concatenation of the invariant and varying spaces performs best across all tasks we investigate, including coarse-grained, fine-grained, and few-shot downstream classification tasks, and various data corruptions.},
  archivePrefix = {arXiv},
  eprint = {2008.05659},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiao et al_2020_what should not be contrastive in contrastive learning.pdf},
  journal = {arXiv:2008.05659 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{xie16_UnsupervisedDeepEmbedding,
  title = {Unsupervised {{Deep Embedding}} for {{Clustering Analysis}}},
  author = {Xie, Junyuan and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  abstract = {Clustering is central to many data-driven application domains and has been studied extensively in terms of distance functions and grouping algorithms. Relatively little work has focused on learning representations for clustering. In this paper, we propose Deep Embedded Clustering (DEC), a method that simultaneously learns feature representations and cluster assignments using deep neural networks. DEC learns a mapping from the data space to a lower-dimensional feature space in which it iteratively optimizes a clustering objective. Our experimental evaluations on image and text corpora show significant improvement over state-of-the-art methods.},
  archivePrefix = {arXiv},
  eprint = {1511.06335},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xie et al_2016_unsupervised deep embedding for clustering analysis.pdf},
  journal = {arXiv:1511.06335 [cs]},
  keywords = {self-supervised},
  primaryClass = {cs}
}

@article{xie19_AdversarialExamplesImprove,
  title = {Adversarial {{Examples Improve Image Recognition}}},
  author = {Xie, Cihang and Tan, Mingxing and Gong, Boqing and Wang, Jiang and Yuille, Alan and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Adversarial examples are commonly viewed as a threat to ConvNets. Here we present an opposite perspective: adversarial examples can be used to improve image recognition models if harnessed in the right manner. We propose AdvProp, an enhanced adversarial training scheme which treats adversarial examples as additional examples, to prevent overfitting. Key to our method is the usage of a separate auxiliary batch norm for adversarial examples, as they have different underlying distributions to normal examples. We show that AdvProp improves a wide range of models on various image recognition tasks and performs better when the models are bigger. For instance, by applying AdvProp to the latest EfficientNet-B7 [28] on ImageNet, we achieve significant improvements on ImageNet (+0.7\%), ImageNet-C (+6.5\%), ImageNet-A (+7.0\%), Stylized-ImageNet (+4.8\%). With an enhanced EfficientNet-B8, our method achieves the state-of-the-art 85.5\% ImageNet top-1 accuracy without extra data. This result even surpasses the best model in [20] which is trained with 3.5B Instagram images (\textasciitilde 3000X more than ImageNet) and \textasciitilde 9.4X more parameters. Models are available at https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1911.09665},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xie et al_2019_adversarial examples improve image recognition.pdf;/home/trung/Zotero/storage/PKRXDS66/1911.html},
  journal = {arXiv:1911.09665 [cs]},
  keywords = {adversarial,batch normalization,Computer Science - Computer Vision and Pattern Recognition,data augmentation},
  primaryClass = {cs}
}

@article{xie19_RegulationElongationPhase,
  title = {Regulation of the {{Elongation Phase}} of {{Protein Synthesis Enhances Translation Accuracy}} and {{Modulates Lifespan}}},
  author = {Xie, Jianling and {de Souza Alves}, Viviane and {von der Haar}, Tobias and O'Keefe, Louise and Lenchine, Roman V. and Jensen, Kirk B. and Liu, Rui and Coldwell, Mark J. and Wang, Xuemin and Proud, Christopher G.},
  year = {2019},
  month = mar,
  volume = {29},
  pages = {737-749.e5},
  issn = {09609822},
  doi = {10.1016/j.cub.2019.01.029},
  file = {/home/trung/Zotero/storage/4M2LNN2M/Xie et al. - 2019 - Regulation of the Elongation Phase of Protein Synt.pdf},
  journal = {Current Biology},
  language = {en},
  number = {5}
}

@article{xie19_ReparameterizableSubsetSampling,
  title = {Reparameterizable {{Subset Sampling}} via {{Continuous Relaxations}}},
  author = {Xie, Sang Michael and Ermon, Stefano},
  year = {2019},
  month = jul,
  abstract = {Many machine learning tasks require sampling a subset of items from a collection based on a parameterized distribution. The Gumbel-softmax trick can be used to sample a single item, and allows for low-variance reparameterized gradients with respect to the parameters of the underlying distribution. However, stochastic optimization involving subset sampling is typically not reparameterizable. To overcome this limitation, we define a continuous relaxation of subset sampling that provides reparameterization gradients by generalizing the Gumbel-max trick. We use this approach to sample subsets of features in an instance-wise feature selection task for model interpretability, subsets of neighbors to implement a deep stochastic k-nearest neighbors model, and sub-sequences of neighbors to implement parametric t-SNE by directly comparing the identities of local neighbors. We improve performance in all these tasks by incorporating subset sampling in end-to-end training.},
  archivePrefix = {arXiv},
  eprint = {1901.10517},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xie et al_2019_reparameterizable subset sampling via continuous relaxations.pdf},
  journal = {arXiv:1901.10517 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xie19_SelftrainingNoisyStudent,
  title = {Self-Training with {{Noisy Student}} Improves {{ImageNet}} Classification},
  author = {Xie, Qizhe and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {We present a simple self-training method that achieves 87.4\% top-1 accuracy on ImageNet, which is 1.0\% better than the state-of-the-art model that requires 3.5B weakly labeled Instagram images. On robustness test sets, it improves ImageNet-A top-1 accuracy from 16.6\% to 74.2\%, reduces ImageNet-C mean corruption error from 45.7 to 31.2, and reduces ImageNet-P mean flip rate from 27.8 to 16.1. To achieve this result, we first train an EfficientNet model on labeled ImageNet images and use it as a teacher to generate pseudo labels on 300M unlabeled images. We then train a larger EfficientNet as a student model on the combination of labeled and pseudo labeled images. We iterate this process by putting back the student as the teacher. During the generation of the pseudo labels, the teacher is not noised so that the pseudo labels are as good as possible. But during the learning of the student, we inject noise such as data augmentation, dropout, stochastic depth to the student so that the noised student is forced to learn harder from the pseudo labels.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.04252},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xie et al_2019_self-training with noisy student improves imagenet classification.pdf},
  journal = {arXiv:1911.04252 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,pseudolabel,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xie19_UnsupervisedDataAugmentation,
  title = {Unsupervised {{Data Augmentation}}},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  year = {2019},
  month = apr,
  abstract = {Despite its success, deep learning still needs large labeled datasets to succeed. Data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA outperforms the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN with 1,000 examples, UDA outperforms all previous approaches and reduces more than \$30\textbackslash\%\$ of the error rates of state-of-the-art methods: going from 7.66\% to 5.27\% and from 3.53\% to 2.46\% respectively. UDA also works well on datasets that have a lot of labeled data. For example, on ImageNet, with 1.3M extra unlabeled data, UDA improves the top-1/top-5 accuracy from 78.28/94.36\% to 79.04/94.45\% when compared to AutoAugment.},
  archivePrefix = {arXiv},
  eprint = {1904.12848},
  eprinttype = {arxiv},
  journal = {arXiv:1904.12848 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xie19_UnsupervisedDataAugmentationa,
  title = {Unsupervised {{Data Augmentation}} for {{Consistency Training}}},
  author = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  year = {2019},
  month = sep,
  abstract = {Semi-supervised learning lately has shown much promise in improving deep learning models when labeled data is scarce. Common among recent approaches is the use of consistency training on a large amount of unlabeled data to constrain model predictions to be invariant to input noise. In this work, we present a new perspective on how to effectively noise unlabeled examples and argue that the quality of noising, specifically those produced by advanced data augmentation methods, plays a crucial role in semi-supervised learning. By substituting simple noising operations with advanced data augmentation methods, our method brings substantial improvements across six language and three vision tasks under the same consistency training framework. On the IMDb text classification dataset, with only 20 labeled examples, our method achieves an error rate of 4.20, outperforming the state-of-the-art model trained on 25,000 labeled examples. On a standard semi-supervised learning benchmark, CIFAR-10, our method outperforms all previous approaches and achieves an error rate of 2.7\% with only 4,000 examples, nearly matching the performance of models trained on 50,000 labeled examples. Our method also combines well with transfer learning, e.g., when finetuning from BERT, and yields improvements in high-data regime, such as ImageNet, whether when there is only 10\% labeled data or when a full labeled set with 1.3M extra unlabeled examples is used. Code is available at https://github.com/google-research/uda.},
  archivePrefix = {arXiv},
  eprint = {1904.12848},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xie et al_2019_unsupervised data augmentation for consistency training.pdf;/home/trung/Zotero/storage/L7GKUDR9/1904.html},
  journal = {arXiv:1904.12848 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xie20_MixtureSpeakertypePLDAs,
  title = {Mixture of {{Speaker}}-Type {{PLDAs}} for {{Children}}'s {{Speech Diarization}}},
  author = {Xie, Jiamin and Sia, Suzanna and Garcia, Paola and Povey, Daniel and Khudanpur, Sanjeev},
  year = {2020},
  month = aug,
  abstract = {In diarization, the PLDA is typically used to model an inference structure which assumes the variation in speech segments be induced by various speakers. The speaker variation is then learned from the training data. However, human perception can differentiate speakers by age, gender, among other characteristics. In this paper, we investigate a speaker-type informed model that explicitly captures the known variation of speakers. We explore a mixture of three PLDA models, where each model represents an adult female, male, or child category. The weighting of each model is decided by the prior probability of its respective class, which we study. The evaluation is performed on a subset of the BabyTrain corpus. We examine the expected performance gain using the oracle speaker type labels, which yields an 11.7\% DER reduction. We introduce a novel baby vocalization augmentation technique and then compare the mixture model to the single model. Our experimental result shows an effective 0.9\% DER reduction obtained by adding vocalizations. We discover empirically that a balanced dataset is important to train the mixture PLDA model, which outperforms the single PLDA by 1.3\% using the same training data and achieving a 35.8\% DER. The same setup improves over a standard baseline by 2.8\% DER.},
  archivePrefix = {arXiv},
  eprint = {2008.13213},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xie et al_2020_mixture of speaker-type pldas for children's speech diarization.pdf},
  journal = {arXiv:2008.13213 [eess]},
  primaryClass = {eess}
}

@article{xie20_scAIDEclusteringlargescale,
  title = {{{scAIDE}}: Clustering of Large-Scale Single-Cell {{RNA}}-Seq Data Reveals Putative and Rare Cell Types},
  author = {Xie, Kaikun and Huang, Yu and Zeng, Feng and Liu, Zehua and Chen, Ting},
  year = {2020},
  month = oct,
  volume = {2},
  issn = {2631-9268},
  doi = {10.1093/nargab/lqaa082},
  abstract = {Recent advancements in both single-cell RNA-sequencing technology and computational resources facilitate the study of cell types on global populations. Up to millions of cells can now be sequenced in one experiment; thus, accurate and efficient computational methods are needed to provide clustering and post-analysis of assigning putative and rare cell types. Here, we present a novel unsupervised deep learning clustering framework that is robust and highly scalable. To overcome the high level of noise, scAIDE first incorporates an autoencoder-imputation network with a distance-preserved embedding network (AIDE) to learn a good representation of data, and then applies a random projection hashing based k-means algorithm to accommodate the detection of rare cell types. We analyzed a 1.3 million neural cell dataset within 30 min, obtaining 64 clusters which were mapped to 19 putative cell types. In particular, we further identified three different neural stem cell developmental trajectories in these clusters. We also classified two subpopulations of malignant cells in a small glioblastoma dataset using scAIDE. We anticipate that scAIDE would provide a more in-depth understanding of cell development and diseases.},
  file = {/home/trung/GoogleDrive/Zotero/xie et al_2020_scaide.pdf},
  journal = {NAR Genomics and Bioinformatics},
  number = {lqaa082}
}

@article{xing18_WalkSGD,
  title = {A {{Walk}} with {{SGD}}},
  author = {Xing, Chen and Arpit, Devansh and Tsirigotis, Christos and Bengio, Yoshua},
  year = {2018},
  month = may,
  abstract = {We present novel empirical observations regarding how stochastic gradient descent (SGD) navigates the loss landscape of over-parametrized deep neural networks (DNNs). These observations expose the qualitatively different roles of learning rate and batch-size in DNN optimization and generalization. Specifically we study the DNN loss surface along the trajectory of SGD by interpolating the loss surface between parameters from consecutive \textbackslash textit\{iterations\} and tracking various metrics during training. We find that the loss interpolation between parameters before and after each training iteration's update is roughly convex with a minimum (\textbackslash textit\{valley floor\}) in between for most of the training. Based on this and other metrics, we deduce that for most of the training update steps, SGD moves in valley like regions of the loss surface by jumping from one valley wall to another at a height above the valley floor. This 'bouncing between walls at a height' mechanism helps SGD traverse larger distance for small batch sizes and large learning rates which we find play qualitatively different roles in the dynamics. While a large learning rate maintains a large height from the valley floor, a small batch size injects noise facilitating exploration. We find this mechanism is crucial for generalization because the valley floor has barriers and this exploration above the valley floor allows SGD to quickly travel far away from the initialization point (without being affected by barriers) and find flatter regions, corresponding to better generalization.},
  annotation = {ZSCC: 0000025},
  archivePrefix = {arXiv},
  eprint = {1802.08770},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xing et al_2018_a walk with sgd.pdf;/home/trung/Zotero/storage/Q7ELDJ2S/1802.html},
  journal = {arXiv:1802.08770 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xiong19_SCALEmethodsinglecell,
  title = {{{SCALE}} Method for Single-Cell {{ATAC}}-Seq Analysis via Latent Feature Extraction},
  author = {Xiong, Lei and Xu, Kui and Tian, Kang and Shao, Yanqiu and Tang, Lei and Gao, Ge and Zhang, Michael and Jiang, Tao and Zhang, Qiangfeng Cliff},
  year = {2019},
  month = dec,
  volume = {10},
  pages = {4576},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-12630-7},
  file = {/home/trung/GoogleDrive/Zotero/2019/false;/home/trung/GoogleDrive/Zotero/xiong et al_2019_scale method for single-cell atac-seq analysis via latent feature extraction.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{xiu20_VariationalDisentanglementRare,
  title = {Variational {{Disentanglement}} for {{Rare Event Modeling}}},
  author = {Xiu, Zidi and Tao, Chenyang and Gao, Michael and Davis, Connor and Goldstein, Benjamin and Heano, Ricardo},
  year = {2020},
  month = sep,
  abstract = {Combining the increasing availability and abundance of healthcare data and the current advances in machine learning methods have created renewed opportunities to improve clinical decision support systems. However, in healthcare risk prediction applications, the proportion of cases with the condition (label) of interest is often very low relative to the available sample size. Though very prevalent in healthcare, such imbalanced classification settings are also common and challenging in many other scenarios. So motivated, we propose a variational disentanglement approach to semi-parametrically learn from rare events in heavily imbalanced classification problems. Specifically, we leverage the imposed extreme-distribution behavior on a latent space to extract information from low-prevalence events, and develop a robust prediction arm that joins the merits of the generalized additive model and isotonic neural nets. Results on synthetic studies and diverse real-world datasets, including mortality prediction on a COVID-19 cohort, demonstrate that the proposed approach outperforms existing alternatives.},
  archivePrefix = {arXiv},
  eprint = {2009.08541},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xiu et al_2020_variational disentanglement for rare event modeling.pdf},
  journal = {arXiv:2009.08541 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{xu00_HOWPOWERFULARE,
  title = {{{HOW POWERFUL ARE GRAPH NEURAL NETWORKS}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  pages = {15},
  abstract = {Graph Neural Networks (GNNs) for representation learning of graphs broadly follow a neighborhood aggregation framework, where the representation vector of a node is computed by recursively aggregating and transforming feature vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs in capturing different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_how powerful are graph neural networks.pdf},
  language = {en}
}

@article{xu00_ShallowVAEsRealNVP,
  title = {Shallow {{VAEs}} with {{RealNVP Prior Can Perform}} as {{Well}} as {{Deep Hierarchical VAEs}}},
  author = {Xu, Haowen and Chen, Wenxiao and Lai, Jinlin and Li, Zhihan and Zhao, Youjian and Pei, Dan},
  pages = {9},
  abstract = {Learn the prior of VAE is a new approach to improve the evidence lower-bound. We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures. We provide the theoretical optimal decoder for Benoulli p(x|z). We demonstrate that, with learned RealNVP prior, {$\beta$}-VAE can have better rate-distortion curve than using fixed Gaussian prior.},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_shallow vaes with realnvp prior can perform as well as deep hierarchical vaes.pdf},
  language = {en}
}

@article{xu00_ShallowVAEsRealNVPa,
  title = {Shallow {{VAEs}} with {{RealNVP Prior Can Perform}} as {{Well}} as {{Deep Hierarchical VAEs}}},
  author = {Xu, Haowen and Chen, Wenxiao and Lai, Jinlin and Li, Zhihan and Zhao, Youjian and Pei, Dan},
  pages = {9},
  abstract = {Learn the prior of VAE is a new approach to improve the evidence lower-bound. We show that using learned RealNVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures. We provide the theoretical optimal decoder for Benoulli p(x|z). We demonstrate that, with learned RealNVP prior, {$\beta$}-VAE can have better rate-distortion curve than using fixed Gaussian prior.},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_shallow vaes with realnvp prior can perform as well as deep hierarchical vaes2.pdf},
  language = {en}
}

@article{xu15_ShowAttendTell,
  title = {Show, {{Attend}} and {{Tell}}: {{Neural Image Caption Generation}} with {{Visual Attention}}},
  shorttitle = {Show, {{Attend}} and {{Tell}}},
  author = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron and Salakhutdinov, Ruslan and Zemel, Richard and Bengio, Yoshua},
  year = {2015},
  month = feb,
  abstract = {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.},
  annotation = {ZSCC: 0000003},
  archivePrefix = {arXiv},
  eprint = {1502.03044},
  eprinttype = {arxiv},
  journal = {arXiv:1502.03044 [cs]},
  keywords = {attention,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,hard attention},
  primaryClass = {cs}
}

@article{xu18_SphericalLatentSpaces,
  title = {Spherical {{Latent Spaces}} for {{Stable Variational Autoencoders}}},
  author = {Xu, Jiacheng and Durrett, Greg},
  year = {2018},
  month = oct,
  abstract = {A hallmark of variational autoencoders (VAEs) for text processing is their combination of powerful encoder-decoder models, such as LSTMs, with simple latent distributions, typically multivariate Gaussians. These models pose a difficult optimization problem: there is an especially bad local optimum where the variational posterior always equals the prior and the model does not use the latent variable at all, a kind of "collapse" which is encouraged by the KL divergence term of the objective. In this work, we experiment with another choice of latent distribution, namely the von Mises-Fisher (vMF) distribution, which places mass on the surface of the unit hypersphere. With this choice of prior and posterior, the KL divergence term now only depends on the variance of the vMF distribution, giving us the ability to treat it as a fixed hyperparameter. We show that doing so not only averts the KL collapse, but consistently gives better likelihoods than Gaussians across a range of modeling conditions, including recurrent language modeling and bag-of-words document modeling. An analysis of the properties of our vMF representations shows that they learn richer and more nuanced structures in their latent representations than their Gaussian counterparts.},
  archivePrefix = {arXiv},
  eprint = {1808.10805},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2018_spherical latent spaces for stable variational autoencoders.pdf},
  journal = {arXiv:1808.10805 [cs]},
  primaryClass = {cs}
}

@article{xu19_HowPowerfulare,
  title = {How {{Powerful}} Are {{Graph Neural Networks}}?},
  author = {Xu, Keyulu and Hu, Weihua and Leskovec, Jure and Jegelka, Stefanie},
  year = {2019},
  month = feb,
  abstract = {Graph Neural Networks (GNNs) are an effective framework for representation learning of graphs. GNNs follow a neighborhood aggregation scheme, where the representation vector of a node is computed by recursively aggregating and transforming representation vectors of its neighboring nodes. Many GNN variants have been proposed and have achieved state-of-the-art results on both node and graph classification tasks. However, despite GNNs revolutionizing graph representation learning, there is limited understanding of their representational properties and limitations. Here, we present a theoretical framework for analyzing the expressive power of GNNs to capture different graph structures. Our results characterize the discriminative power of popular GNN variants, such as Graph Convolutional Networks and GraphSAGE, and show that they cannot learn to distinguish certain simple graph structures. We then develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the WeisfeilerLehman graph isomorphism test. We empirically validate our theoretical findings on a number of graph classification benchmarks, and demonstrate that our model achieves state-of-the-art performance.},
  archivePrefix = {arXiv},
  eprint = {1810.00826},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2019_how powerful are graph neural networks.pdf},
  journal = {arXiv:1810.00826 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{xu19_MetaFunMetaLearningIterative,
  title = {{{MetaFun}}: {{Meta}}-{{Learning}} with {{Iterative Functional Updates}}},
  shorttitle = {{{MetaFun}}},
  author = {Xu, Jin and Ton, Jean-Francois and Kim, Hyunjik and Kosiorek, Adam R. and Teh, Yee Whye},
  year = {2019},
  month = dec,
  abstract = {Few-shot supervised learning leverages experience from previous learning tasks to solve new tasks where only a few labelled examples are available. One successful line of approach to this problem is to use an encoder-decoder meta-learning pipeline, whereby labelled data in a task is encoded to produce task representation, and this representation is used to condition the decoder to make predictions on unlabelled data. We propose an approach that uses this pipeline with two important features. 1) We use infinite-dimensional functional representations of the task rather than fixed-dimensional representations. 2) We iteratively apply functional updates to the representation. We show that our approach can be interpreted as extending functional gradient descent, and delivers performance that is comparable to or outperforms previous state-of-the-art on few-shot classification benchmarks such as miniImageNet and tieredImageNet.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1912.02738},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2019_metafun.pdf;/home/trung/Zotero/storage/CNSR37AV/1912.html},
  journal = {arXiv:1912.02738 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xu19_NecessityEffectivenessLearning,
  title = {On the {{Necessity}} and {{Effectiveness}} of {{Learning}} the {{Prior}} of {{Variational Auto}}-{{Encoder}}},
  author = {Xu, Haowen and Chen, Wenxiao and Lai, Jinlin and Li, Zhihan and Zhao, Youjian and Pei, Dan},
  year = {2019},
  month = may,
  abstract = {Using powerful posterior distributions is a popular approach to achieving better variational inference. However, recent works showed that the aggregated posterior may fail to match unit Gaussian prior, thus learning the prior becomes an alternative way to improve the lower-bound. In this paper, for the first time in the literature, we prove the necessity and effectiveness of learning the prior when aggregated posterior does not match unit Gaussian prior, analyze why this situation may happen, and propose a hypothesis that learning the prior may improve reconstruction loss, all of which are supported by our extensive experiment results. We show that using learned Real NVP prior and just one latent variable in VAE, we can achieve test NLL comparable to very deep state-of-the-art hierarchical VAE, outperforming many previous works with complex hierarchical VAE architectures.},
  annotation = {ZSCC: 0000001},
  archivePrefix = {arXiv},
  eprint = {1905.13452},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2019_on the necessity and effectiveness of learning the prior of variational auto-encoder.pdf;/home/trung/Zotero/storage/R4ULWEXK/1905.html},
  journal = {arXiv:1905.13452 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xu19_RegressionPlanningNetworks,
  title = {Regression {{Planning Networks}}},
  author = {Xu, Danfei and {Mart{\'i}n-Mart{\'i}n}, Roberto and Huang, De-An and Zhu, Yuke and Savarese, Silvio and {Fei-Fei}, Li},
  year = {2019},
  month = sep,
  abstract = {Recent learning-to-plan methods have shown promising results on planning directly from observation space. Yet, their ability to plan for long-horizon tasks is limited by the accuracy of the prediction model. On the other hand, classical symbolic planners show remarkable capabilities in solving long-horizon tasks, but they require predefined symbolic rules and symbolic states, restricting their real-world applicability. In this work, we combine the benefits of these two paradigms and propose a learning-to-plan method that can directly generate a long-term symbolic plan conditioned on high-dimensional observations. We borrow the idea of regression (backward) planning from classical planning literature and introduce Regression Planning Networks (RPN), a neural network architecture that plans backward starting at a task goal and generates a sequence of intermediate goals that reaches the current observation. We show that our model not only inherits many favorable traits from symbolic planning, e.g., the ability to solve previously unseen tasks but also can learn from visual inputs in an end-to-end manner. We evaluate the capabilities of RPN in a grid world environment and a simulated 3D kitchen environment featuring complex visual scenes and long task horizons, and show that it achieves near-optimal performance in completely new task instances.},
  archivePrefix = {arXiv},
  eprint = {1909.13072},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2019_regression planning networks.pdf;/home/trung/Zotero/storage/G58XQ7K9/1909.html},
  journal = {arXiv:1909.13072 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  primaryClass = {cs}
}

@article{xu19_rolecollagencancer,
  title = {The Role of Collagen in Cancer: From Bench to Bedside},
  shorttitle = {The Role of Collagen in Cancer},
  author = {Xu, Shuaishuai and Xu, Huaxiang and Wang, Wenquan and Li, Shuo and Li, Hao and Li, Tianjiao and Zhang, Wuhu and Yu, Xianjun and Liu, Liang},
  year = {2019},
  month = sep,
  volume = {17},
  pages = {309},
  issn = {1479-5876},
  doi = {10.1186/s12967-019-2058-1},
  abstract = {Collagen is the major component of the tumor microenvironment and participates in cancer fibrosis. Collagen biosynthesis can be regulated by cancer cells through mutated genes, transcription factors, signaling pathways and receptors; furthermore, collagen can influence tumor cell behavior through integrins, discoidin domain receptors, tyrosine kinase receptors, and some signaling pathways. Exosomes and microRNAs are closely associated with collagen in cancer. Hypoxia, which is common in collagen-rich conditions, intensifies cancer progression, and other substances in the extracellular matrix, such as fibronectin, hyaluronic acid, laminin, and matrix metalloproteinases, interact with collagen to influence cancer cell activity. Macrophages, lymphocytes, and fibroblasts play a role with collagen in cancer immunity and progression. Microscopic changes in collagen content within cancer cells and matrix cells and in other molecules ultimately contribute to the mutual feedback loop that influences prognosis, recurrence, and resistance in cancer. Nanoparticles, nanoplatforms, and nanoenzymes exhibit the expected gratifying properties. The pathophysiological functions of collagen in diverse cancers illustrate the dual roles of collagen and provide promising therapeutic options that can be readily translated from bench to bedside. The emerging understanding of the structural properties and functions of collagen in cancer will guide the development of new strategies for anticancer therapy.},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2019_the role of collagen in cancer.pdf},
  journal = {Journal of Translational Medicine},
  number = {1}
}

@article{xu19_SNEDomainAdaptationa,
  title = {\$d\$-{{SNE}}: {{Domain Adaptation}} Using {{Stochastic Neighborhood Embedding}}},
  shorttitle = {\$d\$-{{SNE}}},
  author = {Xu, Xiang and Zhou, Xiong and Venkatesan, Ragav and Swaminathan, Gurumurthy and Majumder, Orchid},
  year = {2019},
  month = may,
  abstract = {Deep neural networks often require copious amount of labeled-data to train their scads of parameters. Training larger and deeper networks is hard without appropriate regularization, particularly while using a small dataset. Laterally, collecting well-annotated data is expensive, timeconsuming and often infeasible. A popular way to regularize these networks is to simply train the network with more data from an alternate representative dataset. This can lead to adverse effects if the statistics of the representative dataset are dissimilar to our target. This predicament is due to the problem of domain shift. Data from a shifted domain might not produce bespoke features when a feature extractor from the representative domain is used. In this paper, we propose a new technique (d-SNE) of domain adaptation that cleverly uses stochastic neighborhood embedding techniques and a novel modified-Hausdorff distance. The proposed technique is learnable end-to-end and is therefore, ideally suited to train neural networks. Extensive experiments demonstrate that d-SNE outperforms the current states-of-the-art and is robust to the variances in different datasets, even in the one-shot and semi-supervised learning settings. d-SNE also demonstrates the ability to generalize to multiple domains concurrently.},
  archivePrefix = {arXiv},
  eprint = {1905.12775},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2019_$d$-sne.pdf},
  journal = {arXiv:1905.12775 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{xu20_HowNeuralNetworks,
  title = {How {{Neural Networks Extrapolate}}: {{From Feedforward}} to {{Graph Neural Networks}}},
  shorttitle = {How {{Neural Networks Extrapolate}}},
  author = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  year = {2020},
  month = sep,
  abstract = {We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while multilayer perceptrons (MLPs) do not extrapolate well in certain simple tasks, Graph Neural Network (GNN), a structured network with MLP modules, has shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most non-linear functions. But, they can provably learn a linear target function when the training distribution is sufficiently "diverse". Second, in connection to analyzing successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features.},
  archivePrefix = {arXiv},
  eprint = {2009.11848},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2020_how neural networks extrapolate.pdf},
  journal = {arXiv:2009.11848 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xu20_LearningAutoencodersRelational,
  title = {Learning {{Autoencoders}} with {{Relational Regularization}}},
  author = {Xu, Hongteng and Luo, Dixin and Henao, Ricardo and Shah, Svati and Carin, Lawrence},
  year = {2020},
  month = jun,
  abstract = {A new algorithmic framework is proposed for learning autoencoders of data distributions. We minimize the discrepancy between the model and target distributions, with a \textbackslash emph\{relational regularization\} on the learnable latent prior. This regularization penalizes the fused Gromov-Wasserstein (FGW) distance between the latent prior and its corresponding posterior, allowing one to flexibly learn a structured prior distribution associated with the generative model. Moreover, it helps co-training of multiple autoencoders even if they have heterogeneous architectures and incomparable latent spaces. We implement the framework with two scalable algorithms, making it applicable for both probabilistic and deterministic autoencoders. Our relational regularized autoencoder (RAE) outperforms existing methods, \$e.g.\$, the variational autoencoder, Wasserstein autoencoder, and their variants, on generating images. Additionally, our relational co-training strategy for autoencoders achieves encouraging results in both synthesis and real-world multi-view learning tasks. The code is at https://github.com/HongtengXu/ Relational-AutoEncoders.},
  archivePrefix = {arXiv},
  eprint = {2002.02913},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2020_learning autoencoders with relational regularization.pdf},
  journal = {arXiv:2002.02913 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{xu20_SelftrainingPretrainingare,
  title = {Self-Training and {{Pre}}-Training Are {{Complementary}} for {{Speech Recognition}}},
  author = {Xu, Qiantong and Baevski, Alexei and Likhomanenko, Tatiana and Tomasello, Paden and Conneau, Alexis and Collobert, Ronan and Synnaeve, Gabriel and Auli, Michael},
  year = {2020},
  month = oct,
  abstract = {Self-training and unsupervised pre-training have emerged as effective approaches to improve speech recognition systems using unlabeled data. However, it is not clear whether they learn similar patterns or if they can be effectively combined. In this paper, we show that pseudo-labeling and pre-training with wav2vec 2.0 are complementary in a variety of labeled data setups. Using just 10 minutes of labeled data from Libri-light as well as 53k hours of unlabeled data from LibriVox achieves WERs of 3.0\%/5.2\% on the clean and other test sets of Librispeech - rivaling the best published systems trained on 960 hours of labeled data only a year ago. Training on all labeled data of Librispeech achieves WERs of 1.5\%/3.1\%.},
  archivePrefix = {arXiv},
  eprint = {2010.11430},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2020_self-training and pre-training are complementary for speech recognition.pdf},
  journal = {arXiv:2010.11430 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{xu20_TheoryUsableInformation,
  title = {A {{Theory}} of {{Usable Information Under Computational Constraints}}},
  author = {Xu, Yilun and Zhao, Shengjia and Song, Jiaming and Stewart, Russell and Ermon, Stefano},
  year = {2020},
  month = feb,
  abstract = {We propose a new framework for reasoning about information in complex systems. Our foundation is based on a variational extension of Shannon's information theory that takes into account the modeling power and computational constraints of the observer. The resulting \textbackslash emph\{predictive \$\textbackslash mathcal\{V\}\$-information\} encompasses mutual information and other notions of informativeness such as the coefficient of determination. Unlike Shannon's mutual information and in violation of the data processing inequality, \$\textbackslash mathcal\{V\}\$-information can be created through computation. This is consistent with deep neural networks extracting hierarchies of progressively more informative features in representation learning. Additionally, we show that by incorporating computational constraints, \$\textbackslash mathcal\{V\}\$-information can be reliably estimated from data even in high dimensions with PAC-style guarantees. Empirically, we demonstrate predictive \$\textbackslash mathcal\{V\}\$-information is more effective than mutual information for structure learning and fair representation learning.},
  archivePrefix = {arXiv},
  eprint = {2002.10689},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2020_a theory of usable information under computational constraints.pdf},
  journal = {arXiv:2002.10689 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{xu20_VariationalLearningControllable,
  title = {On {{Variational Learning}} of {{Controllable Representations}} for {{Text}} without {{Supervision}}},
  author = {Xu, Peng and Cheung, Jackie Chi Kit and Cao, Yanshuai},
  year = {2020},
  month = aug,
  abstract = {The variational autoencoder (VAE) can learn the manifold of natural images on certain datasets, as evidenced by meaningful interpolation or extrapolation in the continuous latent space. However, on discrete data such as text, it is unclear if unsupervised learning can discover a similar latent space that allows controllable manipulation. In this work, we find that sequence VAEs trained on text fail to properly decode when the latent codes are manipulated, because the modified codes often land in holes or vacant regions in the aggregated posterior latent space, where the decoding network fails to generalize. Both as a validation of the explanation and as a fix to the problem, we propose to constrain the posterior mean to a learned probability simplex, and perform manipulation within this simplex. Our proposed method mitigates the latent vacancy problem and achieves the first success in unsupervised learning of controllable representations for text. Empirically, our method outperforms unsupervised baselines and strong supervised approaches on text style transfer. On automatic evaluation metrics used in text style transfer, even with the decoding network trained from scratch, our method achieves comparable results with state-of-the-art supervised approaches leveraging large-scale pre-trained models for generation. Furthermore, it is capable of performing more flexible fine-grained control over text generation than existing methods.},
  archivePrefix = {arXiv},
  eprint = {1905.11975},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xu et al_2020_on variational learning of controllable representations for text without supervision.pdf},
  journal = {arXiv:1905.11975 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{xuan19_AnomalousGeneralizationGANs,
  title = {On the {{Anomalous Generalization}} of {{GANs}}},
  author = {Xuan, Jinchen and Yang, Yunchang and Yang, Ze and He, Di and Wang, Liwei},
  year = {2019},
  month = oct,
  abstract = {Generative models, especially Generative Adversarial Networks (GANs), have received significant attention recently. However, it has been observed that in terms of some attributes, e.g. the number of simple geometric primitives in an image, GANs are not able to learn the target distribution in practice. Motivated by this observation, we discover two specific problems of GANs leading to anomalous generalization behaviour, which we refer to as the sample insufficiency and the pixel-wise combination. For the first problem of sample insufficiency, we show theoretically and empirically that the batchsize of the training samples in practice may be insufficient for the discriminator to learn an accurate discrimination function. It could result in unstable training dynamics for the generator, leading to anomalous generalization. For the second problem of pixel-wise combination, we find that besides recognizing the positive training samples as real, under certain circumstances, the discriminator could be fooled to recognize the pixel-wise combinations (e.g. pixel-wise average) of the positive training samples as real. However, those combinations could be visually different from the real samples in the target distribution. With the fooled discriminator as reference, the generator would obtain biased supervision further, leading to the anomalous generalization behaviour. Additionally, in this paper, we propose methods to mitigate the anomalous generalization of GANs. Extensive experiments on benchmark show our proposed methods improve the FID score up to 30\textbackslash\% on natural image dataset.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1909.12638},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/xuan et al_2019_on the anomalous generalization of gans.pdf;/home/trung/Zotero/storage/LBIXPLJW/1909.html},
  journal = {arXiv:1909.12638 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{yacoby20_CharacterizingAvoidingProblematic,
  title = {Characterizing and {{Avoiding Problematic Global Optima}} of {{Variational Autoencoders}}},
  author = {Yacoby, Yaniv and Pan, Weiwei and {Doshi-Velez}, Finale},
  year = {2020},
  month = mar,
  abstract = {Variational Auto-encoders (VAEs) are deep generative latent variable models consisting of two components: a generative model that captures a data distribution p(x) by transforming a distribution p(z) over latent space, and an inference model that infers likely latent codes for each data point (Kingma and Welling, 2013). Recent work shows that traditional training methods tend to yield solutions that violate modeling desiderata: (1) the learned generative model captures the observed data distribution but does so while ignoring the latent codes, resulting in codes that do not represent the data (e.g. van den Oord et al. (2017); Kim et al. (2018)); (2) the aggregate of the learned latent codes does not match the prior p(z). This mismatch means that the learned generative model will be unable to generate realistic data with samples from p(z)(e.g. Makhzani et al. (2015); Tomczak and Welling (2017)). In this paper, we demonstrate that both issues stem from the fact that the global optima of the VAE training objective often correspond to undesirable solutions. Our analysis builds on two observations: (1) the generative model is unidentifiable - there exist many generative models that explain the data equally well, each with different (and potentially unwanted) properties and (2) bias in the VAE objective - the VAE objective may prefer generative models that explain the data poorly but have posteriors that are easy to approximate. We present a novel inference method, LiBI, mitigating the problems identified in our analysis. On synthetic datasets, we show that LiBI can learn generative models that capture the data distribution and inference models that better satisfy modeling assumptions when traditional methods struggle to do so.},
  archivePrefix = {arXiv},
  eprint = {2003.07756},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yacoby et al_2020_characterizing and avoiding problematic global optima of variational autoencoders.pdf},
  journal = {arXiv:2003.07756 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{yacoby20_FailureModesVariational,
  title = {Failure {{Modes}} of {{Variational Autoencoders}} and {{Their Effects}} on {{Downstream Tasks}}},
  author = {Yacoby, Yaniv and Pan, Weiwei and {Doshi-Velez}, Finale},
  year = {2020},
  month = jul,
  abstract = {Variational Auto-encoders (VAEs) are deep generative latent variable models that are widely used for a number of downstream tasks. While it has been demonstrated that VAE training can suffer from a number of pathologies, existing literature lacks characterizations of exactly when these pathologies occur and how they impact down-stream task performance. In this paper we concretely characterize conditions under which VAE training exhibits pathologies and connect these failure modes to undesirable effects on specific downstream tasks \textendash{} learning compressed and disentangled representations, adversarial robustness and semi-supervised learning.},
  archivePrefix = {arXiv},
  eprint = {2007.07124},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yacoby et al_2020_failure modes of variational autoencoders and their effects on downstream tasks2.pdf},
  journal = {arXiv:2007.07124 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{yacoby20_FailureModesVariationala,
  title = {Failure {{Modes}} of {{Variational Autoencoders}} and {{Their Effects}} on {{Downstream Tasks}}},
  author = {Yacoby, Yaniv and Pan, Weiwei and {Doshi-Velez}, Finale},
  year = {2020},
  month = jul,
  abstract = {Variational Auto-encoders (VAEs) are deep generative latent variable models that are widely used for a number of downstream tasks. While it has been demonstrated that VAE training can suffer from a number of pathologies, existing literature lacks characterizations of exactly when these pathologies occur and how they impact down-stream task performance. In this paper we concretely characterize conditions under which VAE training exhibits pathologies and connect these failure modes to undesirable effects on specific downstream tasks - learning compressed and disentangled representations, adversarial robustness and semi-supervised learning.},
  archivePrefix = {arXiv},
  eprint = {2007.07124},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yacoby et al_2020_failure modes of variational autoencoders and their effects on downstream tasks.pdf},
  journal = {arXiv:2007.07124 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{yadan19_Hydraframeworkelegantly,
  title = {Hydra - {{A}} Framework for Elegantly Configuring Complex Applications},
  author = {Yadan, Omry},
  year = {2019},
  howpublished = {Github}
}

@article{yamada00_DisentangledRepresentationsSequence,
  title = {Disentangled {{Representations}} for {{Sequence Data}} Using {{Information Bottleneck Principle}}},
  author = {Yamada, Masanori and Kim, Heecheol and Miyoshi, Kosuke and Iwata, Tomoharu and Yamakawa, Hiroshi},
  pages = {16},
  abstract = {We propose the factorizing variational autoencoder (FAVAE), a generative model for learning disentangled representations from sequential data via the information bottleneck principle without supervision. Real-world data are often generated by a few explanatory factors of variation, and disentangled representation learning obtains these factors from the data. We focus on the disentangled representation of sequential data which can be useful in a wide range of applications, such as video, speech, and stock markets. Factors in sequential data are categorized into dynamic and static ones: dynamic factors are time dependent, and static factors are time independent. Previous models disentangle between static and dynamic factors and between dynamic factors with different time dependencies by explicitly modeling the priors of latent variables. However, these models cannot disentangle representations between dynamic factors with the same time dependency, such as disentangling ``picking up'' and ``throwing'' in robotic tasks. On the other hand, FAVAE can disentangle multiple dynamic factors via the information bottleneck principle where it does not require modeling priors. We conducted experiments to show that FAVAE can extract disentangled dynamic factors on synthetic, video, and speech datasets.},
  file = {/home/trung/GoogleDrive/Zotero/yamada et al_disentangled representations for sequence data using information bottleneck principle.pdf},
  keywords = {_tablet},
  language = {en}
}

@article{yan19_TENERAdaptingTransformer,
  title = {{{TENER}}: {{Adapting Transformer Encoder}} for {{Name Entity Recognition}}},
  shorttitle = {{{TENER}}},
  author = {Yan, Hang and Deng, Bocao and Li, Xiaonan and Qiu, Xipeng},
  year = {2019},
  month = nov,
  abstract = {The Bidirectional long short-term memory networks (BiLSTM) have been widely used as an encoder in models solving the named entity recognition (NER) task. Recently, the Transformer is broadly adopted in various Natural Language Processing (NLP) tasks owing to its parallelism and advantageous performance. Nevertheless, the performance of the Transformer in NER is not as good as it is in other NLP tasks. In this paper, we propose TENER, a NER architecture adopting adapted Transformer Encoder to model the character-level features and word-level features. By incorporating the direction and relative distance aware attention and the un-scaled attention, we prove the Transformer-like encoder is just as effective for NER as other NLP tasks.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.04474},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yan et al_2019_tener.pdf;/home/trung/Zotero/storage/ZSNZGYMJ/1911.html},
  journal = {arXiv:1911.04474 [cs]},
  keywords = {attention,Computer Science - Computation and Language,Computer Science - Machine Learning,transformer,unscaled attention},
  primaryClass = {cs}
}

@article{yan20_StochasticGraphRecurrent,
  title = {Stochastic {{Graph Recurrent Neural Network}}},
  author = {Yan, Tijin and Zhang, Hongwei and Li, Zirui and Xia, Yuanqing},
  year = {2020},
  month = sep,
  abstract = {Representation learning over graph structure data has been widely studied due to its wide application prospects. However, previous methods mainly focus on static graphs while many real-world graphs evolve over time. Modeling such evolution is important for predicting properties of unseen networks. To resolve this challenge, we propose SGRNN, a novel neural architecture that applies stochastic latent variables to simultaneously capture the evolution in node attributes and topology. Specifically, deterministic states are separated from stochastic states in the iterative process to suppress mutual interference. With semi-implicit variational inference integrated to SGRNN, a non-Gaussian variational distribution is proposed to help further improve the performance. In addition, to alleviate KL-vanishing problem in SGRNN, a simple and interpretable structure is proposed based on the lower bound of KL-divergence. Extensive experiments on real-world datasets demonstrate the effectiveness of the proposed model. Code is available at https://github.com/StochasticGRNN/SGRNN.},
  archivePrefix = {arXiv},
  eprint = {2009.00538},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yan et al_2020_stochastic graph recurrent neural network.pdf},
  journal = {arXiv:2009.00538 [cs, stat]},
  primaryClass = {cs, stat}
}

@inproceedings{yanai17_ConditionalFastStyle,
  title = {Conditional {{Fast Style Transfer Network}}},
  booktitle = {Proceedings of the 2017 {{ACM}} on {{International Conference}} on {{Multimedia Retrieval}}},
  author = {Yanai, Keiji and Tanno, Ryosuke},
  year = {2017},
  pages = {434--437},
  publisher = {{ACM}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3078971.3079037},
  abstract = {In this paper, we propose a conditional fast neural style transfer network. We extend the network proposed as a fast neural style transfer network by Johnson et al. [1] so that the network can learn multiple styles at the same time. To do that, we add a conditional input which selects a style to be transferred out of the trained styles. In addition, we show that the proposed network can mix multiple styles, although the network is trained with each of the training styles independently. The proposed network can also transfer different styles to the different parts of a given image at the same time, which we call "spatial style transfer". In the experiments, we confirmed that no quality degradation occurred in the multi-style network compared to the single network, and linear-weighted multi-style fusion enabled us to generate various kinds of new styles which are different from the trained single styles.},
  isbn = {978-1-4503-4701-3},
  series = {{{ICMR}} '17}
}

@inproceedings{yang16_HierarchicalAttentionNetworks,
  title = {Hierarchical {{Attention Networks}} for {{Document Classification}}},
  booktitle = {Proceedings of the 2016 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Yang, Zichao and Yang, Diyi and Dyer, Chris and He, Xiaodong and Smola, Alex and Hovy, Eduard},
  year = {2016},
  pages = {1480--1489},
  publisher = {{Association for Computational Linguistics}},
  address = {{San Diego, California}},
  doi = {10.18653/v1/N16-1174},
  abstract = {We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.},
  annotation = {ZSCC: 0001302},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2016_hierarchical attention networks for document classification.pdf},
  keywords = {attention,hierarchical},
  language = {en}
}

@article{yang17_ImprovedVariationalAutoencoders,
  title = {Improved {{Variational Autoencoders}} for {{Text Modeling}} Using {{Dilated Convolutions}}},
  author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and {Berg-Kirkpatrick}, Taylor},
  year = {2017},
  month = jun,
  abstract = {Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
  archivePrefix = {arXiv},
  eprint = {1702.08139},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/SCJQLSDR/Yang et al. - 2017 - Improved Variational Autoencoders for Text Modelin.pdf},
  journal = {arXiv:1702.08139 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yang17_ImprovedVariationalAutoencodersa,
  title = {Improved {{Variational Autoencoders}} for {{Text Modeling}} Using {{Dilated Convolutions}}},
  author = {Yang, Zichao and Hu, Zhiting and Salakhutdinov, Ruslan and {Berg-Kirkpatrick}, Taylor},
  year = {2017},
  month = jun,
  abstract = {Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder's dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.},
  annotation = {ZSCC: 0000146},
  archivePrefix = {arXiv},
  eprint = {1702.08139},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2017_improved variational autoencoders for text modeling using dilated convolutions.pdf},
  journal = {arXiv:1702.08139 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yang18_GLoMoUnsupervisedlyLearned,
  title = {{{GLoMo}}: {{Unsupervisedly Learned Relational Graphs}} as {{Transferable Representations}}},
  shorttitle = {{{GLoMo}}},
  author = {Yang, Zhilin and Zhao, Jake and Dhingra, Bhuwan and He, Kaiming and Cohen, William W. and Salakhutdinov, Ruslan and LeCun, Yann},
  year = {2018},
  month = jul,
  abstract = {Modern deep transfer learning approaches have mainly focused on learning generic feature vectors from one task that are transferable to other tasks, such as word embeddings in language and pretrained convolutional features in vision. However, these approaches usually transfer unary features and largely ignore more structured graphical representations. This work explores the possibility of learning generic latent relational graphs that capture dependencies between pairs of data units (e.g., words or pixels) from large-scale unlabeled data and transferring the graphs to downstream tasks. Our proposed transfer learning framework improves performance on various tasks including question answering, natural language inference, sentiment analysis, and image classification. We also show that the learned graphs are generic enough to be transferred to different embeddings on which the graphs have not been trained (including GloVe embeddings, ELMo embeddings, and task-specific RNN hidden units), or embedding-free units such as image pixels.},
  annotation = {ZSCC: 0000011},
  archivePrefix = {arXiv},
  eprint = {1806.05662},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2018_glomo.pdf},
  journal = {arXiv:1806.05662 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,graph,relational,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{yang18_GreedyAttackGumbel,
  title = {Greedy {{Attack}} and {{Gumbel Attack}}: {{Generating Adversarial Examples}} for {{Discrete Data}}},
  shorttitle = {Greedy {{Attack}} and {{Gumbel Attack}}},
  author = {Yang, Puyudi and Chen, Jianbo and Hsieh, Cho-Jui and Wang, Jane-Ling and Jordan, Michael I.},
  year = {2018},
  month = may,
  abstract = {We present a probabilistic framework for studying adversarial attacks on discrete data. Based on this framework, we derive a perturbation-based method, Greedy Attack, and a scalable learning-based method, Gumbel Attack, that illustrate various tradeoffs in the design of attacks. We demonstrate the effectiveness of these methods using both quantitative metrics and human evaluation on various state-of-the-art models for text classification, including a word-based CNN, a character-based CNN and an LSTM. As as example of our results, we show that the accuracy of character-based convolutional networks drops to the level of random selection by modifying only five characters through Greedy Attack.},
  annotation = {ZSCC: 0000012},
  archivePrefix = {arXiv},
  eprint = {1805.12316},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2018_greedy attack and gumbel attack.pdf;/home/trung/Zotero/storage/V85QDPEF/1805.html},
  journal = {arXiv:1805.12316 [cs, stat]},
  keywords = {adversarial,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning,discrete,generative,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{yang19_BridgingDisentanglementIndependence,
  title = {Bridging {{Disentanglement}} with {{Independence}} and {{Conditional Independence}} via {{Mutual Information}} for {{Representation Learning}}},
  author = {Yang, Xiaojiang and Bi, Wendong and Cheng, Yu and Yan, Junchi},
  year = {2019},
  month = nov,
  abstract = {Existing works on disentangled representation learning usually lie on a common assumption: all factors in disentangled representations should be independent. This assumption is about the inner property of disentangled representations, while ignoring their relation with external data. To tackle this problem, we propose another assumption to establish an important relation between data and its disentangled representations via mutual information: the mutual information between each factor of disentangled representations and data should be invariant to other factors. We formulate this assumption into mathematical equations, and theoretically bridge it with independence and conditional independence of factors. Meanwhile, we show that conditional independence is satisfied in encoders of VAEs due to factorized noise in reparameterization. To highlight the importance of our proposed assumption, we show in experiments that violating the assumption leads to dramatic decline of disentanglement. Based on this assumption, we further propose to split the deeper layers in encoder to ensure parameters in these layers are not shared for different factors. The proposed encoder, called Split Encoder, can be applied into models that penalize total correlation, and shows significant improvement in unsupervised learning of disentangled representations and reconstructions.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1911.10922},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2019_bridging disentanglement with independence and conditional independence via mutual information for representation learning.pdf;/home/trung/Zotero/storage/EV77P4DY/1911.html},
  journal = {arXiv:1911.10922 [cs, stat]},
  keywords = {_tablet,disentanglement,information},
  primaryClass = {cs, stat}
}

@article{yang19_FeedbackRecurrentAutoEncoder,
  title = {Feedback {{Recurrent AutoEncoder}}},
  author = {Yang, Yang and Sauti{\`e}re, Guillaume and Ryu, J. Jon and Cohen, Taco S.},
  year = {2019},
  month = nov,
  abstract = {In this work, we propose a new recurrent autoencoder architecture, termed Feedback Recurrent AutoEncoder (FRAE), for online compression of sequential data with temporal dependency. The recurrent structure of FRAE is designed to efficiently extract the redundancy along the time dimension and allows a compact discrete representation of the data to be learned. We demonstrate its effectiveness in speech spectrogram compression. Specifically, we show that the FRAE, paired with a powerful neural vocoder, can produce high-quality speech waveforms at a low, fixed bitrate. We further show that by adding a learned prior for the latent space and using an entropy coder, we can achieve an even lower variable bitrate.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1911.04018},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2019_feedback recurrent autoencoder.pdf;/home/trung/Zotero/storage/2Y2GZY7R/1911.html},
  journal = {arXiv:1911.04018 [cs, eess, stat]},
  primaryClass = {cs, eess, stat}
}

@article{yang19_TensorProgramsWide,
  title = {Tensor {{Programs I}}: {{Wide Feedforward}} or {{Recurrent Neural Networks}} of {{Any Architecture}} Are {{Gaussian Processes}}},
  shorttitle = {Tensor {{Programs I}}},
  author = {Yang, Greg},
  year = {2019},
  month = oct,
  abstract = {Wide neural networks with random weights and biases are Gaussian processes, as observed by Neal (1995) for shallow networks, and more recently by Lee et al. (2018) and Matthews et al. (2018) for deep fully-connected networks, as well as by Novak et al. (2019) and Garriga-Alonso et al. (2019) for deep convolutional networks. We show that this Neural Network-Gaussian Process correspondence surprisingly extends to all modern feedforward or recurrent neural networks composed of multilayer perceptron, RNNs (e.g. LSTMs, GRUs), (nD or graph) convolution, pooling, skip connection, attention, batch normalization, and/or layer normalization. More generally, we introduce a language for expressing neural network computations, and our result encompasses all such expressible neural networks. This work serves as a tutorial on the *tensor programs* technique formulated in Yang (2019) and elucidates the Gaussian Process results obtained there. We provide open-source implementations of the Gaussian Process kernels of simple RNN, GRU, transformer, and batchnorm+ReLU network at github.com/thegregyang/GP4A.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.12478},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang_2019_tensor programs i.pdf;/home/trung/Zotero/storage/AZV76HPQ/1910.html},
  journal = {arXiv:1910.12478 [cond-mat, physics:math-ph]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Condensed Matter - Disordered Systems and Neural Networks,explain,gaussian process,Mathematical Physics},
  primaryClass = {cond-mat, physics:math-ph}
}

@article{yang19_UnsupervisedDomainAdaptation,
  title = {Unsupervised {{Domain Adaptation}} via {{Disentangled Representations}}: {{Application}} to {{Cross}}-{{Modality Liver Segmentation}}},
  shorttitle = {Unsupervised {{Domain Adaptation}} via {{Disentangled Representations}}},
  author = {Yang, Junlin and Dvornek, Nicha C. and Zhang, Fan and Chapiro, Julius and Lin, MingDe and Duncan, James S.},
  year = {2019},
  month = aug,
  abstract = {A deep learning model trained on some labeled data from a certain source domain generally performs poorly on data from different target domains due to domain shifts. Unsupervised domain adaptation methods address this problem by alleviating the domain shift between the labeled source data and the unlabeled target data. In this work, we achieve cross-modality domain adaptation, i.e. between CT and MRI images, via disentangled representations. Compared to learning a one-toone mapping as the state-of-art CycleGAN, our model recovers a manyto-many mapping between domains to capture the complex cross-domain relations. It preserves semantic feature-level information by finding a shared content space instead of a direct pixelwise style transfer. Domain adaptation is achieved in two steps. First, images from each domain are embedded into two spaces, a shared domain-invariant content space and a domain-specific style space. Next, the representation in the content space is extracted to perform a task. We validated our method on a crossmodality liver segmentation task, to train a liver segmentation model on CT images that also performs well on MRI. Our method achieved Dice Similarity Coefficient (DSC) of 0.81, outperforming a CycleGAN-based method of 0.72. Moreover, our model achieved good generalization to joint-domain learning, in which unpaired data from different modalities are jointly learned to improve the segmentation performance on each individual modality. Lastly, under a multi-modal target domain with significant diversity, our approach exhibited the potential for diverse image generation and remained effective with DSC of 0.74 on multi-phasic MRI while the CycleGAN-based method performed poorly with a DSC of only 0.52.},
  archivePrefix = {arXiv},
  eprint = {1907.13590},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2019_unsupervised domain adaptation via disentangled representations.pdf},
  journal = {arXiv:1907.13590 [cs, eess]},
  language = {en},
  primaryClass = {cs, eess}
}

@article{yang20_CausalVAEDisentangledRepresentation,
  title = {{{CausalVAE}}: {{Disentangled Representation Learning}} via {{Neural Structural Causal Models}}},
  shorttitle = {{{CausalVAE}}},
  author = {Yang, Mengyue and Liu, Furui and Chen, Zhitang and Shen, Xinwei and Hao, Jianye and Wang, Jun},
  year = {2020},
  month = jul,
  abstract = {Learning disentanglement aims at finding a low dimensional representation which consists of multiple explanatory and generative factors of the observational data. The framework of variational autoencoder (VAE) is commonly used to disentangle independent factors from observations. However, in real scenarios, factors with semantics are not necessarily independent. Instead, there might be an underlying causal structure which renders these factors dependent. We thus propose a new VAE based framework named CausalVAE, which includes a Causal Layer to transform independent exogenous factors into causal endogenous ones that correspond to causally related concepts in data. We further analyze the model identifiabitily, showing that the proposed model learned from observations recovers the true one up to a certain degree. Experiments are conducted on various datasets, including synthetic and real word benchmark CelebA. Results show that the causal representations learned by CausalVAE are semantically interpretable, and their causal relationship as a Directed Acyclic Graph (DAG) is identified with good accuracy. Furthermore, we demonstrate that the proposed CausalVAE model is able to generate counterfactual data through "do-operation" to the causal factors.},
  archivePrefix = {arXiv},
  eprint = {2004.08697},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2020_causalvae.pdf},
  journal = {arXiv:2004.08697 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{yang20_GenerationMedicalDialogues,
  title = {On the {{Generation}} of {{Medical Dialogues}} for {{COVID}}-19},
  author = {Yang, Wenmian and Zeng, Guangtao and Tan, Bowen and Ju, Zeqian and Chakravorty, Subrato and He, Xuehai and Chen, Shu and Yang, Xingyi and Wu, Qingyang and Yu, Zhou and Xing, Eric and Xie, Pengtao},
  year = {2020},
  month = jun,
  abstract = {Under the pandemic of COVID-19, people experiencing COVID19-related symptoms or exposed to risk factors have a pressing need to consult doctors. Due to hospital closure, a lot of consulting services have been moved online. Because of the shortage of medical professionals, many people cannot receive online consultations timely. To address this problem, we aim to develop a medical dialogue system that can provide COVID19-related consultations. We collected two dialogue datasets -- CovidDialog -- (in English and Chinese respectively) containing conversations between doctors and patients about COVID-19. On these two datasets, we train several dialogue generation models based on Transformer, GPT, and BERT-GPT. Since the two COVID-19 dialogue datasets are small in size, which bear high risk of overfitting, we leverage transfer learning to mitigate data deficiency. Specifically, we take the pretrained models of Transformer, GPT, and BERT-GPT on dialog datasets and other large-scale texts, then finetune them on our CovidDialog tasks. We perform both automatic and human evaluation of responses generated by these models. The results show that the generated responses are promising in being doctor-like, relevant to the conversation history, and clinically informative. The data and code are available at https://github.com/UCSD-AI4H/COVID-Dialogue.},
  archivePrefix = {arXiv},
  eprint = {2005.05442},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2020_on the generation of medical dialogues for covid-19.pdf},
  journal = {arXiv:2005.05442 [cs]},
  primaryClass = {cs}
}

@inproceedings{yang20_GraphAttentionTopic,
  title = {Graph {{Attention Topic Modeling Network}}},
  booktitle = {Proceedings of {{The Web Conference}} 2020},
  author = {Yang, Liang and Wu, Fan and Gu, Junhua and Wang, Chuan and Cao, Xiaochun and Jin, Di and Guo, Yuanfang},
  year = {2020},
  month = apr,
  pages = {144--154},
  publisher = {{ACM}},
  address = {{Taipei Taiwan}},
  doi = {10.1145/3366423.3380102},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2020_graph attention topic modeling network.pdf},
  isbn = {978-1-4503-7023-3},
  language = {en}
}

@article{yang20_PurelyUnsupervisedDisentanglement,
  title = {Towards {{Purely Unsupervised Disentanglement}} of {{Appearance}} and {{Shape}} for {{Person Images Generation}}},
  author = {Yang, Hongtao and Zhang, Tong and Huang, Wenbing and He, Xuming and Porikli, Fatih},
  year = {2020},
  month = jul,
  abstract = {There have been a fairly of research interests in exploring the disentanglement of appearance and shape from human images. Most existing endeavours pursuit this goal by either using training images with annotations or regulating the training process with external clues such as human skeleton, body segmentation or cloth patches etc. In this paper, we aim to address this challenge in a more unsupervised manner\textemdash we do not require any annotation nor any external task-specific clues. To this end, we formulate an encoder-decoder-like network to extract both the shape and appearance features from input images at the same time, and train the parameters by three losses: feature adversarial loss, color consistency loss and reconstruction loss. The feature adversarial loss mainly impose little to none mutual information between the extracted shape and appearance features, while the color consistency loss is to encourage the invariance of person appearance conditioned on different shapes. More importantly, our unsupervised1 framework utilizes learned shape features as masks which are applied to the input itself in order to obtain clean appearance features. Without using fixed input human skeleton, our network better preserves the conditional human posture while requiring less supervision. Experimental results on DeepFashion and Market1501 demonstrate that the proposed method achieves clean disentanglement and is able to synthesis novel images of comparable quality with stateof-the-art weakly-supervised or even supervised methods.},
  archivePrefix = {arXiv},
  eprint = {2007.13098},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang et al_2020_towards purely unsupervised disentanglement of appearance and shape for person images generation.pdf},
  journal = {arXiv:2007.13098 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{yang20_TensorProgramsIII,
  title = {Tensor {{Programs III}}: {{Neural Matrix Laws}}},
  shorttitle = {Tensor {{Programs III}}},
  author = {Yang, Greg},
  year = {2020},
  month = sep,
  abstract = {In a neural network (NN), \textbackslash emph\{weight matrices\} linearly transform inputs into \textbackslash emph\{preactivations\} that are then transformed nonlinearly into \textbackslash emph\{activations\}. A typical NN interleaves multitudes of such linear and nonlinear transforms to express complex functions. Thus, the (pre-)activations depend on the weights in an intricate manner. We show that, surprisingly, (pre-)activations of a randomly initialized NN become \textbackslash emph\{independent\} from the weights as the NN's widths tend to infinity, in the sense of \textbackslash emph\{asymptotic freeness\} in random matrix theory. We call this the \textbackslash emph\{Free Independence Principle (FIP)\}, which has these consequences: 1) It rigorously justifies the calculation of asymptotic Jacobian singular value distribution of an NN in Pennington et al. [36,37], essential for training ultra-deep NNs [48]. 2) It gives a new justification of \textbackslash emph\{gradient independence assumption\} used for calculating the \textbackslash emph\{Neural Tangent Kernel\} of a neural network. FIP and these results hold for any neural architecture. We show FIP by proving a Master Theorem for any Tensor Program, as introduced in Yang [50,51], generalizing the Master Theorems proved in those works. As warmup demonstrations of this new Master Theorem, we give new proofs of the semicircle and Marchenko-Pastur laws, which benchmarks our framework against these fundamental mathematical results.},
  archivePrefix = {arXiv},
  eprint = {2009.10685},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yang_2020_tensor programs iii.pdf},
  journal = {arXiv:2009.10685 [cs, math]},
  primaryClass = {cs, math}
}

@article{yariv20_MultiviewNeuralSurface,
  title = {Multiview {{Neural Surface Reconstruction}} by {{Disentangling Geometry}} and {{Appearance}}},
  author = {Yariv, Lior and Kasten, Yoni and Moran, Dror and Galun, Meirav and Atzmon, Matan and Basri, Ronen and Lipman, Yaron},
  year = {2020},
  month = oct,
  abstract = {In this work we address the challenging problem of multiview 3D surface reconstruction. We introduce a neural network architecture that simultaneously learns the unknown geometry, camera parameters, and a neural renderer that approximates the light reflected from the surface towards the camera. The geometry is represented as a zero level-set of a neural network, while the neural renderer, derived from the rendering equation, is capable of (implicitly) modeling a wide set of lighting conditions and materials. We trained our network on real world 2D images of objects with different material properties, lighting conditions, and noisy camera initializations from the DTU MVS dataset. We found our model to produce state of the art 3D surface reconstructions with high fidelity, resolution and detail.},
  archivePrefix = {arXiv},
  eprint = {2003.09852},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yariv et al_2020_multiview neural surface reconstruction by disentangling geometry and appearance.pdf},
  journal = {arXiv:2003.09852 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{yasuda19_Effectchoiceprobability,
  title = {Effect of Choice of Probability Distribution, Randomness, and Search Methods for Alignment Modeling in Sequence-to-Sequence Text-to-Speech Synthesis Using Hard Alignment},
  author = {Yasuda, Yusuke and Wang, Xin and Yamagishi, Junichi},
  year = {2019},
  month = oct,
  abstract = {Sequence-to-sequence text-to-speech (TTS) is dominated by soft-attention-based methods. Recently, hard-attention-based methods have been proposed to prevent fatal alignment errors, but their sampling method of discrete alignment is poorly investigated. This research investigates various combinations of sampling methods and probability distributions for alignment transition modeling in a hard-alignment-based sequence-to-sequence TTS method called SSNT-TTS. We clarify the common sampling methods of discrete variables including greedy search, beam search, and random sampling from a Bernoulli distribution in a more general way. Furthermore, we introduce the binary Concrete distribution to model discrete variables more properly. The results of a listening test shows that deterministic search is more preferable than stochastic search, and the binary Concrete distribution is robust with stochastic search for natural alignment transition.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.12383},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yasuda et al_2019_effect of choice of probability distribution, randomness, and search methods for alignment modeling in sequence-to-sequence text-to-speech synthesis using hard alignment.pdf;/home/trung/Zotero/storage/VWGCRIBK/1910.html},
  journal = {arXiv:1910.12383 [cs, eess, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{ye19_KnowledgeGroundedResponseGeneration,
  title = {Knowledge-{{Grounded Response Generation}} with {{Deep Attentional Latent}}-{{Variable Model}}},
  author = {Ye, Hao-Tong and Lo, Kai-Ling and Su, Shang-Yu and Chen, Yun-Nung},
  year = {2019},
  month = mar,
  abstract = {End-to-end dialogue generation has achieved promising results without using handcrafted features and attributes specific for each task and corpus. However, one of the fatal drawbacks in such approaches is that they are unable to generate informative utterances, so it limits their usage from some real-world conversational applications. This paper attempts at generating diverse and informative responses with a variational generation model, which contains a joint attention mechanism conditioning on the information from both dialogue contexts and extra knowledge.},
  archivePrefix = {arXiv},
  eprint = {1903.09813},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/ye et al_2019_knowledge-grounded response generation with deep attentional latent-variable model.pdf},
  journal = {arXiv:1903.09813 [cs]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs}
}

@article{yeh19_TransformerTransducerEndtoEndSpeech,
  title = {Transformer-{{Transducer}}: {{End}}-to-{{End Speech Recognition}} with {{Self}}-{{Attention}}},
  shorttitle = {Transformer-{{Transducer}}},
  author = {Yeh, Ching-Feng and Mahadeokar, Jay and Kalgaonkar, Kaustubh and Wang, Yongqiang and Le, Duc and Jain, Mahaveer and Schubert, Kjell and Fuegen, Christian and Seltzer, Michael L.},
  year = {2019},
  month = oct,
  abstract = {We explore options to use Transformer networks in neural transducer for end-to-end speech recognition. Transformer networks use self-attention for sequence modeling and comes with advantages in parallel computation and capturing contexts. We propose 1) using VGGNet with causal convolution to incorporate positional information and reduce frame rate for efficient inference 2) using truncated self-attention to enable streaming for Transformer and reduce computational complexity. All experiments are conducted on the public LibriSpeech corpus. The proposed Transformer-Transducer outperforms neural transducer with LSTM/BLSTM networks and achieved word error rates of 6.37 \% on the test-clean set and 15.30 \% on the test-other set, while remaining streamable, compact with 45.7M parameters for the entire system, and computationally efficient with complexity of O(T), where T is input sequence length.},
  archivePrefix = {arXiv},
  eprint = {1910.12977},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yeh et al_2019_transformer-transducer.pdf},
  journal = {arXiv:1910.12977 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{yehuda20_ItNotWhat,
  title = {It's {{Not What Machines Can Learn}}, {{It}}'s {{What We Cannot Teach}}},
  author = {Yehuda, Gal and Gabel, Moshe and Schuster, Assaf},
  year = {2020},
  month = jun,
  abstract = {Can deep neural networks learn to solve any task, and in particular problems of high complexity? This question attracts a lot of interest, with recent works tackling computationally hard tasks such as the traveling salesman problem and satisfiability. In this work we offer a different perspective on this question. Given the common assumption that NP = coNP we prove that any polynomial-time sample generator for an NP-hard problem samples, in fact, from an easier sub-problem. We empirically explore a case study, Conjunctive Query Containment, and show how common data generation techniques generate biased datasets that lead practitioners to over-estimate model accuracy. Our results suggest that machine learning approaches that require training on a dense uniform sampling from the target distribution cannot be used to solve computationally hard problems, the reason being the difficulty of generating sufficiently large and unbiased training sets.},
  archivePrefix = {arXiv},
  eprint = {2002.09398},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yehuda et al_2020_it's not what machines can learn, it's what we cannot teach.pdf},
  journal = {arXiv:2002.09398 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{yildirim14_TechnicalGlanceCosmetic,
  title = {A {{Technical Glance}} on {{Some Cosmetic Oils}}},
  author = {Yildirim, Kenan and Kostem, A Melek},
  year = {2014},
  pages = {11},
  abstract = {The properties of molecular structure, thermal behavior and UVA protection of 10 types of healthy promoting oils which are argania, almond, apricot seed, and jojoba, wheat germ, and sesame seed, avocado, cocoa, carrot and grapes seed oils were studied. FT-IR analysis was used for molecular structure. DSC analysis was used for thermal behavior and UV analysis was used for UV, visible and IR light absorption. Molecular structure and thermal behavior of avocado and cocoa are different from the others. Contrary to the spectra of avocado and cocoa on which the peaks belongs to carboxylic acid very strong, the spectra of the others do not involve carboxylic acid strong peaks. Almond, wheat germ and apricot seed oil absorb all UVA before 350 nm wave length. The UV protection property of grapes seed and cocoa is very well. Protection of wheat germ, almond and apricot seed oils to UVA is well respectively. Absorption of IR rate change from 7\% to 25\% for carrot, apricot seed, wheat germ, argania, avocado and grapes seed respectively.},
  file = {/home/trung/GoogleDrive/Zotero/yildirim et al_2014_a technical glance on some cosmetic oils.pdf},
  language = {en}
}

@incollection{yildiz19_ODE2VAEDeepgenerative,
  title = {{{ODE2VAE}}: {{Deep}} Generative Second Order {{ODEs}} with {{Bayesian}} Neural Networks},
  shorttitle = {{{ODE2VAE}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Yildiz, Cagatay and Heinonen, Markus and Lahdesmaki, Harri},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {13412--13421},
  publisher = {{Curran Associates, Inc.}},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/yildiz et al_2019_ode2vae.pdf;/home/trung/Zotero/storage/M7MDDYBM/9497-ode2vae-deep-generative-second-order-odes-with-bayesian-neural-networks.html}
}

@inproceedings{yin14_dirichletmultinomialmixture,
  title = {A Dirichlet Multinomial Mixture Model-Based Approach for Short Text Clustering},
  booktitle = {Proceedings of the 20th {{ACM SIGKDD}} International Conference on {{Knowledge}} Discovery and Data Mining - {{KDD}} '14},
  author = {Yin, Jianhua and Wang, Jianyong},
  year = {2014},
  pages = {233--242},
  publisher = {{ACM Press}},
  address = {{New York, New York, USA}},
  doi = {10.1145/2623330.2623715},
  abstract = {Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve significantly better performance than three other clustering models.},
  file = {/home/trung/GoogleDrive/Zotero/yin et al_2014_a dirichlet multinomial mixture model-based approach for short text clustering.pdf},
  isbn = {978-1-4503-2956-9},
  language = {en}
}

@article{yin18_Proteinrestrictioncancer,
  title = {Protein Restriction and Cancer},
  author = {Yin, Jie and Ren, Wenkai and Huang, Xingguo and Li, Tiejun and Yin, Yulong},
  year = {2018},
  month = apr,
  volume = {1869},
  pages = {256--262},
  issn = {0304-419X},
  doi = {10.1016/j.bbcan.2018.03.004},
  abstract = {Protein restriction without malnutrition is currently an effective nutritional intervention known to prevent diseases and promote health span from yeast to human. Recently, low protein diets are reported to be associated with lowered cancer incidence and mortality risk of cancers in human. In murine models, protein restriction inhibits tumor growth via mTOR signaling pathway. IGF-1, amino acid metabolic programing, FGF21, and autophagy may also serve as potential mechanisms of protein restriction mediated cancer prevention. Together, dietary intervention aimed at reducing protein intake can be beneficial and has the potential to be widely adopted and effective in preventing and treating cancers.},
  journal = {Biochimica et Biophysica Acta (BBA) - Reviews on Cancer},
  language = {en},
  number = {2}
}

@article{yoon00_VIMEExtendingSuccess,
  title = {{{VIME}}: {{Extending}} the {{Success}} of {{Self}}- and {{Semi}}-Supervised {{Learning}} to {{Tabular Domain}}},
  author = {Yoon, Jinsung and Jordon, James and Zhang, Yao},
  pages = {11},
  abstract = {Self- and semi-supervised learning frameworks have made significant progress in training machine learning models with limited labeled data in image and language domains. These methods heavily rely on the unique structure in the domain datasets (such as spatial relationships in images or semantic relationships in language). They are not adaptable to general tabular data which does not have the same explicit structure as image and language data. In this paper, we fill this gap by proposing novel self- and semi-supervised learning frameworks for tabular data, which we refer to collectively as VIME (Value Imputation and Mask Estimation). We create a novel pretext task of estimating mask vectors from corrupted tabular data in addition to the reconstruction pretext task for self-supervised learning. We also introduce a novel tabular data augmentation method for self- and semi-supervised learning frameworks. In experiments, we evaluate the proposed framework in multiple tabular datasets from various application domains, such as genomics and clinical data. VIME exceeds state-of-the-art performance in comparison to the existing baseline methods.},
  file = {/home/trung/GoogleDrive/Zotero/yoon et al_vime.pdf},
  language = {en}
}

@article{yoon18_InferenceProbabilisticGraphical,
  title = {Inference in {{Probabilistic Graphical Models}} by {{Graph Neural Networks}}},
  author = {Yoon, KiJung and Liao, Renjie and Xiong, Yuwen and Zhang, Lisa and Fetaya, Ethan and Urtasun, Raquel and Zemel, Richard and Pitkow, Xaq},
  year = {2018},
  month = mar,
  abstract = {A fundamental computation for statistical inference and accurate decision-making is to compute the marginal probabilities or most probable states of task-relevant variables. Probabilistic graphical models can efficiently represent the structure of such complex data, but performing these inferences is generally difficult. Message-passing algorithms, such as belief propagation, are a natural way to disseminate evidence amongst correlated variables while exploiting the graph structure, but these algorithms can struggle when the conditional dependency graphs contain loops. Here we use Graph Neural Networks (GNNs) to learn a message-passing algorithm that solves these inference tasks. We first show that the architecture of GNNs is well-matched to inference tasks. We then demonstrate the efficacy of this inference approach by training GNNs on a collection of graphical models and showing that they substantially outperform belief propagation on loopy graphs. Our message-passing algorithms generalize out of the training set to larger graphs and graphs with different structure.},
  archivePrefix = {arXiv},
  eprint = {1803.07710},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yoon et al_2018_inference in probabilistic graphical models by graph neural networks.pdf;/home/trung/Zotero/storage/NIHRSDQF/1803.html},
  journal = {arXiv:1803.07710 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning,variational},
  primaryClass = {cs, stat}
}

@article{yoon19_PluginMethodRepresentation,
  title = {A {{Plug}}-in {{Method}} for {{Representation Factorization}}},
  author = {Yoon, Jee Seok and Roh, Myung-Cheol and Suk, Heung-Il},
  year = {2019},
  month = may,
  abstract = {In this work, we focus on decomposing the latent representations in GANs or learned feature representations in deep auto-encoders into semantically controllable factors in a semi-supervised manner, without modifying the original trained models. Specifically, we propose a Factors Decomposer-Entangler Network (FDEN) that learns to decompose a latent representation into mutually independent factors. Given a latent representation, the proposed framework draws a set of interpretable factors, each aligned to independent factors of variations by maximizing their total correlation in an information-theoretic means. As a plug-in method, we have applied our proposed FDEN to the existing networks of Adversarially Learned Inference and Pioneer Network and conducted computer vision tasks of image-to-image translation in semantic ways, e.g., changing styles while keeping an identify of a subject, and object classification in a few-shot learning scheme. We have also validated the effectiveness of our method with various ablation studies in qualitative, quantitative, and statistical examination.},
  archivePrefix = {arXiv},
  eprint = {1905.11088},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yoon et al_2019_a plug-in method for representation factorization.pdf},
  journal = {arXiv:1905.11088 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{you19_DeepNeuralNetwork,
  title = {Deep {{Neural Network Embeddings}} with {{Gating Mechanisms}} for {{Text}}-{{Independent Speaker Verification}}},
  author = {You, Lanhua and Guo, Wu and Dai, Lirong and Du, Jun},
  year = {2019},
  month = apr,
  abstract = {In this paper, gating mechanisms are applied in deep neural network (DNN) training for x-vector-based text-independent speaker verification. First, a gated convolution neural network (GCNN) is employed for modeling the frame-level embedding layers. Compared with the time-delay DNN (TDNN), the GCNN can obtain more expressive frame-level representations through carefully designed memory cell and gating mechanisms. Moreover, we propose a novel gated-attention statistics pooling strategy in which the attention scores are shared with the output gate. The gated-attention statistics pooling combines both gating and attention mechanisms into one framework; therefore, we can capture more useful information in the temporal pooling layer. Experiments are carried out using the NIST SRE16 and SRE18 evaluation datasets. The results demonstrate the effectiveness of the GCNN and show that the proposed gated-attention statistics pooling can further improve the performance.},
  archivePrefix = {arXiv},
  eprint = {1903.12092},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/you et al_2019_deep neural network embeddings with gating mechanisms for text-independent speaker verification.pdf;/home/trung/GoogleDrive/Zotero/you et al_2019_deep neural network embeddings with gating mechanisms for text-independent speaker verification2.pdf;/home/trung/Zotero/storage/Z5R4U88T/1903.html},
  journal = {arXiv:1903.12092 [cs, eess]},
  keywords = {attention,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,gating},
  primaryClass = {cs, eess}
}

@article{you19_GraphConvolutionalPolicy,
  title = {Graph {{Convolutional Policy Network}} for {{Goal}}-{{Directed Molecular Graph Generation}}},
  author = {You, Jiaxuan and Liu, Bowen and Ying, Rex and Pande, Vijay and Leskovec, Jure},
  year = {2019},
  month = feb,
  abstract = {Generating novel graph structures that optimize given objectives while obeying some given underlying rules is fundamental for chemistry, biology and social science research. This is especially important in the task of molecular graph generation, whose goal is to discover novel molecules with desired properties such as drug-likeness and synthetic accessibility, while obeying physical laws such as chemical valency. However, designing models to find molecules that optimize desired properties while incorporating highly complex and non-differentiable rules remains to be a challenging task. Here we propose Graph Convolutional Policy Network (GCPN), a general graph convolutional network based model for goaldirected graph generation through reinforcement learning. The model is trained to optimize domain-specific rewards and adversarial loss through policy gradient, and acts in an environment that incorporates domain-specific rules. Experimental results show that GCPN can achieve 61\% improvement on chemical property optimization over state-of-the-art baselines while resembling known molecules, and achieve 184\% improvement on the constrained property optimization task.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1806.02473},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/you et al_2019_graph convolutional policy network for goal-directed molecular graph generation.pdf},
  journal = {arXiv:1806.02473 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{you19_MultiTaskLearningHighOrder,
  title = {Multi-{{Task Learning}} with {{High}}-{{Order Statistics}} for {{X}}-Vector Based {{Text}}-{{Independent Speaker Verification}}},
  author = {You, Lanhua and Guo, Wu and Dai, Lirong and Du, Jun},
  year = {2019},
  month = apr,
  abstract = {The x-vector based deep neural network (DNN) embedding systems have demonstrated effectiveness for text-independent speaker verification. This paper presents a multi-task learning architecture for training the speaker embedding DNN with the primary task of classifying the target speakers, and the auxiliary task of reconstructing the first- and higher-order statistics of the original input utterance. The proposed training strategy aggregates both the supervised and unsupervised learning into one framework to make the speaker embeddings more discriminative and robust. Experiments are carried out using the NIST SRE16 evaluation dataset and the VOiCES dataset. The results demonstrate that our proposed method outperforms the original x-vector approach with very low additional complexity added.},
  archivePrefix = {arXiv},
  eprint = {1903.12058},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/you et al_2019_multi-task learning with high-order statistics for x-vector based text-independent speaker verification.pdf;/home/trung/Zotero/storage/WCCGCTCC/1903.html},
  journal = {arXiv:1903.12058 [cs, eess]},
  keywords = {Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing},
  primaryClass = {cs, eess}
}

@article{yu16_Associationsnutconsumption,
  title = {Associations between Nut Consumption and Inflammatory Biomarkers},
  author = {Yu, Zhi and Malik, Vasanti S and Keum, NaNa and Hu, Frank B and Giovannucci, Edward L and Stampfer, Meir J and Willett, Walter C and Fuchs, Charles S and Bao, Ying},
  year = {2016},
  month = sep,
  volume = {104},
  pages = {722--728},
  issn = {0002-9165, 1938-3207},
  doi = {10.3945/ajcn.116.134205},
  abstract = {Background: Increased nut consumption has been associated with reduced risk of cardiovascular disease and type 2 diabetes, as well as a healthy lipid profile. However, the associations between nut consumption and inflammatory biomarkers are unclear. Objective: We investigated habitual nut consumption in relation to inflammatory biomarkers in 2 large cohorts of US men and women. Design: We analyzed cross-sectional data from 5013 participants in the Nurses' Health Study (NHS) and Health Professionals FollowUp Study (HPFS) who were free of diabetes. Nut intake, defined as intake of peanuts and other nuts, was estimated from foodfrequency questionnaires, and cumulative averages from 1986 and 1990 in the NHS and from 1990 and 1994 in the HPFS were used. Plasma biomarkers were collected in 1989\textendash 1990 in the NHS and 1993\textendash 1995 in the HPFS. Multivariate linear regression was used to assess the associations of nut consumption with fasting plasma C-reactive protein (CRP, n = 4941), interleukin 6 (IL-6, n = 2859), and tumor necrosis factor receptor 2 (TNFR2, n = 2905). Results: A greater intake of nuts was associated with lower amounts of a subset of inflammatory biomarkers, after adjusting for demographic, medical, dietary, and lifestyle variables. The relative concentrations (ratios) and 95\% CIs comparing subjects with nut intake of \$5 times/wk and those in the categories of never or almost never were as follows: CRP: 0.80 (0.69, 0.90), P-trend = 0.0003; and IL-6: 0.86 (0.77, 0.97), P-trend = 0.006. These associations remained significant after further adjustment for body mass index. No significant association was observed with TNFR2. Substituting 3 servings of nuts/wk for 3 servings of red meat, processed meat, eggs, or refined grains/wk was associated with significantly lower CRP (all P , 0.0001) and IL-6 (P ranges from 0.001 to 0.017). Conclusion: Frequent nut consumption was associated with a healthy profile of inflammatory biomarkers. Am J Clin Nutr 2016;104:722\textendash 8.},
  annotation = {ZSCC: 0000041},
  file = {/home/trung/GoogleDrive/Zotero/yu et al_2016_associations between nut consumption and inflammatory biomarkers.pdf},
  journal = {The American Journal of Clinical Nutrition},
  language = {en},
  number = {3}
}

@article{yu18_QANetCombiningLocal,
  title = {{{QANet}}: {{Combining Local Convolution}} with {{Global Self}}-{{Attention}} for {{Reading Comprehension}}},
  shorttitle = {{{QANet}}},
  author = {Yu, Adams Wei and Dohan, David and Luong, Minh-Thang and Zhao, Rui and Chen, Kai and Norouzi, Mohammad and Le, Quoc V.},
  year = {2018},
  month = apr,
  abstract = {Current end-to-end machine reading and question answering (Q\textbackslash\&A) models are primarily based on recurrent neural networks (RNNs) with attention. Despite their success, these models are often slow for both training and inference due to the sequential nature of RNNs. We propose a new Q\textbackslash\&A architecture called QANet, which does not require recurrent networks: Its encoder consists exclusively of convolution and self-attention, where convolution models local interactions and self-attention models global interactions. On the SQuAD dataset, our model is 3x to 13x faster in training and 4x to 9x faster in inference, while achieving equivalent accuracy to recurrent models. The speed-up gain allows us to train the model with much more data. We hence combine our model with data generated by backtranslation from a neural machine translation model. On the SQuAD dataset, our single model, trained with augmented data, achieves 84.6 F1 score on the test set, which is significantly better than the best published F1 score of 81.8.},
  annotation = {ZSCC: 0000184},
  archivePrefix = {arXiv},
  eprint = {1804.09541},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yu et al_2018_qanet.pdf;/home/trung/Zotero/storage/HGQN44HW/1804.html},
  journal = {arXiv:1804.09541 [cs]},
  keywords = {attention,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,global attention},
  primaryClass = {cs}
}

@inproceedings{yu19_VAEGANCollaborativeFiltering,
  title = {{{VAEGAN}}: {{A Collaborative Filtering Framework}} Based on {{Adversarial Variational Autoencoders}}},
  shorttitle = {{{VAEGAN}}},
  booktitle = {Proceedings of the {{Twenty}}-{{Eighth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Yu, Xianwen and Zhang, Xiaoning and Cao, Yang and Xia, Min},
  year = {2019},
  month = aug,
  pages = {4206--4212},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Macao, China}},
  doi = {10.24963/ijcai.2019/584},
  abstract = {Recently, Variational Autoencoders (VAEs) have been successfully applied to collaborative filtering for implicit feedback. However, the performance of the resulting model depends a lot on the expressiveness of the inference model and the latent representation is often too constrained to be expressive enough to capture the true posterior distribution. In this paper, a novel framework named VAEGAN is proposed to address the above issue. In VAEGAN, we first introduce Adversarial Variational Bayes (AVB) to train Variational Autoencoders with arbitrarily expressive inference model. By utilizing Generative Adversarial Networks (GANs) for implicit variational inference, the inference model provides better approximation to the posterior and maximum-likelihood assignment. Then the performance of our model is further improved by introducing an auxiliary discriminative network using adversarial training to achieve high accuracy in recommendation. Furthermore, contractive loss is added to the classical reconstruction cost function as a penalty term to yield robust features and improve the generalization performance. Finally, we show that the performance of our proposed VAEGAN significantly outperforms state-ofthe-art baselines on several real-world datasets.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/yu et al_2019_vaegan.pdf},
  isbn = {978-0-9992411-4-1},
  language = {en}
}

@article{yu20_GradientSurgeryMultiTask,
  title = {Gradient {{Surgery}} for {{Multi}}-{{Task Learning}}},
  author = {Yu, Tianhe and Kumar, Saurabh and Gupta, Abhishek and Levine, Sergey and Hausman, Karol and Finn, Chelsea},
  year = {2020},
  month = jan,
  abstract = {While deep learning and deep reinforcement learning (RL) systems have demonstrated impressive results in domains such as image classification, game playing, and robotic control, data efficiency remains a major challenge. Multi-task learning has emerged as a promising approach for sharing structure across multiple tasks to enable more efficient learning. However, the multi-task setting presents a number of optimization challenges, making it difficult to realize large efficiency gains compared to learning tasks independently. The reasons why multi-task learning is so challenging compared to single-task learning are not fully understood. In this work, we identify a set of three conditions of the multi-task optimization landscape that cause detrimental gradient interference, and develop a simple yet general approach for avoiding such interference between task gradients. We propose a form of gradient surgery that projects a task's gradient onto the normal plane of the gradient of any other task that has a conflicting gradient. On a series of challenging multi-task supervised and multi-task RL problems, this approach leads to substantial gains in efficiency and performance. Further, it is model-agnostic and can be combined with previously-proposed multitask architectures for enhanced performance.},
  archivePrefix = {arXiv},
  eprint = {2001.06782},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/W2ZRFZ67/Yu et al. - 2020 - Gradient Surgery for Multi-Task Learning.pdf},
  journal = {arXiv:2001.06782 [cs, stat]},
  keywords = {favorite},
  language = {en},
  primaryClass = {cs, stat}
}

@article{yu20_scATACprocomprehensiveworkbench,
  title = {{{scATAC}}-pro: A Comprehensive Workbench for Single-Cell Chromatin Accessibility Sequencing Data},
  shorttitle = {{{scATAC}}-Pro},
  author = {Yu, Wenbao and Uzun, Yasin and Zhu, Qin and Chen, Changya and Tan, Kai},
  year = {2020},
  month = dec,
  volume = {21},
  pages = {94},
  issn = {1474-760X},
  doi = {10.1186/s13059-020-02008-0},
  abstract = {Single-cell chromatin accessibility sequencing has become a powerful technology for understanding epigenetic heterogeneity of complex tissues. However, there is a lack of open-source software for comprehensive processing, analysis, and visualization of such data generated using all existing experimental protocols. Here, we present scATAC-pro for quality assessment, analysis, and visualization of single-cell chromatin accessibility sequencing data. scATAC-pro computes a range of quality control metrics for several key steps of experimental protocols, with a flexible choice of methods. It generates summary reports for both quality assessment and downstream analysis. scATAC-pro is available at https://github.com/tanlabcode/scATAC-pro.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/yu et al_2020_scatac-pro.pdf},
  journal = {Genome Biology},
  language = {en},
  number = {1}
}

@article{yu20_SurveyKnowledgeEnhancedText,
  title = {A {{Survey}} of {{Knowledge}}-{{Enhanced Text Generation}}},
  author = {Yu, Wenhao and Zhu, Chenguang and Li, Zaitang and Hu, Zhiting and Wang, Qingyun and Ji, Heng and Jiang, Meng},
  year = {2020},
  month = oct,
  abstract = {The goal of text generation is to make machines express in human language. It is one of the most important yet challenging tasks in natural language processing (NLP). Since 2014, various neural encoder-decoder models pioneered by Seq2Seq have been proposed to achieve the goal by learning to map input text to output text. However, the input text alone often provides limited knowledge to generate the desired output, so the performance of text generation is still far from satisfaction in many real-world scenarios. To address this issue, researchers have considered incorporating various forms of knowledge beyond the input text into the generation models. This research direction is known as knowledge-enhanced text generation. In this survey, we present a comprehensive review of the research on knowledge enhanced text generation over the past five years. The main content includes two parts: (i) general methods and architectures for integrating knowledge into text generation; (ii) specific techniques and applications according to different forms of knowledge data. This survey can have broad audiences, researchers and practitioners, in academia and industry.},
  archivePrefix = {arXiv},
  eprint = {2010.04389},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yu et al_2020_a survey of knowledge-enhanced text generation.pdf},
  journal = {arXiv:2010.04389 [cs]},
  primaryClass = {cs}
}

@article{yu20_TutorialVAEsBayes,
  title = {A {{Tutorial}} on {{VAEs}}: {{From Bayes}}' {{Rule}} to {{Lossless Compression}}},
  shorttitle = {A {{Tutorial}} on {{VAEs}}},
  author = {Yu, Ronald},
  year = {2020},
  month = jun,
  abstract = {The Variational Auto-Encoder (VAE) is a simple, efficient, and popular deep maximum likelihood model. Though usage of VAEs is widespread, the derivation of the VAE is not as widely understood. In this tutorial, we will provide an overview of the VAE and a tour through various derivations and interpretations of the VAE objective. From a probabilistic standpoint, we will examine the VAE through the lens of Bayes' Rule, importance sampling, and the change-of-variables formula. From an information theoretic standpoint, we will examine the VAE through the lens of lossless compression and transmission through a noisy channel. We will then identify two common misconceptions over the VAE formulation and their practical consequences. Finally, we will visualize the capabilities and limitations of VAEs using a code example (with an accompanying Jupyter notebook) on toy 2D data.},
  archivePrefix = {arXiv},
  eprint = {2006.10273},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yu_2020_a tutorial on vaes.pdf},
  journal = {arXiv:2006.10273 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{yu20_WaveletFlowFast,
  title = {Wavelet {{Flow}}: {{Fast Training}} of {{High Resolution Normalizing Flows}}},
  shorttitle = {Wavelet {{Flow}}},
  author = {Yu, Jason J. and Derpanis, Konstantinos G. and Brubaker, Marcus A.},
  year = {2020},
  month = oct,
  abstract = {Normalizing flows are a class of probabilistic generative models which allow for both fast density computation and efficient sampling and are effective at modelling complex distributions like images. A drawback among current methods is their significant training cost, sometimes requiring months of GPU training time to achieve state-of-the-art results. This paper introduces Wavelet Flow, a multi-scale, normalizing flow architecture based on wavelets. A Wavelet Flow has an explicit representation of signal scale that inherently includes models of lower resolution signals and conditional generation of higher resolution signals, i.e., super resolution. A major advantage of Wavelet Flow is the ability to construct generative models for high resolution data (e.g., 1024 x 1024 images) that are impractical with previous models. Furthermore, Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15x faster to train.},
  archivePrefix = {arXiv},
  eprint = {2010.13821},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/yu et al_2020_wavelet flow.pdf},
  journal = {arXiv:2010.13821 [cs]},
  primaryClass = {cs}
}

@article{yue18_DeepLearningGenomics,
  title = {Deep {{Learning}} for {{Genomics}}: {{A Concise Overview}}},
  shorttitle = {Deep {{Learning}} for {{Genomics}}},
  author = {Yue, Tianwei and Wang, Haohan},
  year = {2018},
  month = may,
  abstract = {Advancements in genomic research such as high-throughput sequencing techniques have driven modern genomic studies into "big data" disciplines. This data explosion is constantly challenging conventional methods used in genomics. In parallel with the urgent demand for robust algorithms, deep learning has succeeded in a variety of fields such as vision, speech, and text processing. Yet genomics entails unique challenges to deep learning since we are expecting from deep learning a superhuman intelligence that explores beyond our knowledge to interpret the genome. A powerful deep learning model should rely on insightful utilization of task-specific knowledge. In this paper, we briefly discuss the strengths of different deep learning models from a genomic perspective so as to fit each particular task with a proper deep architecture, and remark on practical considerations of developing modern deep learning architectures for genomics. We also provide a concise review of deep learning applications in various aspects of genomic research, as well as pointing out potential opportunities and obstacles for future genomics applications.},
  annotation = {ZSCC: 0000032},
  archivePrefix = {arXiv},
  eprint = {1802.00810},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/S848X232/Yue and Wang - 2018 - Deep Learning for Genomics A Concise Overview.pdf},
  journal = {arXiv:1802.00810 [cs, q-bio]},
  language = {en},
  primaryClass = {cs, q-bio}
}

@inproceedings{yun20_AreTransformersuniversal,
  title = {Are {{Transformers}} Universal Approximators of Sequence-to-Sequence Functions?},
  booktitle = {International Conference on Learning Representations},
  author = {Yun, Chulhee and Bhojanapalli, Srinadh and Rawat, Ankit Singh and Reddi, Sashank and Kumar, Sanjiv},
  year = {2020},
  file = {/home/trung/GoogleDrive/Zotero/yun et al_2020_are transformers universal approximators of sequence-to-sequence functions.pdf}
}

@book{zaccone15_Pythonparallelprogramming,
  title = {Python Parallel Programming Cookbook: Master Efficient Parallel Programming to Build Powerful Applications Using {{Python}} - {{Quick}} Answers to Common Problems},
  shorttitle = {Python Parallel Programming Cookbook},
  author = {Zaccone, Giancarlo},
  year = {2015},
  publisher = {{Packt Publishing}},
  address = {{Birmingham}},
  annotation = {OCLC: 938791996},
  file = {/home/trung/GoogleDrive/Zotero/zaccone_2015_python parallel programming cookbook.pdf},
  isbn = {978-1-78528-958-3},
  language = {en},
  series = {Community Experience Distilled}
}

@techreport{zachariadis20_highlyscalablemethod,
  title = {A Highly Scalable Method for Joint Whole Genome Sequencing and Gene Expression Profiling of Single Cells},
  author = {Zachariadis, Vasilios and Cheng, Huaitao and Andrews, Nathanael J and Enge, Martin},
  year = {2020},
  month = mar,
  institution = {{Genomics}},
  doi = {10.1101/2020.03.04.976530},
  abstract = {Understanding how genetic variation alters gene expression - how genotype affects phenotype - is a central challenge in biology. To address this question in complex cell mixtures, we developed Direct Nuclear Tagmentation and RNA-sequencing (DNTR-seq), which enables whole genome and mRNA sequencing jointly in single cells. When applied to biobanked leukemia samples, DNTR-seq readily identified minor subclones within patients, as well as cell-type specific gene editing such as T-cell receptor rearrangements. mRNA-seq quality is equal to RNA-only methods, and the high yield combined with low positional bias of the genomic library preparation allows detection of sub-megabase aberrations at ultra low coverage of 0.5-3M read pairs per genome. Since each cell library is individually addressable, rare subpopulations can be re-sequenced at increased depth, allowing multi-tiered study designs where depth of sequencing is informed by previous results. In addition, the direct tagmentation protocol enables coverage-independent estimation of ploidy, which can be used to unambiguously identify cell singlets. Thus, DNTR-seq directly links each cell's state to its corresponding genome at a scale enabling routine analysis of heterogeneous tumors and other complex tissues.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/GoogleDrive/Zotero/zachariadis et al_2020_a highly scalable method for joint whole genome sequencing and gene expression profiling of single cells.pdf},
  keywords = {DNTR-seq},
  language = {en},
  type = {Preprint}
}

@article{zafrir19_Q8BERTQuantized8Bit,
  title = {{{Q8BERT}}: {{Quantized 8Bit BERT}}},
  shorttitle = {{{Q8BERT}}},
  author = {Zafrir, Ofir and Boudoukh, Guy and Izsak, Peter and Wasserblat, Moshe},
  year = {2019},
  month = oct,
  abstract = {Recently, pre-trained Transformer based language models such as BERT and GPT, have shown great improvement in many Natural Language Processing (NLP) tasks. However, these models contain a large amount of parameters. The emergence of even larger and more accurate models such as GPT2 and Megatron, suggest a trend of large pre-trained Transformer models. However, using these large models in production environments is a complex task requiring a large amount of compute, memory and power resources. In this work we show how to perform quantization-aware training during the fine-tuning phase of BERT in order to compress BERT by \$4\textbackslash times\$ with minimal accuracy loss. Furthermore, the produced quantized model can accelerate inference speed if it is optimized for 8bit Integer supporting hardware.},
  annotation = {ZSCC: 0000002},
  archivePrefix = {arXiv},
  eprint = {1910.06188},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zafrir et al_2019_q8bert.pdf;/home/trung/Zotero/storage/KZHNNE34/1910.html},
  journal = {arXiv:1910.06188 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{zagoruyko17_PAYINGMOREATTENTION,
  title = {{{PAYING MORE ATTENTION TO ATTENTION}}: {{IMPROVING THE PERFORMANCE OF CONVOLUTIONAL NEURAL NETWORKS VIA ATTENTION TRANSFER}}},
  author = {Zagoruyko, Sergey and Komodakis, Nikos},
  year = {2017},
  pages = {13},
  abstract = {Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at https://github.com/szagoruyko/attention-transfer.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/zagoruyko et al_2017_paying more attention to attention.pdf},
  keywords = {attention,transfer learning},
  language = {en}
}

@article{zaidi20_RobustnessTransformationsDriven,
  title = {Is {{Robustness To Transformations Driven}} by {{Invariant Neural Representations}}?},
  author = {Zaidi, Syed Suleman Abbas and Boix, Xavier and Prasad, Neeraj and {Gilad-Gutnick}, Sharon and {Ben-Ami}, Shlomit and Sinha, Pawan},
  year = {2020},
  month = jul,
  abstract = {Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (e.g. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. Yet, to what extent this hypothesis holds true is an outstanding question, as including transformations in the training set could lead to properties different from invariance, e.g. parts of the network could be specialized to recognize either transformed or non-transformed images. In this paper, we analyze the conditions under which invariance emerges. To do so, we leverage that invariant representations facilitate robustness to transformations for object categories that are not seen transformed during training. Our results with state-of-the-art DCNNs indicate that invariant representations strengthen as the number of transformed categories in the training set is increased. This is much more prominent with local transformations such as blurring and high-pass filtering, compared to geometric transformations such as rotation and thinning, that entail changes in the spatial arrangement of the object. Our results contribute to a better understanding of invariant representations in deep learning, and the conditions under which invariance spontaneously emerges.},
  archivePrefix = {arXiv},
  eprint = {2007.00112},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zaidi et al_2020_is robustness to transformations driven by invariant neural representations.pdf},
  journal = {arXiv:2007.00112 [cs]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs}
}

@article{zamanighomi18_Unsupervisedclusteringepigenetic,
  title = {Unsupervised Clustering and Epigenetic Classification of Single Cells},
  author = {Zamanighomi, Mahdi and Lin, Zhixiang and Daley, Timothy and Chen, Xi and Duren, Zhana and Schep, Alicia and Greenleaf, William J. and Wong, Wing Hung},
  year = {2018},
  month = dec,
  volume = {9},
  pages = {2410},
  issn = {2041-1723},
  doi = {10.1038/s41467-018-04629-3},
  annotation = {ZSCC: 0000021},
  file = {/home/trung/GoogleDrive/Zotero/zamanighomi et al_2018_unsupervised clustering and epigenetic classification of single cells.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{zame20_MachineLearningClinical,
  title = {Machine {{Learning}} for {{Clinical Trials}} in the {{Era}} of {{COVID}}-19},
  author = {Zame, William R. and Bica, Ioana and Shen, Cong and Curth, Alicia and Lee, Hyun-Suk and Bailey, Stuart and Weatherall, James and Wright, David and Bretz, Frank and {van der Schaar}, Mihaela},
  year = {2020},
  month = aug,
  pages = {1--12},
  issn = {1946-6315},
  doi = {10.1080/19466315.2020.1797867},
  file = {/home/trung/GoogleDrive/Zotero/zame et al_2020_machine learning for clinical trials in the era of covid-19.pdf},
  journal = {Statistics in Biopharmaceutical Research},
  language = {en}
}

@article{zand20_NetworkBasedSingleCellRNASeq,
  title = {Network-{{Based Single}}-{{Cell RNA}}-{{Seq Data Imputation Enhances Cell Type Identification}}},
  author = {Zand, Maryam and Ruan, Jianhua},
  year = {2020},
  month = mar,
  volume = {11},
  pages = {377},
  issn = {2073-4425},
  doi = {10.3390/genes11040377},
  abstract = {Single-cell RNA sequencing is a powerful technology for obtaining transcriptomes at single-cell resolutions. However, it suffers from dropout events (i.e., excess zero counts) since only a small fraction of transcripts get sequenced in each cell during the sequencing process. This inherent sparsity of expression profiles hinders further characterizations at cell/gene-level such as cell type identification and downstream analysis. To alleviate this dropout issue we introduce a network-based method, netImpute, by leveraging the hidden information in gene co-expression networks to recover real signals. netImpute employs Random Walk with Restart (RWR) to adjust the gene expression level in a given cell by borrowing information from its neighbors in a gene co-expression network. Performance evaluation and comparison with existing tools on simulated data and seven real datasets show that netImpute substantially enhances clustering accuracy and data visualization clarity, thanks to its effective treatment of dropouts. While the idea of netImpute is general and can be applied with other types of networks such as cell co-expression network or protein\textendash protein interaction (PPI) network, evaluation results show that gene co-expression network is consistently more beneficial, presumably because PPI network usually lacks cell type context, while cell co-expression network can cause information loss for rare cell types. Evaluation results on several biological datasets show that netImpute can more effectively recover missing transcripts in scRNA-seq data and enhance the identification and visualization of heterogeneous cell types than existing methods.},
  annotation = {ZSCC: 0000000},
  file = {/home/trung/Zotero/storage/DWUCPTT8/Zand and Ruan - 2020 - Network-Based Single-Cell RNA-Seq Data Imputation .pdf},
  journal = {Genes},
  language = {en},
  number = {4}
}

@article{zeinali18_HowImproveYour,
  title = {How to {{Improve Your Speaker Embeddings Extractor}} in {{Generic Toolkits}}},
  author = {Zeinali, Hossein and Burget, Lukas and Rohdin, Johan and Stafylakis, Themos and Cernocky, Jan},
  year = {2018},
  month = nov,
  abstract = {Recently, speaker embeddings extracted with deep neural networks became the state-of-the-art method for speaker verification. In this paper we aim to facilitate its implementation on a more generic toolkit than Kaldi, which we anticipate to enable further improvements on the method. We examine several tricks in training, such as the effects of normalizing input features and pooled statistics, different methods for preventing overfitting as well as alternative non-linearities that can be used instead of Rectifier Linear Units. In addition, we investigate the difference in performance between TDNN and CNN, and between two types of attention mechanism. Experimental results on Speaker in the Wild, SRE 2016 and SRE 2018 datasets demonstrate the effectiveness of the proposed implementation.},
  archivePrefix = {arXiv},
  eprint = {1811.02066},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zeinali et al_2018_how to improve your speaker embeddings extractor in generic toolkits.pdf;/home/trung/Zotero/storage/XCV9CM25/1811.html},
  journal = {arXiv:1811.02066 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,x-vector},
  primaryClass = {cs, eess}
}

@techreport{zeng20_AccuratelyClusteringSinglecell,
  title = {Accurately {{Clustering Single}}-Cell {{RNA}}-Seq Data by {{Capturing Structural Relations}} between {{Cells}} through {{Graph Convolutional Network}}},
  author = {Zeng, Yuansong and Zhou, Xiang and Rao, Jiahua and Lu, Yutong and Yang, Yuedong},
  year = {2020},
  month = sep,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.09.02.278804},
  abstract = {Recent advances in single-cell RNA sequencing (scRNA-seq) technologies provide a great opportunity to study gene expression at cellular resolution, and the scRNA-seq data has been routinely conducted to unfold cell heterogeneity and diversity. A critical step for the scRNA-seq analyses is to cluster the same type of cells, and many methods have been developed for cell clustering. However, existing clustering methods are limited to extract the representations from expression data of individual cells, while ignoring the high-order structural relations between cells. Here, we proposed a new method (GraphSCC) to cluster cells based on scRNA-seq data by accounting structural relations between cells through a graph convolutional network. The representation learned from the graph convolutional network, together with another representation output from a denoising autoencoder network, are optimized by a dual self-supervised module for better cell clustering. Extensive experiments indicate that GraphSCC model outperforms state-of-the-art methods in various evaluation metrics on both simulated and real datasets. Further visualizations show that GraphSCC provides representations for better intra-cluster compactness and inter-cluster separability.},
  file = {/home/trung/GoogleDrive/Zotero/zeng et al_2020_accurately clustering single-cell rna-seq data by capturing structural relations between cells through graph convolutional network.pdf},
  language = {en},
  type = {Preprint}
}

@article{zhang00_DVAEVariationalAutoencoder,
  title = {D-{{VAE}}: {{A Variational Autoencoder}} for {{Directed Acyclic Graphs}}},
  author = {Zhang, Muhan and Jiang, Shali and Cui, Zhicheng and Garnett, Roman and Chen, Yixin},
  pages = {13},
  abstract = {Graph structured data are abundant in the real world. Among different graph types, directed acyclic graphs (DAGs) are of particular interest to machine learning researchers, as many machine learning models are realized as computations on DAGs, including neural networks and Bayesian networks. In this paper, we study deep generative models for DAGs, and propose a novel DAG variational autoencoder (D-VAE). To encode DAGs into the latent space, we leverage graph neural networks. We propose an asynchronous message passing scheme that allows encoding the computations on DAGs, rather than using existing simultaneous message passing schemes to encode local graph structures. We demonstrate the effectiveness of our proposed D-VAE through two tasks: neural architecture search and Bayesian network structure learning. Experiments show that our model not only generates novel and valid DAGs, but also produces a smooth latent space that facilitates searching for DAGs with better performance through Bayesian optimization.},
  annotation = {ZSCC: 0000003},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_d-vae.pdf},
  language = {en}
}

@article{zhang00_StackingVAEGAN,
  title = {Stacking {{VAE}} and {{GAN}} for {{Context}}-Aware {{Text}}-to-{{Image Generation}}},
  author = {Zhang, Chenrui and Peng, Yuxin},
  pages = {5},
  abstract = {Generating high-quality images based on text descriptions is an appealing research topic, which has widespread applications in various fields. However, it is quite challenging since that images and language descriptions in real world are noisy with great variability. Most existing text-to-image methods aim to generate images in a holistic manner, which ignore the difference between images' foreground and background, resulting in that objects in images are easily disturbed by the background. Moreover, they commonly ignore the complementarity of different kinds of generative models. In this paper, we propose a context-aware approach to perform text-to-image generation, which separates background and foreground for generating highquality images, as well as utilizes complementarity between Variational Autoencoder (VAE) and Generative Adversarial Network (GAN) for robust text-to-image generation. First, context-aware conditional VAE is proposed to capture images' basic layout and color based on text, which pays different attention on the background and foreground of images for effective text-image alignment. Then, conditional GAN is adopted for refining the generation of VAE, which recovers lost details and corrects the defects for realistic image generation. Attributed to such stacked VAE-GAN structure, two kinds of generative models can boost each other for more effective and stable text-to-image generation. Experimental results on 2 widely-used datasets empirically verify the effectiveness of our proposed approach.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_stacking vae and gan for context-aware text-to-image generation.pdf},
  language = {en}
}

@article{zhang00_UTDCRSSSystems2018,
  title = {{{UTD}}-{{CRSS Systems}} for 2018 {{NIST Speaker Recognition Evaluation}}},
  author = {Zhang, Chunlei and Bahmaninezhad, Fahimeh and Ranjan, Shivesh and Dubey, Harishchandra and Xia, Wei and Hansen, John H L},
  pages = {5},
  abstract = {In this study, we present systems submitted by the Center for Robust Speech Systems (CRSS) from UTDallas to NIST SRE 2018 (SRE18). Three alternative front-end speaker embedding frameworks are investigated, that includes: (i) i-vector, (ii) x-vector, (iii) and a modified triplet speaker embedding system (t-vector). Similar to the previous SRE, language mismatch between training and enrollment/test data, the so-called domain mismatch, remains as a major challenge in this evaluation. In addition, SRE18 also introduces a small portion of audio from an unstructured video corpus in which speaker detection/diarization is supposedly needed to be effectively integrated into speaker recognition for system robustness. In our system development, we focused on: (i) building novel deep neural network based speaker discriminative embedding systems as utterance level feature representations, (ii) exploring alternative dimension reduction methods, back-end classifiers, score normalization techniques which can incorporate unlabeled in-domain data for domain adaptation, (iii) finding an improved data set configurations for the speaker embedding network, LDA/PLDA, and score calibration training (v) and finally, investigating effective score calibration and fusion strategies. The final resulting systems are shown to be both complementary and effective in achieving overall improved speaker recognition performance.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_utd-crss systems for 2018 nist speaker recognition evaluation.pdf},
  language = {en}
}

@article{zhang17_InformationPotentialAutoEncoders,
  title = {Information {{Potential Auto}}-{{Encoders}}},
  author = {Zhang, Yan and Ozay, Mete and Sun, Zhun and Okatani, Takayuki},
  year = {2017},
  month = aug,
  abstract = {In this paper, we suggest a framework to make use of mutual information as a regularization criterion to train Auto-Encoders (AEs). In the proposed framework, AEs are regularized by minimization of the mutual information between input and encoding variables of AEs during the training phase. In order to estimate the entropy of the encoding variables and the mutual information, we propose a non-parametric method. We also give an information theoretic view of Variational AEs (VAEs), which suggests that VAEs can be considered as parametric methods that estimate entropy. Experimental results show that the proposed non-parametric models have more degree of freedom in terms of representation learning of features drawn from complex distributions such as Mixture of Gaussians, compared to methods which estimate entropy using parametric approaches, such as Variational AEs.},
  annotation = {ZSCC: 0000011},
  archivePrefix = {arXiv},
  eprint = {1706.04635},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2017_information potential auto-encoders.pdf;/home/trung/Zotero/storage/ABPSEBU3/1706.html},
  journal = {arXiv:1706.04635 [cs, math, stat]},
  primaryClass = {cs, math, stat}
}

@article{zhang17_LearninghumansDeep,
  title = {Learning like Humans with {{Deep Symbolic Networks}}},
  author = {Zhang, Qunzhi and Sornette, Didier},
  year = {2017},
  month = jul,
  abstract = {We introduce the Deep Symbolic Network (DSN) model, which aims at becoming the white-box version of Deep Neural Networks (DNN). The DSN model provides a simple, universal yet powerful structure, similar to DNN, to represent any knowledge of the world, which is transparent to humans. The conjecture behind the DSN model is that any type of real world objects sharing enough common features are mapped into human brains as a symbol. Those symbols are connected by links, representing the composition, correlation, causality, or other relationships between them, forming a deep, hierarchical symbolic network structure. Powered by such a structure, the DSN model is expected to learn like humans, because of its unique characteristics. First, it is universal, using the same structure to store any knowledge. Second, it can learn symbols from the world and construct the deep symbolic networks automatically, by utilizing the fact that real world objects have been naturally separated by singularities. Third, it is symbolic, with the capacity of performing causal deduction and generalization. Fourth, the symbols and the links between them are transparent to us, and thus we will know what it has learned or not - which is the key for the security of an AI system. Fifth, its transparency enables it to learn with relatively small data. Sixth, its knowledge can be accumulated. Last but not least, it is more friendly to unsupervised learning than DNN. We present the details of the model, the algorithm powering its automatic learning ability, and describe its usefulness in different use cases. The purpose of this paper is to generate broad interest to develop it within an open source project centered on the Deep Symbolic Network (DSN) model towards the development of general AI.},
  archivePrefix = {arXiv},
  eprint = {1707.03377},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2017_learning like humans with deep symbolic networks.pdf;/home/trung/Zotero/storage/M67GCLEF/1707.html},
  journal = {arXiv:1707.03377 [cond-mat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,symbolic},
  primaryClass = {cond-mat}
}

@article{zhang17_Understandingdeeplearning,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  month = feb,
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2017_understanding deep learning requires rethinking generalization.pdf;/home/trung/Zotero/storage/UCVLM7GK/1611.html},
  journal = {arXiv:1611.03530 [cs]},
  keywords = {Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{zhang18_CapProNetDeepFeature,
  title = {{{CapProNet}}: {{Deep Feature Learning}} via {{Orthogonal Projections}} onto {{Capsule Subspaces}}},
  shorttitle = {{{CapProNet}}},
  author = {Zhang, Liheng and Edraki, Marzieh and Qi, Guo-Jun},
  year = {2018},
  month = oct,
  abstract = {In this paper, we formalize the idea behind capsule nets of using a capsule vector rather than a neuron activation to predict the label of samples. To this end, we propose to learn a group of capsule subspaces onto which an input feature vector is projected. Then the lengths of resultant capsules are used to score the probability of belonging to different classes. We train such a Capsule Projection Network (CapProNet) by learning an orthogonal projection matrix for each capsule subspace, and show that each capsule subspace is updated until it contains input feature vectors corresponding to the associated class. We will also show that the capsule projection can be viewed as normalizing the multiple columns of the weight matrix simultaneously to form an orthogonal basis, which makes it more effective in incorporating novel components of input features to update capsule representations. In other words, the capsule projection can be viewed as a multi-dimensional weight normalization in capsule subspaces, where the conventional weight normalization is simply a special case of the capsule projection onto 1D lines. Only a small negligible computing overhead is incurred to train the network in low-dimensional capsule subspaces or through an alternative hyper-power iteration to estimate the normalization matrix. Experiment results on image datasets show the presented model can greatly improve the performance of the state-of-the-art ResNet backbones by 10 - 20\% and that of the Densenet by 5 - 7\% respectively at the same level of computing and memory expenses. The CapProNet establishes the competitive state-of-the-art performance for the family of capsule nets by significantly reducing test errors on the benchmark datasets.},
  archivePrefix = {arXiv},
  eprint = {1805.07621},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2018_cappronet.pdf},
  journal = {arXiv:1805.07621 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhang18_Ubiquinolsuperiorubiquinone,
  title = {Ubiquinol Is Superior to Ubiquinone to Enhance {{Coenzyme Q10}} Status in Older Men},
  author = {Zhang, Ying and Liu, Jin and Chen, Xiao-qiang and Chen, C.-Y. Oliver},
  year = {2018},
  month = nov,
  volume = {9},
  pages = {5653--5659},
  publisher = {{The Royal Society of Chemistry}},
  issn = {2042-650X},
  doi = {10.1039/C8FO00971F},
  abstract = {Coenzyme Q10 (CoQ10) exerts its functions in the body through the ability of its benzoquinone head group to accept and donate electrons. The primary functions are to relay electrons for ATP production in the electron transport chain and to act as an important lipophilic antioxidant. Ubiquinone, the oxidized form of CoQ10, is commonly formulated in commercial supplements, and it must be reduced to ubiquinol to exert CoQ10's functions after consumption. Thus, we aimed to examine whether as compared to ubiquinone, ubiquinol would be more effective to enhance the CoQ10 status in older men. We conducted a double-blind, randomized, crossover trial with two 2-week intervention phases and a 2-week washout between crossovers. Ten eligible older men were randomized to consume either the ubiquinol or ubiquinone supplement at a dose of 200 mg d-1 with one of the main meals. A total of 4 blood samples were collected after an overnight fast for the determination of ubiquinone and ubiquinol in plasma and PBMC and the assessment of FRAP, total thiol, and malondialdehyde (MDA) in plasma and ATP in PBMC. After 2 weeks of the supplementation, the ubiquinol supplement significantly increased plasma ubiquinone 1.7 fold from 0.2 to 0.6 {$\mu$}mol L-1 and total CoQ10 (the sum of 2 forms) 1.5 fold from 1.3 to 3.4 {$\mu$}mol L-1 (p {$<$} 0.05) and tended to increase the plasma ubiquinol status 1.5 fold from 1.1 to 2.8 {$\mu$}mol L-1, but did not alter the ratio of ubiquinol to total CoQ10. The ubiquinone supplement insignificantly increases plasma ubiquinol, ubiquinone, and total CoQ10 and did not affect the ratio. Of 10 subjects, six were more responsive to the ubiquinol supplement and 2 were more so to the ubiquinone. The supplementation of both CoQ10 forms did not alter the CoQ10 status in PBMC. FRAP, total thiol, and MDA in plasma and ATP in PBMC were not changed during the intervention. The significant increase in plasma CoQ10 status observed after the 2-week supplementation suggested that ubiquinol appeared to be a better supplemental form to enhance the CoQ10 status than ubiquinone in older men. Neither ubiquinol nor ubiquinone supplement affected the measured biomarkers of oxidative stress.},
  journal = {Food \& Function},
  language = {en},
  number = {11}
}

@article{zhang18_VisualInterpretabilityDeep,
  title = {Visual {{Interpretability}} for {{Deep Learning}}: A {{Survey}}},
  shorttitle = {Visual {{Interpretability}} for {{Deep Learning}}},
  author = {Zhang, Quanshi and Zhu, Song-Chun},
  year = {2018},
  month = feb,
  abstract = {This paper reviews recent studies in understanding neural-network representations and learning neural networks with interpretable/disentangled middle-layer representations. Although deep neural networks have exhibited superior performance in various tasks, the interpretability is always the Achilles' heel of deep neural networks. At present, deep neural networks obtain high discrimination power at the cost of low interpretability of their black-box representations. We believe that high model interpretability may help people to break several bottlenecks of deep learning, e.g., learning from very few annotations, learning via human-computer communications at the semantic level, and semantically debugging network representations. We focus on convolutional neural networks (CNNs), and we revisit the visualization of CNN representations, methods of diagnosing representations of pre-trained CNNs, approaches for disentangling pre-trained CNN representations, learning of CNNs with disentangled representations, and middle-to-end learning based on model interpretability. Finally, we discuss prospective trends in explainable artificial intelligence.},
  archivePrefix = {arXiv},
  eprint = {1802.00614},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2018_visual interpretability for deep learning.pdf;/home/trung/Zotero/storage/4WESTQWC/1802.html},
  journal = {arXiv:1802.00614 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhang19_AdvancesVariationalInference,
  title = {Advances in {{Variational Inference}}},
  author = {Zhang, Cheng and B{\"u}tepage, Judith and Kjellstr{\"o}m, Hedvig and Mandt, Stephan},
  year = {2019},
  month = aug,
  volume = {41},
  pages = {2008--2026},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.2018.2889774},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_advances in variational inference2.pdf;/home/trung/Zotero/storage/Y6S5BHIJ/8588399.html},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {disentanglement},
  number = {8}
}

@article{zhang19_AdvancesVariationalInferencea,
  title = {Advances in {{Variational Inference}}},
  author = {Zhang, Cheng and Butepage, Judith and Kjellstrom, Hedvig and Mandt, Stephan},
  year = {2019},
  month = aug,
  volume = {41},
  pages = {2008--2026},
  issn = {0162-8828, 2160-9292, 1939-3539},
  doi = {10.1109/TPAMI.2018.2889774},
  abstract = {Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a highdimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_advances in variational inference.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  language = {en},
  number = {8}
}

@inproceedings{zhang19_AdversarialVariationalEmbedding,
  title = {Adversarial {{Variational Embedding}} for {{Robust Semi}}-Supervised {{Learning}}},
  booktitle = {Proceedings of the 25th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} \& {{Data Mining}}  - {{KDD}} '19},
  author = {Zhang, Xiang and Yao, Lina and Yuan, Feng},
  year = {2019},
  pages = {139--147},
  publisher = {{ACM Press}},
  address = {{Anchorage, AK, USA}},
  doi = {10.1145/3292500.3330966},
  abstract = {Semi-supervised learning is sought for leveraging the unlabelled data when labelled data is difficult or expensive to acquire. Deep generative models (e.g., Variational Autoencoder (VAE)) and semi\- supervised Generative Adversarial Networks (GANs) have recently shown promising performance in semi-supervised classification for the excellent discriminative representing ability. However, the latent code learned by the traditional VAE is not exclusive (re\- peatable) for a specific input sample, which prevents it from ex\- cellent classification performance. In particular, the learned latent representation depends on a non-exclusive component which is stochastically sampled from the prior distribution. Moreover, the semi-supervised GAN models generate data from pre-defined dis\- tribution (e.g., Gaussian noises) which is independent of the in\- put data distribution and may obstruct the convergence and is difficult to control the distribution ofthe generated data. To ad\- dress the aforementioned issues, we propose a novel Adversarial Variational Embedding (AVAE) framework for robust and effective semi-supervised learning to leverage both the advantage ofGAN as a high quality generative model and VAE as a posterior distri\- bution learner. The proposed approach first produces an exclusive latent code by the model which we call VAE++, and meanwhile, provides a meaningful prior distribution for the generator ofGAN. The proposed approach is evaluated over four different real-world applications and we show that our method outperforms the state\- of-the-art models, which confirms that the combination ofVAE++ and GAN can provide significant improvements in semi-supervised classification.},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_adversarial variational embedding for robust semi-supervised learning.pdf},
  isbn = {978-1-4503-6201-6},
  language = {en}
}

@article{zhang19_AdversarialVariationalEmbeddinga,
  title = {Adversarial {{Variational Embedding}} for {{Robust Semi}}-Supervised {{Learning}}},
  author = {Zhang, Xiang and Yao, Lina and Yuan, Feng},
  year = {2019},
  pages = {139--147},
  doi = {10.1145/3292500.3330966},
  abstract = {Semi-supervised learning is sought for leveraging the unlabelled data when labelled data is difficult or expensive to acquire. Deep generative models (e.g., Variational Autoencoder (VAE)) and semisupervised Generative Adversarial Networks (GANs) have recently shown promising performance in semi-supervised classification for the excellent discriminative representing ability. However, the latent code learned by the traditional VAE is not exclusive (repeatable) for a specific input sample, which prevents it from excellent classification performance. In particular, the learned latent representation depends on a non-exclusive component which is stochastically sampled from the prior distribution. Moreover, the semi-supervised GAN models generate data from pre-defined distribution (e.g., Gaussian noises) which is independent of the input data distribution and may obstruct the convergence and is difficult to control the distribution of the generated data. To address the aforementioned issues, we propose a novel Adversarial Variational Embedding (AVAE) framework for robust and effective semi-supervised learning to leverage both the advantage of GAN as a high quality generative model and VAE as a posterior distribution learner. The proposed approach first produces an exclusive latent code by the model which we call VAE++, and meanwhile, provides a meaningful prior distribution for the generator of GAN. The proposed approach is evaluated over four different real-world applications and we show that our method outperforms the state-of-the-art models, which confirms that the combination of VAE++ and GAN can provide significant improvements in semisupervised classification.},
  annotation = {ZSCC: 0000003},
  archivePrefix = {arXiv},
  eprint = {1905.02361},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_adversarial variational embedding for robust semi-supervised learning2.pdf},
  journal = {Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining  - KDD '19},
  language = {en}
}

@article{zhang19_bAIcisNovelBayesian,
  title = {{{{\emph{bAIcis}}}} : {{A Novel Bayesian Network Structural Learning Algorithm}} and {{Its Comprehensive Performance Evaluation Against Open}}-{{Source Software}}},
  shorttitle = {{{{\emph{bAIcis}}}}},
  author = {Zhang, Lixia and Rodrigues, Leonardo O. and Narain, Niven R. and Akmaev, Viatcheslav R.},
  year = {2019},
  month = sep,
  pages = {cmb.2019.0210},
  issn = {1557-8666},
  doi = {10.1089/cmb.2019.0210},
  abstract = {Structural learning of Bayesian networks (BNs) from observational data has gained increasing applied use and attention from various scientific and industrial areas. The mathematical theory of BNs and their optimization is well developed. Although there are several open-source BN learners in the public domain, none of them are able to handle both small and large feature space data and recover network structures with acceptable accuracy. bAIcis\`O is a novel BN learning and simulation software from BERG. It was developed with the goal of learning BNs from ``Big Data'' in health care, often exceeding hundreds of thousands features when research is conducted in genomics or multi-omics. This article provides a comprehensive performance evaluation of bAIcis and its comparison with the open-source BN learners. The study investigated synthetic datasets of discrete, continuous, and mixed data in small and large feature space, respectively. The results demonstrated that bAIcis outperformed the publicly available algorithms in structure recovery precision in almost all of the evaluated settings, achieving the true positive rates of 0.9 and precision of 0.8. In addition, bAIcis supports all data types, including continuous, discrete, and mixed variables. It is effectively parallelized on a distributed system and can work with datasets of thousands of features that are infeasible for any of the publicly available tools with a desired level of recovery accuracy.},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_ibaicis-i.pdf},
  journal = {Journal of Computational Biology},
  keywords = {read},
  language = {en}
}

@article{zhang19_ConsistencyRegularizationGenerative,
  title = {Consistency {{Regularization}} for {{Generative Adversarial Networks}}},
  author = {Zhang, Han and Zhang, Zizhao and Odena, Augustus and Lee, Honglak},
  year = {2019},
  month = oct,
  abstract = {Generative Adversarial Networks (GANs) are known to be difficult to train, despite considerable research effort. Several regularization techniques for stabilizing training have been proposed, but they introduce non-trivial computational overheads and interact poorly with existing techniques like spectral normalization. In this work, we propose a simple, effective training stabilizer based on the notion of consistency regularization---a popular technique in the semi-supervised learning literature. In particular, we augment data passing into the GAN discriminator and penalize the sensitivity of the discriminator to these augmentations. We conduct a series of experiments to demonstrate that consistency regularization works effectively with spectral normalization and various GAN architectures, loss functions and optimizer settings. Our method achieves the best FID scores for unconditional image generation compared to other regularization methods on CIFAR-10 and CelebA. Moreover, Our consistency regularized GAN (CR-GAN) improves state-of-the-art FID scores for conditional generation from 14.73 to 11.67 on CIFAR-10 and from 8.73 to 6.66 on ImageNet-2012.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.12027},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_consistency regularization for generative adversarial networks.pdf;/home/trung/Zotero/storage/QM2XY5R5/1910.html},
  journal = {arXiv:1910.12027 [cs, stat]},
  keywords = {adversarial,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,consistency,gan,regularization,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zhang19_DisentanglingSpatialStructure,
  title = {Disentangling the {{Spatial Structure}} and {{Style}} in {{Conditional VAE}}},
  author = {Zhang, Ziye and Sun, Li and Zheng, Zhilin and Li, Qingli},
  year = {2019},
  month = oct,
  abstract = {This paper aims to disentangle the latent space in cVAE into the spatial structure and the style code, which are complementary to each other, with one of them \$z\_s\$ being label relevant and the other \$z\_u\$ irrelevant. The generator is built by a connected encoder-decoder and a label condition mapping network. Depending on whether the label is related with the spatial structure, the output \$z\_s\$ from the condition mapping network is used either as a style code or a spatial structure code. The encoder provides the label irrelevant posterior from which \$z\_u\$ is sampled. The decoder employs \$z\_s\$ and \$z\_u\$ in each layer by adaptive normalization like SPADE or AdaIN. Extensive experiments on two datasets with different types of labels show the effectiveness of our method.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1910.13062},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_disentangling the spatial structure and style in conditional vae.pdf;/home/trung/Zotero/storage/YMY78CV4/1910.html},
  journal = {arXiv:1910.13062 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,conditional,disentanglement,variational},
  primaryClass = {cs}
}

@inproceedings{zhang19_FullySupervisedSpeaker,
  title = {Fully {{Supervised Speaker Diarization}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Aonan and Wang, Quan and Zhu, Zhenyao and Paisley, John and Wang, Chong},
  year = {2019},
  month = may,
  pages = {6301--6305},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8683892},
  abstract = {In this paper, we propose a fully supervised speaker diarization approach, named unbounded interleaved-state recurrent neural networks (UIS-RNN). Given extracted speaker-discriminative embeddings (a.k.a. d-vectors) from input utterances, each individual speaker is modeled by a parameter-sharing RNN, while the RNN states for different speakers interleave in the time domain. This RNN is naturally integrated with a distance-dependent Chinese restaurant process (ddCRP) to accommodate an unknown number of speakers. Our system is fully supervised and is able to learn from examples where time-stamped speaker labels are annotated. We achieved a 7.6\% diarization error rate on NIST SRE 2000 CALLHOME, which is better than the state-of-the-art method using spectral clustering. Moreover, our method decodes in an online fashion while most state-of-the-art systems rely on offline clustering.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_fully supervised speaker diarization.pdf},
  isbn = {978-1-4799-8131-1},
  language = {en}
}

@article{zhang19_Graphconvolutionalnetworks,
  title = {Graph Convolutional Networks: A Comprehensive Review},
  shorttitle = {Graph Convolutional Networks},
  author = {Zhang, Si and Tong, Hanghang and Xu, Jiejun and Maciejewski, Ross},
  year = {2019},
  month = dec,
  volume = {6},
  pages = {11},
  issn = {2197-4314},
  doi = {10.1186/s40649-019-0069-y},
  abstract = {Graphs naturally appear in numerous application domains, ranging from social analysis, bioinformatics to computer vision. The unique capability of graphs enables capturing the structural relations among data, and thus allows to harvest more insights compared to analyzing data in isolation. However, it is often very challenging to solve the learning problems on graphs, because (1) many types of data are not originally structured as graphs, such as images and text data, and (2) for graph-structured data, the underlying connectivity patterns are often complex and diverse. On the other hand, the representation learning has achieved great successes in many areas. Thereby, a potential solution is to learn the representation of graphs in a low-dimensional Euclidean space, such that the graph properties can be preserved. Although tremendous efforts have been made to address the graph representation learning problem, many of them still suffer from their shallow learning mechanisms. Deep learning models on graphs (e.g., graph neural networks) have recently emerged in machine learning and other related areas, and demonstrated the superior performance in various problems. In this survey, despite numerous types of graph neural networks, we conduct a comprehensive review specifically on the emerging field of graph convolutional networks, which is one of the most prominent graph deep learning models. First, we group the existing graph convolutional network models into two categories based on the types of convolutions and highlight some graph convolutional network models in details. Then, we categorize different graph convolutional networks according to the areas of their applications. Finally, we present several open challenges in this area and discuss potential directions for future research.},
  annotation = {ZSCC: 0000011},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_graph convolutional networks.pdf},
  journal = {Computational Social Networks},
  language = {en},
  number = {1}
}

@article{zhang19_HierarchicalGraphPooling,
  title = {Hierarchical {{Graph Pooling}} with {{Structure Learning}}},
  author = {Zhang, Zhen and Bu, Jiajun and Ester, Martin and Zhang, Jianfeng and Yao, Chengwei and Yu, Zhi and Wang, Can},
  year = {2019},
  month = nov,
  abstract = {Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.05954},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_hierarchical graph pooling with structure learning.pdf;/home/trung/Zotero/storage/LNAQESW3/1911.html},
  journal = {arXiv:1911.05954 [cs, stat]},
  keywords = {Computer Science - Machine Learning,hierarchical,pooling,Statistics - Machine Learning,structure},
  primaryClass = {cs, stat}
}

@article{zhang19_Learninglatentrepresentations,
  title = {Learning Latent Representations for Style Control and Transfer in End-to-End Speech Synthesis},
  author = {Zhang, Ya-Jie and Pan, Shifeng and He, Lei and Ling, Zhen-Hua},
  year = {2019},
  month = feb,
  abstract = {In this paper, we introduce the Variational Autoencoder (VAE) to an end-to-end speech synthesis model, to learn the latent representation of speaking styles in an unsupervised manner. The style representation learned through VAE shows good properties such as disentangling, scaling, and combination, which makes it easy for style control. Style transfer can be achieved in this framework by first inferring style representation through the recognition network of VAE, then feeding it into TTS network to guide the style in synthesizing speech. To avoid Kullback-Leibler (KL) divergence collapse in training, several techniques are adopted. Finally, the proposed model shows good performance of style control and outperforms Global Style Token (GST) model in ABX preference tests on style transfer.},
  annotation = {ZSCC: 0000007},
  archivePrefix = {arXiv},
  eprint = {1812.04342},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_learning latent representations for style control and transfer in end-to-end speech synthesis.pdf;/home/trung/Zotero/storage/FJA5CHYW/1812.html},
  journal = {arXiv:1812.04342 [cs, eess]},
  keywords = {Computer Science - Computation and Language,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,favorite,kl collapse,representation,variational},
  primaryClass = {cs, eess}
}

@article{zhang19_LookaheadOptimizersteps,
  title = {Lookahead {{Optimizer}}: K Steps Forward, 1 Step Back},
  shorttitle = {Lookahead {{Optimizer}}},
  author = {Zhang, Michael R. and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},
  year = {2019},
  month = jul,
  abstract = {The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by \textbackslash emph\{looking ahead\} at the sequence of "fast weights" generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.},
  archivePrefix = {arXiv},
  eprint = {1907.08610},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_lookahead optimizer.pdf;/home/trung/Zotero/storage/HDGMMKSD/1907.html},
  journal = {arXiv:1907.08610 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zhang19_Probabilisticcelltypeassignment,
  title = {Probabilistic Cell-Type Assignment of Single-Cell {{RNA}}-Seq for Tumor Microenvironment Profiling},
  author = {Zhang, Allen W. and O'Flanagan, Ciara and Chavez, Elizabeth A. and Lim, Jamie L. P. and Ceglia, Nicholas and McPherson, Andrew and Wiens, Matt and Walters, Pascale and Chan, Tim and Hewitson, Brittany and Lai, Daniel and Mottok, Anja and Sarkozy, Clementine and Chong, Lauren and Aoki, Tomohiro and Wang, Xuehai and Weng, Andrew P. and McAlpine, Jessica N. and Aparicio, Samuel and Steidl, Christian and Campbell, Kieran R. and Shah, Sohrab P.},
  year = {2019},
  month = oct,
  volume = {16},
  pages = {1007--1015},
  issn = {1548-7105},
  doi = {10.1038/s41592-019-0529-1},
  abstract = {Single-cell RNA sequencing has enabled the decomposition of complex tissues into functionally distinct cell types. Often, investigators wish to assign cells to cell types through unsupervised clustering followed by manual annotation or via `mapping' to existing data. However, manual interpretation scales poorly to large datasets, mapping approaches require purified or pre-annotated data and both are prone to batch effects. To overcome these issues, we present CellAssign, a probabilistic model that leverages prior knowledge of cell-type marker genes to annotate single-cell RNA sequencing data into predefined or de novo cell types. CellAssign automates the process of assigning cells in a highly scalable manner across large datasets while controlling for batch and sample effects. We demonstrate the advantages of CellAssign through extensive simulations and analysis of tumor microenvironment composition in high-grade serous ovarian cancer and follicular lymphoma.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_probabilistic cell-type assignment of single-cell rna-seq for tumor microenvironment profiling.pdf},
  journal = {Nature Methods},
  number = {10}
}

@article{zhang19_SelfAttentionGenerativeAdversarial,
  title = {Self-{{Attention Generative Adversarial Networks}}},
  author = {Zhang, Han and Goodfellow, Ian and Metaxas, Dimitris and Odena, Augustus},
  year = {2019},
  month = jun,
  abstract = {In this paper, we propose the Self-Attention Generative Adversarial Network (SAGAN) which allows attention-driven, long-range dependency modeling for image generation tasks. Traditional convolutional GANs generate high-resolution details as a function of only spatially local points in lower-resolution feature maps. In SAGAN, details can be generated using cues from all feature locations. Moreover, the discriminator can check that highly detailed features in distant portions of the image are consistent with each other. Furthermore, recent work has shown that generator conditioning affects GAN performance. Leveraging this insight, we apply spectral normalization to the GAN generator and find that this improves training dynamics. The proposed SAGAN achieves the state-of-the-art results, boosting the best published Inception score from 36.8 to 52.52 and reducing Frechet Inception distance from 27.62 to 18.65 on the challenging ImageNet dataset. Visualization of the attention layers shows that the generator leverages neighborhoods that correspond to object shapes rather than local regions of fixed shape.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1805.08318},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_self-attention generative adversarial networks.pdf;/home/trung/Zotero/storage/NV2US25Q/1805.html},
  journal = {arXiv:1805.08318 [cs, stat]},
  keywords = {adversarial,attention,Computer Science - Machine Learning,gan,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@inproceedings{zhang19_SemisupervisedLearningGenerative,
  title = {Semi-Supervised {{Learning}} with {{Generative Adversarial Networks}} for {{Arabic Dialect Identification}}},
  booktitle = {{{ICASSP}} 2019 - 2019 {{IEEE International Conference}} on {{Acoustics}}, {{Speech}} and {{Signal Processing}} ({{ICASSP}})},
  author = {Zhang, Chunlei and Zhang, Qian and Hansen, John H.L.},
  year = {2019},
  month = may,
  pages = {5986--5990},
  publisher = {{IEEE}},
  address = {{Brighton, United Kingdom}},
  doi = {10.1109/ICASSP.2019.8682629},
  abstract = {Dialect Identification (DID) refers to the process of identifying different dialects within the same language class. Compared with more general language identification (LID), DID is a more challenging task because of the substantial similarity between dialects. For an i-vector based LID/DID, prior studies have shown advancements with deep neural networks (DNNs) over Gaussian Mixture Models (GMMs) in acoustic modeling. In this study, a novel i-vector representation which is based on unsupervised bottleneck features is examined as the feature to identify dialects from Arabic broadcast speech. To utilize the unlabeled training data, semi-supervised learning with generative adversarial networks (GANs) are incorporated in the back-end classifier development. Experiments with the proposed method in the third release version of the Multi-Genre Broadcast (MGB-3) Challenge yields the best single system performance among all submitted systems. An overall classification accuracy of 73.8\% achieves a +28.8\% relative improvement over the MGB3 baseline with an accuracy of 57.3\%, which is the state-of-the-art performance in this DID task. The fused system further achieves an improvement of +39.4\% in accuracy.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2019_semi-supervised learning with generative adversarial networks for arabic dialect identification.pdf},
  isbn = {978-1-4799-8131-1},
  language = {en}
}

@misc{zhang19_zhangqianhuiAdversarialNetsPapers,
  title = {Zhangqianhui/{{AdversarialNetsPapers}}},
  author = {Zhang, Jichao},
  year = {2019},
  month = nov,
  abstract = {The classical paper list with code about generative adversarial nets},
  annotation = {ZSCC: NoCitationData[s0]},
  keywords = {adversarial-networks,deep-learning,gan,image-translation}
}

@article{zhang20_CausalViewRobustness,
  title = {A {{Causal View}} on {{Robustness}} of {{Neural Networks}}},
  author = {Zhang, Cheng and Zhang, Kun and Li, Yingzhen},
  year = {2020},
  month = may,
  abstract = {We present a causal view on the robustness of neural networks against input manipulations, which applies not only to traditional classification tasks but also to general measurement data. Based on this view, we design a deep causal manipulation augmented model (deep CAMA) which explicitly models possible manipulations on certain causes leading to changes in the observed effect. We further develop data augmentation and test-time fine-tuning methods to improve deep CAMA's robustness. When compared with discriminative deep neural networks, our proposed model shows superior robustness against unseen manipulations. As a by-product, our model achieves disentangled representation which separates the representation of manipulations from those of other latent causes.},
  archivePrefix = {arXiv},
  eprint = {2005.01095},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_a causal view on robustness of neural networks.pdf},
  journal = {arXiv:2005.01095 [cs, stat]},
  keywords = {causal},
  primaryClass = {cs, stat}
}

@article{zhang20_ContrastiveLearningMedical,
  title = {Contrastive {{Learning}} of {{Medical Visual Representations}} from {{Paired Images}} and {{Text}}},
  author = {Zhang, Yuhao and Jiang, Hang and Miura, Yasuhide and Manning, Christopher D. and Langlotz, Curtis P.},
  year = {2020},
  month = oct,
  abstract = {Learning visual representations of medical images is core to medical image understanding but its progress has been held back by the small size of hand-labeled datasets. Existing work commonly relies on transferring weights from ImageNet pretraining, which is suboptimal due to drastically different image characteristics, or rule-based label extraction from the textual report data paired with medical images, which is inaccurate and hard to generalize. We propose an alternative unsupervised strategy to learn medical visual representations directly from the naturally occurring pairing of images and textual data. Our method of pretraining medical image encoders with the paired text data via a bidirectional contrastive objective between the two modalities is domain-agnostic, and requires no additional expert input. We test our method by transferring our pretrained weights to 4 medical image classification tasks and 2 zero-shot retrieval tasks, and show that our method leads to image representations that considerably outperform strong baselines in most settings. Notably, in all 4 classification tasks, our method requires only 10\% as much labeled training data as an ImageNet initialized counterpart to achieve better or comparable performance, demonstrating superior data efficiency.},
  archivePrefix = {arXiv},
  eprint = {2010.00747},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_contrastive learning of medical visual representations from paired images and text.pdf},
  journal = {arXiv:2010.00747 [cs]},
  keywords = {self-supervised},
  primaryClass = {cs}
}

@article{zhang20_Determiningsequencingdepth,
  title = {Determining Sequencing Depth in a Single-Cell {{RNA}}-Seq Experiment},
  author = {Zhang, Martin Jinye and Ntranos, Vasilis and Tse, David},
  year = {2020},
  month = dec,
  volume = {11},
  pages = {774},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-14482-y},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/Zotero/storage/VAHQ7T42/Zhang et al. - 2020 - Determining sequencing depth in a single-cell RNA-.pdf},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@article{zhang20_Dimensionalityreductionsingle,
  title = {Dimensionality Reduction for Single Cell {{RNA}} Sequencing Data Using Constrained Robust Non-Negative Matrix Factorization},
  author = {Zhang, Shuqin and Yang, Liu and Yang, Jinwen and Lin, Zhixiang and Ng, Michael K},
  year = {2020},
  month = sep,
  volume = {2},
  pages = {lqaa064},
  issn = {2631-9268},
  doi = {10.1093/nargab/lqaa064},
  abstract = {Single cell RNA-sequencing (scRNA-seq) technology, a powerful tool for analyzing the entire transcriptome at single cell level, is receiving increasing research attention. The presence of dropouts is an important characteristic of scRNA-seq data that may affect the performance of downstream analyses, such as dimensionality reduction and clustering. Cells sequenced to lower depths tend to have more dropouts than those sequenced to greater depths. In this study, we aimed to develop a dimensionality reduction method to address both dropouts and the non-negativity constraints in scRNA-seq data. The developed method simultaneously performs dimensionality reduction and dropout imputation under the non-negative matrix factorization (NMF) framework. The dropouts were modeled as a non-negative sparse matrix. Summation of the observed data matrix and dropout matrix was approximated by NMF. To ensure the sparsity pattern was maintained, a weighted 1 penalty that took into account the dependency of dropouts on the sequencing depth in each cell was imposed. An efficient algorithm was developed to solve the proposed optimization problem. Experiments using both synthetic data and real data showed that dimensionality reduction via the proposed method afforded more robust clustering results compared with those obtained from the existing methods, and that dropout imputation improved the differential expression analysis.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_dimensionality reduction for single cell rna sequencing data using constrained robust non-negative matrix factorization.pdf},
  journal = {NAR Genomics and Bioinformatics},
  language = {en},
  number = {3}
}

@article{zhang20_Extendinginformationmaximization,
  title = {Extending Information Maximization from a Rate-Distortion Perspective},
  author = {Zhang, Yan and Hu, Junjie and Okatani, Takayuki},
  year = {2020},
  month = jul,
  volume = {399},
  pages = {285--295},
  issn = {0925-2312},
  doi = {10.1016/j.neucom.2020.02.061},
  abstract = {In this paper, we propose a new interpretation of the information maximization method (InfoMax) from a perspective of the rate distortion theory. We show that under specific conditions, InfoMax is equivalent to the minimization of a compression rate under the constraint of zero distortion. Zero distortion, or equivalently, zero reconstruction error between the input and its reconstruction, does not provide meaningful solutions in many cases. Based on the new interpretation, we extend InfoMax to be able to deal with non-zero distortion and also to learn under/over-complete representations. Experimental results on synthetic as well as real data show the effectiveness of our method.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_extending information maximization from a rate-distortion perspective.pdf},
  journal = {Neurocomputing},
  keywords = {information,Information maximization,Rate distortion,Unsupervised representation learning}
}

@article{zhang20_LearningInvariantRepresentations,
  title = {Learning {{Invariant Representations}} for {{Reinforcement Learning}} without {{Reconstruction}}},
  author = {Zhang, Amy and McAllister, Rowan and Calandra, Roberto and Gal, Yarin and Levine, Sergey},
  year = {2020},
  month = jun,
  abstract = {We study how representation learning can accelerate reinforcement learning from rich observations, such as images, without relying either on domain knowledge or pixel-reconstruction. Our goal is to learn representations that both provide for effective downstream control and invariance to task-irrelevant details. Bisimulation metrics quantify behavioral similarity between states in continuous MDPs, which we propose using to learn robust latent representations which encode only the task-relevant information from observations. Our method trains encoders such that distances in latent space equal bisimulation distances in state space. We demonstrate the effectiveness of our method at disregarding task-irrelevant information using modified visual MuJoCo tasks, where the background is replaced with moving distractors and natural videos, while achieving SOTA performance. We also test a first-person highway driving task where our method learns invariance to clouds, weather, and time of day. Finally, we provide generalization results drawn from properties of bisimulation metrics, and links to causal inference.},
  archivePrefix = {arXiv},
  eprint = {2006.10742},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_learning invariant representations for reinforcement learning without reconstruction.pdf},
  journal = {arXiv:2006.10742 [cs, stat]},
  keywords = {causal,disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhang20_PushingLimitsSemiSupervised,
  title = {Pushing the {{Limits}} of {{Semi}}-{{Supervised Learning}} for {{Automatic Speech Recognition}}},
  author = {Zhang, Yu and Qin, James and Park, Daniel S. and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V. and Wu, Yonghui},
  year = {2020},
  month = oct,
  abstract = {We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4\%/2.6\% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7\%/3.3\%.},
  archivePrefix = {arXiv},
  eprint = {2010.10504},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_pushing the limits of semi-supervised learning for automatic speech recognition.pdf},
  journal = {arXiv:2010.10504 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{zhang20_PushingLimitsSemiSuperviseda,
  title = {Pushing the {{Limits}} of {{Semi}}-{{Supervised Learning}} for {{Automatic Speech Recognition}}},
  author = {Zhang, Yu and Qin, James and Park, Daniel S. and Han, Wei and Chiu, Chung-Cheng and Pang, Ruoming and Le, Quoc V. and Wu, Yonghui},
  year = {2020},
  month = oct,
  abstract = {We employ a combination of recent developments in semi-supervised learning for automatic speech recognition to obtain state-of-the-art results on LibriSpeech utilizing the unlabeled audio of the Libri-Light dataset. More precisely, we carry out noisy student training with SpecAugment using giant Conformer models pre-trained using wav2vec 2.0 pre-training. By doing so, we are able to achieve word-error-rates (WERs) 1.4\%/2.6\% on the LibriSpeech test/test-other sets against the current state-of-the-art WERs 1.7\%/3.3\%.},
  archivePrefix = {arXiv},
  eprint = {2010.10504},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_pushing the limits of semi-supervised learning for automatic speech recognition2.pdf},
  journal = {arXiv:2010.10504 [cs, eess]},
  primaryClass = {cs, eess}
}

@article{zhang20_Singlecelllandscapeimmunological,
  title = {Single-Cell Landscape of Immunological Responses in Patients with {{COVID}}-19},
  author = {Zhang, Ji-Yuan and Wang, Xiang-Ming and Xing, Xudong and Xu, Zhe and Zhang, Chao and Song, Jin-Wen and Fan, Xing and Xia, Peng and Fu, Jun-Liang and Wang, Si-Yu and Xu, Ruo-Nan and Dai, Xiao-Peng and Shi, Lei and Huang, Lei and Jiang, Tian-Jun and Shi, Ming and Zhang, Yuxia and Zumla, Alimuddin and Maeurer, Markus and Bai, Fan and Wang, Fu-Sheng},
  year = {2020},
  month = sep,
  volume = {21},
  pages = {1107--1118},
  issn = {1529-2916},
  doi = {10.1038/s41590-020-0762-x},
  abstract = {In coronavirus disease 2019 (COVID-19), caused by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) infection, the relationship between disease severity and the host immune response is not fully understood. Here we performed single-cell RNA sequencing in peripheral blood samples of 5 healthy donors and 13 patients with COVID-19, including moderate, severe and convalescent cases. Through determining the transcriptional profiles of immune cells, coupled with assembled T cell receptor and B cell receptor sequences, we analyzed the functional properties of immune cells. Most cell types in patients with COVID-19 showed a strong interferon-{$\alpha$} response and an overall acute inflammatory response. Moreover, intensive expansion of highly cytotoxic effector T cell subsets, such as CD4+ effector-GNLY (granulysin), CD8+ effector-GNLY and NKT CD160, was associated with convalescence in moderate patients. In severe patients, the immune landscape featured a deranged interferon response, profound immune exhaustion with skewed T cell receptor repertoire and broad T cell expansion. These findings illustrate the dynamic nature of immune responses during disease progression.},
  file = {/home/trung/GoogleDrive/Zotero/zhang et al_2020_single-cell landscape of immunological responses in patients with covid-19.pdf},
  journal = {Nature Immunology},
  number = {9}
}

@article{zhao00_Dirichletbeliefnetworks,
  title = {Dirichlet Belief Networks for Topic Structure Learning},
  author = {Zhao, He and Du, Lan and Buntine, Wray and Zhou, Mingyuan},
  pages = {12},
  abstract = {Recently, considerable research effort has been devoted to developing deep architectures for topic models to learn topic structures. Although several deep models have been proposed to learn better topic proportions of documents, how to leverage the benefits of deep structures for learning word distributions of topics has not yet been rigorously studied. Here we propose a new multi-layer generative process on word distributions of topics, where each layer consists of a set of topics and each topic is drawn from a mixture of the topics of the layer above. As the topics in all layers can be directly interpreted by words, the proposed model is able to discover interpretable topic hierarchies. As a self-contained module, our model can be flexibly adapted to different kinds of topic models to improve their modelling accuracy and interpretability. Extensive experiments on text corpora demonstrate the advantages of the proposed model.},
  annotation = {ZSCC: 0000004},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_dirichlet belief networks for topic structure learning.pdf},
  language = {en}
}

@article{zhao00_InterIntraTopic,
  title = {Inter and {{Intra Topic Structure Learning}} with {{Word Embeddings}}},
  author = {Zhao, He and Du, Lan and Buntine, Wray and Zhou, Mingyaun},
  pages = {10},
  abstract = {One important task of topic modeling for text analysis is interpretability. By discovering structured topics one is able to yield improved interpretability as well as modeling accuracy. In this paper, we propose a novel topic model with a deep structure that explores both inter-topic and intra-topic structures informed by word embeddings. Specifically, our model discovers inter topic structures in the form of topic hierarchies and discovers intra topic structures in the form of sub-topics, each of which is informed by word embeddings and captures a fine-grained thematic aspect of a normal topic. Extensive experiments demonstrate that our model achieves the state-of-the-art performance in terms of perplexity, document classification, and topic quality. Moreover, with topic hierarchies and sub-topics, the topics discovered in our model are more interpretable, providing an illuminating means to understand text data.},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_inter and intra topic structure learning with word embeddings.pdf},
  language = {en}
}

@article{zhao00_StructuredBayesianLatent,
  title = {Structured {{Bayesian Latent Factor Models}} with {{Meta}}-Data},
  author = {Zhao, He},
  pages = {193},
  file = {/home/trung/GoogleDrive/Zotero/zhao_structured bayesian latent factor models with meta-data.pdf},
  language = {en}
}

@article{zhao00_WordEmbeddingsInformed,
  title = {A {{Word Embeddings Informed Focused Topic Model}}},
  author = {Zhao, He},
  pages = {16},
  abstract = {In natural language processing and related fields, it has been shown that the word embeddings can successfully capture both the semantic and syntactic features of words. They can serve as complementary information to topics models, especially for the cases where word co-occurrence data is insufficient, such as with short texts. In this paper, we propose a focused topic model where how a topic focuses on words is informed by word embeddings. Our models is able to discover more informed and focused topics with more representative words, leading to better modelling accuracy and topic quality. With the data argumentation technique, we can derive an efficient Gibbs sampling algorithm that benefits from the fully local conjugacy of the model. We conduct extensive experiments on several real world datasets, which demonstrate that our model achieves comparable or improved performance in terms of both perplexity and topic coherence, particularly in handling short text data.},
  file = {/home/trung/GoogleDrive/Zotero/zhao_a word embeddings informed focused topic model.pdf},
  language = {en}
}

@article{zhao17_DeeperUnderstandingVariational,
  title = {Towards {{Deeper Understanding}} of {{Variational Autoencoding Models}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2017},
  month = feb,
  abstract = {We propose a new family of optimization criteria for variational auto-encoding models, generalizing the standard evidence lower bound. We provide conditions under which they recover the data distribution and learn latent features, and formally show that common issues such as blurry samples and uninformative latent features arise when these conditions are not met. Based on these new insights, we propose a new sequential VAE model that can generate sharp samples on the LSUN image dataset based on pixel-wise reconstruction loss, and propose an optimization criterion that encourages unsupervised learning of informative latent features.},
  annotation = {ZSCC: 0000048},
  archivePrefix = {arXiv},
  eprint = {1702.08658},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2017_towards deeper understanding of variational autoencoding models.pdf;/home/trung/Zotero/storage/JCSN952J/1702.html},
  journal = {arXiv:1702.08658 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{zhao17_DeeperUnderstandingVariationala,
  title = {Towards {{Deeper Understanding}} of {{Variational Autoencoding Models}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2017},
  month = feb,
  abstract = {We propose a new family of optimization criteria for variational auto-encoding models, generalizing the standard evidence lower bound. We provide conditions under which they recover the data distribution and learn latent features, and formally show that common issues such as blurry samples and uninformative latent features arise when these conditions are not met. Based on these new insights, we propose a new sequential VAE model that can generate sharp samples on the LSUN image dataset based on pixel-wise reconstruction loss, and propose an optimization criterion that encourages unsupervised learning of informative latent features.},
  annotation = {ZSCC: 0000052},
  archivePrefix = {arXiv},
  eprint = {1702.08658},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/DKTLHWDN/Zhao et al. - 2017 - Towards Deeper Understanding of Variational Autoen.pdf},
  journal = {arXiv:1702.08658 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhao18_BiasGeneralizationDeep,
  title = {Bias and {{Generalization}} in {{Deep Generative Models}}: {{An Empirical Study}}},
  shorttitle = {Bias and {{Generalization}} in {{Deep Generative Models}}},
  author = {Zhao, Shengjia and Ren, Hongyu and Yuan, Arianna and Song, Jiaming and Goodman, Noah and Ermon, Stefano},
  year = {2018},
  month = nov,
  abstract = {In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images. Inspired by experimental methods from cognitive psychology, we probe each learning algorithm with carefully designed training datasets to characterize when and how existing models generate novel attributes and their combinations. We identify similarities to human psychology and verify that these patterns are consistent across commonly used models and architectures.},
  annotation = {ZSCC: 0000009},
  archivePrefix = {arXiv},
  eprint = {1811.03259},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2018_bias and generalization in deep generative models.pdf},
  journal = {arXiv:1811.03259 [cs, stat]},
  keywords = {disentanglement},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhao18_InformationAutoencodingFamily,
  title = {The {{Information Autoencoding Family}}: {{A Lagrangian Perspective}} on {{Latent Variable Generative Models}}},
  shorttitle = {The {{Information Autoencoding Family}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2018},
  month = jul,
  abstract = {A large number of objectives have been proposed to train latent variable generative models. We show that many of them are Lagrangian dual functions of the same primal optimization problem. The primal problem optimizes the mutual information between latent and visible variables, subject to the constraints of accurately modeling the data distribution and performing correct amortized inference. By choosing to maximize or minimize mutual information, and choosing different Lagrange multipliers, we obtain different objectives including InfoGAN, ALI/BiGAN, ALICE, CycleGAN, beta-VAE, adversarial autoencoders, AVB, AS-VAE and InfoVAE. Based on this observation, we provide an exhaustive characterization of the statistical and computational trade-offs made by all the training objectives in this class of Lagrangian duals. Next, we propose a dual optimization method where we optimize model parameters as well as the Lagrange multipliers. This method achieves Pareto optimal solutions in terms of optimizing information and satisfying the constraints.},
  archivePrefix = {arXiv},
  eprint = {1806.06514},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/7Q9UYJAI/Zhao et al. - 2018 - The Information Autoencoding Family A Lagrangian .pdf},
  journal = {arXiv:1806.06514 [cs, stat]},
  keywords = {information},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhao19_InfoVAEBalancingLearning,
  title = {{{InfoVAE}}: {{Balancing Learning}} and {{Inference}} in {{Variational Autoencoders}}},
  author = {Zhao, Shengjia and Song, Jiaming and Ermon, Stefano},
  year = {2019},
  pages = {14},
  abstract = {A key advance in learning generative models is the use of amortized inference distributions that are jointly trained with the models. We find that existing training objectives for variational autoencoders can lead to inaccurate amortized inference distributions and, in some cases, improving the objective provably degrades the inference quality. In addition, it has been observed that variational autoencoders tend to ignore the latent variables when combined with a decoding distribution that is too flexible. We again identify the cause in existing training criteria and propose a new class of objectives (InfoVAE) that mitigate these problems. We show that our model can significantly improve the quality of the variational posterior and can make effective use of the latent features regardless of the flexibility of the decoding distribution. Through extensive qualitative and quantitative analyses, we demonstrate that our models outperform competing approaches on multiple performance metrics.},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2019_infovae.pdf;/home/trung/GoogleDrive/Zotero/zhao et al_2019_infovae2.pdf},
  journal = {AAAI},
  keywords = {disentanglement,information},
  language = {en}
}

@article{zhao19_LearningNeuralNetworks,
  title = {Learning {{Neural Networks}} with {{Adaptive Regularization}}},
  author = {Zhao, Han and Tsai, Yao-Hung Hubert and Salakhutdinov, Ruslan and Gordon, Geoffrey J.},
  year = {2019},
  month = oct,
  abstract = {Feed-forward neural networks can be understood as a combination of an intermediate representation and a linear hypothesis. While most previous works aim to diversify the representations, we explore the complementary direction by performing an adaptive and data-dependent regularization motivated by the empirical Bayes method. Specifically, we propose to construct a matrix-variate normal prior (on weights) whose covariance matrix has a Kronecker product structure. This structure is designed to capture the correlations in neurons through backpropagation. Under the assumption of this Kronecker factorization, the prior encourages neurons to borrow statistical strength from one another. Hence, it leads to an adaptive and data-dependent regularization when training networks on small datasets. To optimize the model, we present an efficient block coordinate descent algorithm with analytical solutions. Empirically, we demonstrate that the proposed method helps networks converge to local optima with smaller stable ranks and spectral norms. These properties suggest better generalizations and we present empirical results to support this expectation. We also verify the effectiveness of the approach on multiclass classification and multitask regression problems with various network structures.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1907.06288},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2019_learning neural networks with adaptive regularization.pdf;/home/trung/Zotero/storage/6J955YWE/1907.html},
  journal = {arXiv:1907.06288 [cs, stat]},
  keywords = {Computer Science - Machine Learning,regularization,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zhao19_VariationalAutoencodersSparse,
  title = {Variational {{Autoencoders}} for {{Sparse}} and {{Overdispersed Discrete Data}}},
  author = {Zhao, He and Rai, Piyush and Du, Lan and Buntine, Wray and Zhou, Mingyuan},
  year = {2019},
  month = may,
  abstract = {Many applications, such as text modelling, high-throughput sequencing, and recommender systems, require analysing sparse, high-dimensional, and overdispersed discrete (count-valued or binary) data. Although probabilistic matrix factorisation and linear/nonlinear latent factor models have enjoyed great success in modelling such data, many existing models may have inferior modelling performance due to the insufficient capability of modelling overdispersion in count-valued data and model misspecification in general. In this paper, we comprehensively study these issues and propose a variational autoencoder based framework that generates discrete data via negative-binomial distribution. We also examine the model's ability to capture properties, such as self- and cross-excitations in discrete data, which is critical for modelling overdispersion. We conduct extensive experiments on three important problems from discrete data analysis: text analysis, collaborative filtering, and multi-label learning. Compared with several state-of-the-art baselines, the proposed models achieve significantly better performance on the above problems.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1905.00616},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2019_variational autoencoders for sparse and overdispersed discrete data.pdf},
  journal = {arXiv:1905.00616 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhao19_VariationalAutoencodersSparsea,
  title = {Variational {{Autoencoders}} for {{Sparse}} and {{Overdispersed Discrete Data}}},
  author = {Zhao, He and Rai, Piyush and Du, Lan and Buntine, Wray and Zhou, Mingyuan},
  year = {2019},
  month = may,
  abstract = {Many applications, such as text modelling, high-throughput sequencing, and recommender systems, require analysing sparse, high-dimensional, and overdispersed discrete (count-valued or binary) data. Although probabilistic matrix factorisation and linear/nonlinear latent factor models have enjoyed great success in modelling such data, many existing models may have inferior modelling performance due to the insufficient capability of modelling overdispersion in count-valued data and model misspecification in general. In this paper, we comprehensively study these issues and propose a variational autoencoder based framework that generates discrete data via negative-binomial distribution. We also examine the model's ability to capture properties, such as self- and cross-excitations in discrete data, which is critical for modelling overdispersion. We conduct extensive experiments on three important problems from discrete data analysis: text analysis, collaborative filtering, and multi-label learning. Compared with several state-of-the-art baselines, the proposed models achieve significantly better performance on the above problems.},
  archivePrefix = {arXiv},
  eprint = {1905.00616},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2019_variational autoencoders for sparse and overdispersed discrete data2.pdf;/home/trung/GoogleDrive/Zotero/zhao et al_2019_variational autoencoders for sparse and overdispersed discrete data3.pdf},
  journal = {arXiv:1905.00616 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{zhao20_DiscretizedBottleneckVAE,
  title = {Discretized {{Bottleneck}} in {{VAE}}: {{Posterior}}-{{Collapse}}-{{Free Sequence}}-to-{{Sequence Learning}}},
  shorttitle = {Discretized {{Bottleneck}} in {{VAE}}},
  author = {Zhao, Yang and Yu, Ping and Mahapatra, Suchismit and Su, Qinliang and Chen, Changyou},
  year = {2020},
  month = apr,
  abstract = {Variational autoencoders (VAEs) are important tools in end-to-end representation learning. VAEs can capture complex data distributions and have been applied extensively in many natural-language-processing (NLP) tasks. However, a common pitfall in sequence-to-sequence learning with VAEs is the posterior-collapse issue in latent space, wherein the model tends to ignore latent variables when a strong auto-regressive decoder is implemented. In this paper, we propose a principled approach to eliminate this issue by applying a discretized bottleneck in the latent space. Specifically, we impose a shared discrete latent space where each input is learned to choose a combination of shared latent atoms as its latent representation. Compared with VAEs employing continuous latent variables, our model endows more promising capability in modeling underlying semantics of discrete sequences and can thus provide more interpretative latent structures. Empirically, we demonstrate the efficiency and effectiveness of our model on a broad range of tasks, including language modeling, unaligned text style transfer, dialog response generation, and neural machine translation.},
  archivePrefix = {arXiv},
  eprint = {2004.10603},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2020_discretized bottleneck in vae.pdf},
  journal = {arXiv:2004.10603 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhao20_LatentVariablesSpheres,
  title = {Latent {{Variables}} on {{Spheres}} for {{Autoencoders}} in {{High Dimensions}}},
  author = {Zhao, Deli and Zhu, Jiapeng and Zhang, Bo},
  year = {2020},
  month = feb,
  abstract = {Variational Auto-Encoder (VAE) has been widely applied as a fundamental generative model in machine learning. For complex samples like imagery objects or scenes, however, VAE suffers from the dimensional dilemma between reconstruction precision that needs high-dimensional latent codes and probabilistic inference that favors a low-dimensional latent space. By virtue of high-dimensional geometry, we propose a very simple algorithm, called Spherical Auto-Encoder (SAE), completely different from existing VAEs to address the issue. SAE is in essence the vanilla autoencoder with spherical normalization on the latent space. We analyze the unique characteristics of random variables on spheres in high dimensions and argue that random variables on spheres are agnostic to various prior distributions and data modes when the dimension is sufficiently high. Therefore, SAE can harness a high-dimensional latent space to improve the inference precision of latent codes while maintain the property of stochastic sampling from priors. The experiments on sampling and inference validate our theoretical analysis and the superiority of SAE.},
  archivePrefix = {arXiv},
  eprint = {1912.10233},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhao et al_2020_latent variables on spheres for autoencoders in high dimensions.pdf},
  journal = {arXiv:1912.10233 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{zhen20_LearningLearnVariational,
  title = {Learning to {{Learn Variational Semantic Memory}}},
  author = {Zhen, Xiantong and Du, Yingjun and Xiong, Huan and Qiu, Qiang and Snoek, Cees G. M. and Shao, Ling},
  year = {2020},
  month = oct,
  abstract = {In this paper, we introduce variational semantic memory into meta-learning to acquire long-term knowledge for few-shot learning. The variational semantic memory accrues and stores semantic information for the probabilistic inference of class prototypes in a hierarchical Bayesian framework. The semantic memory is grown from scratch and gradually consolidated by absorbing information from tasks it experiences. By doing so, it is able to accumulate long-term, general knowledge that enables it to learn new concepts of objects. We formulate memory recall as the variational inference of a latent memory variable from addressed contents, which offers a principled way to adapt the knowledge to individual tasks. Our variational semantic memory, as a new long-term memory module, confers principled recall and update mechanisms that enable semantic information to be efficiently accrued and adapted for few-shot learning. Experiments demonstrate that the probabilistic modelling of prototypes achieves a more informative representation of object classes compared to deterministic vectors. The consistent new state-of-the-art performance on four benchmarks shows the benefit of variational semantic memory in boosting few-shot recognition.},
  archivePrefix = {arXiv},
  eprint = {2010.10341},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhen et al_2020_learning to learn variational semantic memory.pdf},
  journal = {arXiv:2010.10341 [cs]},
  keywords = {_tablet,favorite,representation},
  primaryClass = {cs}
}

@article{zheng18_DAGsNOTEARS,
  title = {{{DAGs}} with {{NO TEARS}}: {{Continuous Optimization}} for {{Structure Learning}}},
  shorttitle = {{{DAGs}} with {{NO TEARS}}},
  author = {Zheng, Xun and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
  year = {2018},
  month = nov,
  abstract = {Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: We formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree. Code implementing the proposed algorithm is open-source and publicly available at https://github.com/xunzheng/notears.},
  annotation = {ZSCC: 0000023},
  archivePrefix = {arXiv},
  eprint = {1803.01422},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/MACUZ97W/Zheng et al. - 2018 - DAGs with NO TEARS Continuous Optimization for St.pdf},
  journal = {arXiv:1803.01422 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zheng18_DegenerationVAELight,
  title = {Degeneration in {{VAE}}: In the {{Light}} of {{Fisher Information Loss}}},
  shorttitle = {Degeneration in {{VAE}}},
  author = {Zheng, Huangjie and Yao, Jiangchao and Zhang, Ya and Tsang, Ivor W.},
  year = {2018},
  month = sep,
  abstract = {While enormous progress has been made to Variational Autoencoder (VAE) in recent years, similar to other deep networks, VAE with deep networks suffers from the problem of degeneration, which seriously weakens the correlation between the input and the corresponding latent codes, deviating from the goal of the representation learning. To investigate how degeneration affects VAE from a theoretical perspective, we illustrate the information transmission in VAE and analyze the intermediate layers of the encoders/decoders. Specifically, we propose a Fisher Information measure for the layer-wise analysis. With such measure, we demonstrate that information loss is ineluctable in feed-forward networks and causes the degeneration in VAE. We show that skip connections in VAE enable the preservation of information without changing the model architecture. We call this class of VAE equipped with skip connections as SCVAE and perform a range of experiments to show its advantages in information preservation and degeneration mitigation.},
  archivePrefix = {arXiv},
  eprint = {1802.06677},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zheng et al_2018_degeneration in vae.pdf},
  journal = {arXiv:1802.06677 [cs, stat]},
  keywords = {_tablet,information},
  primaryClass = {cs, stat}
}

@article{zheng18_UnderstandingVAEsFisherShannon,
  title = {Understanding {{VAEs}} in {{Fisher}}-{{Shannon Plane}}},
  author = {Zheng, Huangjie and Yao, Jiangchao and Zhang, Ya and Tsang, Ivor W. and Wang, Jia},
  year = {2018},
  month = sep,
  abstract = {In information theory, Fisher information and Shannon information (entropy) are respectively used to quantify the uncertainty associated with the distribution modeling and the uncertainty in specifying the outcome of given variables. These two quantities are complementary and are jointly applied to information behavior analysis in most cases. The uncertainty property in information asserts a fundamental trade-off between Fisher information and Shannon information, which enlightens us the relationship between the encoder and the decoder in variational auto-encoders (VAEs). In this paper, we investigate VAEs in the Fisher-Shannon plane and demonstrate that the representation learning and the log-likelihood estimation are intrinsically related to these two information quantities. Through extensive qualitative and quantitative experiments, we provide with a better comprehension of VAEs in tasks such as high-resolution reconstruction, and representation learning in the perspective of Fisher information and Shannon information. We further propose a variant of VAEs, termed as Fisher auto-encoder (FAE), for practical needs to balance Fisher information and Shannon information. Our experimental results have demonstrated its promise in improving the reconstruction accuracy and avoiding the non-informative latent code as occurred in previous works.},
  archivePrefix = {arXiv},
  eprint = {1807.03723},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zheng et al_2018_understanding vaes in fisher-shannon plane.pdf},
  journal = {arXiv:1807.03723 [cs, stat]},
  keywords = {information},
  primaryClass = {cs, stat}
}

@article{zheng20_DisentanglingUserInterest,
  title = {Disentangling {{User Interest}} and {{Popularity Bias}} for {{Recommendation}} with {{Causal Embedding}}},
  author = {Zheng, Yu and Gao, Chen and Li, Xiang and He, Xiangnan and Li, Yong and Jin, Depeng},
  year = {2020},
  month = jun,
  abstract = {Recommendation models are usually trained on observational data. However, observational data exhibits various bias, such as popularity bias. Biased data results in a gap between online environment and offline evaluation, which makes it difficult to measure the real effect of recommendation. To bridge this gap, most existing methods exploit causal inference to eliminate the bias in historical observational data, e.g., by re-weighting training samples or leveraging a small fraction of unbiased data. However, different causes of an interaction are bundled together as a unified representation in these algorithms. In this paper, we present DICE, a general framework that learns representations where user interest and item popularity are structurally disentangled. We leverage separate embeddings for different causes, and make each embedding capture only one cause by training with cause-specific samples and imposing direct disentanglement supervision. Our algorithm outperforms state-of-the-art baselines with remarkable improvements on two real-world datasets with various backbone models. We further demonstrate that the learned embeddings successfully capture the desired causes.},
  archivePrefix = {arXiv},
  eprint = {2006.11011},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zheng et al_2020_disentangling user interest and popularity bias for recommendation with causal embedding.pdf},
  journal = {arXiv:2006.11011 [cs]},
  keywords = {causal},
  language = {en},
  primaryClass = {cs}
}

@article{zheng20_LearningSparseNonparametric,
  title = {Learning {{Sparse Nonparametric DAGs}}},
  author = {Zheng, Xun and Dan, Chen and Aragam, Bryon and Ravikumar, Pradeep and Xing, Eric P.},
  year = {2020},
  month = mar,
  abstract = {We develop a framework for learning sparse nonparametric directed acyclic graphs (DAGs) from data. Our approach is based on a recent algebraic characterization of DAGs that led to a fully continuous program for score-based learning of DAG models parametrized by a linear structural equation model (SEM). We extend this algebraic characterization to nonparametric SEM by leveraging nonparametric sparsity based on partial derivatives, resulting in a continuous optimization problem that can be applied to a variety of nonparametric and semiparametric models including GLMs, additive noise models, and index models as special cases. Unlike existing approaches that require specific modeling choices, loss functions, or algorithms, we present a completely general framework that can be applied to general nonlinear models (e.g. without additive noise), general differentiable loss functions, and generic black-box optimization routines. The code is available at https://github.com/xunzheng/notears.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1909.13189},
  eprinttype = {arxiv},
  file = {/home/trung/Zotero/storage/4RRTPWI4/Zheng et al. - 2020 - Learning Sparse Nonparametric DAGs.pdf},
  journal = {arXiv:1909.13189 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhong18_GhostVLADsetbasedface,
  title = {{{GhostVLAD}} for Set-Based Face Recognition},
  author = {Zhong, Yujie and Arandjelovi{\'c}, Relja and Zisserman, Andrew},
  year = {2018},
  month = oct,
  abstract = {The objective of this paper is to learn a compact representation of image sets for template-based face recognition. We make the following contributions: first, we propose a network architecture which aggregates and embeds the face descriptors produced by deep convolutional neural networks into a compact fixed-length representation. This compact representation requires minimal memory storage and enables efficient similarity computation. Second, we propose a novel GhostVLAD layer that includes \{\textbackslash em ghost clusters\}, that do not contribute to the aggregation. We show that a quality weighting on the input faces emerges automatically such that informative images contribute more than those with low quality, and that the ghost clusters enhance the network's ability to deal with poor quality images. Third, we explore how input feature dimension, number of clusters and different training techniques affect the recognition performance. Given this analysis, we train a network that far exceeds the state-of-the-art on the IJB-B face recognition dataset. This is currently one of the most challenging public benchmarks, and we surpass the state-of-the-art on both the identification and verification protocols.},
  annotation = {ZSCC: 0000009},
  archivePrefix = {arXiv},
  eprint = {1810.09951},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhong et al_2018_ghostvlad for set-based face recognition.pdf;/home/trung/Zotero/storage/YWX5X7PF/1810.html},
  journal = {arXiv:1810.09951 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{zhou13_AugmentandConquerNegativeBinomial,
  title = {Augment-and-{{Conquer Negative Binomial Processes}}},
  author = {Zhou, Mingyuan and Carin, Lawrence},
  year = {2013},
  month = feb,
  abstract = {By developing data augmentation methods unique to the negative binomial (NB) distribution, we unite seemingly disjoint count and mixture models under the NB process framework. We develop fundamental properties of the models and derive efficient Gibbs sampling inference. We show that the gamma-NB process can be reduced to the hierarchical Dirichlet process with normalization, highlighting its unique theoretical, structural and computational advantages. A variety of NB processes with distinct sharing mechanisms are constructed and applied to topic modeling, with connections to existing algorithms, showing the importance of inferring both the NB dispersion and probability parameters.},
  archivePrefix = {arXiv},
  eprint = {1209.1119},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2013_augment-and-conquer negative binomial processes.pdf},
  journal = {arXiv:1209.1119 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{zhou13_NegativeBinomialProcess,
  title = {Negative {{Binomial Process Count}} and {{Mixture Modeling}}},
  author = {Zhou, Mingyuan and Carin, Lawrence},
  year = {2013},
  month = oct,
  abstract = {The seemingly disjoint problems of count and mixture modeling are united under the negative binomial (NB) process. A gamma process is employed to model the rate measure of a Poisson process, whose normalization provides a random probability measure for mixture modeling and whose marginalization leads to an NB process for count modeling. A draw from the NB process consists of a Poisson distributed finite number of distinct atoms, each of which is associated with a logarithmic distributed number of data samples. We reveal relationships between various count- and mixture-modeling distributions and construct a Poisson-logarithmic bivariate distribution that connects the NB and Chinese restaurant table distributions. Fundamental properties of the models are developed, and we derive efficient Bayesian inference. It is shown that with augmentation and normalization, the NB process and gamma-NB process can be reduced to the Dirichlet process and hierarchical Dirichlet process, respectively. These relationships highlight theoretical, structural and computational advantages of the NB process. A variety of NB processes, including the betageometric, beta-NB, marked-beta-NB, marked-gamma-NB and zero-inflated-NB processes, with distinct sharing mechanisms, are also constructed. These models are applied to topic modeling, with connections made to existing algorithms under Poisson factor analysis. Example results show the importance of inferring both the NB dispersion and probability parameters.},
  archivePrefix = {arXiv},
  eprint = {1209.3442},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2013_negative binomial process count and mixture modeling.pdf},
  journal = {arXiv:1209.3442 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{zhou17_NonparametricBayesianNegative,
  title = {Nonparametric {{Bayesian Negative Binomial Factor Analysis}}},
  author = {Zhou, Mingyuan},
  year = {2017},
  month = oct,
  abstract = {A common approach to analyze a covariate-sample count matrix, an element of which represents how many times a covariate appears in a sample, is to factorize it under the Poisson likelihood. We show its limitation in capturing the tendency for a covariate present in a sample to both repeat itself and excite related ones. To address this limitation, we construct negative binomial factor analysis (NBFA) to factorize the matrix under the negative binomial likelihood, and relate it to a Dirichlet-multinomial distribution based mixed-membership model. To support countably infinite factors, we propose the hierarchical gamma-negative binomial process. By exploiting newly proved connections between discrete distributions, we construct two blocked and a collapsed Gibbs sampler that all adaptively truncate their number of factors, and demonstrate that the blocked Gibbs sampler developed under a compound Poisson representation converges fast and has low computational complexity. Example results show that NBFA has a distinct mechanism in adjusting its number of inferred factors according to the sample lengths, and provides clear advantages in parsimonious representation, predictive power, and computational complexity over previously proposed discrete latent variable models, which either completely ignore burstiness, or model only the burstiness of the covariates but not that of the factors.},
  archivePrefix = {arXiv},
  eprint = {1604.07464},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou_2017_nonparametric bayesian negative binomial factor analysis.pdf},
  journal = {arXiv:1604.07464 [stat]},
  language = {en},
  primaryClass = {stat}
}

@article{zhou19_ContinuityRotationRepresentations,
  title = {On the {{Continuity}} of {{Rotation Representations}} in {{Neural Networks}}},
  author = {Zhou, Yi and Barnes, Connelly and Lu, Jingwan and Yang, Jimei and Li, Hao},
  year = {2019},
  month = apr,
  abstract = {In neural networks, it is often desirable to work with various representations of the same space. For example, 3D rotations can be represented with quaternions or Euler angles. In this paper, we advance a definition of a continuous representation, which can be helpful for training deep neural networks. We relate this to topological concepts such as homeomorphism and embedding. We then investigate what are continuous and discontinuous representations for 2D, 3D, and n-dimensional rotations. We demonstrate that for 3D rotations, all representations are discontinuous in the real Euclidean spaces of four or fewer dimensions. Thus, widely used representations such as quaternions and Euler angles are discontinuous and difficult for neural networks to learn. We show that the 3D rotations have continuous representations in 5D and 6D, which are more suitable for learning. We also present continuous representations for the general case of the n-dimensional rotation group SO(n). While our main focus is on rotations, we also show that our constructions apply to other groups such as the orthogonal group and similarity transforms. We finally present empirical results, which show that our continuous rotation representations outperform discontinuous ones for several practical problems in graphics and vision, including a simple autoencoder sanity test, a rotation estimator for 3D point clouds, and an inverse kinematics solver for 3D human poses.},
  annotation = {ZSCC: 0000016},
  archivePrefix = {arXiv},
  eprint = {1812.07035},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2019_on the continuity of rotation representations in neural networks.pdf;/home/trung/Zotero/storage/6FZBR9TU/1812.html},
  journal = {arXiv:1812.07035 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{zhou19_EstablishingEvaluationMetric,
  title = {Establishing an {{Evaluation Metric}} to {{Quantify Climate Change Image Realism}}},
  author = {Zhou, Sharon and Luccioni, Alexandra and Cosne, Gautier and Bernstein, Michael S. and Bengio, Yoshua},
  year = {2019},
  month = oct,
  abstract = {With success on controlled tasks, generative models are being increasingly applied to humanitarian applications [1, 2]. In this paper, we focus on the evaluation of a conditional generative model that illustrates the consequences of climate change-induced flooding to encourage public interest and awareness on the issue. Because metrics for comparing the realism of different modes in a conditional generative model do not exist, we propose several automated and human-based methods for evaluation. To do this, we adapt several existing metrics, and assess the automated metrics against gold standard human evaluation. We find that using Fr\'echet Inception Distance (FID) with embeddings from an intermediary InceptionV3 layer that precedes the auxiliary classifier produces results most correlated with human realism. While insufficient alone to establish a human-correlated automatic evaluation metric, we believe this work begins to bridge the gap between human and automated generative evaluation procedures.},
  annotation = {ZSCC: 0000000},
  archivePrefix = {arXiv},
  eprint = {1910.10143},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2019_establishing an evaluation metric to quantify climate change image realism.pdf},
  journal = {arXiv:1910.10143 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhou20_ComparisonDiscreteLatent,
  title = {A {{Comparison}} of {{Discrete Latent Variable Models}} for {{Speech Representation Learning}}},
  author = {Zhou, Henry and Baevski, Alexei and Auli, Michael},
  year = {2020},
  month = oct,
  abstract = {Neural latent variable models enable the discovery of interesting structure in speech audio data. This paper presents a comparison of two different approaches which are broadly based on predicting future time-steps or auto-encoding the input signal. Our study compares the representations learned by vq-vae and vq-wav2vec in terms of sub-word unit discovery and phoneme recognition performance. Results show that future time-step prediction with vq-wav2vec achieves better performance. The best system achieves an error rate of 13.22 on the ZeroSpeech 2019 ABX phoneme discrimination challenge},
  archivePrefix = {arXiv},
  eprint = {2010.14230},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2020_a comparison of discrete latent variable models for speech representation learning.pdf},
  journal = {arXiv:2010.14230 [cs, eess]},
  keywords = {_tablet,favorite,representation},
  primaryClass = {cs, eess}
}

@article{zhou20_EvaluatingDisentanglementDeep,
  title = {Evaluating the {{Disentanglement}} of {{Deep Generative Models}} through {{Manifold Topology}}},
  author = {Zhou, Sharon and Zelikman, Eric and Lu, Fred and Ng, Andrew Y. and Ermon, Stefano},
  year = {2020},
  month = jun,
  abstract = {Learning disentangled representations is regarded as a fundamental task for improving the generalization, robustness, and interpretability of generative models. However, measuring disentanglement has been challenging and inconsistent, often dependent on an ad-hoc external model or specific to a certain dataset. To address this, we present a method for quantifying disentanglement that only uses the generative model, by measuring the topological similarity of conditional submanifolds in the learned representation. This method showcases both unsupervised and supervised variants. To illustrate the effectiveness and applicability of our method, we empirically evaluate several state-of-the-art models across multiple datasets. We find that our method ranks models similarly to existing methods.},
  archivePrefix = {arXiv},
  eprint = {2006.03680},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2020_evaluating the disentanglement of deep generative models through manifold topology.pdf},
  journal = {arXiv:2006.03680 [cs, stat]},
  keywords = {disentanglement},
  primaryClass = {cs, stat}
}

@article{zhou20_GaussianMixtureVariational,
  title = {Gaussian {{Mixture Variational Autoencoder}} for {{Semi}}-{{Supervised Topic Modeling}}},
  author = {Zhou, Cangqi and Ban, Hao and Zhang, Jing and Li, Qianmu and Zhang, Yinghua},
  year = {2020},
  volume = {8},
  pages = {106843--106854},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2020.3001184},
  abstract = {Topic models are widely explored for summarizing a corpus of documents. Recent advances in Variational AutoEncoder (VAE) have enabled the development of black-box inference methods for topic modeling in order to alleviate the drawbacks of classical statistical inference. Most existing VAE based approaches assume a unimodal Gaussian distribution for the approximate posterior of latent variables, which limits the flexibility in encoding the latent space. In addition, the unsupervised architecture hinders the incorporation of extra label information, which is ubiquitous in many applications. In this paper, we propose a semi-supervised topic model under the VAE framework. We assume that a document is modeled as a mixture of classes, and a class is modeled as a mixture of latent topics. A multimodal Gaussian mixture model is adopted for latent space. The parameters of the components and the mixing weights are encoded separately. These weights, together with partially labeled data, also contribute to the training of a classifier. The objective is derived under the Gaussian mixture assumption and the semi-supervised VAE framework. Modules of the proposed framework are appropriately designated. Experiments performed on three benchmark datasets demonstrate the effectiveness of our method, comparing to several competitive baselines.},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2020_gaussian mixture variational autoencoder for semi-supervised topic modeling.pdf},
  journal = {IEEE Access},
  language = {en}
}

@techreport{zhou20_Hyperbolicgeometrygene,
  title = {Hyperbolic Geometry of Gene Expression},
  author = {Zhou, Yuansheng and Sharpee, Tatyana},
  year = {2020},
  month = aug,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.08.27.270264},
  abstract = {Understanding the patterns of gene expression is key to elucidating the differences between cell types and across disease conditions. The overwhelmingly large number of genes involved generally makes this problem intractable. Yet, we find that gene expression patterns in five different data datasets can all be described using a small number of variables. These variables describe differences between cells according to a hyperbolic metric. We reach this conclusion by developing methods that, starting with an initial assumption of a Euclidean geometry, can detect the presence of other geometries in the data. The Euclidean metric is used in most of current studies of gene expression, primarily because it is difficult to use other non-linear metrics in high dimensional spaces. The hyperbolic metric is much more suitable for describing data produced by a hierarchically organized network, which is relevant for many biological processes. We find that the hyperbolic effects, but not the space dimensionality, increase with the number of genes that are taken into account. The hyperbolic curvature was the smallest for mouse embryonic stem cells, stronger for mouse kidney, lung and brain cells, and reached the largest value in a set of human cells integrated from multiple sources. We show that taking into account hyperbolic geometry strongly improves the visualization of gene expression data compared to leading visualization methods. These results demonstrate the advantages of knowing the underlying geometry when analyzing high-dimensional data.},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2020_hyperbolic geometry of gene expression.pdf},
  language = {en},
  type = {Preprint}
}

@techreport{zhou20_Hyperbolicgeometrygenea,
  title = {Hyperbolic Geometry of Gene Expression},
  author = {Zhou, Yuansheng and Sharpee, Tatyana},
  year = {2020},
  month = aug,
  institution = {{Bioinformatics}},
  doi = {10.1101/2020.08.27.270264},
  abstract = {Understanding the patterns of gene expression is key to elucidating the differences between cell types and across disease conditions. The overwhelmingly large number of genes involved generally makes this problem intractable. Yet, we find that gene expression patterns in five different data datasets can all be described using a small number of variables. These variables describe differences between cells according to a hyperbolic metric. We reach this conclusion by developing methods that, starting with an initial assumption of a Euclidean geometry, can detect the presence of other geometries in the data. The Euclidean metric is used in most of current studies of gene expression, primarily because it is difficult to use other non-linear metrics in high dimensional spaces. The hyperbolic metric is much more suitable for describing data produced by a hierarchically organized network, which is relevant for many biological processes. We find that the hyperbolic effects, but not the space dimensionality, increase with the number of genes that are taken into account. The hyperbolic curvature was the smallest for mouse embryonic stem cells, stronger for mouse kidney, lung and brain cells, and reached the largest value in a set of human cells integrated from multiple sources. We show that taking into account hyperbolic geometry strongly improves the visualization of gene expression data compared to leading visualization methods. These results demonstrate the advantages of knowing the underlying geometry when analyzing high-dimensional data.},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2020_hyperbolic geometry of gene expression2.pdf},
  language = {en},
  type = {Preprint}
}

@article{zhou20_pneumoniaoutbreakassociated,
  title = {A Pneumonia Outbreak Associated with a New Coronavirus of Probable Bat Origin},
  author = {Zhou, Peng and Yang, Xing-Lou and Wang, Xian-Guang and Hu, Ben and Zhang, Lei and Zhang, Wei and Si, Hao-Rui and Zhu, Yan and Li, Bei and Huang, Chao-Lin and Chen, Hui-Dong and Chen, Jing and Luo, Yun and Guo, Hua and Jiang, Ren-Di and Liu, Mei-Qin and Chen, Ying and Shen, Xu-Rui and Wang, Xi and Zheng, Xiao-Shuang and Zhao, Kai and Chen, Quan-Jiao and Deng, Fei and Liu, Lin-Lin and Yan, Bing and Zhan, Fa-Xian and Wang, Yan-Yi and Xiao, Geng-Fu and Shi, Zheng-Li},
  year = {2020},
  month = feb,
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/s41586-020-2012-7},
  annotation = {ZSCC: NoCitationData[s0]},
  file = {/home/trung/Zotero/storage/6SQZ5CS2/Zhou et al. - 2020 - A pneumonia outbreak associated with a new coronav.pdf},
  journal = {Nature},
  language = {en}
}

@article{zhou20_Unsupervisedanomalylocalization,
  title = {Unsupervised Anomaly Localization Using {{VAE}} and Beta-{{VAE}}},
  author = {Zhou, Leixin and Deng, Wenxiang and Wu, Xiaodong},
  year = {2020},
  month = may,
  abstract = {Variational Auto-Encoders (VAEs) have shown great potential in the unsupervised learning of data distributions. An VAE trained on normal images is expected to only be able to reconstruct normal images, allowing the localization of anomalous pixels in an image via manipulating information within the VAE ELBO loss. The ELBO consists of KL divergence loss (image-wise) and reconstruction loss (pixel-wise). It is natural and straightforward to use the later as the predictor. However, usually local anomaly added to a normal image can deteriorate the whole reconstructed image, causing segmentation using only naive pixel errors not accurate. Energy based projection was proposed to increase the reconstruction accuracy of normal regions/pixels, which achieved the state-of-the-art localization accuracy on simple natural images. Another possible predictors are ELBO and its components gradients with respect to each pixels. Previous work claimed that KL gradient is a robust predictor. In this paper, we argue that the energy based projection in medical imaging is not as useful as on natural images. Moreover, we observe that the robustness of KL gradient predictor totally depends on the setting of the VAE and dataset. We also explored the effect of the weight of KL loss within beta-VAE and predictor ensemble in anomaly localization.},
  archivePrefix = {arXiv},
  eprint = {2005.10686},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhou et al_2020_unsupervised anomaly localization using vae and beta-vae.pdf},
  journal = {arXiv:2005.10686 [cs]},
  primaryClass = {cs}
}

@article{zhu00_MedLDAMaximumMargin,
  title = {{{MedLDA}}: {{Maximum Margin Supervised Topic Models}}},
  author = {Zhu, Jun and Ahmed, Amr and Xing, Eric P},
  pages = {42},
  abstract = {A supervised topic model can use side information such as ratings or labels associated with documents or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective functions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet allocation (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) under a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or regression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, especially for classification.},
  file = {/home/trung/GoogleDrive/Zotero/zhu et al_medlda.pdf},
  language = {en}
}

@article{zhu17_UnpairedImagetoImageTranslation,
  title = {Unpaired {{Image}}-to-{{Image Translation}} Using {{Cycle}}-{{Consistent Adversarial Networks}}},
  author = {Zhu, Jun-Yan and Park, Taesung and Isola, Phillip and Efros, Alexei A.},
  year = {2017},
  month = mar,
  abstract = {Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between an input image and an output image using a training set of aligned image pairs. However, for many tasks, paired training data will not be available. We present an approach for learning to translate an image from a source domain X to a target domain Y in the absence of paired examples. Our goal is to learn a mapping G : X \textrightarrow{} Y such that the distribution of images from G(X) is indistinguishable from the distribution Y using an adversarial loss. Because this mapping is highly under-constrained, we couple it with an inverse mapping F : Y \textrightarrow{} X and introduce a cycle consistency loss to enforce F (G(X)) {$\approx$} X (and vice versa). Qualitative results are presented on several tasks where paired training data does not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc. Quantitative comparisons against several prior methods demonstrate the superiority of our approach.},
  archivePrefix = {arXiv},
  eprint = {1703.10593},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhu et al_2017_unpaired image-to-image translation using cycle-consistent adversarial networks.pdf},
  journal = {arXiv:1703.10593 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{zhu18_SelfAttentiveSpeakerEmbeddings,
  title = {Self-{{Attentive Speaker Embeddings}} for {{Text}}-{{Independent Speaker Verification}}},
  booktitle = {Interspeech 2018},
  author = {Zhu, Yingke and Ko, Tom and Snyder, David and Mak, Brian and Povey, Daniel},
  year = {2018},
  month = sep,
  pages = {3573--3577},
  publisher = {{ISCA}},
  doi = {10.21437/Interspeech.2018-1158},
  abstract = {This paper introduces a new method to extract speaker embeddings from a deep neural network (DNN) for text-independent speaker verification. Usually, speaker embeddings are extracted from a speaker-classification DNN that averages the hidden vectors over the frames of a speaker; the hidden vectors produced from all the frames are assumed to be equally important. We relax this assumption and compute the speaker embedding as a weighted average of a speaker's frame-level hidden vectors, and their weights are automatically determined by a self-attention mechanism. The effect of multiple attention heads are also investigated to capture different aspects of a speaker's input speech. Finally, a PLDA classifier is used to compare pairs of embeddings. The proposed self-attentive speaker embedding system is compared with a strong DNN embedding baseline on NIST SRE 2016. We find that the self-attentive embeddings achieve superior performance. Moreover, the improvement produced by the self-attentive speaker embeddings is consistent with both short and long testing utterances.},
  file = {/home/trung/GoogleDrive/Zotero/zhu et al_2018_self-attentive speaker embeddings for text-independent speaker verification.pdf},
  language = {en}
}

@article{zhu20_BatchNormalizedInference,
  title = {A {{Batch Normalized Inference Network Keeps}} the {{KL Vanishing Away}}},
  author = {Zhu, Qile and Su, Jianlin and Bi, Wei and Liu, Xiaojiang and Ma, Xiyao and Li, Xiaolin and Wu, Dapeng},
  year = {2020},
  month = may,
  abstract = {Variational Autoencoder (VAE) is widely used as a generative model to approximate a model's posterior on latent variables by combining the amortized variational inference and deep neural networks. However, when paired with strong autoregressive decoders, VAE often converges to a degenerated local optimum known as ``posterior collapse''. Previous approaches consider the KullbackLeibler divergence (KL) individual for each datapoint. We propose to let the KL follow a distribution across the whole dataset, and analyze that it is sufficient to prevent posterior collapse by keeping the expectation of the KL's distribution positive. Then we propose Batch NormalizedVAE (BN-VAE), a simple but effective approach to set a lower bound of the expectation by regularizing the distribution of the approximate posterior's parameters. Without introducing any new model component or modifying the objective, our approach can avoid the posterior collapse effectively and efficiently. We further show that the proposed BN-VAE can be extended to conditional VAE (CVAE). Empirically, our approach surpasses strong autoregressive baselines on language modeling, text classification and dialogue generation, and rivals more complex approaches while keeping almost the same training time as VAE.},
  archivePrefix = {arXiv},
  eprint = {2004.12585},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhu et al_2020_a batch normalized inference network keeps the kl vanishing away.pdf},
  journal = {arXiv:2004.12585 [cs]},
  keywords = {_tablet,favorite,vae_issues},
  language = {en},
  primaryClass = {cs}
}

@article{zhu20_DeepLearningLearning,
  title = {Deep {{Learning}} for {{Learning Graph Representations}}},
  author = {Zhu, Wenwu and Wang, Xin and Cui, Peng},
  year = {2020},
  volume = {866},
  pages = {169--210},
  doi = {10.1007/978-3-030-31756-0_6},
  abstract = {Mining graph data has become a popular research topic in computer science and has been widely studied in both academia and industry given the increasing amount of network data in the recent years. However, the huge amount of network data has posed great challenges for efficient analysis. This motivates the advent of graph representation which maps the graph into a low-dimension vector space, keeping original graph structure and supporting graph inference. The investigation on efficient representation of a graph has profound theoretical significance and important realistic meaning, we therefore introduce some basic ideas in graph representation/network embedding as well as some representative models in this chapter.},
  archivePrefix = {arXiv},
  eprint = {2001.00293},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhu et al_2020_deep learning for learning graph representations.pdf;/home/trung/Zotero/storage/J4MZ6QHY/2001.html},
  journal = {arXiv:2001.00293 [cs, stat]},
  primaryClass = {cs, stat}
}

@article{zhu20_LearningDisentangledRepresentations,
  title = {Learning {{Disentangled Representations}} with {{Latent Variation Predictability}}},
  author = {Zhu, Xinqi and Xu, Chang and Tao, Dacheng},
  year = {2020},
  month = jul,
  abstract = {Latent traversal is a popular approach to visualize the disentangled latent representations. Given a bunch of variations in a single unit of the latent representation, it is expected that there is a change in a single factor of variation of the data while others are fixed. However, this impressive experimental observation is rarely explicitly encoded in the objective function of learning disentangled representations. This paper defines the variation predictability of latent disentangled representations. Given image pairs generated by latent codes varying in a single dimension, this varied dimension could be closely correlated with these image pairs if the representation is well disentangled. Within an adversarial generation process, we encourage variation predictability by maximizing the mutual information between latent variations and corresponding image pairs. We further develop an evaluation metric that does not rely on the ground-truth generative factors to measure the disentanglement of latent representations. The proposed variation predictability is a general constraint that is applicable to the VAE and GAN frameworks for boosting disentanglement of latent representations. Experiments show that the proposed variation predictability correlates well with existing ground-truth-required metrics and the proposed algorithm is effective for disentanglement learning.},
  archivePrefix = {arXiv},
  eprint = {2007.12885},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhu et al_2020_learning disentangled representations with latent variation predictability.pdf},
  journal = {arXiv:2007.12885 [cs]},
  keywords = {disentanglement},
  primaryClass = {cs}
}

@article{zhu20_NeuralGenerativeModel,
  title = {A {{Neural Generative Model}} for {{Joint Learning Topics}} and {{Topic}}-{{Specific Word Embeddings}}},
  author = {Zhu, Lixing and He, Yulan and Zhou, Deyu},
  year = {2020},
  month = aug,
  abstract = {We propose a novel generative model to explore both local and global context for joint learning topics and topic-specific word embeddings. In particular, we assume that global latent topics are shared across documents, a word is generated by a hidden semantic vector encoding its contextual semantic meaning, and its context words are generated conditional on both the hidden semantic vector and global latent topics. Topics are trained jointly with the word embeddings. The trained model maps words to topic-dependent embeddings, which naturally addresses the issue of word polysemy. Experimental results show that the proposed model outperforms the word-level embedding methods in both word similarity evaluation and word sense disambiguation. Furthermore, the model also extracts more coherent topics compared with existing neural topic models or other models for joint learning of topics and word embeddings. Finally, the model can be easily integrated with existing deep contextualized word embedding learning methods to further improve the performance of downstream tasks such as sentiment classification.},
  archivePrefix = {arXiv},
  eprint = {2008.04702},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhu et al_2020_a neural generative model for joint learning topics and topic-specific word embeddings.pdf},
  journal = {arXiv:2008.04702 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zhu20_NovelCoronavirusPatients,
  title = {A {{Novel Coronavirus}} from {{Patients}} with {{Pneumonia}} in {{China}}, 2019},
  author = {Zhu, Na and Zhang, Dingyu and Wang, Wenling and Li, Xingwang and Yang, Bo and Song, Jingdong and Zhao, Xiang and Huang, Baoying and Shi, Weifeng and Lu, Roujian and Niu, Peihua and Zhan, Faxian and Ma, Xuejun and Wang, Dayan and Xu, Wenbo and Wu, Guizhen and Gao, George F. and Tan, Wenjie},
  year = {2020},
  month = jan,
  volume = {0},
  pages = {null},
  issn = {0028-4793},
  doi = {10.1056/NEJMoa2001017},
  annotation = {ZSCC: 0000001},
  file = {/home/trung/Zotero/storage/CQTAGWXY/NEJMoa2001017.html},
  journal = {New England Journal of Medicine},
  number = {0}
}

@article{zhuang13_RegularizedSemiSupervisedLatent,
  title = {Regularized {{Semi}}-{{Supervised Latent Dirichlet Allocation}} for Visual Concept Learning},
  author = {Zhuang, Liansheng and Gao, Haoyuan and Luo, Jiebo and Lin, Zhouchen},
  year = {2013},
  month = nov,
  volume = {119},
  pages = {26--32},
  issn = {09252312},
  doi = {10.1016/j.neucom.2012.04.043},
  abstract = {Topic model is a popular tool for visual concept learning. Most topic models are either unsupervised or fully supervised. In this paper, to take advantage of both limited labeled training images and rich unlabeled images, we propose a novel regularized Semi-Supervised Latent Dirichlet Allocation (r-SSLDA) for learning visual concept classifiers. Instead of introducing a new complex topic model, we attempt to find an efficient way to learn topic models in a semi-supervised way. Our r-SSLDA considers both semi-supervised properties and supervised topic model simultaneously in a regularization framework. Furthermore, to improve the performance of r-SSLDA, we introduce the low rank graph to the framework. Experiments on Caltech 101 and Caltech 256 have shown that r-SSLDA outperforms both unsupervised LDA and achieves competitive performance against fully supervised LDA with much fewer labeled images.},
  file = {/home/trung/GoogleDrive/Zotero/zhuang et al_2013_regularized semi-supervised latent dirichlet allocation for visual concept learning.pdf},
  journal = {Neurocomputing},
  language = {en}
}

@article{zhuang19_AttributedSequenceEmbedding,
  title = {Attributed {{Sequence Embedding}}},
  author = {Zhuang, Zhongfang and Kong, Xiangnan and Rundensteiner, Elke and Zouaoui, Jihane and Arora, Aditya},
  year = {2019},
  month = nov,
  abstract = {Mining tasks over sequential data, such as clickstreams and gene sequences, require a careful design of embeddings usable by learning algorithms. Recent research in feature learning has been extended to sequential data, where each instance consists of a sequence of heterogeneous items with a variable length. However, many real-world applications often involve attributed sequences, where each instance is composed of both a sequence of categorical items and a set of attributes. In this paper, we study this new problem of attributed sequence embedding, where the goal is to learn the representations of attributed sequences in an unsupervised fashion. This problem is core to many important data mining tasks ranging from user behavior analysis to the clustering of gene sequences. This problem is challenging due to the dependencies between sequences and their associated attributes. We propose a deep multimodal learning framework, called NAS, to produce embeddings of attributed sequences. The embeddings are task independent and can be used on various mining tasks of attributed sequences. We demonstrate the effectiveness of our embeddings of attributed sequences in various unsupervised learning tasks on real-world datasets.},
  annotation = {ZSCC: NoCitationData[s0]},
  archivePrefix = {arXiv},
  eprint = {1911.00949},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhuang et al_2019_attributed sequence embedding.pdf;/home/trung/Zotero/storage/YPZUIDKK/1911.html},
  journal = {arXiv:1911.00949 [cs, stat]},
  keywords = {attribute,Computer Science - Computation and Language,Computer Science - Databases,Computer Science - Machine Learning,embedding,semi-supervised,sequence,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zhuang20_ComprehensiveSurveyTransfer,
  title = {A {{Comprehensive Survey}} on {{Transfer Learning}}},
  author = {Zhuang, Fuzhen and Qi, Zhiyuan and Duan, Keyu and Xi, Dongbo and Zhu, Yongchun and Zhu, Hengshu and Xiong, Hui and He, Qing},
  year = {2020},
  month = jun,
  abstract = {Transfer learning aims at improving the performance of target learners on target domains by transferring the knowledge contained in different but related source domains. In this way, the dependence on a large number of target domain data can be reduced for constructing target learners. Due to the wide application prospects, transfer learning has become a popular and promising area in machine learning. Although there are already some valuable and impressive surveys on transfer learning, these surveys introduce approaches in a relatively isolated way and lack the recent advances in transfer learning. Due to the rapid expansion of the transfer learning area, it is both necessary and challenging to comprehensively review the relevant studies. This survey attempts to connect and systematize the existing transfer learning researches, as well as to summarize and interpret the mechanisms and the strategies of transfer learning in a comprehensive way, which may help readers have a better understanding of the current research status and ideas. Unlike previous surveys, this survey paper reviews more than forty representative transfer learning approaches, especially homogeneous transfer learning approaches, from the perspectives of data and model. The applications of transfer learning are also briefly introduced. In order to show the performance of different transfer learning models, over twenty representative transfer learning models are used for experiments. The models are performed on three different datasets, i.e., Amazon Reviews, Reuters-21578, and Office-31. And the experimental results demonstrate the importance of selecting appropriate transfer learning models for different applications in practice.},
  archivePrefix = {arXiv},
  eprint = {1911.02685},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zhuang et al_2020_a comprehensive survey on transfer learning.pdf},
  journal = {arXiv:1911.02685 [cs, stat]},
  language = {en},
  primaryClass = {cs, stat}
}

@article{ziegenfuss18_EffectsAqueousExtract,
  title = {Effects of an {{Aqueous Extract}} of {{Withania}} Somnifera on {{Strength Training Adaptations}} and {{Recovery}}: {{The STAR Trial}}},
  shorttitle = {Effects of an {{Aqueous Extract}} of {{Withania}} Somnifera on {{Strength Training Adaptations}} and {{Recovery}}},
  author = {Ziegenfuss, Tim N. and Kedia, Anurag W. and Sandrock, Jennifer E. and Raub, Betsy J. and Kerksick, Chad M. and Lopez, Hector L.},
  year = {2018},
  month = nov,
  volume = {10},
  issn = {2072-6643},
  doi = {10.3390/nu10111807},
  abstract = {Withania somnifera (Ashwagandha) is an Ayurvedic herb categorized as having ``rasayana'' (rejuvenator), longevity, and revitalizing properties. Sensoril\textregistered{} is a standardized aqueous extract of the roots and leaves of Withania somnifera. Purpose: To examine the impact of Sensoril\textregistered{} supplementation on strength training adaptations. Methods: Recreationally active men (26.5 {$\pm$} 6.4 years, 181 {$\pm$} 6.8 cm, 86.9 {$\pm$} 12.5 kg, 24.5 {$\pm$} 6.6\% fat) were randomized in a double-blind fashion to placebo (PLA, n = 19) or 500 mg/d Sensoril\textregistered{} (S500, n = 19). Body composition (DEXA), muscular strength, power, and endurance, 7.5 km cycling time trial, and clinical blood chemistries were measured at baseline and after 12 weeks of supplementation and training. Subjects were required to maintain their normal dietary habits and to follow a specific, progressive overload resistance-training program (4-day/week, upper body/lower body split). 2 \texttimes{} 2 mixed factorial ANOVA was used for analysis and statistical significance was set a priori at p {$\leq$} 0.05. Results: Gains in 1-RM squat (S500: +19.1 {$\pm$} 13.0 kg vs. PLA +10.0 {$\pm$} 6.2 kg, p = 0.009) and bench press (S500: +12.8 {$\pm$} 8.2 kg vs. PLA: +8.0 {$\pm$} 6.0 kg, p = 0.048) were significantly greater in S500. Changes in DEXA-derived android/gynoid ratio (S500: +0.0 {$\pm$} 0.14 vs. PLA: +0.09 {$\pm$} 0.1, p = 0.03) also favored S500. No other between-group differences were found for body composition, visual analog scales for recovery and affect, or systemic hemodynamics, however, only the S500 group experienced statistically significant improvements in average squat power, peak bench press power, 7.5 km time trial performance, and perceived recovery scores. Clinical chemistry analysis indicated a slight polycythemia effect in PLA, with no other statistical or clinically relevant changes being noted. Conclusions: A 500 mg dose of an aqueous extract of Ashwagandha improves upper and lower-body strength, supports a favorable distribution of body mass, and was well tolerated clinically in recreationally active men over a 12-week resistance training and supplementation period.},
  file = {/home/trung/GoogleDrive/Zotero/ziegenfuss et al_2018_effects of an aqueous extract of withania somnifera on strength training adaptations and recovery.pdf},
  journal = {Nutrients},
  number = {11},
  pmcid = {PMC6266766},
  pmid = {30463324}
}

@article{zou20_StylizedNeuralPainting,
  title = {Stylized {{Neural Painting}}},
  author = {Zou, Zhengxia and Shi, Tianyang and Qiu, Shuang and Yuan, Yi and Shi, Zhenwei},
  year = {2020},
  month = nov,
  abstract = {This paper proposes an image-to-painting translation method that generates vivid and realistic painting artworks with controllable styles. Different from previous image-to-image translation methods that formulate the translation as pixel-wise prediction, we deal with such an artistic creation process in a vectorized environment and produce a sequence of physically meaningful stroke parameters that can be further used for rendering. Since a typical vector render is not differentiable, we design a novel neural renderer which imitates the behavior of the vector renderer and then frame the stroke prediction as a parameter searching process that maximizes the similarity between the input and the rendering output. We explored the zero-gradient problem on parameter searching and propose to solve this problem from an optimal transportation perspective. We also show that previous neural renderers have a parameter coupling problem and we re-design the rendering network with a rasterization network and a shading network that better handles the disentanglement of shape and color. Experiments show that the paintings generated by our method have a high degree of fidelity in both global appearance and local textures. Our method can be also jointly optimized with neural style transfer that further transfers visual style from other images. Our code and animated results are available at \textbackslash url\{https://jiupinjia.github.io/neuralpainter/\}.},
  archivePrefix = {arXiv},
  eprint = {2011.08114},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zou et al_2020_stylized neural painting.pdf},
  journal = {arXiv:2011.08114 [cs]},
  language = {en},
  primaryClass = {cs}
}

@article{zrnic19_PowerBatchingMultiple,
  title = {The {{Power}} of {{Batching}} in {{Multiple Hypothesis Testing}}},
  author = {Zrnic, Tijana and Jiang, Daniel L. and Jordan, Michael I.},
  year = {2019},
  month = oct,
  abstract = {One important partition of algorithms for controlling the false discovery rate (FDR) in multiple testing is into offline and online algorithms. The first generally achieve significantly higher power of discovery, while the latter allow making decisions sequentially as well as adaptively formulating hypotheses based on past observations. Using existing methodology, it is unclear how one could trade off the benefits of these two broad families of algorithms, all the while preserving their formal FDR guarantees. To this end, we introduce \$\textbackslash text\{Batch\}\_\{\textbackslash text\{BH\}\}\$ and \$\textbackslash text\{Batch\}\_\{\textbackslash text\{St-BH\}\}\$, algorithms for controlling the FDR when a possibly infinite sequence of batches of hypotheses is tested by repeated application of one of the most widely used offline algorithms, the Benjamini-Hochberg (BH) method or Storey's improvement of the BH method. We show that our algorithms interpolate between existing online and offline methodology, thus trading off the best of both worlds.},
  archivePrefix = {arXiv},
  eprint = {1910.04968},
  eprinttype = {arxiv},
  file = {/home/trung/GoogleDrive/Zotero/zrnic et al_2019_the power of batching in multiple hypothesis testing.pdf;/home/trung/Zotero/storage/SX4VP88Z/1910.html},
  journal = {arXiv:1910.04968 [stat]},
  keywords = {Statistics - Machine Learning,Statistics - Methodology},
  primaryClass = {stat}
}


